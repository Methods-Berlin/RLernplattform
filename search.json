[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Lernplattform",
    "section": "",
    "text": "1 Inhalte"
  },
  {
    "objectID": "Installation.html#windows",
    "href": "Installation.html#windows",
    "title": "2  Installation und Aktualisierung von R und RStudio",
    "section": "2.1 Windows",
    "text": "2.1 Windows\nDie folgenden Installationen wurden unter Nutzung von Firefox 70.0.1 durchgeführt.\n\n2.1.1 Installation von R\nAuf dieser Seite klicken wir auf Download R for Windows und auf der folgenden Seite auf Install R for the first time. Dann kommen wir auf eine Seite, auf der wir die aktuellste R-Version finden.\n\n\n\n\n\nDiese Version läuft unter Windows XP, Windows Vista, Windows 8 und Windows 10. Sollten wir eine dieser Windows Versionen auf unserem Computer haben, drücken wir auf Download R 3.6.1 for Windows.\n\n\nWie finde ich meine Windows Version heraus?\n\nUm die aktuelle Version unseres Windows Systems herauszufinden, drücken wir die Windows-Taste und die Pause Taste gleichzeitig.\n\nAchtung: Bei manchen Rechner muss man zur Aktivierung der Pause-Taste zusätzlich noch die fn-Taste drücken.\n\n\n\n\n\n\nEs öffnet sich ein Fenster mit den grundlegenden Informationen über unseren Computer.\n\n\n\n\n\nGanz oben sehen wir dort unter Windows Edition, welche Windows Version wir installiert haben. Wichtig ist auch der Eintrag unter Systemtyp. Hier steht entweder 64 bit-Betriebssystem oder 32 bit-Betriebssystem. R gibt es sowohl für das 64- als auch für das 32 bit-Betriebssystem. Wenn wir auf Download R 3.6.1 for Windows drücken, laden wir beide Versionen herunter und R erkennt dann automatisch, welche installiert werden kann.\n\n\n\nÄltere Windows-Versionen\n\nFalls unser Betriebssystem nicht kompatibel ist, können wir hier ältere R-Versionen finden. Wir müssen jeweils auf die Version klicken und kommen dann auf eine Seite, die genau so aufgebaut ist wie die Seite der aktuellsten Version. Um zu erfahren, ob die Version mit unserem Betriebssystem kompatibel ist, müssen wir jeweils auf Does R run under my version of Windows? klicken. Wenn wir die passende Version gefunden haben, klicken wir auf Download R [Versionsnummer] for Windows. Die Versionsnummer ist abhängig davon, welche R Version tatsächlich mit unserem Betriebssystem kompatibel ist.\n\nNach dem Herunterladen der .exe-Datei können wir diese öffnen. Es folgen einige Einstellungen und Zustimmungen, die wir uns kurz anschauen wollen.\nWir werden gefragt, ob wir Änderungen an unseren Gerät zulassen wollen und wir bejahen.\nDann können wir die Standardsprache für R einstellen. Diese kann man später auch im Programm noch ändern.\n\n\n\n\n\n\n\n\n\n\nAchtung: Es kann hilfreich sein, Englisch als Sprache festzulegen. So würden wir Fehlermeldungen auf englisch ausgegeben bekommen, zu denen wir bei Suchen im Internet zumeist mehr finden.\n\nSpäter können wir noch den Zielordner festlegen und auswählen, welche Komponenten installiert werden sollen. Normalerweise sollten alle wichtigen Komponenten automatisch ausgewählt sein. Wenn wir ein 64 bit-Betriebssystem haben, werden sowohl die 64- als auch die 32 bit-Version von R ausgewählt. Zweiteres können wir abwählen, damit wir das Programm nicht zweimal installieren.\n\n\n\n\n\n\n\n\n\nNun werden wir gefragt, ob wir die Startoptionen von R ändern wollen. Wir können hier erstmal verneinen und diese später im Programm noch anpassen.\n\n\n\n\n\n\n\n\n\n\n\nWas sind Startoptionen?\n\nBei den Startoptionen können wir z.B. einstellen, wie R später die Hilfe-Seiten einzelner Funktionen anzeigt. Da wir mit der Entwicklungsumgebung RStudio arbeiten werden, können wir diese Einstellungen einfach überspringen.\n\nAnschließend können wir den Startmenü-Ordner auswählen, d.h. festlegen, ob und wo Verknüpfungen zum Programm erstellt werden sollen. Da wir R später über RStudio aufrufen werden, können wir auch hier einfach mit der Voreinstellung auf Weiter drücken.\n\n\n\n\n\n\n\n\n\nNun können wir noch zusätzliche Aufgaben auswählen z.B. ob auf dem Startbildschirm eine Verknüpfung zum Programm erstellt werden soll.\n\n\n\n\n\n\n\n\n\nDanach müssen wir die Installation nur noch fertigstellen.\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Installation von RStudio\nNachdem wir R heruntergeladen haben, können wir nun auch die Entwicklungsumgebung RStudio herunterladen.\nDazu gehen wir auf die RStudio-Seite und scrollen auf der Seite nach unten. Hier überprüfen wir, ob die uns empfohlene Version mit unserem Betriebssystem sowie unserer R-Version kompatibel ist. Wenn ja, können wir auf Download RStudio for Windows klicken.\n\n\n\n\n\n\n\n\n\n\n\nBei mir steht zwar Download RStudio Desktop, aber ich kann nicht darauf klicken.\n\nIn diesem Fall können wir auf der Seite weiter nach unten scrollen. Dort werden unter All Installers alle aktuellen Versionen von RStudio Desktop aufgeführt. Unter dem Eintrag OS sind alle Betriebssysteme aufgelistet, für die es die aktuellste RStudio Desktop Version gibt. Wenn wir unser Betriebssystem gefunden haben, wählen wir in der Spalte Download die passende RStudio Desktop Version aus und laden diese herunter.\n\n\n\nWas bedeutet “RStudio 1.2 requires a 64-bit operating system. If you are on a 32 bit system, you can use an older version of RStudio.”?\n\nOb wir die 32- oder die 64-bit Version von RStudio Desktop brauchen, hängt von unserem Computer ab. Für fast alle Computer können wir die 64-bit Version herunterladen. Sind wir uns nicht sicher, welche Version wir brauchen, können wir hier nachschauen. Wenn wir ein 32 bit-Betriebssystem haben, müssen wir eine ältere Version von RStudio Desktop herunterladen. Diese finden wir unter diesem Link.\n\n\n\n\n\n\n\n\n\nAndernfalls können wir einfach die aktuelle Version mit Download RStudio for Windows herunterladen.\n\nWir speichern die .exe-Datei und klicken dann auf diese. Wir müssen auch hier wieder zustimmen, dass Änderungen am System vorgenommen werden. Dann öffnet sich Fenster mit dem Installationsassistent.\n\n\n\n\n\n\n\n\n\nHier werden ähnliche Einstellungen wie bei der Installation von R besprochen. Abschließend klicken wir auch hier auf fertigstellen.\n\n\n2.1.3 Aktualisierung von R mit Übernahme der Pakete aus der älteren Version (Paket installr)\n\n\n\nWenn eine aktuellere (für unser Betriebssystem kompatible) Version von R vorhanden ist, ist es ratsam, diese herunterzuladen. Mit Aktualisierungen werden etwaige Fehler und Sicherheitslücken behoben und ggf. neue Funktionen eines Programms implementiert.\nFür Windows können wir zur Aktualisierung von R auf das Paket installr zurück greifen, welches den Prozess weitestgehend automatisiert.\n\nAchtung: Wenn wir von einer älteren R -Version auf R 4.0.0 wechseln, müssen wir all unsere Pakete neu installieren. Das Verschieben dieser mit dem Paket installr funktioniert hier nicht.\n\nDazu installieren wir das Paket z.B. via install.packages(\"installr\"). Nachdem wir das Paket mit library(\"installr\") geladen haben, führen wir die enthaltene Funktion updateR() aus.\nWenn wir die aktuellste R-Version installiert haben, bekommen wir ein FALSE ausgegeben.\nWenn es eine aktuellere (kompatible) R-Version gibt, öffnet sich ein neues Fenster.\n\n\n\n\n\n\n\n\n\nWenn wir auf OK geklickt haben, werden wir gefragt, ob wir uns die Neuerungen dieser Version anschauen wollen (optional). Diese würden sich in einem neuen Tab im Browser öffnen.\nAnschließend können wir die neuere Version installieren, indem wir auf Ja klicken.\n\n\n\n\n\n\n\n\n\nWenn wir in RStudio sind, werden wir gefragt, ob wir die Installation via updateR() lieber in R ausführen wollen. Wir können die Installation aber auch einfach in RStudio fortführen.\nNun kommen die gleichen Einstellungen und Zustimmungen wie bei der Installation von R.\nNach Abschluss der Installation werden wir gefragt, ob wir die Pakete unserer alten R-Version in die neue übernehmen wollen …\n\n\n\n\n\n\n\n\n\n… und die Pakete aus den alten Ordnern löschen wollen. Hier können wir Ja anklicken, da wir nicht vorhaben, die alte R-Version noch zu nutzen (diese könnten wir also auch löschen).\n\n\n\n\n\n\n\n\n\nWir können unsere Starteinstellungen für R (Rprofile.site) ebenso in die neuere Version übernehmen.\n\n\n\n\n\n\n\n\n\nAbschließend werden wir sogar noch gefragt, ob wir unsere (verschobenen) Pakete aktualisieren wollen.\n\n\n\n\n\n\n\n\n\n\n\n2.1.4 Aktualisierung von RStudio\nAktualisierungen für die Entwicklungsumgebung RStudio gibt es wesentlich seltener als für das dahinter liegende Basisprogramm R.\nZur Überprüfung, ob eine aktuellere Version vorliegt, können wir in RStudio auf Help –> Check for Updates klicken (in R gibt es diese Option nicht).\n\n\n\n\n\n\n\n\n\nWenn es eine aktuellere Version gibt, öffnet sich die RStudio-Seite im Browser.\n\n\nIch finde die Option Check for Updates nicht.\n\nManchmal gibt es die Option Check für Updates nicht im Help-Menü. Analog dazu können wir auch manuell unsere bestehende mit der aktuellsten RStudio-Version abgleichen. Dazu klicken wir in RStudio auf RStudio –> About RStudio …\n\n\n\n\n\n\n\n\n\n… so dass sich folgendes Fenster öffnet, in dem wir unsere bestehende RStudio-Version in Erfahrung bringen können.\n\n\n\n\n\n\n\n\n\nInformationen über die aktuellste RStudio-Version finden wir wieder unten auf der RStudio-Seite.\n\nWir überprüfen, ob die uns empfohlene Version mit unserem Betriebssystem sowie unserer R-Version kompatibel ist. Wenn ja, können wir auf Download RStudio for Windows klicken.\nDas weitere Vorgehen ist ebenso analog zur Installation von RStudio."
  },
  {
    "objectID": "Installation.html#mac",
    "href": "Installation.html#mac",
    "title": "2  Installation und Aktualisierung von R und RStudio",
    "section": "2.2 Mac",
    "text": "2.2 Mac\nDie folgenden Installationen und Aktualisierungen wurden unter Nutzung von Safari 13.0.3 durchgeführt.\n\n2.2.1 Installation von R\nUnter folgendem Link unter Download R for (Mac) OS X finden wir die aktuellste R-Version ganz oben. Wir kommen dann auf die folgende Seite:\n\n\n\n\n\n\nAchtung: Wir werden auf der Seite darauf hingewiesen, dass wir ab Mac OS X 10.9 (Mavericks) XQuartz nach jedem Upgrade der Betriebssoftware neu installieren sollten. Falls wir das noch nicht gemacht haben, sollten wir das vor der Installation von R noch tun.\n\n\n\nInstallation von XQuartz\n\nWir klicken auf die Verlinkung und gelangen auf die XQuartz-Seite.\n\n\n\n\n\nZuerst schauen wir, ob die aktuelle XQuartz-Version mit unserem Betriebssystem kompatibel ist. Wenn das der Fall sein sollte, laden wir sie herunter. Anschließend klicken wir auf die .dmg-Datei und ein neues Fenster öffnet sich.\n\n\n\n\n\n\n\n\n\nWir klicken auf die .pkg-Datei. Dadurch öffnet sich das Installationsmenü.\n\n\n\n\n\n\n\n\n\nWir klicken jeweils auf Fortfahren und stimmen dem Lizenzvertrag zu. Wir müssen den Änderungen an unserem System zustimmen, indem wir unser Benutzerpasswort eingeben. Anschließend ist die Installation abgeschlossen.\n\n\n\n\n\n\n\n\n\n\nNun müssen wir schauen, ob unser Betriebssystem den Mindestanforderungen für die aktuellste R Version entspricht. Wenn ja, können wir auf die .pkg-Datei klicken und den Download starten.\n\n\nWie finde ich meine Mac OS Version heraus?\n\nUnsere Version des Betriebssystems bestimmt, welche Versionen von R und R Studio wir herunterladen können.\nWir finden unsere Betriebssystem-Version heraus, indem wir auf den Apfel in der Menüleiste am oberen Bildschirm und dann auf Über diesen Mac klicken.\n\n\n\n\n\n\n\n\n\nDaraufhin erscheint ein Fenster, in dem wir die Informationen ablesen können.\n\n\n\n\n\n\n\n\n\n\nFalls die Mindestanforderungen nicht erfüllt sind, können wir trotzdem auf der gleichen Seite runterscrollen und nachschauen, ob eine ältere Version (für Mac OS X 10.9+ oder 10.6-10.8) vorliegt. Noch ältere R-Versionen für Mac OS X 10.4 und älter finden wir hier.\nWenn die .pkg- bzw. .dmg-Datei heruntergeladen wurde, klicken wir auf diese. Dann öffnet sich folgendes Fenster:\n\n\n\n\n\n\n\n\n\nNun müssen wir uns durchklicken und den Bedingungen zustimmen. Wenn die Installation erfolgreich war, erscheint folgendes Bild:\n\n\n\n\n\n\n\n\n\nDie .pkg- bzw. .dmg-Datei(en) können wir nach der Installation löschen. .dmg-Dateien müssen vorher noch ausgeworfen werden. Wir finden diese bei Geräte im Finder.\n\n\n2.2.2 Installation von RStudio\nNun können wir auf die RStudio-Seite gehen, nach unten scrollen und prüfen, ob die uns empfohlene Version mit unserem Betriebssystem sowie unserer R-Version kompatibel ist.\n\n\n\n\n\n\n\n\n\nWenn unser Betriebssystem älter ist, können wir hier eine ältere RStudio-Version herunterladen.\nGanz oben finden wir hier kompatible RStudio-Versionen für Mac OS X 10.11 (El Capitan) und ältere Betriebssystemversionen.\n\n\n\n\n\n\n\n\n\nWenn wir die für unser Betriebssystem kompatible Version heruntergeladen haben, können wir die .dmg-Datei öffnen und installieren. Um diesen Prozess abzuschließen, müssen wir das Programm in den Applications-Ordner verschieben.\n\n\n\n\n\n\n\n\n\nDie .dmg-Datei können wir nach der Installation löschen. Vorher muss diese noch ausgeworfen werden. Wir finden die .dmg-Datei bei Geräte im Finder.\n\n\n2.2.3 Aktualisierung von R\nWenn eine aktuellere (für unser Betriebssystem kompatible) Version von R vorhanden ist, ist es ratsam, diese herunterzuladen. Mit Aktualisierungen werden etwaige Fehler und Sicherheitslücken behoben und ggf. neue Funktionen eines Programms implementiert.\nZur Überprüfung, ob Aktualisierungen vorhanden sind, können wir in R oben in der Menüleiste auf R –> Nach R Updates suchen klicken (in RStudio gibt es diese Option nicht).\n\n\n\n\n\n\n\n\n\nEntweder wir bekomen in der Konsole nun die Ausgabe, dass unsere Version aktuell ist …\n\n\n\n\n\n\n\n\n\n… oder in unserem Browser öffnet sich die Seite von CRAN, auf der wir die aktuellste Version von R herunterladen können.\n\n\n\n\n\n\n\n\n\nNun müssen wir noch überprüfen, ob die Mindestanforderungen an unser Betriebssystem erfüllt sind. Falls die Mindestanforderungen nicht erfüllt sind, können wir trotzdem auf der gleichen Seite runterscrollen und nachschauen, ob eine aktuellere als unsere derzeitige Version vorliegt.\nDas weiterführende Vorgehen ist das Gleiche wie bei der initialen Installation von R: Wir laden die .pkg-Datei herunter, öffnen sie und führen die Installation aus.\n\nAchtung: Aus unbekannten Gründen kann es vorkommen, dass uns über Nach R Updates suchen nicht mitgeteilt wird, dass unsere derzeitige Version nicht die aktuellste ist. Wir können auch analog unsere derzeitige mit der aktuellsten Version abgleichen.\n\nDie Information über unsere derzeitige R-Version wird uns sowohl in R als auch in RStudio nach Öffnen des Programms ganz oben in der Konsole angezeigt.\n\n\n\n\n\n\n\n\n\n\n\nDer Screenshot ist aus RStudio, aber in R bekommen wir dieselbe Information.\n\nDann müssen wir diese nur noch mit der aktuellsten Version auf CRAN abgleichen.\n\n2.2.3.1 Pakete aus der älteren in die neuere R-Version übernehmen\n\nAchtung: Die nachfolgend beschriebene Verschiebung von Paketen ist nur notwendig, wenn sich die R-Version in der ersten Nachkommastelle ändert z.B. Version 3.5 zu 3.6. Bei kleineren Updates z.B. Version 3.6.0 zu 3.6.1 verändert sich der Pfad der Pakete nicht.\n\n\nAchtung: Wenn wir von einer älteren R -Version auf R 4.0.0 wechseln, müssen wir all unsere Pakete neu installieren. Das manuelle Verschieben dieser mit dem nachfolgend vorgestellten Weg funktioniert hier nicht.\n\nWenn wir eine neuere R-Version heruntergeladen haben, sind unsere Pakete, die wir unter der vorherigen R-Version installiert haben, nicht mehr nutzbar im aktualisierten Programm. Das liegt daran, dass ein neuer Ordner für die aktuelle Version erstellt wurde, in den die Pakete nicht automatisch verschoben wurden.\nUm unsere Pakete im aktualisierten Programm nutzen zu können, müssen wir diese in den neuen Ordner verschieben. Dazu öffnen wir den Finder und nutzen den Kurzbefehl shift + cmd + G, um die Dateipfadsuche zu öffnen. In dieser suchen wir den Pfad /Library/Frameworks/R.framework/Versions/.\n\n\n\n\n\n\n\n\n\nWir bekommen nun die Ordner der verschiedenen R-Versionen angezeigt.\n\n\n\n\n\n\n\n\n\nWir machen nun einen Rechtsklick auf den Ordner der vorherig genutzten R-Version (hier: 3.5) und öffnen diesen in einem neuen Tab. Der Ordner enthält nur den Ordner Resources und dieser wiederum nur den Ordner library. Wen wir diesen öffnen, sehen wir die Ordner aller Pakete. Wir wählen schonmal alle Ordner an.\n\n\n\n\n\n\n\n\n\nNun gehen wir zurück auf den anderen Tab, in dem die Ordner der verschiedenen Versionen von R gelistet sind. Wir öffnen den Ordner der aktuellesten Version (hier: 3.6) und klicken auch hier auf den Ordner Resources und dann library.\nWir gehen jetzt zurück auf den anderen Tab (der älteren R-Version) und ziehen die Pakete in den library-Ordner der neuen R-Version.\n\nAchtung: Wenn Paket-Ordner doppelt vorliegen, dann werden wir gefragt, ob wir diese ersetzen wollen. Wir sollten verneinen, da wir ansonsten Probleme mit der Nutzung der alten Standard-Pakete in der neuen R-Version bekommen könnten.\n\n\n\n\nTipp: Nach der Aktualisierung von R können wir mit großer Wahrscheinlichkeit auch einige Pakete aktualisieren. Für Hilfe dabei können wir uns das gleichnamige Kapitel anschauen.\n\n\n\n2.2.4 Aktualisierung von RStudio\nAktualisierungen für die Entwicklungsumgebung RStudio gibt es wesentlich seltener als für das dahinter liegende Basisprogramm R.\nZur Überprüfung, ob eine aktuellere Version vorliegt, können wir in RStudio auf Help –> Check for Updates klicken (in R gibt es diese Option nicht).\n\n\n\n\n\n\n\n\n\nWenn es eine aktuellere Version gibt, öffnet sich die RStudio-Seite im Browser.\n\n\nIch finde die Option Check for Updates nicht.\n\nManchmal gibt es die Option Check für Updates nicht im Help-Menü. Analog dazu können wir auch manuell unsere bestehende mit der aktuellsten RStudio-Version abgleichen. Dazu klicken wir in RStudio auf RStudio –> About RStudio …\n\n\n\n\n\n\n\n\n\n… so dass sich folgendes Fenster öffnet, in dem wir unsere bestehende RStudio-Version in Erfahrung bringen können.\n\n\n\n\n\n\n\n\n\nInformationen über die aktuellste RStudio-Version finden wir wieder auf der RStudio-Seite.\n\nWenn unsere Version nicht mehr aktuell ist, überprüfen wir, ob die uns empfohlene Version mit unserem Betriebssystem sowie unserer R-Version kompatibel ist.\nDas weitere Vorgehen ist weitestgehend deckungsgleich mit dem der initialen Installation von RStudio. Einziger Unterschied ist, dass wir die ältere durch die neuere RStudio-Version ersetzen."
  },
  {
    "objectID": "Installation.html#hinweis-zur-replizierbarkeit-von-analysen",
    "href": "Installation.html#hinweis-zur-replizierbarkeit-von-analysen",
    "title": "2  Installation und Aktualisierung von R und RStudio",
    "section": "2.3 Hinweis zur Replizierbarkeit von Analysen",
    "text": "2.3 Hinweis zur Replizierbarkeit von Analysen\nWir sollten bei Analysen in R immer berichten, in welcher Version unser Betriebssystem, R und unsere genutzten Pakete vorliegen. Sonst kann die Replikation unserer Ergebnisse Anderen schwer fallen, z.B. weil die Funktionen sich in ihrer Berechnung oder Funktionsweise zwischen verschiedenen Versionen von Paketen unterscheiden können. Mit der Funktion sessionInfo() bekommen wir all diese Informationen auf einen Schlag."
  },
  {
    "objectID": "Einfuehrung_in_R.html#funktionen-pakete",
    "href": "Einfuehrung_in_R.html#funktionen-pakete",
    "title": "3  Einführung in R",
    "section": "3.1 Funktionen & Pakete",
    "text": "3.1 Funktionen & Pakete\nFangen wir damit an, was Funktionen und Pakete sind, warum wir diese nutzen und wie wir diese in R anwenden können.\n\nAchtung: Alles, was wir in einer Zeile hinter eine Raute (#) schreiben, wird nicht als Funktion, sondern als Kommentar interpretiert. Unseren Code zu kommentieren ist sehr nützlich, da es uns und Anderen die Nachvollziehbarkeit unseres Vorgehens erleichtert.\n\n\n3.1.1 Funktionen\n\n\n\n\nFunktionen sind (Unter)Programme, die eine gewisse Funktionalität haben, d.h. eine bestimmte Aufgabe ausführen.\n\nWarum ist es sinnvoll, Funktionen zu nutzen?\nWenn wir beispielsweise den Mittelwert einer Zahlenreihe errechnen wollen, nutzen wir folgenden Code:\n\n# Mittelwert der Zahlenreihe selbst berechnen\n(4+3+6+2+3)/5\n\n[1] 3.6\n\n\nDabei müssen wir die Zahlen aufsummieren und durch deren Anzahl teilen. Wenn wir weiter mit der Zahlenreihe arbeiten wollen, müssen wir sie außerdem wieder eingeben.\nViel einfacher können wir die Aufgabe ausführen, indem wir vorgefertigte Funktionen nutzen.\n\n# Zahlenreihe (Vektor) erstellen\nnums <- c(4, 3, 6, 2, 3)\nnums\n\n[1] 4 3 6 2 3\n\n# Mittelwert errechnen lassen\nmean(x = nums)\n\n[1] 3.6\n\n\nWir können Funktionen variierenden Input übergeben. Dieser steht immer in Klammern direkt hinter der Funktion. Beispielsweise könnten wir auch den Mittelwert einer anderen Zahlenreihe als nums berechnen.\nDen Input übergeben wir an einen Parameter. Das ist eine (formale) Variable einer Funktion (z.B. mean(x)), die in der Funktionsdefinition festgelegt ist. Der tatsächliche Input nennt sich Argument (z.B. mean(x = nums)). Die Parameter einer Funktion werden mit Kommata getrennt. Schematisch sieht eine Funktion somit folgendermaßen aus: function(parameter_1 = argument_1, parameter_2 = argument_2, ...).\nWerfen wir einmal einen Blick in die Funktionsdefinition von mean():\n\n\n\n\n\n\n\n\n\nDie Funktionsdefinition finden wir in der Dokumentation, welche wir in RStudio unter Files, Plots, Packages, Help & Viewer.\nManche Parameter besitzen voreingestellte Argumente (z.B. na.rm=FALSE), diese bezeichnet man als Defaults. Funktionen mit (min. einem) Parameter ohne Default (z.B. mean()) werden ohne Spezifikation dieser nicht ausgeführt. Beispielsweise müssen wir dem Parameter x einen Vektor, von dem wir den Mittelwert berechnen wollen, übergeben.\nParameter ermöglichen uns aber nicht nur, eine Aufgabe mit verschiedenen Daten durchzuführen, sondern auch weitere Optionen zu wählen (z.B. na.rm: Ausschluss von fehlenden Werten; trim: trimmen der Enden der Verteilung der Zahlenreihe vor Berechnung des Mittelwerts).\nWenn unsere Zahlenreihe beispielsweise fehlende Werte (in R: NA) besitzt, müssen wir den Default von mean(..., na.rm=FALSE) ändern, sodass fehlende Werte aus der Berechnung entfernt werden.\n\n# fehlenden Wert hinzufügen\nnums <- c(nums, NA)\nnums\n\n[1]  4  3  6  2  3 NA\n\n# Mittelwert errechnen lassen\nmean(nums) # funktioniert nicht, weil unklar ist, was mit NA passieren soll\n\n[1] NA\n\nmean(nums, na.rm=TRUE) # funktioniert, weil NA aus Berechnung entfernt wird\n\n[1] 3.6\n\n\n\n\nWann können wir Parameternamen weglassen (wie bei mean(nums))?\n\nWir müssen x=nums nicht ausschreiben, weil x der erste Parameter in der Funktionsdefinition ist, und unser Argument num automatisch dem Parameter x zugeordnet wird.\nDas funktioniert mit jedem Argument solange wir die Reihenfolge der Parameter in der Funktionsdefinition beachten.\n\nmean(nums, TRUE) \n# funktioniert nicht (wir würden eine Fehlermeldung erhalten), ...\n# ... weil trim (und nicht na.rm) an zweiter Stelle steht ...\n# ... und trim numerischen Input (eine Zahl zwischen 0 und 0.5) ...\n# ... und na.rm logischen Input (TRUE oder FALSE) verlangt\n\n\nmean(nums, 0, TRUE) \n\n[1] 3.6\n\n\n\n\n\n\nSo erleichtern uns Funktionen unsere Arbeit. Zusammengefasst hat die Nutzung von Funktionen folgende Vorteile:\n\nOrganisation\nProgramme (z.B. statistische Analysen) können sehr komplex werden. Durch die Nutzung von Funktionen teilen wir unser komplexes Programm in mehrere, kleinere (Unter)Programme (z.B. Vektor erstellen, Mittelwert berechnen, …).\nWiederverwendbarkeit\nWir können Funktionen immer wieder aufrufen. So wird unser Programm kompakter (DRY-Prinzip, Don’t Repeat Yourself) und wir reduzieren Fehler durch Kopieren von Code (z.B. Verzählen bei den Elementen durch die wir teilen wollen).\nTesten\nWeil wir weniger (redundanten) Code haben, können wir schneller Fehler finden (z.B. Tippfehler, fehlende Elemente).\nErweiterbarkeit\nFunktionen können erweitert werden, um verschiedene Szenarien zu händeln. Meist hat das schon jemand für uns gemacht. Wir müssen den Parametern einer Funktion nur verschiedene Argumente übergeben (z.B. na.rm=TRUE zum Ausschluss von fehlenden Werten).\nAbstraktion\nWir müssen die Funktion nicht im Detail verstehen. Es reicht zu wissen, wie der Name der Funktion ist (mean()), welchen Input wir übergeben (x; trim und na.rm optional weil mit Default) und welchen Output (arithmetisches Mittel) wir bekommen und wo wir die Funktion finden, d.h. aus welchem Paket (base) sie stammt. All diese Informationen erhalten wir in der Dokumentation der Funktion z.B. in RStudio unter Help.\n\nAuf freecodecamp.org finden wir eine kurze Einführung zu R: R Programming Language explained. Es lohnt sich, vertiefend den Part zu Funktionen anzuschauen (~ 10min), um ein besseres Verständnis für die Arbeit mit R zu bekommen.\n\n\n3.1.2 Pakete\n\nFunktionen (und Dateien) werden in sogenannten Paketen gespeichert. Dabei sind in einem Paket (häufig) Funktionen, die für einen begrenzten Aufgabenbereich genutzt werden.\n\nEs gibt Standardpakete, die man automatisch mit dem Download von R erhält und deren Funktionen und Dateien man einfach nutzen kann. Diese sind base (basale Funktionen wie z.B. c() und mean(), die wir gerade genutzt haben), datasets (Beispieldatensätze), graphics (Grafiken erstellen), grDevices (Farben und Schriften), methods (Methoden und Klassen erstellen bzw. Informationen erhalten), stats (statistische Methoden) und utils (z.B. Informationen zu Add-On Paketen erhalten und diese herunterladen).\nZum Beispiel können wir mit der Funktion data() auf die in datasets enthaltenen Datensatze zugreifen.\n\ndata {utils}\n\n\ndata(women) \n\nDie Größe des Datensatzes sehen wir mit dim().\n\ndim {base}\n\n\ndim(women)\n\n[1] 15  2\n\n\nDer Datensatz women enthält 15 Fälle (Zeilen) und zwei Variablen (Spalten).\nDie Namen der Variablen erfahren wir mit names().\n\nnames {base}\n\n\nnames(women)\n\n[1] \"height\" \"weight\"\n\n\nMit den Funktionen aus den Standardpaketen können wir schon vieles machen. Weil R open-source ist, kann jeder eigene Pakete schreiben und Anderen zugänglich machen. Wir können auf diese Add-on Pakete, die andere R-NutzerInnen erstellt haben, über CRAN (Comprehensive R Archive Network) zugreifen. Natürlich können wir selbstgeschriebene Funktionen, die wir häufig nutzen, auch in eigenen Paketen speichern, um sie unkompliziert wieder nutzen zu können oder sie der R-Community zur Verfügung zu stellen.\nAdd-on Pakete müssen wir einmalig herunterladen und jedes Mal, wenn wir sie (in einer R-Session) nutzen wollen, laden.\nEin bei PsychologInnen beliebtes Paket ist psych. Dieses enthält Funktionen, die häufig in der Persönlichkeitspsychologie und Psychometrie genutzt werden.\nWir können ein Add-on Paket mit folgendem Befehl herunterladen:\n\ninstall.packages(\"psych\") # auf Anführungszeichen achten!\n\nDie im Paket enthaltene Funktion describe() gibt uns beispielsweise eine kompakte Übersicht relevanter deskriptiv-statistischer Kennwerte von Daten aus.\n\nWenn wir enthaltene Funktionen nutzen wollen, müssen wir das Paket zuerst laden.\n\nlibrary(psych) # keine Anführungszeichen notwendig\ndescribe(women)\n\n       vars  n   mean    sd median trimmed   mad min max range skew kurtosis\nheight    1 15  65.00  4.47     65   65.00  5.93  58  72    14 0.00    -1.44\nweight    2 15 136.73 15.50    135  136.31 17.79 115 164    49 0.23    -1.34\n         se\nheight 1.15\nweight 4.00\n\n\nMit describe erhalten wir folgende Informationen zu jeder Variable im Data Frame bzw. Matrix: Namen, Spaltennummer (vars), Anzahl (gültiger) Fälle (n), Mittelwert (mean), Standarabweichung (sd), Median (median), getrimmter Mittelwert (trimmed), Median Absolute Deviation (mad), Minimum (min), Maximum (max), Schiefe (skew), Exzess (kurtosis) und Standardfehler des Mittelwerts (se).\n\nDiese Informationen finden wir in der Dokumentation, welche wir unter Help in RStudio ansehen können.\n\nWenn wir R bzw. RStudio schließen und erneut öffnen, müssen wir zusätzliche Pakete vor der Nutzung erneut laden.\nFür mehr Informationen zu Paketen, u.a. wie wir diese aktualisieren können, könnt ihr euch dazugehöriges Kapitel anschauen."
  },
  {
    "objectID": "Einfuehrung_in_R.html#daten",
    "href": "Einfuehrung_in_R.html#daten",
    "title": "3  Einführung in R",
    "section": "3.2 Daten",
    "text": "3.2 Daten\nUm zu verstehen, wie R arbeitet, benötigen wir ein Verständnis dafür, wie Daten in R repräsentiert werden. Dazu schauen wir uns drei wichtige Konzepte an: Datentypen, Datenstrukturen und Objekte.\nBevor wir uns die einzelnen Konzepte im Detail anschauen, sehen wir nachfolgend eine Veranschaulichung des Zusammenhangs dieser, um bereits eine grobe Vorstellung zu haben, was uns in den folgenden Abschnitten erwartet.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchtung: Die Gliederung nach “Datentyp” und “Datenstruktur” sind getreu des Manuals von R. Man stößt in anderen Quellen teils auf abweichende Benennungen.\n\n\n3.2.1 Datentypen\n\nDer Datentyp gibt die Art der Daten an, d.h. welche konkreten Werte(bereiche) die Daten annehmen können und welche Operationen darauf anwendbar sind.\n\nWir beschäftigen uns in R zumeist mit den folgenden Datentypen: character, logical, integer und double. Die letzten beiden werden (häufig) als numeric zusammengefasst.\nNachfolgend finden wir eine Übersicht dieser Datentypen.\n\n\n\n\n \n  \n    Art der Daten \n    Werte \n    Operationen \n    Datentyp in R \n\n     \n  \n \n\n  \n    Zeichen(ketten) \n    z.B. \"Ball\" oder '@' \n    gleich oder ungleich \n    character \n\n     \n  \n  \n    Wahrheitswerte \n    TRUE, FALSE \n    (einige) Logische Operatoren \n    logical \n\n    _ \n\n  \n  \n    Ganze Zahlen \n    z.B. 2 \n    Arithmetische und Logische Operatoren \n    integer \n\n    numeric \n\n  \n  \n    Kommazahlen \n    z.B. 3.4 \n    Arithmetische und Logische Operatoren \n    double \n\n    numeric \n\n  \n\n\n\n\n\n\n\n\n\nEs gibt in R noch zwei weitere Datentypen, mit denen wir uns aber nicht weiter beschäftigen werden: complex und raw.\n\n\nAchtung: Kommazahlen werden mit . und nicht mit , dargestellt, weil Kommata genutzt werden, um Argumente einer Funktion voneinander zu trennen.\n\n\n\n\n\n\nLogische Operatoren in R\n\nEin logischer Operator ist ein Operator, dessen Ergebnis ein Wahrheitswert (logical; TRUE oder FALSE) ist. \n\n\n\n\n \n  \n    Operator \n    Vergleich \n    Beispiel \n  \n \n\n  \n    < \n    kleiner \n    1 < 1FALSE \n  \n  \n    <= \n    kleiner gleich \n    1 <= 1TRUE \n  \n  \n    > \n    größer \n    2 > 1TRUE \n  \n  \n    >= \n    größer gleich \n    1 >= 3FALSE \n  \n  \n    == \n    (genau) gleich \n    TRUE == FALSEFALSE \n  \n  \n    != \n    ungleich \n    TRUE != FALSETRUE \n  \n  \n    ! \n    nicht (Negation von Bedingungen) \n    !TRUEFALSE \n  \n  \n    | oder || \n    oder \n    5 > 3 | 5 == 4TRUE \n  \n  \n    & oder && \n    und \n    5 > 3 & 9 < 8FALSE \n  \n\n\n\n\n\n\n\n\n\nArithmetische Operatoren in R\n\nÜber (die meisten) arithmetischen Operatoren sind wir wohl schon zu Grundschulzeiten gestoßen. Das sind Operatoren, die wir zum Rechnen mit Zahlen (numeric) benötigen.\n\n\n\n\n\n\n\n\nOperator\nRechenoperationen\nBeispiel\n\n\n\n\n-\nAddition\n1+2 = 3\n\n\n+\nSubtraktion\n4-3 = 1\n\n\n*\nMultiplikation\n2*3 = 6\n\n\n/\nDivision\n5/3 = 1.6667\n\n\n^ oder **\nExponenzieren\n8^2 = 64\n\n\n%%\nganzzahliger Rest bei der Division (Modulo)\n5 %% 3 = 2\n\n\n%/%\nganzzahliger Quotient\n5 %/% 3 = 1\n\n\n\n\n\n3.2.1.1 Messniveaus und Datentypen\n\n\nRecap: Messniveaus\n\nDas Messniveau (oder auch Skalenniveau) ist eine wichtige Eigenschaft von Merkmalen (Variablen) von Untersuchungseinheiten. Es beschreibt, welche Informationen in unseren Messwerten abgebildet werden und damit auch welche mathematischen Transformationen mit den Messwerten sinnvoll sind (z.B. das Berechnen von Mittelwerten). Somit begrenzt das Messniveau auch die zulässigen Datenauswertungsverfahren unserer Variablen.\nDie Kodierung von nominalskalierten Merkmalen ist insofern willkürlich, als dass lediglich auf Gleichheit versus Ungleichheit geachtet werden muss (z.B. 1, 4, 9 oder A, Y, M).\nDie Kodierung von ordinalskalierten Merkmalen geschieht der Größe nach, d.h. dass die Rangfolge der Kodierungen einzelner Gruppen relevant ist (z.B. 1 < 4 < 9 oder A < M < Y). Man kann aber auch eine eigene Sortierung festlegen, die nicht der “natürlichen” Rangfolge (Zahlen: aufsteigend; Buchstaben: alphabetisch) entspricht (z.B. Y < A < M). Ein Realschulabschluss ist beispielsweise besser als ein Hauptschulabschluss. Wir können aber nicht festlegen, wie viel besser er ist.\nBei der Kodierung von intervallskalierten Merkmalen sind sowohl die Rangfolge als auch die Abstände zwischen den Ausprägungen relevant (z.B. 1, 4, 7; jeweils mit gleichem Abstand zueinander; oder 1.4, 1.5, 2.3; jeweils mit verschiedenen Abständen zueinander). Ein Beispiel dafür ist die Temperatur in Grad Celsius oder Grad Fahrenheit.\nBei der Kodierung von verhältnisskalierten Merkmalen ist zusätzlich noch ein Nullpunkt vorhanden. Dieser erlaubt es, dass Quotienten zwischen Werten gebildet werden können. Ein beliebtes Beispiel ist die Kelvin Skala. Bei dieser ist bei 0°K keine Bewegungsenergie mehr vorhanden und 20°K sind halb so viel wie 40°K.\nZu guter Letzt gibt es noch absolutskalierte Merkmale, welche sowohl einen eindeutigen Nullpunkt als auch eine eindeutige Einheit der Skala (z.B. Anzahl der Kinder) vorweisen kann. Die Kodierung entspricht der natürlichen Einheit.\nNachfolgend finden wir eine Tabelle der möglichen Unterscheidungen der jeweiligen Messniveaus.\n\n\n\n\n \n  \n      \n    (Un-)\nGleichheit \n\n    Rangordnung \n    Abstände \n    Verhältnisse \n    natürliche\nEinheit \n  \n \n\n  \n    Nominal \n    X \n     \n     \n     \n     \n  \n  \n    Ordinal \n    X \n    X \n     \n     \n     \n  \n  \n    Intervall \n    X \n    X \n    X \n     \n     \n  \n  \n    Verhältnis \n    X \n    X \n    X \n    X \n     \n  \n  \n    Absolut \n    X \n    X \n    X \n    X \n    X \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBildquelle: https://de.wikipedia.org/wiki/Datei:Skalenniveau.png\n\nDie verschiedenen Messniveaus können mit unterschiedlichen Datentypen repräsentiert werden. Hauptsächlich nutzt man dafür character und numeric. Nachfolgend finden wir eine Übersicht der möglichen Kodierungen der Messniveaus.\n\n\n\n\n \n\n_\nArt der Skala:\n\n  \n     \n     \n    Nominal- \n    Ordinal- \n    Intervall- \n    Verhältnis- \n    Absolut- \n  \n \n\n  \n    Datentyp: \n    character \n    X \n    X2 \n     \n     \n     \n  \n  \n    Datentyp: \n    numeric \n    X1 \n    X2 \n    X \n    X \n    X \n  \n\n\n1 Faktorisieren (unordered factor) notwendig wenn keine Indikatorvariable(n) genutzt) 2 Faktorisieren (ordered factor) notwendig\n\n\n\n\n\n\n\n\n\n\n\n\nEin Faktor ist eine Art von Vektor. Mehr dazu im nächsten Abschnitt.\n\n\n\nKönnen Merkmale auch mit logical kodiert werden?\n\nWir könnten auch logische Werte nutzen, um Merkmale zu kodieren, allerdings kann es sich dabei nur um dichotome nominalskalierte Merkmale handeln (d.h. diese können nur zwei diskrete Ausprägungen besitzen).\nLogische Werte und Operatoren kommen hauptsächlich in der Indexierung von Vektoren (diese lernen wir im nächsten Abschnitt kennen) …\n\nx <- c(7,4,3,6,1) # Vektor x erstellen\nx\n\n[1] 7 4 3 6 1\n\n# Welche Elemente in Vektor x sind größer als 5? \nx > 5 # Output: logischer Vektor (mit T/F zu jedem Element) ...\n\n[1]  TRUE FALSE FALSE  TRUE FALSE\n\nx[x > 5] # ... den wir auf x anwenden können, um die Elemente zu erhalten.\n\n[1] 7 6\n\n\n… und der konditionalen Programmierung vor.\n\nfor (i in 1:length(x)) { # Für jedes Element in x, begonnen bei 1 ...\n  if (x[i] > 5) {         # ... wenn i-tes Element in x größer als 5 ...\n    x[i] = x[i] * 2         # ... multipliziere Element mit 2\n  }\n}\nx\n\n[1] 14  4  3 12  1\n\n\nDamit werden wir uns erst später beschäftigen. Anwendung findet die konditionale Programmierung z.B. wenn wir subsetten, d.h. Elemente, auf die eine Kondition (z.B. größer als 5) zutrifft, einer Datenstruktur (z.B. Vektor, Matrix, Dataframe) entnehmen wollen.\n\n\n\n\n3.2.2 Datenstrukturen\n\n\n\n\nDie Datenstruktur bestimmt die Organisation und Speicherung von Daten(typen), und folglich auch, welche Funktionen wir anwenden können.\n\nDatenstrukturen können nach Dimensionalität und enthaltenen Datentypen klassifiziert werden. Nachfolgend befindet sich eine Übersicht der in R enthaltenen Datenstrukturen.\n\n\n\n\n \n\n_\nBeinhaltet unterschiedliche Datentypen?\n\n  \n     \n     \n    nein (homogen) \n    ja (heterogen) \n  \n \n\n  \n    Anzahl der Dimensionen \n    1 \n    Vektor \n    Liste \n  \n  \n    Anzahl der Dimensionen \n    2 \n    Matrix \n    Data Frame \n  \n  \n    Anzahl der Dimensionen \n    n \n    Array \n     \n  \n\n\n\n\n\n\n\n3.2.2.1 Vektor\nVektoren sind die elementare Datenstruktur, aus der sich alle anderen Datenstrukturen zusammensetzen. Sie besitzen nur eine Dimension. Mit c() können wir Vektoren erstellen.\nGenerell können sie unterschiedlichen Typs sein …\n\nvek_1 <- c(\"A\", 'B')  # egal ob \" oder ' \nvek_2 <- c(F, T, T)  # Abkürzung von FALSE und TRUE\n\n… aber ein Vektor kann nur einen Datentyp beinhalten.\n\nvek_3 <- c(1, \"3\")  # alles wird zu character\n\nZahlen können wir auf unterschiedliche Weisen speichern.\n\nvek_4 <- c(1, 2, 3) # ganze Zahlen\nvek_5 <- c(1.3, 4.5) # Kommazahlen\nvek_6 <- c(1L, 4L) # ganze Zahlen\n\nMit str() können wir uns den Datentyp, die Länge der Dimension (Anzahl der Elemente) und die ersten 10 Elemente ausgeben lassen.\n\nstr(vek_1)\n\n chr [1:2] \"A\" \"B\"\n\n# chr --> Datentyp character\n# [1:2] --> enthält zwei Elemente\n# \"A\" \"B\" --> ersten zwei (von max. 10) Elementen\n\n\nstr(vek_2)\n\n logi [1:3] FALSE TRUE TRUE\n\nstr(vek_3)\n\n chr [1:2] \"1\" \"3\"\n\nstr(vek_4)\n\n num [1:3] 1 2 3\n\nstr(vek_5)\n\n num [1:2] 1.3 4.5\n\nstr(vek_6)\n\n int [1:2] 1 4\n\n\n\n\nWarum sind die numerischen Vektoren nur vom Typ numeric oder integer?\n\nBei Betrachtung der numerischen Vektoren fällt auf, dass vek_4 und vek_5 als numeric und vek_6 als integer gespeichert wurden.\nAber warum wurden vek_4 und vek_5 als numeric gespeichert, obwohl wir integer (ganze Zahlen) bzw. double (Kommazahlen) erwartet hätten?\nDas liegt daran, dass R alle Zahlen (d.h. ganze, reelle und komplexe Zahlen) als numeric zusammenfasst (wie bereits in der Einführung zu Datentypen erwähnt), solange wir diese nicht explizit (als integer) definieren.\nGenau genommen lautete der exakte Datentyp von ganzen und reellen Zahlen, die als numeric zusammengefasst sind, double. Mit typeof() sehen wir diesen.\n\ntypeof(vek_4)\n\n[1] \"double\"\n\ntypeof(vek_5)\n\n[1] \"double\"\n\n\nWarum double (und nicht integer)?\nWenn wir arithmetische Operatoren (v.a. Division) anwenden, dann werden unsere ganzen Zahlen zu Kommazahlen. Daher werden ganze und gebrochene Zahlen in numeric “präventiv” als double gespeichert.\nUnd warum ist vek_6 vom Typ integer?\nHier haben wir mit dem L hinter den (ganzen) Zahlen (c(1L, 4L)) explizit festgelegt, dass wir diese als integer speichern wollen.\nWenn wir integer-Zahlenfolgen erstellen wollen, können wir das auch mit Anfang:Ende machen.\n\nvek_7 <- 2:5\nvek_8 <- c(6:9, 1:4)\n\n\nstr(vek_7)\n\n int [1:4] 2 3 4 5\n\nstr(vek_8)\n\n int [1:8] 6 7 8 9 1 2 3 4\n\n\nGenerell reicht für uns aber die Unterscheidung zwischen numeric und den anderen, nicht-numerischen Datentypen. Ob integer oder double ist zumeist nicht von Relevanz.\n\nWenn wir Zahlenfolgen (numeric) erstellen wollen, können wir seq() nutzen.\n\n# seq(from,to,by)\nvek_9 <- seq(1,10,2)\nvek_10 <- seq(1,10,0.5)\n\n\nstr(vek_9)\n\n num [1:5] 1 3 5 7 9\n\nstr(vek_10)\n\n num [1:19] 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 ...\n\n\nWenn wir wollen, dass sich Elemente wiederholen, können wir die Funktion rep() nutzen.\n\n# rep(x, times)\nvek_11 <- rep(\"A\", 10)\nvek_12 <- c(rep(1, 3), rep(2:3, 3))\n\n\nstr(vek_11)\n\n chr [1:10] \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\"\n\nstr(vek_12)\n\n num [1:9] 1 1 1 2 3 2 3 2 3\n\n\n\n\nWie genau funktioniert rep()?\n\nDem Parameter x übergeben wir die Zeichen(folge), die wir wiederholen wollen; times übergeben wir die Anzahl der Wiederholungen der Zeichenfolge bzw. each die Anzahl der Wiederholungen der einzelnen Zeichen.\nDie Zahl 1 wird 10 mal (times) wiederholt:\n\nrep(1, 10) # das gleiche wie: rep(x=1, times=10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\nDie Zahlenfolge 0, 1 bzw. die Zeichenfolge \"A\", \"B wird 10 mal (times) wiederholt:\n\nrep(0:1, 10)\n\n [1] 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n\nrep(c(\"A\", \"B\"), 10)\n\n [1] \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\"\n[20] \"B\"\n\n\nWenn wir erst 10 mal die 0 bzw. \"A\" und anschließend 10 mal die 1 bzw. \"B\" haben wollen, nutzen wir den Parameter each.\n\nrep(0:1, each=10) # das gleiche wie c(rep(0, 10), rep(1, 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n\nrep(c(\"A\", \"B\"), each=10)\n\n [1] \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n[20] \"B\"\n\n\n\nDurch Indexierung via [] können wir uns einzelne Elemente ausgeben lassen.\n\nvek_1[2] # zweites Element\n\n[1] \"B\"\n\nvek_5[1] # erstes Element\n\n[1] 1.3\n\n\n\n3.2.2.1.1 Spezialfall Faktor\n\n\n\nEin Faktor ist ein spezieller Vektor, der genutzt wird, um diskrete Klassifikationen zu kodieren.\nMit der Funktion factor() können wir Vektoren in ungeordnete und geordnete Faktoren umwandeln.\nUnsortierte Faktoren können nominalskalierte Merkmale kodieren.\n\n# Vektor erstellen\nx <- c(1,3,2,3,2)\ny <- c(\"f\", \"B\", \"c\", \"b\", \"c\")\n\nnominal_x <- factor(x)\nnominal_y <- factor(y)\n\n\nstr(nominal_x)\n\n Factor w/ 3 levels \"1\",\"2\",\"3\": 1 3 2 3 2\n\nstr(nominal_y)\n\n Factor w/ 4 levels \"b\",\"B\",\"c\",\"f\": 4 2 3 1 3\n\n\nDie Zahlen hinter den Ausprägungen zeigen die interne Kodierung. Bei numerischen Vektoren wie nominal_x entsprechen diese auch den möglichen Ausprägungen.\nSortierte Faktoren können ordinalskalierte Merkmale kodieren. Um Faktoren zu sortieren, müssen wir dem Parameter ordered das Argument TRUE übergeben.\n\nordinal_x <- factor(x, ordered=TRUE)\nordinal_y <- factor(y, ordered=TRUE)\n\n\nstr(ordinal_x)\n\n Ord.factor w/ 3 levels \"1\"<\"2\"<\"3\": 1 3 2 3 2\n\nstr(ordinal_y)\n\n Ord.factor w/ 4 levels \"b\"<\"B\"<\"c\"<\"f\": 4 2 3 1 3\n\n\nWie wir sehen wurde automatisch eine Sortierung festgelegt. Zahlen werden standardmäßig aufsteigend; Zeichen alphabetisch sortiert (wobei Kleinbuchstaben vor Großbuchstaben auftauchen). Bei den unsortierten Faktoren gab es diese Sortierung auch bereits, allerdings wird diese nur zum Darstellen der Ausprägungen genutzt (bei nominal_ sind die Ausprägungen mit , getrennt; bei ordinal_ mit <).\nMit dem Parameter levels können wir auch eigene Sortierungen festlegen. Das übergebene Argument muss selbst ein Vektor mit den möglichen Ausprägungen sein.\n\nordinal_x.2 <- factor(x, ordered=TRUE, levels=c(3,2,1))\nordinal_y.2 <- factor(y, ordered=TRUE, levels=c(\"B\", \"f\", \"b\", \"c\"))\nstr(ordinal_x.2)\n\n Ord.factor w/ 3 levels \"3\"<\"2\"<\"1\": 3 1 2 1 2\n\nstr(ordinal_y.2)\n\n Ord.factor w/ 4 levels \"B\"<\"f\"<\"b\"<\"c\": 2 1 4 3 4\n\n\nJetzt sehen wir auch, dass sich (mit einer anderen als der natürlichen Sortierung) auch die internen Kodierungen geändert haben.\n\n\n\n3.2.2.2 Matrix\nMatrizen sind zweidimensionale Vektoren, die nur einen Datentyp beinhalten können. In mathematischen Kontexten werden Matrizen uns häufiger begegnen.\nWir erstellen sie mit matrix(data, nrow, ncol).\n\nmat_1 <- matrix(data=c(1,2,3,4), # Daten-Vektor\n                nrow=2, # Anzahl Zeilen\n                ncol=2, # Anzahl Spalten\n                # eine Angabe (Zeilen oder Spalten) reicht auch\n                byrow=TRUE) # reihenweise Eintragen der Daten\n\n\nmat_1\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nstr(mat_1) # [Länge Zeilen, Länge Spalten]\n\n num [1:2, 1:2] 1 3 2 4\n\n\nWenn wir einzelne Elemente indexieren wollen, müssen wir zwei Indizes angeben, weil Matrizen zweidimensional sind.\n\nZuerst die Zeile,\ndann die Spalte.\n\n\n# [Zeile, Spalte]\nmat_1[1,2] # Zeile 1, Spalte 2\n\n[1] 2\n\nmat_1[2,1] # Zeile 2, Spalte 1\n\n[1] 3\n\n\nWir können auch nur einen Index angeben, um uns die komplette Zeile bzw. Spalte ausgeben zu lassen. Dabei müssen wir aber daran denken, das Komma zu setzen!\n\nmat_1[1,] # komplette erste Zeile\n\n[1] 1 2\n\nmat_1[,2] # komplette zweite Spalte\n\n[1] 2 4\n\n\nZusätzlich können wir die Spalten und Zeilen von Matrizen benennen.\n\ncolnames(mat_1) <- c(\"A\", \"B\") # Spalten benennen\nrownames(mat_1) <- c(\"Vpn_1\", \"Vpn_2\")\nmat_1\n\n      A B\nVpn_1 1 2\nVpn_2 3 4\n\n\n\n\n3.2.2.3 Liste\n\n\n\nListen bestehen aus geordneten Sammlungen von Objekten (Komponenten) unterschiedlichen Datentyps. Diese Objekte können wiederum selbst Vektoren, Matrizen oder Dataframes sein. Listen haben nur eine Dimension.\nMit list() können wir eigene Listen erstellen.\n\nlist_kurs <- list(kurs=\"Programmieren\",\n                  teilnehmer=3,\n                  namen.teilnehmer=c(\"Tina\", \"Paul\", \"Lena\"),\n                  vorerfahrung=c(T, F, F))\nlist_kurs\n\n$kurs\n[1] \"Programmieren\"\n\n$teilnehmer\n[1] 3\n\n$namen.teilnehmer\n[1] \"Tina\" \"Paul\" \"Lena\"\n\n$vorerfahrung\n[1]  TRUE FALSE FALSE\n\n\n\nstr(list_kurs) # \"List of ...\" gibt die Länge der (einen) Dimension der Liste an\n\nList of 4\n $ kurs            : chr \"Programmieren\"\n $ teilnehmer      : num 3\n $ namen.teilnehmer: chr [1:3] \"Tina\" \"Paul\" \"Lena\"\n $ vorerfahrung    : logi [1:3] TRUE FALSE FALSE\n\n\nWir können Komponenten bzw. ihre Elemente auf verschiedene Arten indexieren.\n\nlist_kurs[3] # Name und Elemente der dritten Komponente\n\n$namen.teilnehmer\n[1] \"Tina\" \"Paul\" \"Lena\"\n\nlist_kurs[[3]] # nur Elemente der dritten Komponente\n\n[1] \"Tina\" \"Paul\" \"Lena\"\n\nlist_kurs[[3]][2] # zweites Element der dritten Komponente\n\n[1] \"Paul\"\n\n\nWir können alle Elemente einer Komponente ebenso mit ihren Namen extrahieren.\n\nlist_kurs$vorerfahrung\n\n[1]  TRUE FALSE FALSE\n\n\n\n\n\nListen werden uns häufiger als Output statistischer Funktionen begegnen.\n\n\n3.2.2.4 Data Frame\nIn der Psychologie arbeiten wir zumeist mit Data Frames. Diese haben, wie Matrizen, zwei Dimensionen, aber sie können auch unterschiedliche Datentypen beinhalten.\nUm Data Frames zu erstellen, spezifizieren wir zuerst Vektoren (unterschiedlichen Typs) und führen diese dann mit data.frame() zusammen.\n\n# Vektoren erstellen: \neins <- c(1, 3, 2, 1) \nzwei <- c(\"A\", \"A\", \"B\", \"B\")\n\n# in Data Frame zusammenführen\ndf_1 <- data.frame(eins, zwei)\n\n\ndf_1\n\n  eins zwei\n1    1    A\n2    3    A\n3    2    B\n4    1    B\n\nstr(df_1) # obs. = Länge Zeilen, variables = Länge Spalten\n\n'data.frame':   4 obs. of  2 variables:\n $ eins: num  1 3 2 1\n $ zwei: chr  \"A\" \"A\" \"B\" \"B\"\n\n\nDie Benennung der Vektoren wird als Spaltenbenennung übernommen. Per Default werden Daten vom Typ character (z.B. zwei) als Faktoren gespeichert (stringsAsFactors = default.stringsAsFactors(), was wiederum TRUE ist).\nMit colnames() bzw. rownames() können wir wieder Spalten- bzw. Zeilennamen ändern bzw. hinzufügen.\n\ncolnames(df_1) <- c(\"AV\", \"UV\")\nrownames(df_1) <- c(\"Fall_1\", \"Fall_2\", \"Fall_3\", \"Fall_4\")\n# mit data.frame(..., row.names) könnten wir auch initial Zeilennamen übergeben\ndf_1\n\n       AV UV\nFall_1  1  A\nFall_2  3  A\nFall_3  2  B\nFall_4  1  B\n\n\nWir können mit Indexieren wieder einzelne Elemente oder Spalten bzw. Zeilen extrahieren. Spalten bzw. Zeilen können wir hier auch mit ihren Namen ansprechen.\n\ndf_1[3,2] # dritte Zeile, zweite Spalte\n\n[1] \"B\"\n\ndf_1[\"Fall_3\", \"UV\"] # dritte Zeile, zweite Spalte\n\n[1] \"B\"\n\ndf_1[1,] # erste Zeile\n\n       AV UV\nFall_1  1  A\n\ndf_1[\"Fall_1\",] # erste Zeile\n\n       AV UV\nFall_1  1  A\n\n\nSpalten können wir uns auch mit dem $-Operator ausgeben lassen mit der Form df_name$spalten_name.\n\ndf_1$AV\n\n[1] 1 3 2 1\n\n\nEinige Funktionen verlangen Data Frames als Input. Wir können z.B. Matrizen mittels as.data.frame() in Data Frames umwandeln.\n\n\n\n3.2.3 Objekte\n\n\n\n\n“Everything that exists in R is an object”\n- John Chambers (Entwicklungsteam von R)\n\nR arbeitet mit sogenannten Objekten. Alle Entitäten, mit denen wir in R operieren, sind Objekte. So sind alle Datenstrukturen, die wir gerade kennengelernt haben, Objekte sobald wir ihnen einen Namen zugewiesen haben.\nUm ein Objekt zu erstellen, nutzen wir den Zuweisungspfeil <-.\n\nobj <- c(1, 2, 3)\n\nWir können uns Objekte anschauen, indem wir ihren Namen ausführen oder indem wir View() nutzen. Zweiteres öffnet das Objekt im Data Viewer (in RStudio).\n\nobj\n\n[1] 1 2 3\n\n\nAlle Objekte, die derzeit in R vorhanden sind, bekommen wir mit ls() angezeigt.\nWenn wir Objekte löschen wollen, nutzen wir rm(). Mit rm(objekt_1, objekt_2, ...) löschen wir einzelne Objekte; mit rm(list = ls()) löschen wir alle.\nZur Benennung von Objekten ist folgendes zu wissen:\n\nalle alphanumerischen Zeichen sowie . und _ sind erlaubt\nIn Deutsch schließt das Groß- und Kleinbuchstaben des gesamten Alphabets und der Umlaute sowie die Zahlen 0-9 ein. Um Enkodierungsprobleme (u.a. zwischen verschiedenen Systemen) zu reduzieren, sollten wir aber auf Umlaute verzichten; auch in der Benennung unserer Ordner außerhalb von R. Namen sollten nicht mit . oder _ beginnen.\nGroß- und Kleinschreibung beachten (case-sensitivity)\nDas gilt auch für Funktionen; z.B. funktioniert View() nur, wenn der erste Buchstabe groß geschrieben wird.\nBestehende Objekte können de facto nicht mehr umbenannt werden. Wir können sie aber in einem neuen Objekt (mit einem neuen Namen) speichern (und ggf. das alte Objekt löschen)."
  },
  {
    "objectID": "Einfuehrung_in_R.html#weitere-hilfen",
    "href": "Einfuehrung_in_R.html#weitere-hilfen",
    "title": "3  Einführung in R",
    "section": "3.3 Weitere Hilfen",
    "text": "3.3 Weitere Hilfen\n\n\n\n3.3.1 Andere Lernplattformen und Übungen\nWie bereits im Abschnitt Funktionen erwähnt, können wir auf freecodecamp.org eine weiterführende Vertiefung zu Funktionen im gleichnamigen Abschnitt bekommen. Das dauert nur ca. 10min und ist gut investierte Zeit, wenn man wenig Erfahrung mit dem Programmieren hat.\nWenn ihr einen amüsanten Einstieg in R haben wollte, schaut euch YaRrr! The Pirate’s Guide to R (auf englisch) an. Nach einer charismatischen Einführung befinden sich mehrere Abschnitte, teils mit Abschlussübung z.B. zu Skalaren und Vektoren, Vektor-Funktionen, Indexierung, Plotten, uvm..\nwww.r-exercises.com bietet eine Fülle an Übungen (mit Lösungen) zu verschiedenen Themenbereichen, wie z.B. Vektoren und Data Frames. Mit Ausnahme der mit Protected gekennzeichneten Seiten können wir alle nutzen. Rechts unter Filter by Topic können wir die Themen filtern.\nWenn ihr den Umgang mit R direkt in R lernen wollt, dann schaut euch das Paket swirl an. Es gibt mehrere Kurse mit mehreren kleinen Einheiten zu absolvieren.\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] kableExtra_1.3.4 dplyr_1.1.2      psych_2.3.3     \n\nloaded via a namespace (and not attached):\n [1] jsonlite_1.8.4    compiler_4.3.0    webshot_0.5.4     tidyselect_1.2.0 \n [5] xml2_1.3.3        stringr_1.5.0     parallel_4.3.0    systemfonts_1.0.4\n [9] scales_1.2.1      yaml_2.3.7        fastmap_1.1.1     lattice_0.21-8   \n[13] R6_2.5.1          generics_0.1.3    knitr_1.42        htmlwidgets_1.6.2\n[17] tibble_3.2.1      munsell_0.5.0     svglite_2.1.1     pillar_1.9.0     \n[21] rlang_1.1.0       utf8_1.2.3        stringi_1.7.12    xfun_0.39        \n[25] viridisLite_0.4.1 cli_3.6.1         magrittr_2.0.3    digest_0.6.31    \n[29] rvest_1.0.3       grid_4.3.0        rstudioapi_0.14   lifecycle_1.0.3  \n[33] nlme_3.1-162      vctrs_0.6.2       mnormt_2.1.1      evaluate_0.20    \n[37] glue_1.6.2        fansi_1.0.4       colorspace_2.1-0  rmarkdown_2.21   \n[41] httr_1.4.5        tools_4.3.0       pkgconfig_2.0.3   htmltools_0.5.5  \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Einfuehrung_in_RStudio.html#allgemeines-zu-rstudio",
    "href": "Einfuehrung_in_RStudio.html#allgemeines-zu-rstudio",
    "title": "4  Einführung in RStudio",
    "section": "4.1 Allgemeines zu RStudio",
    "text": "4.1 Allgemeines zu RStudio\nRStudio ist eine integrierte Entwicklungsumgebung (integrated development environment, IDE) für die Statistiksoftware R. Eine IDE bietet uns verschiedene Werkzeuge an, die uns den Umgang mit einer (grundlegenden) Software erleichtern.\nIn R steht uns standardmäßig nur die Konsole und das Skript zur Verfügung.\nIn RStudio gibt es u.a. zusätzlich:\n\nCode Hervorhebung\nIn Abhängigkeit davon, um was für Code es sich handelt (z.B. Funktionen, Datentyp, bestehende Objekte, Kommentare; Fehlermeldungen) wird dieser farblich verschieden im Skript und auch in der Konsole hervorgehoben.\nBefehlszeilenergänzung (Autovervollständigung)\nWenn wir anfangen, Code einzutippen, bekommen wir bereits Vorschläge, welche Funktionen oder bestehenden Objekte wir meinen könnten. Wenn wir mit der Maus über eine vorgeschlagene Funktion fahren, bekommen wir außerdem eine kurze Erklärung, was diese macht und welche Parameter sie besitzt. Wir können auf die Vorschläge klicken, um unseren Code (d.h. den Namen der Funktion oder des Objekts) automatisch vervollständigen zu lassen. Die Befehlszeilenergänzung können wir sowohl in der Konsole als auch im Skript nutzen.\nCode Diagnostik\nWenn wir Code im Skript schreiben, bekommen wir schon vor der Ausführung Hinweise auf Probleme bzw. Unvollständigkeiten. Unser Skript muss dazu aber bereits gespeichert sein. Auf die Code Diagnostik gehen wir im Kapitel zu Fehlermeldungen noch etwas detaillierter ein.\n\n\n\nWir können aus verschiedenen Möglichkeiten zur Code Hervorhebung wählen. Diese finden wir in der oberen Menüleiste auf Tools > Global Options… > Appearance > Editor Theme.\n\nBeim ersten Öffnen sieht RStudio folgendermaßen aus:\n\n\n\n\n\n\n\n\n\nZu allererst öffnen wir ein neues Skript, in das wir unseren Code schreiben werden. Dazu gehen wir in der oberen Leiste ganz links auf  und dann auf R Script.\n\n\n\nNun gliedert sich die Entwicklungsumgebung RStudio in vier verschiedene Bereiche:\nIn das Skript schreiben wir unseren Code. In der Konsole wird dieser ausgeführt und die Ergebnisse angezeigt. Bestehende Objekte sehen wir im Environment. Eine Übersicht unserer Pakete finden wir unter Packages und Informationen zu R und zu Funktionen finden wir unter Help.\n\n\n\n\n\n\n\n\n\n\n\n\nDie Größe der Bereiche lässt sich an den Kanten oder an den Icons in der oberen rechten Ecke verändern. Die Aufteilung der Panels können wir über Tools > Global Options… und dann im neuen Fenster links unter Pane Layout ändern.\n\nDie verschiedenen Bereiche (Panels) und ihre (für uns wichtigsten) Features schauen wir uns im Folgenden einmal genauer an.\n\n4.1.1 Skript\nHier schreiben wir unseren Code und Kommentare rein.\n\n\n\n\n\n\n\n\n\nAlle Zeichen, die in einer Zeile nach einem # erscheinen, gelten als Kommentar. Von der Kommentarfunktion sollten wir ausgiebig Gebrauch machen. So können wir (und Andere) zu einem späteren Zeitpunkt noch nachvollziehen, was genau wir gemacht haben.\nUnsere Skripte sollten wir regelmäßig (oder spätestens vor Beendigung des Programms) speichern. Um die Datei anzulegen gehen wir in der oberen Leiste auf File > Save as…. Wir geben der R-Datei einen Namen und wählen einen Speicherort.\n\nSkript Speichern:\nWin: Strg + S\nMac: cmd + S\n\nBeim allerersten Speichern müssen wir außerdem die Zeichen-Enkodierung festlegen. Wir wählen UTF-8, das ist die am häufigsten genutzte Kodierung für Unicode-Zeichen.\n\n\n\nAb dann können wir unser Skript über File > Save speichern.\nUnseren Code führen wir aus, indem wir ihn markieren und auf Run klicken.\n\n\n\n\n\n\n\n\n\n\nAktuelle Linie bzw. markierten Code ausführen:\nWin: Strg + enter\nMac: cmd + enter\n\n\n\n4.1.2 Konsole\nHier bekommen wir den Output unseres Codes ausgegeben. Kommentare werden nicht ausgeführt, sondern nur wiedergegeben.\n\n\n\n\n\n\n\n\n\nWir bekommen in der Konsole einen Hinweis, wenn wir eine eindeutig unvollständige Funktion, d.h. wenn wir nur am Ende einer Funktion die schließende Klammer vergessen haben, ausführen wollen. Dann wird uns in der Konsole in der nachfolgenden Zeile ein + ausgegeben.\n\n\n\n\n\n\n\n\n\nIn der Konsole bekommen wir manchmal auch Fehler- (error) und Warnmeldungen (warning) sowie andere Informationen zu einer Funktion (message) angezeigt.\nWenn wir einen Fehler in einer Funktion gemacht haben, wird die Ausführung unterbrochen und wir bekommen eine Fehlermeldungen. Wenn wir z.B. ein Komma zwischen den Elementen in c() vergessen hätten, bekämen wir folgende Fehlermeldung:\n\n\n\n\n\nMehr Informationen zur Interpretation von Fehlermeldungen finden wir im gleichnamigen Kapitel.\nDer Inhalt der Konsole wird nach jedem Schließen des Programms gelöscht und kann nicht ohne weiteres gespeichert werden. Deswegen ist es wichtig, die eigene Arbeit in Skripten zu speichern.\n\n\n4.1.3 Environment & History\nIm Environment sehen wir alle Objekte, die derzeit in R geladen sind. Wenn wir (externe) Datensätze einlesen, sehen wir diese auch hier. Wir bekommen außerdem weitere Informationen zu den Objekten und wir sehen ggf. die ersten Elemente.\nBeispielsweise haben wir gerade das Objekt vektor erstellt, welches uns hier angezeigt wird.\n\n\n\n\n\n\n\n\n\nWir sehen, dass es sich um einen numerischen Vektor mit zwei Elementen, 1 und 3, handelt.\nWas für Informationen wir zu einem Objekt angezeigt bekommen ist abhängig von seiner Datenstruktur. Schauen wir uns das einmal am Beispiel der Objekte, die wir im Abschnitt zu Daten im Kapitel Einführung in R erstellt haben, an.\n\n\n\n\n\n\n\n\n\nDie Objekte werden in zwei Kategorien aufgeteilt:\n\nUnter Data finden wir Data Frames, Listen und Matrizen. Bei allen bekommen wir die Länge der einzelnen Dimensionen angezeigt (Dataframe: obs. = Zeilen, variables = Spalten; Matrix: [Zeilen, Spalten]; Liste: List of …). Data Frames werden mit einem vorangestellten  markiert. Bei Matrizen bekommen wir zusätzlich die ersten Elemente angezeigt. Durch Klicken können wir uns diese Objekte im Data Viewer anschauen (alternativ zu View()).\nUnter Values finden wir Vektoren. Wir bekommen die selben Informationen angezeigt, die uns str(vektor) gibt: Datentyp, Länge, ersten 10 Elemente. Bei Faktoren wird uns de facto nicht der Datentyp sondern die -struktur angezeigt. Der Datentyp ist eigentlich auch irrelevant, weil es nur eine diskrete Anzahl an Ausprägungen gibt, die uns auch angezeigt wird. Die Länge des Faktors erfahren wir auch nicht.\n\n\nDen Datentyp eines Faktors können wir mit typeof(), die Länge mit length() in Erfahrung bringen.\n\nZum Löschen aller Objekte können wir auf  klicken (alternativ zu rm(list=ls())).\nZusätzlich können wir über das Environment auch externe Datensätze einlesen. Wie wir das machen, schauen wir uns im Kapitel zum Daten einlesen im Abschnitt dazu an.\nIn der History sehen wir den zuletzt ausgeführten Code.\n\n\n\n\n\n\n\n\n\nMit To Source bekommen wir den markierten Code in unser Skript; mit To Console in die Konsole.\nDer Vorteil gegenüber der Konsole ist, dass der Inhalt der History nicht mit Beenden einer R-Session gelöscht wird, sondern wir auf den Code zugreifen können bis dieser explizit gelöscht wird (auch mit einem ).\n\n\n4.1.4 Files, Plots, Packages, Help & Viewer\nIm Folgenden werden wir hauptsächlich die Reiter Packages und Help besprechen. Auf die Dokumentation, auf die wir mit letzerem direkt in RStudio (anstatt im Browser) zugreifen können, gehen wir besonders stark ein.\nUnter Packages sehen wir die Standardpakete und die von uns installierten Add-On Pakete.\n\n\n\n\n\n\n\n\n\nWir sehen eine kurze Beschreibung des Pakets und seine Versionsnummer. Im Kästchen ganz links sehen wir außerdem, ob Pakete derzeit geladen sind. Standardpakete sind immer geladen; Add-On Pakete müssen wir bei jeder Session neu laden (wenn wir sie nutzen wollen).\nMehr Informationen zum Installieren, Laden und Aktualisieren von Paketen mit Funktionen sowie der Entwicklungsumgebung R finden wir im gleichnamigen Kapitel.\nUnter Files sehen wir die Ordner(struktur) auf unserem Rechner. Im Kapitel zu Daten einlesen erfahren wir, wie wir diesen bereich nutzen können.\nUnter Plots werden (von uns erstellte) Grafiken angezeigt; unter Viewer (von uns erstellte) Tabellen.\nÜber Help bekommen wir Zugang zur R-Dokumentation, welche wir uns nachfolgend etwas genauer anschauen wollen.\n\n4.1.4.1 R-Dokumentation\nDie R-Dokumentation bietet uns umfassende Hilfe zum Umgang mit R im Allgemeinen und zu Funktionen an.\n\n\n\n\n\n\n\n\n\nDen Namen der Funktion geben wir in das Suchfeld ein. Alternativ können wir auch die Funktionen help(funktion) oder ?funktion nutzen. Wenn wir auf  klicken, öffnet sich die Dokumentationsseite in einem neuen Fenster, was die Nutzung wesentlich übersichtlicher gestaltet.\n\n\n\nNeben der Informationen, aus welchem Paket eine Funktion stammt, finden wir hier zumeist folgende Abschnitte:\n\nDescription: Beschreibung, was die Funktion macht\nUsage: Funktionsdefinition (Parameter der Funktion und ggf. Defaults)\nArguments: Beschreibung der Parameter und ihrer möglichen Argumente\nDetails: detaillierte Beschreibung zur Nutzung der Funktion und etwaigen Sonderfällen\nSee Also: verwandte Funktionen (meist aus dem gleichen Paket)\nExamples: Beispiele zur Nutzung der Funktion\n\nIm Folgenden schauen wir uns die R-Dokumentation exemplarisch für die Funktion matrix() an.\nDazu öffnen wir die dazugehörige R-Dokumentations-Seite, indem wir matrix in das Suchfeld eingeben, oder eine der Hilfe-Funktionen, help(matrix) oder ?matrix, ausführen. Folgende Seite sollte sich nun öffnen:\n\n\n\n\n\n\n\n\n\nOben links sehen wir, dass die Funktion aus dem Basispaket base stammt.\n\n4.1.4.1.1 Description\nMan findet hier zu einer Funktion bzw. einem Set an verwandten Funktionen kurze Ausführungen zum Zweck einer Funktion.\nDie Funktion matrix erstellt eine Matrix von einem gegebenem Set an Werten. Es gibt noch zwei verwandte Funktionen - as.matrix und is.matrix - welche ein Objekt in eine Matrix umwandeln bzw. überprüfen, ob das (als Argument) übergebene Objekt eine Matrix ist.\n\n\n4.1.4.1.2 Usage\nHier sehen wir die Funktionsdefinition. Diese zeigt, welche Parameter die Funktion besitzt und ggf. welche dieser Parameter voreingestellte Argumente (Defaults) besitzen.\n\n\n\n\n\n\n\n\n\nPer Default …:\n\n… wird an data nur ein NA (Missing) übergeben\n… gibt es eine Reihe (nrow=1)\n… und eine Spalte (ncol=1)\n… wird die Matrix spaltenweise (d.h. von oben nach unten) mit Werten befüllt (byrow=FALSE)\n… gibt es keine Spalten- und Zeilenbenennung (dimnames=NULL)\n\nUm Defaults besser zu verstehen, führen wir matrix() ohne Spezifikation der Parameter aus und schauen uns den Output an.\n\n\n     [,1]\n[1,]   NA\n\n\nBis auf byrow=FALSE können wir so alle Voreinstellungen im Output nachvollziehen. Dessen Funktionsweise sieht man erst bei mehreren Elementen in einer Matrix.\n\n\n\n4.1.4.1.3 Arguments\nArgumente sind Einstellungen, die wir für eine Funktion festlegen können. In der R-Dokumentation sieht man links den Namen des Arguments (z.B. byrow) und rechts eine Beschreibung dazu (z.B. logical) verbunden mit verschiedenen Einstellungsoptionen (z.B. FALSE - spaltenweise Befüllen der Matrix).\n\n\n4.1.4.1.4 Details\nIm Abschnitt Details stehen weitere Detailinformationen zur Nutzung der Funktion.\n1. Abschnitt von matrix():\nWir erfahren hier, dass wenn nrow (Zeilenanzahl) oder ncol (Spaltenanzahl) nicht festgelegt wird, versucht wird, auf dessen Länge zu schließen. Das schauen wir uns mal an einem Beispiel an.\n\n# Beispiel 1:\nmatrix(data=c(1,1,2,2,3,3,4,4,5,5), nrow=5)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    1    4\n[3,]    2    4\n[4,]    2    5\n[5,]    3    5\n\n\nWir haben einen Vektor mit 10 Elementen reingegeben und nrow=5 festlegt. Daraus kann R schließen, dass die Matrix zwei Spalten (ncol=2) haben muss.\n2. Abschnitt matrix():\nWenn die Anzahl der Elemente, die wir an data übergeben, kleiner ist, als die Anzahl der gewünschten Elemente in Matrix (nrow x ncol), dann wird data recycled. Das bedeutet, dass data nochmal genutzt wird, um die Matrix zu befüllen.\n\n# Beispiel 2:\nmatrix(data=c(1,1,2,2,3), nrow=3, ncol=2)\n\nWarning in matrix(data = c(1, 1, 2, 2, 3), nrow = 3, ncol = 2): data length [5]\nis not a sub-multiple or multiple of the number of rows [3]\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    1\n\n\nIn Beispiel 1 und 2 können wir auch die Auswirkung des Default byrow=FALSE sehen. Bei beiden wurden die Elemente spaltenweise in die Matrix eingetragen, ohne dass wir das so festgelegt haben.\n\nHinweis: Es ist ratsam, vor der Nutzung einer (unbekannten) Funktionen oder bei einer Fehlermeldung zu einer Funktion im Abschnitt Details nachzuschauen. Oftmals findet man hier hilfreiche Erklärungen.\n\n\n\n4.1.4.1.5 See also\nIn diesem Abschnitt bekommt man Funktionen angezeigt, die mit der vorliegenden Funktion in enger Verbindung stehen. Diese sind häufig mit einer kurzen Erklärung versehen.\nTeilweise findet man hier auch Funktionen, die besser für das eigene Vorhaben geeignet sind.\n\n\n4.1.4.1.6 Examples\nHier findet man einige beispielhafte Anwendungen der Funktion. Anhand der Beispiele bekommt man ein besseres Verständnis von der Syntax und Funktionsweise der Funktion. Was genau gemacht wird ist oftmals in kurzen Kommentaren (# ...) erklärt.\n\n\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] kableExtra_1.3.4 dplyr_1.1.2     \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       svglite_2.1.1     httr_1.4.5        cli_3.6.1        \n [5] knitr_1.42        rlang_1.1.0       xfun_0.39         stringi_1.7.12   \n [9] generics_0.1.3    jsonlite_1.8.4    glue_1.6.2        colorspace_2.1-0 \n[13] htmltools_0.5.5   scales_1.2.1      fansi_1.0.4       rmarkdown_2.21   \n[17] munsell_0.5.0     evaluate_0.20     tibble_3.2.1      fastmap_1.1.1    \n[21] yaml_2.3.7        lifecycle_1.0.3   stringr_1.5.0     compiler_4.3.0   \n[25] rvest_1.0.3       htmlwidgets_1.6.2 pkgconfig_2.0.3   rstudioapi_0.14  \n[29] systemfonts_1.0.4 digest_0.6.31     viridisLite_0.4.1 R6_2.5.1         \n[33] tidyselect_1.2.0  utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3   \n[37] webshot_0.5.4     tools_4.3.0       xml2_1.3.3       \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Pakete.html#pakete-installieren",
    "href": "Pakete.html#pakete-installieren",
    "title": "5  Pakete",
    "section": "5.1 Pakete Installieren",
    "text": "5.1 Pakete Installieren\nZuerst einmal schauen wir uns an, wie wir diese zusätzlichen Pakete in R installieren können. Die meisten bekannten Pakete werden über CRAN zur Verfügung gestellt und können von dort heruntergeladen werden.\nZur Installation von Paketen schauen wir uns beide Herangehensweisen, das Nutzen von Funktionen sowie der grafischen Benutzeroberfläche, an.\nVorweg: Bei beiden Wegen wird uns nach erfolgreicher Installation folgender Output in der Konsole angezeigt:\n\n\n\n\n\n\n5.1.1 Über die Funktion install.packages()\nWir können psych installieren, indem wir install.packages(\"psych\", dependencies = TRUE) ausführen. Hierbei ist darauf zu achten, dass wir den Namen des Pakets in Anführungszeichen setzen müssen. Mit dependencies=TRUE geben wir an, dass noch nicht installierte Pakete, welche von dem Zielpaket benötigt oder empfohlen werden, ebenfalls installiert werden sollen.\n\nHinweis: Die Funktion install.packages() ist Bestandteil des Standardpakets utils.\n\n\n\n5.1.2 Über das Icon Install oder den Menüpunkt Install Packages…\nWir können das Paket auch installieren, indem wir im Packages-Tab auf das Icon Install …\n\n\n\n\n\n\n\n\n\n… oder in der Menüleiste am oberen Bildschirmrand auf Tools –> Install Packages… klicken.\n\n\n\n\n\n\n\n\n\nNun öffnet sich ein neues Fenster, in welchem wir unter dem Reiter Packages (…) den Namen des Pakets eingeben können. Anschließen müssen wir noch auf das Icon Install klicken.\n\n\n\n\n\n\n\n\n\nMit dem Häkchen in dem Kästchen bei Install Dependencies werden von dem Zielpaket benötigte oder empfohlene bisher nicht installierte Pakete auch heruntergeladen."
  },
  {
    "objectID": "Pakete.html#pakete-laden",
    "href": "Pakete.html#pakete-laden",
    "title": "5  Pakete",
    "section": "5.2 Pakete Laden",
    "text": "5.2 Pakete Laden\nNachdem wir das benötigte Paket installiert haben, müssen wir es noch laden. Nur dann können wir die im Paket enthaltenen Funktionen nutzen. Hierzu schauen wir uns wieder die beiden Herangehensweisen, über eine Funktion oder die grafische Benutzeroberfläche, an.\nVorweg: Bei beiden Möglichkeiten erscheint nach erfolgreichem Laden des Pakets folgender Output in der Konsole:\n\n\n\n\n\n\n5.2.1 Über die Funktion library()\nWir können das Paket psych laden, indem wir library(psych) ausführen. Hierbei ist es nicht wichtig, den Namen des Pakets in Anführungszeichen zu setzen. Wir können diese auch weglassen.\n\nHinweis: Die Funktion library() ist Bestandteil des Standardpakets base.\n\nAlternativ kann man auch die Syntax paket::funktion(), z.B. psych::alpha(), nutzen. Was dieser Weg für Vorteile hat, erfahren wir im Unterabschnitt Maskierung.\n\n\n5.2.2 Über das Häkchen-Setzen in der System Library\nAlternativ können wir im Packages-Tab ein Häkchen bei dem Paket setzen, welches wir laden wollen. Um das gewünschte Paket schneller zu finden, können wir das Suchfeld nutzen.\n\n\n\n\n\n\n\n\n\n\n\n5.2.3 Maskierung: Wenn verschiedene Pakete gleich benannte Funktionen enthalten\nEs kann vorkommen, dass Funktionen aus verschiedenen Paketen die gleiche Bezeichnung haben. Beispielsweise gibt es in psych und in ggplot2 eine Funktion mit dem Namen alpha(). Wenn wir ein Paket laden, und vorher ein anderes Paket geladen wurde, in dem eine gleichnamige Funktion vorkommt, bekommen wir folgende Meldung in der Konsole ausgegeben:\n\n\n\n\n\nDie Funktion des zuletzt eingeladenen Pakets wird mit dem gemeinsamen Funktionsnamen (hier: alpha() aus dem Paket psych) aufgerufen. Die Funktion aus dem anderen Paket wird maskiert, d.h. wir können sie jetzt erstmal nicht mehr nutzen.\nNachfolgend schauen wir uns drei Möglichkeiten an, Probleme mit dem Maskieren von Funktionen handzuhaben: 1) deaktivieren und neu laden von Paketen, 2) eindeutige Referenzierung von Funktionen und 3) das Paket conflicted nutzen.\nWenn wir die Funktion aus dem anderen Paket (ggplot2) nutzen wollen, können wir das Paket erst deaktivieren und dann neu laden. Wir können Pakete deaktivieren, indem wir detach(\"package:ggplot2\", unload = TRUE) nutzen oder indem wir das Häkchen neben dem Paket im Packages-Tab entfernen.\nWenn wir das Paket mit der gewünschten Funktion laden, erhalten wir folgende Meldung.\n\n\n\n\n\nEs handelt sich um die gleiche Meldung wie oben, nur das nun das Paket ggplot2 als zweites Paket eingelesen wurde und entsprechend die Funktionen im Paket psych maskiert wurden.\nEine Alternative zu dem Laden und Deaktivieren von Paketen ist die exakte Referenzierung der Funktion auf das Paket mittels ::. Das schreiben wir zwischen das Paket und die Funktion z.B. psych::alpha() oder ggplot2::alpha(). So weiß R eindeutig, welche Funktion wir nutzen wollen.\n\nHinweis: Es kommt häufiger zu Problemen bei der Ausführung der Funktionen filter(), select() und summarise() aus dem Paket dplyr, wenn die Pakete stats (Basispaket; filter()), MASS (select()) oder plyr (summarise()) ebenfalls geladen sind. Die eindeutige Auswahl von Funktionen mittels :: kann bestehende Probleme lösen. Mehr Informationen zur Problematik finden wir in diesem Forumseintrag.\n\nAußerdem können wir das Paket conflicted nutzen. Wenn wir dieses zu Beginn laden, wird uns jedes Mal, wenn wir eine Funktion nutzen wollen, die nicht eindeutig einem geladenen Paket zugeordnet werden kann, eine detaillierte Fehlermeldung ausgegeben. So können wir Probleme durch die Nutzung falscher Funktionen und/oder Ratlosigkeit bzgl. missverständlicher Fehlermeldungen vermeiden. Wir haben außerdem die Möglichkeit, einmalig bzw. für das gesamte Skript eine von mehreren gleichnamigen Funktionen festzulegen. Mehr Informationen zum Paket und dessen Anwendung finden wir hier."
  },
  {
    "objectID": "Pakete.html#pakete-aktualisieren",
    "href": "Pakete.html#pakete-aktualisieren",
    "title": "5  Pakete",
    "section": "5.3 Pakete Aktualisieren",
    "text": "5.3 Pakete Aktualisieren\nPakete werden von Zeit zu Zeit aktualisiert. Wir sollten hin und wieder überprüfen, ob es Updates für unsere installierten Pakete gibt. Das können wir wieder wahlweise mit Funktionen oder der grafischen Benutzeroberfläche machen.\n\n5.3.1 Über die Funktion update.packages()\nWir können update.packages() ausführen, und bekommen so für jedes Paket, für das eine aktuellere Version vorliegt, in der Konsole die Frage gestellt, ob wir dieses aktualisieren wollen. Mit Yes oder No bzw. deren Anfangsbuchstaben können wir antworten.\n\nHinweis: Die Funktion update.packages() ist Bestandteil des Standardpakets utils.\n\n\n\n\n\n\nEs kann vorkommen, dass für einige der Pakete, die wir aktualisieren wollen, noch nicht die Binärcodes der aktuellsten Versionen der Pakete für unser Betriebssystem auf CRAN bereit gestellt wurde.\n\n\n\n\n\n\n\nWas sind Binär- und Quellcode?\n\nBinärcode ist eine Sprache, die zur Verarbeitung digitaler Informationen, d.h. von Rechnern, genutzt wird (Synonym: Maschinencode). Es heißt binär weil es zwei mögliche Zeichen gibt: 0 und 1. Quellcode bezeichnet für Menschen lesbare Programmiersprachen. Quellcode wird in Binärcode übersetzt und kann dann von Rechnern ausgeführt werden.\n\nWir können die jüngsten Binärcodes für die Pakete herunterladen oder die Quellcodes der aktuellsten Versionen kompilieren. Allerdings müssen wir dafür spezielle Tools in R installiert haben, welche man unter Mac OS und einer älteren als R-Version 4.0.0 zusätzlich herunterladen muss (z.B. hier). Wir können hier auch pauschal nein antworten. Es ist ausreichend dafür ein n in die Konsole zu tippen. Nach einiger Zeit sollten wir erneut versuchen, die aktuellste Version herunterzuladen, da dann häufig auch die Binärcodes zur Verfügung stehen.\n\n\n5.3.2 Über das Icon Update oder den Menüpunkt Check for Package Updates…\nDazu können wir im Packages-Tab auf das Icon Update …\n\n\n\n\n\n\n\n\n\n… oder in der Menüleiste am oberen Bildschirmrand auf Tools –> Check for Package Updates… klicken.\n\n\n\n\n\n\n\n\n\nDamit öffnet sich ein neues Fenster, in dem all unsere Pakete angezeigt werden, für die es Aktualisierungen gibt.\n\n\n\n\n\n\n\n\n\nWir sehen hier welche Version eines Pakets wir haben (Installed), welche neuere Version verfügbar ist (Available) und welche Änderungen in der neueren Version vorgenommen wurden (NEWS). Bei letzterem müssen wir das jeweilige Seiten-Symbol klicken und werden auf eine Website verwiesen, in der eine Übersicht der Änderungen zu finden ist.\nWenn wir alle Pakete aktualisieren wollen, wählen wir Select All. Dann müssen wir nur noch auf Install Updates klicken.\nWenn wir Pakete, die wir installieren wollen, derzeit geladen haben, öffnet sich ein Fenster, in dem werden wir gefragt, ob wir R neustarten möchten bevor die gewählten Pakete installiert werden sollen. Wir sollten hier Ja anklicken. Manchmal kann es vorkommen, dass uns dieses Fenster immer wieder angezeigt wird. Wenn das der Fall ist, sollten wir auf Nein klicken.\n\n\n\n\n\n\n\n\n\n\n\n\nManchmal werden die Updates nicht einfach installiert, sondern der Vorgang wird unterbrochen und wir bekommen eine Meldung in der Konsole angezeigt.\n\n\n\n\n\nWir werden auch hier darauf hingewiesen, dass für einige der Pakete, die wir aktualisieren wollen, noch nicht die Binärcodes der aktuellsten Versionen der Pakete für unser Betriebssystem auf CRAN bereit gestellt wurden.\nWir sollen uns nun entscheiden, ob wir die betroffenen Pakete mit den Quellcodes der aktuellsten Versionen kompilieren wollen. Dafür müssen wir aber gewisse Tools in R implementiert haben, welche man mit Mac OS zusätzlich herunterladen muss (z.B. hier) Besser ist es, hier pauschal nein anzuwählen. Dafür reicht es auch, ein n in die Konsole zu tippen. Wir haben so aber nicht die aktuellste Version der Pakete und sollten in nächster Zeit erneut versuchen, diese zu aktualisieren.\n\n\n5.3.3 Entwicklerpakete runterladen\n\n\n\nDas Paket devtools enthält verschiedene Funktion zum Installieren von Paketen von verschiedenen Quellen (engl.: repositories, z.B. CRAN).\nMit diesem können wir u.a. sogenannte Entwicklerpakete herunterladen. Das sind Pakete, die man nicht direkt von CRAN runterladen kann (z.B. weil die Dokumentation des Pakets und seiner Funktionen nicht den CRAN-Standards entspricht).\nSolche Entwicklerpakete können häufig von GitHub runtergeladen werden. Zuerst müssen wir dafür das Paket devtools installieren. Dann müssen wir dieses laden und anschließend können wir das gewünschte Zielpaket herunterladen.\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\n\n# Beispiel: Paket horst\ninstall_github(\"kthorstmann/horst\") # EntwicklerIn / Paket\n\nMit devtools kann man auch ältere Versionen von Paketen runterladen. Für mehr Informationen dazu siehe Ältere Paket-Versionen installieren."
  },
  {
    "objectID": "Pakete.html#wichtige-hinweise-zur-replizierbarkeit",
    "href": "Pakete.html#wichtige-hinweise-zur-replizierbarkeit",
    "title": "5  Pakete",
    "section": "5.4 Wichtige Hinweise zur Replizierbarkeit",
    "text": "5.4 Wichtige Hinweise zur Replizierbarkeit\n\n5.4.1 Replizierbarkeit von R-Skripten\nWir haben beide Möglichkeiten, die Nutzung von Funktionen und die der grafischen Benutzeroberfläche vorgestellt, aber die Verwendung von Funktionen ist mit Hinblick auf die Replizierbarkeit von Skripten zu bevorzugen. Wenn man die grafische Benutzeroberfläche nutzt, findet man im Skript keine Hinweise darüber, welche Pakete genutzt wurden, d.h. library(paket) erscheint nur in der Konsole und eben nicht im R-Skript.\n\n\n5.4.2 Replizierbarkeit von Analysen\nEs ist sinnvoll, bei Analysen zu vermerken, mit welcher Version eines Pakets wir diese durchgeführt haben. Wenn sich die Funktionen eines Pakets durch Updates ändern, kann das auch die Ergebnisse unserer Analysen beeinflussen. Gleiches gilt auch für zu der Zeit genutzte Versionen von unserem Betriebssystem und von R.\nMit der Funktion sessionInfo() aus dem Standardpaket utils bekommen wir viele wichtige Informationen auf einen Schlag:\n\nR-Version\nBetriebssystem-Version\ngeladene Pakete mit Angaben zur Version\n\n\n\n\n\n\n\nHinweis: Hiermit werden nur Pakete gelistet, die mit library() oder mit Hilfe des GUIs geladen wurden. Wenn wir die eindeutige Zuweisung von Funktionen zu Paketen aus dem Abschnitt Maskierung nutzen z.B. psych::alpha(), werden unsere genutzten Pakete nicht gelistet. In diesem Fall müssten wir die Informationen zu den Versionen der Pakete selbst in Erfahrung bringen.\n\nFür noch detailliertere Informationen zu unseren geladenen Paketen können wir session_info() aus dem Paket devtools nutzen.\nHier sehen wir z.B. bei source wo wir die Paket heruntergeladen haben (z.B. CRAN) und bei version mit welcher R-Version wir sie erstellt haben (z.B. R 3.6.0)."
  },
  {
    "objectID": "Pakete.html#weitere-hilfen",
    "href": "Pakete.html#weitere-hilfen",
    "title": "5  Pakete",
    "section": "5.5 Weitere Hilfen",
    "text": "5.5 Weitere Hilfen\n\n5.5.1 Probleme mit Paketen und Funktionen\nBei Problemen mit dem Installieren bzw. Updaten oder Laden von Paketen lohnt es sich, in unserem FAQ-Eintrag nachzuschauen.\nWenn wir bestimmte Funktionen nicht ausführen können, obwohl wir das notwendige Paket geladen haben, kann das auf Maskierung von gleichnamigen Funktionen aus verschiedenen geladenen Paketen zurückzuführen sein.\n\n\n5.5.2 Ältere Paket-Versionen installieren\nManchmal möchten wir ältere Versionen von Paketen nutzen. Das kann z.B. der Fall sein, wenn wir eine Analyse, die mit einer älteren Version eines Pakets durchgeführt wurde, replizieren möchten. Hier finden wir eine Anleitung, wie wir dafür devtools() oder URLs nutzen können."
  },
  {
    "objectID": "Pakete.html#faq",
    "href": "Pakete.html#faq",
    "title": "5  Pakete",
    "section": "5.6 FAQ",
    "text": "5.6 FAQ\nManchmal kann es vorkommen, dass wir bestimmte Pakete nicht laden oder gar nicht erst installieren können. Dafür kann es vielfältige Ursachen geben. Im Folgenden schauen wir uns an, wie man das Problem (mit großer Wahrscheinlichkeit) lösen kann.\nWir führen die folgenden drei Möglichkeiten der Reihe nach durch und überprüfen nach jeder Möglichkeit, ob wir das Paket schon nutzen können.\n\nWir schauen uns das exemplarisch für das Paket car an. Du änderst nur noch den Namen des Pakets bzw. der Pakete. Manchmal werden noch weitere Pakete - sog. dependencies - geladen. Diese solltest du in die folgenden Lösungswege auch mit einbeziehen.\n\n\n5.6.1 Die Pakete via Befehl deinstallieren und neu installieren\nremove.packages('car') bzw. remove.packages(c('car', 'survey'))\ninstall.packages('car', dependencies=TRUE)\n\n\nHilfe bei der Installation von Paketen finden wir hier.\n\n\n\n5.6.2 Die neuste R-Version von R und RStudio auf dem Computer installieren und dann versuchen, das Paket neu zu installieren bzw. Schritt 1 durchzuführen.\nAchte dabei auf die Kompatibilität mit deiner Hardware und Software. Unter Umständen kannst du vielleicht nicht die neueste Version installieren, aber eine neuere als deine aktuelle Version. Ältere Versionen von R finden wir auch unter obigem Link. Ältere Versionen von RStudio finden wir hier.\n\nHilfe bei der Installation von R und RStudio finden wir unserem Kapitel.\n\n\n\n5.6.3 Die Pakete deinstallieren und manuell neu installieren\nremove.packages('car') bzw. remove.packages(c('car', 'survey'))\n\nDas Paket bzw. die Pakete von CRAN als zip-Datei/en runterladen, dann öffnen (entpacken), den R Library Ordner lokalisieren (in dem werden alle R-internen Dateien gespeichert) und den/die Paket-Ordner dorthin verschieben.\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] devtools_2.4.5 usethis_2.1.6 \n\nloaded via a namespace (and not attached):\n [1] miniUI_0.1.1.1    jsonlite_1.8.4    compiler_4.3.0    crayon_1.5.2     \n [5] promises_1.2.0.1  Rcpp_1.0.10       stringr_1.5.0     callr_3.7.3      \n [9] later_1.3.0       yaml_2.3.7        fastmap_1.1.1     mime_0.12        \n[13] R6_2.5.1          knitr_1.42        htmlwidgets_1.6.2 profvis_0.3.7    \n[17] shiny_1.7.4       rlang_1.1.0       stringi_1.7.12    cachem_1.0.7     \n[21] httpuv_1.6.9      xfun_0.39         fs_1.6.1          pkgload_1.3.2    \n[25] memoise_2.0.1     cli_3.6.1         magrittr_2.0.3    ps_1.7.5         \n[29] digest_0.6.31     processx_3.8.1    xtable_1.8-4      remotes_2.4.2    \n[33] lifecycle_1.0.3   prettyunits_1.1.1 vctrs_0.6.2       evaluate_0.20    \n[37] glue_1.6.2        urlchecker_1.0.1  sessioninfo_1.2.2 pkgbuild_1.4.0   \n[41] rmarkdown_2.21    purrr_1.0.1       tools_4.3.0       ellipsis_0.3.2   \n[45] htmltools_0.5.5  \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Daten-importieren.html#vor-dem-einlesen-in-r",
    "href": "Daten-importieren.html#vor-dem-einlesen-in-r",
    "title": "6  Daten importieren",
    "section": "6.1 Vor dem Einlesen in R",
    "text": "6.1 Vor dem Einlesen in R\nIn diesem Abschnitt schauen wir uns an, wie wir Dateien aus dem Internet herunterladen (z.B. aus moodle) und in unseren Arbeitsordner verschieben können. Wir sollten der Übersichtlichkeit halber für jedes neue Projekt einen neuen Ordner anlegen.\nDie beiden Schritte schauen wir uns jeweils für das Windows- und Mac-Betriebssystem bzw. die Browser Google Chrome und Safari an.\nDieser Abschnitt ist optional. Wenn du bereits weißt, wie du Daten herunterlädst und verschiebst, kannst du diesen Abschnitt überspringen."
  },
  {
    "objectID": "Daten-importieren.html#windows",
    "href": "Daten-importieren.html#windows",
    "title": "6  Daten importieren",
    "section": "6.2 Windows",
    "text": "6.2 Windows\nIm nachfolgenden Beispiel wird gezeigt, wie wir unter Benutzung des Browsers Google Chrome eine Datei aus einem Moodle-Kurs herunterladen und in unseren Arbeitsordner verschieben.\n\n6.2.1 Datei herunterladen\nWir begeben uns in den entsprechenden Moodle-Kurs und wählen die Datei mit einem Rechtsklick an, wählen die Option Link speichern unter… sowie den gewünschten Zielordner zur Ablage aus.\n\n\n\n\n\n\n\n\n\nSehr wichtig ist es, sich immer zu merken, in welchem Ordner die heruntergeladene Datei gespeichert wird. Es ist sinnvoll, die Datei bereits jetzt im Arbeitsordner zu speichern (in dem wir unser R-Skript später speichern möchten).\nIn Google Chrome können wir den Zielordner herausfinden, indem wir rechts oben auf die drei Punkte klickt und den Menüpunkt Downloads anwählen. Es öffnet sich ein neuer Tab im Browser, in dem wir die Option In Ordner anzeigen auswählen können.\n\n\n6.2.2 In Arbeitsordner verschieben\nWir wählen die Datei mit einem Rechtsklick im Ordner aus und klicken dann auf die Option Ausschneiden. Im Gegensatz zu Kopieren entfernt das Ausschneiden die Datei auch aus dem ursprünglichen Ordner.\n\n\n\n\n\n\n\n\n\nAls nächstes begeben wir uns in unseren Arbeitsordner (ggf. müssen wir diesen vorher noch erstellen). Wir machen einen Rechtsklick und wählen die Option Einfügen aus.\n\n\n\n\n\n\n\n\n\nJetzt befindet sich die Datei in unserem Arbeitsordner und wir können nun RStudio öffnen, um die Datei einzulesen."
  },
  {
    "objectID": "Daten-importieren.html#mac",
    "href": "Daten-importieren.html#mac",
    "title": "6  Daten importieren",
    "section": "6.3 Mac",
    "text": "6.3 Mac\nIm folgenden Beispiel wird gezeigt, wie wir unter Benutzung des Browsers Safari eine Datei aus einem Moodle-Kurs herunterladen und in unseren Arbeitsordner verschieben.\n\n6.3.1 Datei herunterladen\nWir begeben uns in den entsprechenden Moodle-Kurs und öffnen die Datei im Browser. Dann machen wir einen Rechtsklick (dabei darf nichts markiert sein) und klicken auf Seite sichern unter….\n\n\n\n\n\n\n\n\n\nIn dem Fenster, welches sich dann öffnet, müssen wir bei Format noch festlegen, dass wir den Quelltext der Seite herunterladen wollen. Wir könnten auch schon unseren Arbeitsordner als Zielordner festlegen.\n\n\n\n\n\n\n\n\n\n\nAchtung: Manchmal werden wir beim Speichern gefragt, ob die Endung .txt angehängt werden soll (d.h. ob die Datei als Textformat gespeichert werden soll). Das sollten wir verneinen, da ansonsten unser (.csv-)Dateiformat geändert wird.\n\nOben rechts im Browser sehen wir einen nach unten zeigenden Pfeil . Wenn wir auf diesen klicken, können wir uns die heruntergeladene Datei im Finder anzeigen lassen. Standardmäßig werden heruntergeladene Dateien im Ordner Downloads gespeichert.\n\n\n6.3.2 In Arbeitsordner verschieben\nWir machen einen Rechtsklick auf die Datei. Nun öffnet sich ein Menü, in welchem wir Kopieren auswählen.\n\n\n\n\n\n\n\n\n\nAls nächstes begeben wir uns in unseren Arbeitsordner (ggf. müssen wir diesen vorher noch erstellen). Wir machen einen Rechtsklick und wählen die Option Objekt einsetzen aus.\n\n\n\n\n\n\n\n\n\nDie Datei ist nun im Arbeitsordner gespeichert; wir können sie nun auch aus dem Download-Ordner löschen. Jetzt öffnen wir RStudio, um die Datei einzulesen."
  },
  {
    "objectID": "Daten-importieren.html#weg-1-environment-import-dataset",
    "href": "Daten-importieren.html#weg-1-environment-import-dataset",
    "title": "6  Daten importieren",
    "section": "6.4 Weg 1: Environment > Import Dataset",
    "text": "6.4 Weg 1: Environment > Import Dataset\nEine Variante, Daten in R ganz ohne Code zu importieren, ist es, das Icon Import Dataset zu nutzen. Dieses finden wir im rechten oberen Panel bei Environment.\n\n\n\n\n\n\n\n\n\nNun klicken wir auf From CSV. Daraufhin öffnet sich ein Fenster, in dem wir verschiedene Optionen zum Einlesen haben.\n\nAchtung: In neueren RStudio-Versionen gibt es die Optionen From Text (base) und From Text (readr) (anstatt zusammengefasst From CSV). Beides kann genutzt werden, um .csv-Dateien einzulesen. base ist ein Standardpaket, welches in R von Beginn an vorinstalliert ist. Um readr nutzen zu können, müssen wir erst das gleichnamige Paket herunterladen. Die nachfolgend genannten Schritte beziehen sich auf die Benutzung von From Text (readr); das Fenster bei From Text (base) sieht auch anders aus.\n\nNachdem wir eine Option ausgewählt haben, öffnet sich ein Fenster, in welchem wir die gewünschte Datei in unserem Arbeitsordner auswählen können.\nDann öffnet sich ein neues Fenster, welches eine Vorschau beinhaltet, die uns zeigt, wie die Datei mit den derzeitig festgelegten Optionen in R aussehen würde. Wenn es Probleme gibt (z.B. mit der Interpretation der Trennungszeichen), sehen wir das sofort an der Darstellung der Daten.\n\n\n\n\n\n\n\n\n\nZum Einlesen sind folgende Schritte nötig:\n\nAnklicken des Browse-Buttons. Daraufhin öffnet sich ein Fenster, in welchem wir die Datei auswählen können.\nÜberprüfen der Vorschau, ob die Daten korrekt dargestellt werden. Die häufigsten Probleme bei der Repräsentation der Daten kommen durch die Trennungzeichen (zwischen den einzelnen Datenelementen) zustande. Diese können wir bei Delimiter ändern.\nAnschließend drücken des Import-Buttons.\n\nWenn die Datei neuro.csv erfolgreich eingelesen wurde, erscheint das neu erstellte Objekt neuro (oder welchen anderen Namen wir dem Objekt gegeben haben) im rechten oberen Panel bei Environment."
  },
  {
    "objectID": "Daten-importieren.html#weg-2-files-import-dataset",
    "href": "Daten-importieren.html#weg-2-files-import-dataset",
    "title": "6  Daten importieren",
    "section": "6.5 Weg 2: Files > Import Dataset",
    "text": "6.5 Weg 2: Files > Import Dataset\nSchauen wir uns einen weiteren Weg an, mit der Benutzeroberfläche der Entwicklungsumgebung RStudio Dateien einzulesen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDas Vorgehen hier ist weitestgehend analog zu Weg 1.\nWir klicken auf die Datei neuro.csv in unserem Arbeitsordner und dann auf die Option Import Dataset.\n\n\n\n\n\n\n\n\n\nEs öffnet sich (weitestgehend) das gleiche Fenster wie in Weg 1.\n\n\n\n\n\n\n\n\n\nWir haben hier mit Hilfe der Vorschau wieder die Möglichkeit vor dem Einlesen zu Überprüfen, ob die Datei von R richtig repräsentiert wird. Die meisten Probleme hängen mit den Trennungszeichen zwischen den einzelnen Datenelementen zusammen. Diese Option können wir unter Delimiter anpassen. Stimmt die Vorschau mit unseren Erwartungen überein, können wir rechts unten auf Importieren klicken.\nIm Workspace sollten wir nun den eben eingelesenen Dataframe neuro finden."
  },
  {
    "objectID": "Daten-importieren.html#weg-3-manuell-importieren-mit-funktionen",
    "href": "Daten-importieren.html#weg-3-manuell-importieren-mit-funktionen",
    "title": "6  Daten importieren",
    "section": "6.6 Weg 3: Manuell Importieren mit Funktionen",
    "text": "6.6 Weg 3: Manuell Importieren mit Funktionen\nAuch wenn Möglichkeiten existieren, Dateien mithilfe der Benutzeroberfläche von RStudio einzulesen, ist es ratsam, auch einmal selbst Funktionen zu nutzen. Für die meisten Arbeiten in R nutzen wir nämlich Funktionen.\nWelche Funktion hierfür angebracht ist, hängt von der Struktur der Datei ab. Nachfolgend schauen wir uns an, welche Funktionen wir für .csv, .txt und .dat nutzen können.\n\n\n# nutzbare Funktionen zum Einlesen von .csv, .txt. und .dat\ndaten <- read.table(\"Dateipfad/neuro.csv\")\ndaten <- read.delim(\"Dateipfad/neuro.csv\")\ndaten <- read.csv(\"Dateipfad/neuro.csv\")\n\n\n\nEinen Dateipfad kopieren\n\n\nWindows\nUnter Windows können wir auf  shift  drücken und dann einen Rechtsklick auf die Datei machen. Nun öffnet sich ein Menü, in welchem wir Als Dateipfad kopieren auswählen. Wichtig dabei ist, dass wir noch alle \\ (backslashes) aus dem kopierten Pfad in / (forwardslashes) ändern müssen.\nMac\nWir klicken einmal auf die Datei (sodass sie markiert ist; dann ist sie blau hinterlegt) und führen dann den Kurzbefehl  alt  +  cmd  +  C  aus.\n\n\n\n\n\n\n\n\n\n\nDiese drei Funktionen sind sehr ähnlich aufgebaut. Sind haben aber teilweise unterschiedliche Voreinstellungen (sog. “Defaults”). Zum Beispiel nimmt read.csv() an, dass einzelne Datenelemente mit Kommata (Default: sep=\",\") getrennt werden. Dafür werden bei read.table() standarmäßig Spaltennamen nicht eingelesen (Default: header=FALSE).\n\nAchtung: Alle Funktionsdefinitionen (mit Defaults) finden wir in der R-Dokumentation, die wir im unteren rechten Panel bei Help finden. Alternativ können wir sie auch mit der Hilfefunktion ?, z.B. ?read.table, öffnen.\n\nIn Abhängigkeit der Speicherung der Dateien müssen wir manchmal den Parametern der Funktionen andere Argumente übergeben. Die zwei wichtigsten Parameter sind header und sep.\n\nheader\n\nob Spaltennamen übernommen werden sollen\nTRUE oder FALSE möglich\nWenn es Spaltennamen gibt, aber header = FALSE festgelegt ist, stehen diese in der ersten Zeile und die Spalten werden alternativ mit V1, V2, V3, … benannt.\n\nsep\n\nwie (angenommen wird, dass) einzelne Datenelemente getrennt sind\nu.a. Komma (,), Semikolon (,) und Freizeichen () möglich\nDass wir hier etwas ändern müssen erkennen wir daran, dass nicht die gesamte Anzahl an Spalten im R-Objekt vorhanden sind. Wenn wir uns das Objekt anschauen, dann sehen wir, mit welchem Zeichen die Elemente getrennt sind.\n\n\n\nAchtung: Es kann dabei sein, dass unterschiedliche Personen zum korrekten Einlesen derselben Datei andere Argumenten nutzen. Das kann auf unterschiedliche Betriebssysteme oder Programme zum Öffnen der Dateien zurückzuführen sein.\n\nWenn wir den Dataframe eingelesen haben, erscheint er im Environment.\n\n\n\n\n\n\n\n\n\n\n\nProbleme? Nutze Trial-and-Error!\n\nUm in Erfahrung zu bringen, welche Argumente wir nutzen müssen, um die Daten korrekt einzulesen, können wir einen Trial-and-Error Ansatz verwenden:\n\nWir lesen die Datei erstmal ohne Spezifikation von Argumenten ein z.B. mit read.table(\"Dateipfad\").\nDann schauen wir uns die Datei in R ein und beurteilen, ob diese korrekt angezeigt wird. Schauen wir uns dazu beispielhaft einmal folgende .csv-Datei aus dem lavaan - Paket an:\n\n\n\n\n\n\n\n\n\nV1\n\n\n\n\ny1,y2,y3,y4,y5,y6,y7,y8,x1,x2,x3\n\n\n2.5,0,3.333333,0,1.25,0,3.72636,3.333333,4.442651,3.637586,2.557615\n\n\n1.25,0,3.333333,0,6.25,1.1,6.666666,0.736999,5.384495,5.062595,3.568079\n\n\n7.5,8.8,9.999998,9.199991,8.75,8.094061,9.999998,8.211809,5.961005,6.25575,5.224433\n\n\n8.9,8.8,9.999998,9.199991,8.907948,8.127979,9.999998,4.615086,6.285998,7.567863,6.267495\n\n\n10,3.333333,9.999998,6.666666,7.5,3.333333,9.999998,6.666666,5.863631,6.818924,4.573679\n\n\n\n\n\n\nDie Datei wird scheinbar nicht korrekt angezeigt. Dabei fallen zwei Sachen ins Auge:\n\nEs gibt keine Spaltennamen bzw. stehen diese in der ersten Zeile.\n\nDaher müssen wird das Argument header=TRUE nutzen, damit die Spaltennamen als solche übernommen werden.\n\nEs existiert nur ein Spalte.\n\nDie Daten aus verschiedenen Spalten werden alle in einer Zeile dargestellt. Wenn man sich das genauer anschaut sieht man, dass die einzelnen Daten jeweils mit einem Komma voneinander getrennt sind. Folglich müssen wird das Argument sep=\",\" benutzen, damit die Spalten korrekt getrennt werden.\n\nJetzt lesen wir die Datei nochmal mit diesen Argumenten ein:\nread.table(\"Dateipfad\", header=TRUE, sep=\",\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\ny7\ny8\nx1\nx2\nx3\n\n\n\n\n2.50\n0.000000\n3.333333\n0.000000\n1.250000\n0.000000\n3.726360\n3.333333\n4.442651\n3.637586\n2.557615\n\n\n1.25\n0.000000\n3.333333\n0.000000\n6.250000\n1.100000\n6.666666\n0.736999\n5.384495\n5.062595\n3.568079\n\n\n7.50\n8.800000\n9.999998\n9.199991\n8.750000\n8.094061\n9.999998\n8.211809\n5.961005\n6.255750\n5.224433\n\n\n8.90\n8.800000\n9.999998\n9.199991\n8.907948\n8.127979\n9.999998\n4.615086\n6.285998\n7.567863\n6.267495\n\n\n10.00\n3.333333\n9.999998\n6.666666\n7.500000\n3.333333\n9.999998\n6.666666\n5.863631\n6.818924\n4.573679\n\n\n7.50\n3.333333\n6.666666\n6.666666\n6.250000\n1.100000\n6.666666\n0.368500\n5.533389\n5.135798\n3.892270\n\n\n\n\n\nNun wird die Datei korrekt dargestellt."
  },
  {
    "objectID": "Daten-importieren.html#faq",
    "href": "Daten-importieren.html#faq",
    "title": "6  Daten importieren",
    "section": "6.7 FAQ",
    "text": "6.7 FAQ\nUm Daten in R aufbereiten und bearbeiten zu können, muss man diese erst einmal einlesen können. Manchmal gestaltet sich das leider schwieriger als erwartet.\nIm Folgenden wollen wir uns einige Methoden zum Einlesen gängiger Dateiformate in R anschauen. Das Dateiformat einer Datei erkennt man an seiner Endung.\n\n\nKurzbefehle zum Kopieren des Dateipfads\n\n\nWindows\nUnter Windows können wir auf  shift  drücken und dann einen Rechtsklick auf die Datei machen. Nun öffnet sich ein Menü, in welchem wir Als Dateipfad kopieren auswählen. Wichtig dabei ist, dass wir noch alle \\ (backslashes) aus dem kopierten Pfad in / (forwardslashes) ändern müssen.\nMac\nWir klicken einmal auf die Datei (sodass sie markiert ist; dann ist sie blau hinterlegt) und führen dann den Kurzbefehl  alt  +  cmd  +  C  aus.\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.1 .csv, .txt und .dat\nDiese Dateiformate sind die am weitesten verbreiteten Tabellendateien. Alle drei können u.a. mit den folgenden Funktionen eingelesen werden:\n\nread.table(\"Dateipfad\")\nread.delim(\"Dateipfad\")\nread.csv(\"Dateipfad\")\n\n\n\n6.7.2 .xls und .xlsx\nDiese Endungen gehören zum Programm Excel. Man muss externe Pakete installieren, um Excel-Dateien in R einlesen zu können.\nUm .xlsx-Dateien einzulesen, nutzen wir das Paket openxlsx.\n\nlibrary(openxlsx)\ndaten <- read.xlsx(\"Dateipfad\")\n\nAlternativ können wir das Paket readxl nutzen. Dieses ermöglicht uns sogar .xlsx- und .xls-Dateien einzulesen.\n\nlibrary(readxl)\ndaten <- read_xlsx(\"Dateipfad\")\n\n\n\n6.7.3 .sav\nDie Endung .sav wird z.B. für SPSS-Dateien genutzt wird. Hierfür muss man wieder ein zusätzliches Paket (z.B. foreign) runterladen, um SPSS-Dateien in R einlesen zu können.\n\nlibrary(foreign)\ndaten <- read.spss(\"Dateipfad\", to.data.frame = TRUE)\n# ohne to.data.frame wird eine Liste erzeugt\n\n\n\n6.7.4 Dateien via URL direkt aus dem Internet laden\nMan kann Dateien auch direkt aus dem Internet laden mit Hilfe ihrer URL.\n\ndaten <- load(url(\"Webadresse\"))\n\nAlternativ kann man die Datei auch herunterladen und dann in Abhängigkeit ihres Formats mit einem der oberen Befehle einlesen.\n\n\n6.7.5 .R, .Rda und .Rmd\nR-eigene Dateien kann man am besten öffnen, indem man auf sie klickt. Mac-Benutzer haben dabei manchmal das Problem, das sich die Datei per default in R öffnet.\n\nAuf Mac-Rechnern kann man R-Studio folgendermaßen zum Standardprogramm zum Öffnen von R-Dateien machen (siehe Abb. unten)\n\nRechtsklick auf die Datei\nauf Informationen klicken\nUnter Öffnen mit R-Studio auswählen und auf Alle ändern… klicken\n\nWenn man nur die eine R-Datei regulär mit R-Studio öffnen möchte:\n\nRechtsklick auf die Datei\nÖffnen mit\nAnderem Programm …\nR-Studio auswählen\nunten mittig ein Häkchen in das Kästchen Immer öffnen mit setzen (wenn das nicht angezeigt wird, muss man erst unten rechts auf Optionen klicken)\nÖffnen"
  },
  {
    "objectID": "Daten-importieren.html#weiterführende-hilfe",
    "href": "Daten-importieren.html#weiterführende-hilfe",
    "title": "6  Daten importieren",
    "section": "6.8 Weiterführende Hilfe",
    "text": "6.8 Weiterführende Hilfe\nFalls wir ein seltener genutztes Dateiformat (z.B. Stata, JSON) in R einlesen möchten, können wir in dem Data Import Tutorial auf Datacamp nachschauen."
  },
  {
    "objectID": "Daten-importieren.html#übung",
    "href": "Daten-importieren.html#übung",
    "title": "6  Daten importieren",
    "section": "6.9 Übung",
    "text": "6.9 Übung\nIn diesem Abschnitt finden wir verschiedene Dateien, die wir zur Übung in R einlesen können. Wie wir dabei vorgehen (d.h. welchen Weg wir nutzen) bleibt ganz uns überlassen. Wenn wir möchten, können wir die Tipps nutzen, um die Aufgaben zu lösen. Zur Überprüfung finden wir mögliche Lösungswege und die eingelesenen Daten.\nWenn wir Hilfe beim Einlesen von Daten brauchen, können wir uns das ausführliche Kapitel dazu anschauen.\nWenn wir Probleme beim Installieren oder Laden von Paketen haben, können wir unseren Eintrag dazu anschauen.\n\n\n6.9.1 Übung 1: .csv\nLade dir von openpsychometrics.org die zip-Datei NPI runter. Entpacke diese und lese data.csv in R ein.\n\n\n\n\n\nTipp 1\n\nDer Datensatz besteht aus 11243 Zeilen und 44 Spalten.\n\n\n\nLösung\n\nMan kan die Datei z.B. mit read.csv(\"Dateipfad\") oder mit read_csv(\"Dateipfad\") (aus dem Paket readr) korrekt einlesen.\nHier siehst du, wie die ersten 6 Zeilen der insgesamt 44 Spalten der Datei.\n\n\n\n\n \n  \n    score \n    Q1 \n    Q2 \n    Q3 \n    Q4 \n    Q5 \n    Q6 \n    Q7 \n    Q8 \n    Q9 \n    Q10 \n    Q11 \n    Q12 \n    Q13 \n    Q14 \n    Q15 \n    Q16 \n    Q17 \n    Q18 \n    Q19 \n    Q20 \n    Q21 \n    Q22 \n    Q23 \n    Q24 \n    Q25 \n    Q26 \n    Q27 \n    Q28 \n    Q29 \n    Q30 \n    Q31 \n    Q32 \n    Q33 \n    Q34 \n    Q35 \n    Q36 \n    Q37 \n    Q38 \n    Q39 \n    Q40 \n    elapse \n    gender \n    age \n  \n \n\n  \n    18 \n    2 \n    2 \n    2 \n    2 \n    1 \n    2 \n    1 \n    2 \n    2 \n    2 \n    1 \n    1 \n    2 \n    1 \n    1 \n    1 \n    2 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    2 \n    2 \n    2 \n    1 \n    2 \n    2 \n    2 \n    1 \n    2 \n    1 \n    1 \n    1 \n    2 \n    2 \n    2 \n    1 \n    2 \n    211 \n    1 \n    50 \n  \n  \n    6 \n    2 \n    2 \n    2 \n    1 \n    2 \n    2 \n    1 \n    2 \n    1 \n    1 \n    2 \n    2 \n    2 \n    1 \n    2 \n    2 \n    1 \n    1 \n    2 \n    1 \n    2 \n    2 \n    1 \n    2 \n    2 \n    2 \n    2 \n    1 \n    2 \n    2 \n    2 \n    1 \n    2 \n    2 \n    1 \n    2 \n    2 \n    2 \n    2 \n    1 \n    149 \n    1 \n    40 \n  \n  \n    27 \n    1 \n    2 \n    2 \n    1 \n    2 \n    1 \n    2 \n    1 \n    2 \n    2 \n    2 \n    1 \n    1 \n    1 \n    1 \n    1 \n    2 \n    2 \n    1 \n    1 \n    2 \n    2 \n    2 \n    2 \n    1 \n    2 \n    1 \n    1 \n    2 \n    1 \n    2 \n    2 \n    1 \n    1 \n    2 \n    1 \n    1 \n    2 \n    1 \n    2 \n    168 \n    1 \n    28 \n  \n  \n    29 \n    1 \n    1 \n    2 \n    2 \n    2 \n    1 \n    2 \n    1 \n    1 \n    2 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    2 \n    2 \n    1 \n    2 \n    1 \n    1 \n    1 \n    2 \n    1 \n    2 \n    1 \n    2 \n    2 \n    1 \n    1 \n    2 \n    1 \n    1 \n    2 \n    1 \n    2 \n    2 \n    1 \n    1 \n    230 \n    1 \n    37 \n  \n  \n    6 \n    1 \n    2 \n    1 \n    1 \n    1 \n    2 \n    1 \n    2 \n    1 \n    2 \n    2 \n    2 \n    2 \n    2 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    2 \n    1 \n    2 \n    2 \n    1 \n    2 \n    1 \n    2 \n    2 \n    2 \n    1 \n    2 \n    2 \n    1 \n    2 \n    2 \n    2 \n    0 \n    1 \n    389 \n    1 \n    50 \n  \n  \n    19 \n    1 \n    2 \n    2 \n    1 \n    2 \n    1 \n    1 \n    1 \n    2 \n    2 \n    1 \n    1 \n    1 \n    2 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    2 \n    1 \n    1 \n    1 \n    2 \n    1 \n    1 \n    2 \n    1 \n    2 \n    1 \n    1 \n    2 \n    2 \n    2 \n    2 \n    361 \n    1 \n    27 \n  \n\n\n\n\n\n\n\n\n\n\n6.9.2 Übung 2: .csv\nLade dir von openpsychometrics.org die zip-Datei 16PF runter. Entpacke diese und lese data.csv in R ein.\n\n\n\n\n\nTipp 1\n\nDer Datensatz besteht aus 49159 Zeilen und 169 Spalten.\n\n\n\nTipp 2\n\nDie einzelnen Zellen (d.h. Elemente der Tabelle) sind durch Leerzeichen (white space) getrennt.\n\n\n\nTipp 3\n\nDie Information, wie die Zellen getrennt sind (siehe Tipp 2) übergibt man dem Argument sep.\n\n\n\nLösung\n\nMan kan die Datei z.B. mit read.csv(\"Dateipfad\", sep=\"\") oder mit read_table2(\"Dateipfad\") (aus dem Paket readr) korrekt einlesen.\nHier siehst du die ersten 6 Zeilen der ersten 50 Spalten der Datei.\n\n\n\n\n \n  \n    A1 \n    A2 \n    A3 \n    A4 \n    A5 \n    A6 \n    A7 \n    A8 \n    A9 \n    A10 \n    B1 \n    B2 \n    B3 \n    B4 \n    B5 \n    B6 \n    B7 \n    B8 \n    B9 \n    B10 \n    B11 \n    B12 \n    B13 \n    C1 \n    C2 \n    C3 \n    C4 \n    C5 \n    C6 \n    C7 \n    C8 \n    C9 \n    C10 \n    D1 \n    D2 \n    D3 \n    D4 \n    D5 \n    D6 \n    D7 \n    D8 \n    D9 \n    D10 \n    E1 \n    E2 \n    E3 \n    E4 \n    E5 \n    E6 \n    E7 \n  \n \n\n  \n    1 \n    4 \n    2 \n    3 \n    3 \n    2 \n    3 \n    4 \n    4 \n    3 \n    4 \n    4 \n    5 \n    4 \n    5 \n    4 \n    5 \n    4 \n    1 \n    2 \n    1 \n    1 \n    1 \n    4 \n    5 \n    4 \n    4 \n    2 \n    4 \n    4 \n    3 \n    3 \n    2 \n    4 \n    3 \n    5 \n    5 \n    4 \n    4 \n    3 \n    2 \n    4 \n    3 \n    1 \n    1 \n    4 \n    3 \n    4 \n    5 \n    1 \n  \n  \n    4 \n    3 \n    4 \n    3 \n    4 \n    4 \n    4 \n    4 \n    2 \n    2 \n    4 \n    4 \n    4 \n    4 \n    5 \n    4 \n    3 \n    2 \n    3 \n    2 \n    4 \n    1 \n    1 \n    1 \n    2 \n    3 \n    3 \n    2 \n    5 \n    4 \n    4 \n    3 \n    3 \n    4 \n    2 \n    4 \n    4 \n    4 \n    5 \n    4 \n    2 \n    3 \n    1 \n    1 \n    2 \n    4 \n    1 \n    4 \n    2 \n    2 \n  \n  \n    3 \n    4 \n    4 \n    4 \n    4 \n    4 \n    4 \n    3 \n    2 \n    2 \n    4 \n    4 \n    5 \n    5 \n    4 \n    4 \n    4 \n    4 \n    2 \n    2 \n    2 \n    2 \n    2 \n    2 \n    4 \n    4 \n    3 \n    3 \n    3 \n    4 \n    2 \n    3 \n    0 \n    3 \n    3 \n    2 \n    2 \n    3 \n    4 \n    3 \n    1 \n    3 \n    3 \n    1 \n    1 \n    3 \n    1 \n    4 \n    2 \n    3 \n  \n  \n    4 \n    5 \n    4 \n    4 \n    4 \n    3 \n    3 \n    2 \n    2 \n    2 \n    4 \n    2 \n    4 \n    5 \n    4 \n    5 \n    4 \n    4 \n    3 \n    3 \n    3 \n    2 \n    4 \n    3 \n    2 \n    3 \n    4 \n    3 \n    3 \n    2 \n    2 \n    3 \n    4 \n    3 \n    2 \n    3 \n    4 \n    2 \n    3 \n    3 \n    3 \n    4 \n    3 \n    3 \n    2 \n    4 \n    1 \n    4 \n    4 \n    1 \n  \n  \n    4 \n    0 \n    4 \n    4 \n    4 \n    3 \n    5 \n    1 \n    2 \n    4 \n    2 \n    4 \n    4 \n    5 \n    5 \n    4 \n    4 \n    5 \n    4 \n    1 \n    5 \n    1 \n    2 \n    2 \n    4 \n    3 \n    3 \n    4 \n    4 \n    4 \n    4 \n    3 \n    2 \n    5 \n    4 \n    3 \n    4 \n    5 \n    4 \n    1 \n    1 \n    1 \n    3 \n    1 \n    1 \n    3 \n    1 \n    4 \n    2 \n    4 \n  \n  \n    3 \n    5 \n    4 \n    4 \n    4 \n    5 \n    5 \n    1 \n    1 \n    4 \n    4 \n    1 \n    4 \n    5 \n    3 \n    4 \n    3 \n    3 \n    2 \n    2 \n    1 \n    2 \n    2 \n    2 \n    3 \n    4 \n    2 \n    4 \n    2 \n    2 \n    4 \n    3 \n    1 \n    4 \n    4 \n    5 \n    5 \n    4 \n    3 \n    2 \n    2 \n    2 \n    3 \n    3 \n    4 \n    3 \n    2 \n    4 \n    3 \n    2 \n  \n\n\n\n\n\n\n\n\n\n\n6.9.3 Übung 3: .sav\nLade dir die Datei ges7.sav von metheval.uni-jena.de herunter und lese diese in R ein.\n\n\n\n\n\nTipp 1\n\nDie Endung .sav kennzeichnet SPSS-Dateien. Um diese einzulesen benötigt man zusätzliche Pakete, weil es in base R keine Funktion dafür gibt.\n\n\n\nTipp 2\n\nDer Datensatz besteht aus 503 Zeilen und 1650 Spalten.\n\n\n\nLösung\n\nMan kan die Datei z.B. mit read.spss(\"Dateipfad\", to.data.frame = TRUE) (aus dem Paket foreign) oder mit read_sav(\"Dateipfad\") (aus dem Paket haven) korrekt einlesen.\nHier siehst du die ersten 6 Zeilen der ersten 50 Spalten der Datei.\n\n\n\n\n \n  \n    CODE \n    T1SEX \n    T1AGE \n    T1KNR \n    T1MZP \n    T1TIME \n    T1DAY \n    T1MON \n    T1SB \n    T1ST01 \n    T1ST02 \n    T1ST03 \n    T1ST04 \n    T1ST05 \n    T1ST06 \n    T1ST07 \n    T1ST08 \n    T1ST09 \n    T1ST10 \n    T1ST11 \n    T1ST12 \n    T1ST13 \n    T1ST14 \n    T1ST15 \n    T1ST16 \n    T1ST17 \n    T1ST18 \n    T1ST19 \n    T1ST20 \n    T1ST21 \n    T1ST22 \n    T1ST23 \n    T1ST24 \n    T1ST25 \n    T1ST26 \n    T1ST27 \n    T1ST28 \n    T1ST29 \n    T1ST30 \n    T1ST31 \n    T1ST32 \n    T1ST33 \n    T1ST34 \n    T1ST35 \n    T1ST36 \n    T1ST37 \n    T1ST38 \n    T1ST39 \n    T1ST40 \n    T1ST41 \n  \n \n\n  \n    aa02 \n    2 \n    22 \n    1 \n    1 \n    23 \n    NA \n    NA \n    1 \n    4 \n    3 \n    1 \n    3 \n    4 \n    4 \n    1 \n    1 \n    4 \n    1 \n    1 \n    1 \n    1 \n    3 \n    2 \n    3 \n    4 \n    4 \n    2 \n    1 \n    2 \n    3 \n    3 \n    3 \n    2 \n    4 \n    4 \n    1 \n    2 \n    4 \n    3 \n    1 \n    5 \n    1 \n    4 \n    1 \n    3 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    aa19 \n    1 \n    35 \n    1 \n    1 \n    11 \n    8 \n    5 \n    2 \n    3 \n    3 \n    4 \n    5 \n    2 \n    3 \n    3 \n    4 \n    2 \n    5 \n    5 \n    5 \n    3 \n    2 \n    4 \n    3 \n    2 \n    3 \n    4 \n    5 \n    4 \n    2 \n    1 \n    1 \n    5 \n    2 \n    4 \n    4 \n    3 \n    2 \n    4 \n    5 \n    5 \n    4 \n    1 \n    4 \n    1 \n    5 \n    3 \n    3 \n    4 \n  \n  \n    ab17 \n    1 \n    58 \n    1 \n    1 \n    21 \n    NA \n    NA \n    1 \n    1 \n    3 \n    4 \n    2 \n    1 \n    2 \n    4 \n    1 \n    1 \n    3 \n    2 \n    4 \n    1 \n    3 \n    3 \n    1 \n    1 \n    3 \n    5 \n    1 \n    4 \n    1 \n    1 \n    3 \n    3 \n    1 \n    2 \n    1 \n    2 \n    4 \n    4 \n    2 \n    1 \n    1 \n    4 \n    1 \n    4 \n    2 \n    2 \n    4 \n    3 \n  \n  \n    ac03 \n    2 \n    53 \n    1 \n    1 \n    19 \n    15 \n    5 \n    1 \n    5 \n    4 \n    3 \n    1 \n    3 \n    1 \n    3 \n    1 \n    4 \n    2 \n    1 \n    4 \n    1 \n    5 \n    2 \n    1 \n    4 \n    2 \n    1 \n    1 \n    4 \n    3 \n    3 \n    1 \n    1 \n    3 \n    1 \n    1 \n    1 \n    4 \n    5 \n    1 \n    1 \n    1 \n    3 \n    1 \n    3 \n    3 \n    1 \n    4 \n    3 \n  \n  \n    ac09 \n    1 \n    25 \n    1 \n    1 \n    22 \n    3 \n    5 \n    1 \n    5 \n    5 \n    1 \n    1 \n    5 \n    5 \n    1 \n    1 \n    5 \n    1 \n    1 \n    1 \n    1 \n    5 \n    1 \n    1 \n    5 \n    3 \n    1 \n    1 \n    1 \n    5 \n    5 \n    1 \n    1 \n    5 \n    1 \n    1 \n    1 \n    5 \n    3 \n    1 \n    1 \n    1 \n    4 \n    1 \n    5 \n    4 \n    1 \n    4 \n    4 \n  \n  \n    ad03 \n    2 \n    23 \n    1 \n    1 \n    16 \n    10 \n    4 \n    1 \n    3 \n    3 \n    2 \n    1 \n    3 \n    3 \n    4 \n    1 \n    4 \n    1 \n    1 \n    1 \n    1 \n    4 \n    1 \n    1 \n    4 \n    3 \n    1 \n    1 \n    1 \n    3 \n    3 \n    1 \n    2 \n    2 \n    2 \n    1 \n    3 \n    2 \n    1 \n    2 \n    1 \n    2 \n    2 \n    1 \n    2 \n    2 \n    3 \n    3 \n    2 \n  \n\n\n\n\n\n\n\n\n\n\n6.9.4 Übung 4: .xlsx\nLade dir die Bahnsteigdaten (RNI) von data.deutschebahn.com herunter und lese diese in R ein.\n\n\n\n\n\nTipp 1\n\nDie Endung .xlsx kennzeichnet Excel-Dateien. Um diese einzulesen benötigt man zusätzliche Pakete, weil es in base R keine Funktion dafür gibt.\n\n\n\nTipp 2\n\nDer Datensatz besteht aus 345 Zeilen und 4 Spalten.\n\n\n\nLösung\n\nMan kan die Datei z.B. mit read_xlsx(\"Dateipfad\") (aus dem Paket readxl) oder mit read.xlsx(\"Dateipfad\") (aus dem Paket openxlsx) korrekt einlesen.\nHier siehst du die ersten 6 Zeilen der insgesamt 4 Spalten der Datei.\n\n\n\n\n \n  \n    bf_nr \n    Bahnsteig_Nr \n    Bahnsteig_Hoehe_cm \n    Nettobahnsteiglaenge_m \n  \n \n\n  \n    8263 \n    1 \n    38 \n    115 \n  \n  \n    8263 \n    2 \n    38 \n    115 \n  \n  \n    2616 \n    1 \n    38 \n    115 \n  \n  \n    6618 \n    1 \n    38 \n    113 \n  \n  \n    6618 \n    2 \n    38 \n    123 \n  \n  \n    33 \n    1 \n    38 \n    115 \n  \n\n\n\n\n\n\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] readxl_1.4.2     foreign_0.8-84   kableExtra_1.3.4 knitr_1.42      \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       httr_1.4.5        svglite_2.1.1     cli_3.6.1        \n [5] rlang_1.1.0       xfun_0.39         highr_0.10        stringi_1.7.12   \n [9] jsonlite_1.8.4    glue_1.6.2        colorspace_2.1-0  htmltools_0.5.5  \n[13] fansi_1.0.4       scales_1.2.1      rmarkdown_2.21    cellranger_1.1.0 \n[17] tibble_3.2.1      evaluate_0.20     munsell_0.5.0     fastmap_1.1.1    \n[21] yaml_2.3.7        lifecycle_1.0.3   stringr_1.5.0     compiler_4.3.0   \n[25] rvest_1.0.3       pkgconfig_2.0.3   htmlwidgets_1.6.2 rstudioapi_0.14  \n[29] systemfonts_1.0.4 digest_0.6.31     viridisLite_0.4.1 R6_2.5.1         \n[33] utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3    webshot_0.5.4    \n[37] tools_4.3.0       xml2_1.3.3       \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Fehlermeldungen.html#tools-die-uns-helfen-fehler-zu-vermeiden",
    "href": "Fehlermeldungen.html#tools-die-uns-helfen-fehler-zu-vermeiden",
    "title": "7  Fehlermeldungen",
    "section": "7.1 Tools die uns helfen, Fehler zu vermeiden",
    "text": "7.1 Tools die uns helfen, Fehler zu vermeiden\nBevor wir uns dem Verstehen von konkreten (Syntax-)Fehlermeldungen widmen und anschauen, wie wir effizient im Internet suchen können, schauen wir uns zwei Hilfsmittel an, die uns die Entwicklungsumgebung RStudio gibt, um Fehler zu vermeiden.\n\n7.1.1 R-Dokumentation\nWenn wir eine uns noch unbekannte Funktion nutzen (aber auch wenn wir aus einer Fehlermeldung nicht schlau werden) können wir die Dokumentation nutzen. Diese können wir in RStudio im unteren rechten Panel unter Help öffnen. In das Suchfeld geben wir den Namen der Funktion ein.\nAlternativ können wir die Hilfefunktionen ? oder help() nutzen (z.B. ?matrix oder help(matrix)).\nMit Klick auf das eingekreiste Icon kann man sich die Hilfe-Seite auch in einem extra Fenster anzeigen lassen.\n\nWir finden die R-Dokumentation auch im Internet, allerdings ermöglicht uns RStudio, diese ohne Internetverbindung und direkt in der Entwicklungsumgebung zu öffnen.\n\n\n\n\n\n\n\n\n\n\nWir bekommen hier (zumeist) folgende Informationen: aus welchem Paket die Funktion ist (oben links in { }), was die Funktion macht (Description), die Funktionsdefinition ggf. mit Defaults (Usage), ihre Parameter mit jeweils gültigen Argumenten (Arguments), weitere Details zur Nutzung (Details), ähnliche Funktionen (See also) und Beispiele zur Nutzung (Examples).\n\n\nWas sind Parameter, Argumente und Defaults?\n\nParameter bezeichnet die (formalen) Variablen einer Funktion (z.B. nrow aus matrix()), denen wir unsere (tatsächlichen) Argumente (z.B. 3), d.h. unseren Input, übergeben. Default bezeichnet ein voreingestelltes Argument (z.B. nrow=1).\nOhne explizite Spezifikation des Parameters unsererseits wird (wenn vorhanden) der Default verwendet. Wir könnten natürlich auch ein anderes (als das voreingestellte) Argument festlegen.\nFunktionen, die ausschließlich Parameter mit Defaults besitzen, (z.B. matrix()) werden auch ohne Spezifikation ausgeführt. Funktionen mit (min. einem) Parameter ohne Default (z.B. mean()) werden ohne Spezifikation dieser nicht ausgeführt (wir müssen dem Parameter x einen Vektor, von dem wir den Mittelwert berechnen wollen, übergeben). Mehr Informationen zu Funktionsdefinition und etwaigen Defaults finden wir im Abschnitt Usage in der R-Dokumentation.\nMehr Informationen zum Aufbau von Funktionen finden wir in der Einführung in R\n\nEine detaillierte Einführung zur Nutzung der Dokumentation befindet sich im Kapitel Einführung in R Studio.\n\n\n7.1.2 Code Diagnostik\n\nAchtung: Wir können die Code Diagnostik nur nutzen, wenn wir unser Skript gespeichert haben.\n\nVor Ausführung unseres Codes erhalten wir Hinweise, beispielsweise wenn Argumente einer Funktion fehlen oder unerwartete Zeichen auftauchen.\nDer Fehler wird unterstrichen und es erscheint außerdem ein Symbol links neben der Zeilennummerierung. Zusätzlich erhalten wir einen ausformulierten Hinweis, wenn wir mit der Maus über das Symbol fahren.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLösung\n\nDas dritte Argument, welches nach dem zweiten Komma in c() folgen sollte, ist leer. In unserem Beispiel sind die Argumente von c() Zahlen (zur Indexierung der Spalten vom Dataframe daten). Wenn wir die Spalten 1, 2 und 3 extrahieren wollten, würden wir das überflüssige Komma löschen: daten[,c(1,2,3)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLösung\n\nUnser Code endet mit einer schließenden runden Klammer, welche kein öffnendes Pendant hat. Außerdem hat unsere öffnende eckige Klammer kein abschließendes Pendant. Wahrscheinlich haben wir uns vertippt und anstatt einer schließenden eckigen Klammer eine schließende runde Klammer eingefügt. So würde der richtige Code aussehen: daten[,c(1,2,3)]\n\n\n\nAuf dieser Seite wird die Möglichkeiten der Code Diagnostics noch detaillierter erklärt und wir erhalten eine Anleitung dazu, wie wir verschiedene Einstellungen tätigen können (in der oberen Menüleiste über Tools > Global Options > Code > Diagnostics)."
  },
  {
    "objectID": "Fehlermeldungen.html#fehlermeldungen-verstehen",
    "href": "Fehlermeldungen.html#fehlermeldungen-verstehen",
    "title": "7  Fehlermeldungen",
    "section": "7.2 Fehlermeldungen verstehen",
    "text": "7.2 Fehlermeldungen verstehen\nSchauen wir uns einmal den typischen Aufbau von Fehlermeldungen an:\n\n\n\n\n\n\n\n\n\n\n\nDiese Fehlermeldung werden wir im Abschnitt Suchen im Internet noch aufklären.\n\nDie Information vor dem Doppelpunkt gibt uns an, in welcher Funktion der Fehler steckt; die Information nach dem Doppelpunkt gibt Aufschluss über die Art des Fehlers. Zweiteres ist für die Fehlersuche (zumeist) von großer Bedeutung.\nIm Folgenden konzentrieren wir uns auf eine Art von Fehler, mit der vor allem AnfängerInnen häufig konfrontiert sind: Syntaxfehler.\n\nSyntaxfehler sind Fehler, die durch eine Verletzung des formalen Aufbaus einer Funktion zustande kommen. Manchmal entstehen sie auch nur durch Vertippen.\n\nSyntaxfehler können zumeist relativ einfach durch Korrigieren der fehlerhaften Syntax, i.d.R. dem Einfügen oder Entfernen von bestimmten Zeichen, zumeist Kommata oder Klammern, gelöst werden.\nSchauen wir uns nun einige Beispiele an:\nWenn ein oder mehrere Zeichen überflüssig sind bzw. fehlen, dann bekommt man Unerwartete(s) '...' in \"...\" ausgegeben. Hierbei teilt uns die Meldung mit, wo der Fehler liegt: In den Anführungszeichen \"...\" wird nur ein Teil des Codes ausgegeben und der Fehler ist zumeist das zuletzt ausgegebene Zeichen (oder es liegt unmittelbar davor).\n\n\n\n\n\n\n\n\n\n\n\nLösung\n\nz.B. daten[2]\nDamit extrahiert man auch die zweite Spalte aus daten, aber hier wird der Name der Spalte (Variable) nicht mit ausgegeben. Außerdem werden die Elemente der Spalte zeilwenweise wiedergegeben.\n\n\n\nWenn mehrere Syntaxfehler enthalten sind, und man nicht gleich alle erkennt, muss man diese nach und nach beseitigen.\n\n\n\n\n\n\n\n\n\n\n\nLösung\n\nz.B. daten[2]\nZuerst wurde nur auf das fehlerhafte $ hingewiesen. Nachdem dieses gelöscht wurde, wird erst auf das falsche ) verwiesen.\n\n\n\nManchmal bekommt man auch nur unerwartetes Symbol in: \"...\" ausgegeben. In den Anführungszeichen \"...\" wird hier nur der Teil des Codes ausgegeben, in dem der Fehler ist, nicht aber (wie oben), auch das problematische Zeichen.\n\n\n\n\n\n\n\n\n\n\n\nLösung\n\nz.B. daten[,2] oder daten[2]\nHier wurde nicht bemerkt, dass bei der ersten Funktion daten[,2 noch ein ] fehlt und einfach schon die nächste Funktion daten[,3] ausgeführt. So nahm R an, dass beide zusammen gehören würden.\n\n\n\nWenn man Komma-Fehler hat, bekommt man beispielsweise folgende Meldung angezeigt.\n\n\n\n\n\n\n\n\n\n\n\nLösung\n\nz.B. daten[c(1,2)] oder daten[c(1,2,3)]\nHier weist uns R darauf hin, dass das dritte Argument, welches nach dem zweiten Komma folgen sollte, leer ist. In Abhängigkeit davon, welche Spalten aus Daten extrahiert werden sollen, löscht man das Komma (um Spalte 1 und 2 zu extrahieren) oder man ergänzt einen weiteren Spaltenindex (z.B. 3; um Spalte 1, 2 und 3 zu extrahieren)."
  },
  {
    "objectID": "Fehlermeldungen.html#suchen-im-internet",
    "href": "Fehlermeldungen.html#suchen-im-internet",
    "title": "7  Fehlermeldungen",
    "section": "7.3 Suchen im Internet",
    "text": "7.3 Suchen im Internet\nNun gibt es auch Fehlermeldungen, die wir nicht auf Anhieb verstehen. Dafür kann es vielfältige Ursachen geben. Wenn wir schon in der R-Dokumentation nachgeschaut haben und trotzdem noch ratlos sind, können wir im Internet nach Hilfe suchen. Wir können dafür Suchmaschinen oder direkt spezielle Foren nutzen.\nHierbei gibt es nicht den einen Weg, fündig zu werden. Häufig gibt es eine Fehlermeldung, die bei verschiedenen Problemen (mit unterschiedlichen Funktionen) angezeigt wird. Dann muss man filtern, welche Seite für das eigene Anliegen relevant sein könnte. Es kann ebenso möglich sein, dass es zu manchen Problemen einfach noch keine Lösung gibt. Dieser Fall ist eher seltener und tritt vor allem bei neueren oder seltener genutzen Paketen auf.\nIm Folgenden schauen wir uns einige wichtige Aspekte an, auf die man achten sollte, wenn man im Internet nach Lösungen zu Fehlermeldungen sucht.\nDafür schauen wir uns folgendes Beispiel an: Wir wollen eine Korrelationstabelle erstellen, in der wir aus daten die ersten beiden Spalten [1:2] mit den letzten beiden Spalten [3:4] korrelieren. Dazu wurde die Funktion cor.test() genutzt. Wir bekommen folgende Fehlermeldung:\n\n\n\n\n\n\n\n\n\n\n7.3.1 Passende Suchbegriffe nutzen\nSinnvoll ist es im Suchtext drei Aspekte miteinzubeziehen:\n\ndas Programm, mit dem es Probleme gibt\ndie Funktion, mit der es Probleme gibt\n\nauch ersichtlich an der Information vor dem Doppelpunkt\n\nAusschnitte aus der Fehlermeldung\n\nvon den Informationen nach dem Doppelpunkt\n\n\nUnsere Suche könnte folgendermaßen aussehen:\n\n\n\n\n\n\n\n\n\nAnstatt eine globale Suchmaschine wie google oder ecosia zu nutzen, kann man auch eine spezifisch für Anliegen in R konzipierte Suchmaschine wie rseek nutzen. Dann kann man im Suchtext den Namen des Programms weglassen, beispielsweise nur cor.test x numerischer Vektor suchen.\n\n\n7.3.2 Ergebnisse filtern\nNachfolgend sehen wir die ersten fünf Ergebnisse der Suche.\n\n\n\n\n\n\n\n\n\n\nSuche mit ecosia am 05.09.2019 um 17:30Uhr\n\nMit Ausnahme des zweiten Links sieht es so aus, als würden die Seiten zu unspezifisch für unser Problem sein.\nDer zweite Link weist gleich zwei Übereinstimmungen auf: Wir wollten einen correlation test rechnen und haben die selbe Fehlermeldung (‘x’ must be a numeric vector) bekommen.\nSchauen wir uns also das zweite Ergebnis an.\n\n\n7.3.3 Lösungsvorschläge ausprobieren\nIn Foren findet man in aller Regel oben die Problembeschreibung und darunter die geposteten Lösungsvorschläge.\nIn unserem Beispiel, hat der/die Fragende mit der gleichen Funktion (cor.test()) gearbeitet.\nSchauen wir uns den Lösungsvorschlag an: Es wird vorgeschlagen den Datensatz mit str() zu überprüfen, ob die enthaltenen Variablen numerische Vektoren sind. Wenn dem nicht so ist, muss man sie dementsprechend umwandeln.\n\nstr(daten)\n\n'data.frame':   5 obs. of  4 variables:\n $ Var_1: num  0 1 3 2 2\n $ Var_2: num  3 2 0 3 1\n $ Var_3: num  3 0 1 3 1\n $ Var_4: num  1 2 1 0 3\n\n\nBei uns scheinen alle Spalten numerisch (num) zu sein. Diese Lösung scheint also nicht passend für unser Problem zu sein.\n\n\n7.3.4 Zurück zu b) Ergebnisse filtern und c) Lösungsvorschläge ausprobieren\nGehen wir also zurück zu unserer Suche und schauen uns andere Seiten an. So sehen die nächsten fünf Ergebnisse der Suche aus.\n\n\n\n\n\n\n\n\n\nBis auf den zweiten Link sehen die Vorschläge hier auch wieder sehr unspezifisch aus. Bei diesem gibt es zwei Parallelen zu unserem Problem: cor.test wird auf einen data frame angewendet. Sehen wir uns die Seite mal an.\nWir landen in einem Foreneintrag. Die Antwort von dasonk weißt uns darauf hin, dass man an cor.test() nur zwei Vektoren übergeben kann. Wir haben aber 4 Vektoren übergeben. In der nächsten Antwort von anbende wird vorgeschlagen, die Korrelationstabelle mit cor() zu erstellen. Allerdings muss man die Signifikanztestung immer noch mit cor.test() machen.\nDie Antwort hat uns schon weitergeholfen: Wir haben eine Vorstellung davon, wo der Fehler liegen könnte. Jetzt ist es sinnvoll, eine neue Suche zu starten, um nach einer Funktion zu suchen, die sowohl eine Korrelationstabelle erstellt, als auch eine Signifikanztestung durchführt.\nHier kombinieren wir Suchwörter zum Programm und zu dem, was die Funktion, die wir suchen, leisten soll.\n\n\n\n\n\n\n\n\n\nDann schauen wir uns wieder die ersten 5 Vorschläge dazu an:\n\n\n\n\n\n\n\n\n\n\nSuche mit ecosia am 06.09.2019 um 09:40Uhr\n\nDer erste Vorschlag ist nicht ganz das, was wir wollen. Hier geht es scheinbar mehr um elegante Korrelationstabellen.\nDer zweite Vorschlag hingegen scheint vielversprechender. Schauen wir uns diesen einmal an.\nEs ist eine Seite, auf der verschiedene Aspekte der Umsetzung von Korrelationstabellen in R erklärt werden. Oben finden wir eine Gliederung. Wir springen gleich zum Punkt ‘Correlation matrix with significance levels (p-value)’.\nHier erfahren wir, dass man die Funktion rcorr() aus dem Hmisc-Paket nutzen kann. Außerdem wird man darauf hingewiesen, dass diese Funktion nur mit Matrizen arbeiten kann. Der Befehl, mit dem man Objekte in Matrizen umwandeln kann, wird auch angegeben.\nProbieren wir diesen Weg einmal aus.\n\n# install.packages(\"Hmisc\")\nlibrary(Hmisc)\nrcorr(as.matrix(daten[1:2]), as.matrix(daten[3:4]))\n\n      Var_1 Var_2 Var_3 Var_4\nVar_1  1.00 -0.74 -0.29 -0.04\nVar_2 -0.74  1.00  0.66 -0.44\nVar_3 -0.29  0.66  1.00 -0.69\nVar_4 -0.04 -0.44 -0.69  1.00\n\nn= 5 \n\n\nP\n      Var_1  Var_2  Var_3  Var_4 \nVar_1        0.1528 0.6309 0.9510\nVar_2 0.1528        0.2279 0.4616\nVar_3 0.6309 0.2279        0.2006\nVar_4 0.9510 0.4616 0.2006       \n\n\nWir erhalten zwei Matrizen als Output. Eine enthält die Korrelationen, die andere die p-Werte. Wir haben also gefunden, wonach wir gesucht haben."
  },
  {
    "objectID": "Fehlermeldungen.html#und-wenn-man-keine-lösung-gefunden-hat",
    "href": "Fehlermeldungen.html#und-wenn-man-keine-lösung-gefunden-hat",
    "title": "7  Fehlermeldungen",
    "section": "7.4 Und wenn man keine Lösung gefunden hat?",
    "text": "7.4 Und wenn man keine Lösung gefunden hat?\nWenn man im ersten Durchlauf nichts gefunden hat, ist es beispielsweise sinnvoll, andere Ausschnitte aus der Fehlermeldung in den Suchtext zu inkludieren.\nManchmal ist ein Fehler so global, dass er bei verschiedenen Funktionen auftaucht. Daher lohnt es sich manchmal auch, eine Suche ohne den Funktionsnamen durchzuführen. Spätestens, wenn man bei der Suche mit allen drei Suchbegriffen nichts gefunden hat, ist es sinnvoll, nur mit Programm + Fehlermeldung zu suchen.\nWir können auch auf allgemeine Tipps zum Suchen im Internet zurückgreifen, u.a.:\n\nmit Zitaten exakte Phrasen suchen\n\nz.B. “Fehler in cor.test.default”\n\nalternative Begriffe suchen (logische Operatoren)\n\nAND ist der Default wenn wir mehrere Wörter eingeben\nmit OR (oder |) können wir mehrere Optionen angeben z.B. “cor.test | cor.test.default”\n\naus- bzw. einschließen von Wörtern\n\nvor allem in Kombination mit Zitaten ist dieses Tool sehr nützlich\nausschließen z.B. “Fehler in cor.test.default” -str()”\neinschließen z.B. “Fehler in cor.test.default +r”\n\n\nEin weiterer Tipp ist es, auf Englisch zu suchen. Die Wahrscheinlichkeit ist hier viel größer, dass man Lösungsvorschläge findet.\nWenn man, wie in diesem Kapitel zu sehen, die Fehlermeldungen auf deutsch ausgegeben bekommt, kann man das folgendermaßen auf englisch ändern:\n\nSys.setenv(LANGUAGE=\"en\")"
  },
  {
    "objectID": "Fehlermeldungen.html#weiterführende-hilfe",
    "href": "Fehlermeldungen.html#weiterführende-hilfe",
    "title": "7  Fehlermeldungen",
    "section": "7.5 Weiterführende Hilfe",
    "text": "7.5 Weiterführende Hilfe\nGanz generell kann es sehr hilfreich sein, wenn man ein bisschen mit der Funktion “spielt” - etwas wegnimmt oder verändert - und schaut, ob es dann funktioniert oder wie sich die Fehlermeldung verändert. Das hilft meist, das Problem etwas einzugrenzen - und man lernt nebenbei die Funktion besser kennen.\nIm R-Fehlermeldungsleitfaden der Uni Münster (unter 3. Fehlertextliste), auf R-bloggers sowie PROGRAMMINGR findet man jeweils eine Übersicht über geläufige Fehlermeldungen mit Lösungen und teilweise auch Beispielen.\n\nEine Einführung in weitere Möglichkeitens des Debuggings und der dafür in R eingebauten Tools finden wir auf Advanced R."
  },
  {
    "objectID": "Fehlermeldungen.html#automatische-suche-mit-dem-paket-errorist",
    "href": "Fehlermeldungen.html#automatische-suche-mit-dem-paket-errorist",
    "title": "7  Fehlermeldungen",
    "section": "7.6 Automatische Suche mit dem Paket errorist",
    "text": "7.6 Automatische Suche mit dem Paket errorist\nEs gibt das Paket errorist, mit dem bei jeder Fehler- und Warnmeldung eine automatische Suche in Google gestartet wird. Schauen wir uns die Funktionsweise des Pakets am im Abschnitt Suchen im Internet genutzten Beispiels an.\nZuerst installieren wir das Paket:\n\ninstall.packages(\"errorist\", dependencies=TRUE)\n\nDann laden wir das Paket und schauen uns an, was bei Ausführung unseres fehlerhaften Codes passiert.\n\nlibrary(errorist)\ncor.test(daten[1:2], daten[3:4])\n\nSofort öffnet sich ein neues Fenster im Browser, in dem in Google folgender Text gesucht wird: “Fehler in cor.test.default(daten[1:2], daten[3:4]) : ‘x’ muss ein numerischer Vektor sein r programming”.\n\n\n\n\n\n\n\n\n\nDas Paket übernimmt sozusagen den ersten Schritt bei der Suche im Internet, passende Suchbegriffe zu nutzen, für uns. Die Ergebnisse müssen wir nach wie vor alleine filtern, um eine geeignete Lösung zu finden.\nWenn wir uns den Aufbau des Suchtextes anschauen, sehen wir, dass die gesamte Fehlermeldung und “r programming” gesucht wird. In unserer manuellen Suche haben wir die gleichen Begriffe genutzt; wir haben lediglich nicht die gesamte Fehlermeldung genutzt, sondern daraus den Namen der Funktion und die eigentliche Fehlermeldung. Unser manueller Suchtext enthielt folglich weniger Wörter und generiert damit u.U. mehr Ergebnisse.\n\nVergleich der Ergebnisse der Suchen für unser Beispiel: manuell 448.000 vs. errorist 17.000.\nAllerdings wurden auch verschiedene Suchmaschinen genutzt (ecosia vs. google).\n\nOb ihr das Paket nutzen möchtet oder lieber analog sucht bleibt euch überlassen.\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] Hmisc_5.0-1\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3      jsonlite_1.8.4    dplyr_1.1.2       compiler_4.3.0   \n [5] rpart_4.1.19      tidyselect_1.2.0  htmlTable_2.4.1   stringr_1.5.0    \n [9] gridExtra_2.3     cluster_2.1.4     scales_1.2.1      yaml_2.3.7       \n[13] fastmap_1.1.1     ggplot2_3.4.2     R6_2.5.1          generics_0.1.3   \n[17] Formula_1.2-5     knitr_1.42        backports_1.4.1   htmlwidgets_1.6.2\n[21] checkmate_2.1.0   tibble_3.2.1      munsell_0.5.0     nnet_7.3-18      \n[25] pillar_1.9.0      rlang_1.1.0       utf8_1.2.3        stringi_1.7.12   \n[29] xfun_0.39         cli_3.6.1         magrittr_2.0.3    digest_0.6.31    \n[33] grid_4.3.0        rstudioapi_0.14   base64enc_0.1-3   lifecycle_1.0.3  \n[37] vctrs_0.6.2       data.table_1.14.8 evaluate_0.20     glue_1.6.2       \n[41] fansi_1.0.4       colorspace_2.1-0  foreign_0.8-84    rmarkdown_2.21   \n[45] tools_4.3.0       pkgconfig_2.0.3   htmltools_0.5.5  \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Datenvorbereitung.html#grundlegende-erste-schritte",
    "href": "Datenvorbereitung.html#grundlegende-erste-schritte",
    "title": "8  Datenvorbereitung",
    "section": "8.1 Grundlegende erste Schritte",
    "text": "8.1 Grundlegende erste Schritte\nZuerst widmen wir unsere Aufmerksamkeit der Überprüfung wichtiger übergreifender Punkte der Datenvorbereitung. Diese sind: ob unser Datensatz als Dataframe vorliegt, ob unsere Daten plausibel und fehlende Werte korrekt kodiert sind, ob nominal- und v.a. ordinalskalierte Variablen faktorisiert sind, und ob wir unser bestehendes Tabellenformat ggf. ändern müssen. Die Schritte sind bereits in einer sinnvollen Abfolge angeordnet (z.B. ist es vorteilhaft, erst unplausible und fehlende Werte ausfindig zu machen und umzukodieren, bevor man Variablen faktorisiert).\nOptional können wir vorher unser Wissen zum Messen von Merkmalen und der korrekten Darstellung dieser in R auffrischen.\nWir schauen uns nachfolgend nur die Datensätze airquality und PWE_data an.\n\n8.1.1 Recap: Kodierung von Daten\nGenerell wenn wir mit Daten arbeiten, ist es ratsam, sich zuallererst Gedanken darüber zu machen, welche Informationen wir diesen entnehmen können (Messniveau) und ob sie so gespeichert sind, dass R sie richtig erkennt (Datentypen und -strukturen).\nDie nachfolgende Wiederholung ist eine verkürzte Variante des Abschnitts Daten aus dem Kapitel zu Einführung in R.\n\n8.1.1.1 Messniveaus\n\n\n\nDas Messniveau (oder auch Skalenniveau) ist eine wichtige Eigenschaft von Merkmalen (Variablen) von Untersuchungseinheiten. Es beschreibt, welche Informationen in unseren Messwerten abgebildet werden und damit auch welche mathematischen Transformationen mit den Messwerten sinnvoll sind (z.B. das Berechnen von Mittelwerten). Somit begrenzt das Messniveau auch die zulässigen Datenauswertungsverfahren unserer Variablen.\nDie Kodierung von nominalskalierten Merkmalen ist insofern willkürlich, als dass lediglich auf Gleichheit versus Ungleichheit geachtet werden muss (z.B. 1, 4, 9 oder A, Y, M).\n\nMögliche Unterscheidungen:\nGleichheit/Ungleichheit\n\nairquality: -\nPWE_data: u.a.school, urban, gender\nDie Kodierung von ordinalskalierten Merkmalen geschieht der Größe nach, d.h. dass die Rangfolge der Kodierungen einzelner Gruppen relevant ist (z.B. 1 < 4 < 9 oder A < M < Y). Man kann aber auch eine eigene Sortierung festlegen, die nicht der “natürlichen” Rangfolge entspricht (z.B. Y < A < M). Ein Realschulabschluss ist beispielsweise besser als ein Hauptschulabschluss. Wir können aber nicht festlegen, wie viel besser er ist.\n\nMögliche Unterscheidungen:\nGleichheit/Ungleichheit\nRangordnung\n\nairquality: -\nPWE_data: education\nBei der Kodierung von intervallskalierten Merkmalen sind sowohl die Rangfolge als auch die Abstände zwischen den Ausprägungen relevant (z.B. 1, 4, 7; jeweils mit gleichem Abstand zueinander; oder 1.4, 1.5, 2.3; jeweils mit verschiedenen Abständen zueinander). Ein Beispiel dafür ist die Temperatur in Grad Celsius oder Grad Fahrenheit.\n\nMögliche Unterscheidungen:\nGleichheit/Ungleichheit\nRangordnung\nAbstände\n\nairquality: Temp (Temperatur in Grad Fahrenheit)\nPWE_data: u.a. Antworten (1-5) auf die Items des PWE (Q1A, …, Q19A), Antworten (1-7) auf die Items des TIPI (TIPI1, …, TIPI10)\nBei der Kodierung von verhältnisskalierten Merkmalen ist zusätzlich noch ein Nullpunkt vorhanden. Dieser erlaubt es, dass Quotienten zwischen Werten gebildet werden können. Ein beliebtes Beispiel ist die Kelvin Skala. Bei dieser ist bei 0°K keine Bewegungsenergie mehr vorhanden und 20°K sind doppelt so viel wie 40°K.\n\nMögliche Unterscheidungen:\nGleichheit/Ungleichheit\nRangordnung\nAbstände\nVerhältnisse\n\nairquality: Ozone, Solar.R, Wind\nPWE_data: age\nZu guter Letzt gibt es noch absolutskalierte Merkmale, welche sowohl einen eindeutigen Nullpunkt als auch eine eindeutige Einheit der Skala (z.B. Anzahl der Kinder) vorweisen kann. Die Kodierung entspricht der natürlichen Einheit.\nairquality: -\nPWE_data: familysize\nNachfolgend finden wir eine Tabelle der möglichen Unterscheidungen der jeweiligen Messiniveaus.\n\n\n\n\n \n  \n      \n    (Un-)\nGleichheit \n\n    Rangordnung \n    Abstände \n    Verhältnisse \n    natürliche\nEinheit \n  \n \n\n  \n    Nominal \n    X \n     \n     \n     \n     \n  \n  \n    Ordinal \n    X \n    X \n     \n     \n     \n  \n  \n    Intervall \n    X \n    X \n    X \n     \n     \n  \n  \n    Verhältnis \n    X \n    X \n    X \n    X \n     \n  \n  \n    Absolut \n    X \n    X \n    X \n    X \n    X \n  \n\n\n\n\n\n\n\n8.1.1.2 Datentypen und Datenstrukturen\n\nAchtung: Die Unterteilung nach “Datentyp” und “Datenstruktur” sind getreu des Manuals von R. Man stößt in anderen Quellen aber auch auf abweichende Unterteilungen bzw. Benennungen.\n\n\n\n\nDer Datentyp gibt an, um was für Daten es sich handelt, d.h. welche Werte(bereiche) diese haben und welche Operationen wir auf sie anwenden können. Wir nutzen zumeist Zeichen, Wahrheitswerte und Zahlen. Diese werden in R als character, logical, integer und double gespeichert, wobei letztere beiden häufig als numeric zusammengefasst werden.\nDie Datenstruktur bestimmt die Organisation und Speicherung von Daten(typen). In R gibt es z.B. Vektoren, Matrizen, Dataframes, Listen und Faktoren. Beispielsweise können Vektoren und Matrizen jeweils nur einen Datentypen enthalten, während Dataframes mehrere Datentypen enthalten können.\nDatentyp und -struktur sind ausschlaggebend dafür, welche Funktionen wir anwenden können bzw. welchen Output wir bekommen.\n\nBeispiel 1:\nMan kann nur mit numeric deskriptiv-statistische Kennwerte bilden.\nBeispiel 2:\nWenn mir nominal- bzw. ordinalskalierte Daten nicht adäquat kodieren, kann es zu ungewollten Konsequenzen kommen.\nWenn wir z.B. eine Variable \\(X\\) haben, welche eine Gruppenzugehörigkeit mit 1, 2 und 3 kodiert, und diese als Prädiktor in ein Regressionsmodell aufnehmen, dann wird \\(X\\) als kontinuierliche Variable behandelt und wir würden keine separaten Schätzungen für die Mittelwertsdifferenzen der Gruppen bekommen.\n\nNachfolgend finden wir eine Übersicht zu den Möglichkeiten der Kodierung von Merkmalen mit verschiedenen Skalenniveaus.\n\n\n\n\n \n\n_\nArt der Skala:\n\n  \n     \n     \n    Nominal- \n    Ordinal- \n    Intervall- \n    Verhältnis- \n    Absolut- \n  \n \n\n  \n    Datentyp: \n    character \n    X \n    X2 \n     \n     \n     \n  \n  \n    Datentyp: \n    numeric \n    X1 \n    X2 \n    X \n    X \n    X \n  \n\n\n 1 Faktorisieren (unordered factor) notwendig wenn keine Indikatorvariable(n) genutzt 2 Faktorisieren (ordered factor) notwendig\n\n\n\n\n\nMit der Funktion str() können wir uns eine kompakte Übersicht der enthaltenen Variablen (d.h. ihr Datentyp bzw. die Datenstruktur Faktor sowie jeweils die ersten 10 Werte) der Datenstruktur (hier: Dataframe) ausgeben lassen. So können wir schauen, ob die Daten auch entsprechend ihres Messniveaus kodiert sind.\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\nFür den Beispieldatensatz airquality liegen die verhältnisskalierten Variablen Ozone,Solar.R, Wind, Month und Day korrekterweise als integer bzw. numeric vor. Die intervallskalierte Variable Temp liegt auch wie erwartet als integer vor (weil die Temperatur in Fahrenheit und nicht in Kelvin gemessen wurde ist die Variable nicht verhältnisskaliert).\nFür den Datensatz PWE_data schauen wir uns exemplarisch nur die letzten 15 Variablen an:\n\nls.str(PWE_data[,88:102])\n\nage :  num [1:1350] 24 66 17 23 19 40 50 21 16 27 ...\neducation :  num [1:1350] 3 2 2 2 2 4 4 2 1 2 ...\nengnat :  num [1:1350] 1 1 2 1 1 1 1 1 1 1 ...\nfamilysize :  num [1:1350] 2 5 4 3 2 3 3 1 3 2 ...\ngender :  num [1:1350] 1 2 2 2 2 2 1 3 2 1 ...\nhand :  num [1:1350] 1 2 1 2 1 1 1 3 3 1 ...\nmajor :  chr [1:1350] \"Computer\" NA NA \"dietetics\" NA \"Philosophy\" \"engineering\" ...\nmarried :  num [1:1350] 1 3 1 1 1 3 2 1 1 2 ...\norientation :  num [1:1350] 1 1 1 1 1 2 1 5 2 1 ...\nrace :  num [1:1350] 11 16 16 16 16 16 16 17 16 16 ...\nreligion :  num [1:1350] 6 6 2 2 1 7 6 4 1 2 ...\nscreenh :  num [1:1350] 1080 640 1024 1080 615 ...\nscreenw :  num [1:1350] 1920 360 1280 1920 1093 ...\nurban :  num [1:1350] 2 3 2 2 1 3 1 2 3 2 ...\nvoted :  num [1:1350] 2 2 2 2 2 2 1 2 2 1 ...\n\n\nWir nutzen hier für PWE_data die Funktion ls.str(), weil der Output für ein Tibble Dataframe so weniger ausführlich ist. Allerdings werden die Variablen mit dieser Funktion alphabetisch sortiert (im Gegensatz zu str(), welche die Variablen der Spaltennummerierung nach darstellt).\nWie wir sehen, liegen fast alle Variablen als numeric vor, obwohl viele nominalskaliert sind. So würden sie von R nicht entsprechend ihres Messniveaus behandelt werden. Wir müssen diese also entweder in character umwandeln oder faktorisieren (später mehr dazu).\n\n\n\n8.1.2 Dataframe\n\n\n\nFür viele Anwendungen in R (z.B. für das [Erstellen von Grafiken mit ggplot2][Grafiken ggplot]) ist es notwendig, dass der Datensatz als Dataframe vorliegt. Folgendermaßen können wir prüfen, ob ein Datensatz ein Dataframe ist:\n\nis.data.frame(airquality)\n\n[1] TRUE\n\nis.data.frame(PWE_data)\n\n[1] TRUE\n\n\n\n\nWas genau ist ein Dataframe?\n\nEin Dataframe ist eine Datenstruktur, die es uns erlaubt, Daten tabellarisch zu speichern. Der Dataframe ist eine Liste aus Vektoren mit gleicher Länge. Listen können (im Gegensatz zu Matrizen) Variablen mit unterschiedlichen Datentypen speichern.\nMehr Informationen gibt es im vorhergehenden Abschnitt zu Datentypen und Datenstrukturen.\n\n\n\n\nFalls unser Datensatz kein Dataframe ist, können wir ihn folgendermaßen umwandeln:\n\nairquality <- as.data.frame(airquality)\n\nDie Funktion as.data.frame() enthält das Argument stringsAsFactors, mit dem wir bestimmen können, ob Zeichenketten (character) zu Faktoren umgewandelt werden sollen (TRUE). Falls wir ordinalskalierte Daten haben, müssen wir den Faktor dann aber noch zusätzlich ordnen. Generell ist es sinnvoll, erst nach der Überprüfung auf unplausible und fehlende Werte zu faktorisieren, damit eventuell vorhandene Fehlkodierungen nicht als Faktorstufen behandelt werden.\n\n\n8.1.3 Plausibilitäts-Check\nIm nächsten Schritt lohnt es sich zu überprüfen, ob in den vorangegangen Schritten der Erhebung und Kodierung unserer Daten, Fehler passiert sind. Das ist wichtig damit wir solche Fehler nicht in unsere Analyse übertragen, wo sie sehr viel unwahrscheinlicher auffallen werden. Dafür überprüfen wir, ob die Messniveaus und Ausprägungen unserer interessierenden Variablen auch unseren Erwartungen entsprechen.\nEntweder man hat die Daten selbst erhoben und somit Wissen darüber, welche Werte möglich sind, oder man schaut sich die Dokumentation zu den Daten an.\nFür den Datensatz airquality finden wir die Dokumentation hier.\nKonkretisieren wir einmal einen hypothetischen Fall an der Variable Day. Wir wissen, dass diese mit Zahlen von 1-31 kodiert sein kann. Es wäre also unplausibel, wenn andere Werte (z.B. 0 oder 32) vorliegen würden.\nDas Auftauchen von unplausiblen Werten ist z.B. wahrscheinlicher, wenn Daten manuell digitalisiert (d.h. eingetippt) wurden (z.B. Paper-and-Pencil Tests). Gerade in diesen Fällen sollte man sicher gehen, und die Daten auf unplausible Werte hin überprüfen.\nDie Funktion unique() gibt uns einen Überblick über alle enthaltenen Ausprägungen eines Vektors. Wenn wir diese mit sapply() kombinieren, können wir das für jede Variable im Datensatz anwenden. Wenn wir den Output wiederum erneut an sapply() übergeben, und sort() anwenden, werden die Ausprägungen aufsteigend sortiert (weil der Default decreasing = FALSE ist), was die Überprüfung erleichtert. In sort() können wir außerdem na.last=TRUE nutzen, um uns das Vorhandensein von NAs am Ende der Ausprägungen anzeigen zu lassen.\n\nsapply(sapply(airquality, unique), sort, na.last=TRUE)\n\n$Ozone\n [1]   1   4   6   7   8   9  10  11  12  13  14  16  18  19  20  21  22  23  24\n[20]  27  28  29  30  31  32  34  35  36  37  39  40  41  44  45  46  47  48  49\n[39]  50  52  59  61  63  64  65  66  71  73  76  77  78  79  80  82  84  85  89\n[58]  91  96  97 108 110 115 118 122 135 168  NA\n\n$Solar.R\n  [1]   7   8  13  14  19  20  24  25  27  31  36  37  44  47  48  49  51  59\n [19]  64  65  66  71  77  78  81  82  83  91  92  95  98  99 101 112 115 118\n [37] 120 127 131 135 137 138 139 145 148 149 150 153 157 167 175 183 186 187\n [55] 188 189 190 191 192 193 194 197 201 203 207 212 213 215 220 222 223 224\n [73] 225 229 230 236 237 238 242 244 248 250 252 253 254 255 256 258 259 260\n [91] 264 266 267 269 272 273 274 275 276 279 284 285 286 287 290 291 294 295\n[109] 299 307 313 314 320 322 323 332 334  NA\n\n$Wind\n [1]  1.7  2.3  2.8  3.4  4.0  4.1  4.6  5.1  5.7  6.3  6.9  7.4  8.0  8.6  9.2\n[16]  9.7 10.3 10.9 11.5 12.0 12.6 13.2 13.8 14.3 14.9 15.5 16.1 16.6 18.4 20.1\n[31] 20.7\n\n$Temp\n [1] 56 57 58 59 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81\n[26] 82 83 84 85 86 87 88 89 90 91 92 93 94 96 97\n\n$Month\n[1] 5 6 7 8 9\n\n$Day\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31\n\n\nEinen kompakten Überblick über die Verteilung der Variablen können wir mit der Funktion summary() bekommen. Hierbei interessieren uns vor allem die Extremwerte (Min. und Max.), d.h. der Range der Variablen, und die Missings (NA).\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nInsgesamt sehen die Daten plausibel aus. Für eine spezifischere Einschätzung der Wetter-Variablen (Ozone, Solar.R, Wind und Temp) könnte man sich zusätzlich Vergleichsdaten von anderen Erhebungen in einem ähnlichen Zeitraum und Gebiet anschauen.\nFür den Datensatz PWE_data schauen wir uns hier wieder nur einige Variablen an. Informationen zu den Variablen finden wir in der Codebook, welches sich im Ordner PWE_data befindet.\n\nsapply(sapply(PWE_data[,88:101], unique), sort)\n\n$education\n[1] 0 1 2 3 4\n\n$urban\n[1] 0 1 2 3\n\n$gender\n[1] 0 1 2 3\n\n$engnat\n[1] 0 1 2\n\n$age\n [1] 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37\n[26] 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62\n[51] 63 64 65 66 67 69 70 71 72 74 75 76 77 81 90\n\n$screenw\n [1]    0  320  347  360  375  393  396  412  414  424  455  486  505  570  586\n[16]  600  601  640  720  768  780  800  810  834  864  948  960  962 1024 1080\n[31] 1093 1111 1138 1152 1200 1242 1280 1300 1348 1360 1364 1366 1368 1376 1400\n[46] 1422 1423 1440 1477 1493 1500 1504 1536 1541 1600 1676 1680 1707 1821 1824\n[61] 1920 2048 2400 2560 2561 3072 3840\n\n$screenh\n [1]    0  320  347  360  414  480  486  505  568  569  570  576  592  597  601\n[16]  614  615  618  640  648  658  667  672  692  698  702  720  731  732  736\n[31]  740  741  744  747  748  752  753  758  759  760  762  768  774  780  786\n[46]  800  803  808  809  812  819  820  831  846  848  853  854  863  864  866\n[61]  869  879  888  892  896  900  902  912  933  943  945  960  962 1000 1003\n[76] 1022 1024 1050 1080 1112 1152 1194 1200 1280 1350 1366 1440 1536 1728 1920\n[91] 2160\n\n$hand\n[1] 0 1 2 3\n\n$religion\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12\n\n$orientation\n[1] 0 1 2 3 4 5\n\n$race\n[1] 10 11 12 13 14 15 16 17\n\n$voted\n[1] 0 1 2\n\n$married\n[1] 0 1 2 3\n\n$familysize\n [1]  0  1  2  3  4  5  6  7  8  9 10 12 13 14 33\n\n\nEs fällt auf, dass alle dargestellten Variablen (bis auf age), die Ausprägung 0 enthalten, obwohl diese im Codebook für diese Variablen nicht definiert ist. Auch für die anderen Variablen im Datensatz, mit Ausnahme von VCL1, …, VCL16, gilt, dass 0 nicht als mögliche Ausprägung gegeben wird, obwohl sie vorhanden ist. Wir können daher annehmen, dass 0 wahrscheinlich eine alternative Kodierung für NA ist. Besser wäre es natürlich, wenn wir die Wissenschaftler*innen, welche die Daten erhoben haben, kontaktieren und nachfragen würden.\nWie wir sehen, geht die Überprüfung von plausiblen und fehlenden Werten häufig ineinander über. Werte, die außerhalb des Ranges der betrachteten Variable liegen, können eine Kodierung für fehlende Werte darstellen.\nWenn wir unplausible Werte in unseren Daten finden, können wir zu 5. Kodierung ändern springen, und diese umkodieren. Wenn wir fehlkodierte Missings finden, können wir auch case_when() oder alternative Möglichkeiten aus dem Abschnitt [Sind die Missings einheitlich kodiert][Sind die Missings einheitlich kodiert?] des Kapitels Fehlende Werte nutzen (z.B. die im Folgenden illustrierte Umkodierung des gesamten Datensatzes).\nWeil wir in PWE_data sehr viele Variablen haben und es zu umständlich wäre, alle einzeln umzukodieren, kodieren wir erst im gesamten Datensatz 0 zu NA um, und ändern danach wieder die Kodierung für die Variablen VCL1 bisVCL16.\n\n# Umkodierung für gesamten Datensatz\nPWE_data[PWE_data == 0] <- NA\n\n# \"Rückkodierung\" für Variablen, die regulär 0 enthalten\n# library(dplyr)\nPWE_data$VCL1 <- case_when(is.na(PWE_data$VCL1) ~ 0, # Umkodierung von NA zu 0\n                           PWE_data$VCL1 == 1 ~ 1) # bleibt gleich\nPWE_data$VCL2 <- case_when(is.na(PWE_data$VCL2) ~ 0, PWE_data$VCL2 == 1 ~ 1)\nPWE_data$VCL3 <- case_when(is.na(PWE_data$VCL3) ~ 0, PWE_data$VCL3 == 1 ~ 1)\nPWE_data$VCL4 <- case_when(is.na(PWE_data$VCL4) ~ 0, PWE_data$VCL4 == 1 ~ 1)\nPWE_data$VCL5 <- case_when(is.na(PWE_data$VCL5) ~ 0, PWE_data$VCL5 == 1 ~ 1)\nPWE_data$VCL6 <- case_when(is.na(PWE_data$VCL6) ~ 0, PWE_data$VCL6 == 1 ~ 1)\nPWE_data$VCL7 <- case_when(is.na(PWE_data$VCL7) ~ 0, PWE_data$VCL7 == 1 ~ 1)\nPWE_data$VCL8 <- case_when(is.na(PWE_data$VCL8) ~ 0, PWE_data$VCL8 == 1 ~ 1)\nPWE_data$VCL9 <- case_when(is.na(PWE_data$VCL9) ~ 0, PWE_data$VCL9 == 1 ~ 1)\nPWE_data$VCL10 <- case_when(is.na(PWE_data$VCL10) ~ 0, PWE_data$VCL10 == 1 ~ 1)\nPWE_data$VCL11 <- case_when(is.na(PWE_data$VCL11) ~ 0, PWE_data$VCL11 == 1 ~ 1)\nPWE_data$VCL12 <- case_when(is.na(PWE_data$VCL12) ~ 0, PWE_data$VCL12 == 1 ~ 1)\nPWE_data$VCL13 <- case_when(is.na(PWE_data$VCL13) ~ 0, PWE_data$VCL13 == 1 ~ 1)\nPWE_data$VCL14 <- case_when(is.na(PWE_data$VCL14) ~ 0, PWE_data$VCL14 == 1 ~ 1)\nPWE_data$VCL15 <- case_when(is.na(PWE_data$VCL15) ~ 0, PWE_data$VCL15 == 1 ~ 1)\nPWE_data$VCL16 <- case_when(is.na(PWE_data$VCL16) ~ 0, PWE_data$VCL16 == 1 ~ 1)\n\n\n\n\n\n\nDie Funktion case_when() wird im Abschnitt 5. Kodierung ändern ausführlich erklärt.\n\nNun schauen wir uns noch die Verteilungen der Variablen an.\n\nsummary(PWE_data[,88:101])\n\n   education         urban           gender          engnat     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :3.000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :2.653   Mean   :2.135   Mean   :1.528   Mean   :1.283  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :4.000   Max.   :3.000   Max.   :3.000   Max.   :2.000  \n NA's   :17      NA's   :19      NA's   :7       NA's   :2      \n      age           screenw        screenh            hand      \n Min.   :13.00   Min.   : 320   Min.   : 320.0   Min.   :1.000  \n 1st Qu.:20.00   1st Qu.: 414   1st Qu.: 736.0   1st Qu.:1.000  \n Median :26.00   Median :1366   Median : 800.0   Median :1.000  \n Mean   :29.74   Mean   :1169   Mean   : 850.3   Mean   :1.165  \n 3rd Qu.:36.00   3rd Qu.:1536   3rd Qu.: 948.8   3rd Qu.:1.000  \n Max.   :90.00   Max.   :3840   Max.   :2160.0   Max.   :3.000  \n                 NA's   :2      NA's   :2        NA's   :5      \n    religion       orientation         race           voted     \n Min.   : 1.000   Min.   :1.000   Min.   :10.00   Min.   :1.00  \n 1st Qu.: 2.000   1st Qu.:1.000   1st Qu.:16.00   1st Qu.:1.00  \n Median : 4.000   Median :1.000   Median :16.00   Median :2.00  \n Mean   : 4.452   Mean   :1.606   Mean   :15.43   Mean   :1.51  \n 3rd Qu.: 6.000   3rd Qu.:2.000   3rd Qu.:16.00   3rd Qu.:2.00  \n Max.   :12.000   Max.   :5.000   Max.   :17.00   Max.   :2.00  \n NA's   :15       NA's   :25                      NA's   :11    \n    married        familysize    \n Min.   :1.000   Min.   : 1.000  \n 1st Qu.:1.000   1st Qu.: 2.000  \n Median :1.000   Median : 2.000  \n Mean   :1.358   Mean   : 2.616  \n 3rd Qu.:2.000   3rd Qu.: 3.000  \n Max.   :3.000   Max.   :33.000  \n NA's   :6       NA's   :28      \n\n\nHier sehen wir auch, dass für nominalskalierte Merkmale, wie z.B. urban, orientation und married, deskriptiv-statistische Kennwerte wie der Mittelwert gebildet werden (d.h. diese werden als mindestens intervallskaliert behandelt), weil sie als numeric vorliegen. Später werden wir diese noch faktorisieren.\n\n\n8.1.4 Fehlende Werte\nGenerell werden fehlende Werte (Missings) in R mit NA dargestellt. In anderen Programmen mag das anders sein (z.B. werden Missings in Unipark mit 99 oder -99 kodiert). Wie im vorhergehenden Abschnitt demonstriert, überschneidet sich die Überprüfung von plausiblen und fehlenden Werten häufig.\nNeben der im letzten Abschnitt vorgestellten Varianten, Missings mit summary() zu finden, gibt es noch weitere Optionen.\nBeispielsweise können wir mit der Kombination von colSums() und is.na() spaltenweise Missings zählen.\n\ncolSums(is.na(airquality))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\n\n\ncolSums(is.na(PWE_data))\n\n         Q1A          Q1I          Q1E          Q2A          Q2I          Q2E \n           1            1            1            1            1            1 \n         Q3A          Q3I          Q3E          Q4A          Q4I          Q4E \n           1            1            1            1            1            1 \n         Q5A          Q5I          Q5E          Q6A          Q6I          Q6E \n           1            1            1            1            1            1 \n         Q7A          Q7I          Q7E          Q8A          Q8I          Q8E \n           1            1            1            1            1            1 \n         Q9A          Q9I          Q9E         Q10A         Q10I         Q10E \n           1            1            1            1            1            1 \n        Q11A         Q11I         Q11E         Q12A         Q12I         Q12E \n           1            1            1            1            1            1 \n        Q13A         Q13I         Q13E         Q14A         Q14I         Q14E \n           1            1            1            1            1            1 \n        Q15A         Q15I         Q15E         Q16A         Q16I         Q16E \n           1            1            1            1            1            1 \n        Q17A         Q17I         Q17E         Q18A         Q18I         Q18E \n           1            1            1            1            1            1 \n        Q19A         Q19I         Q19E      country  introelapse   testelapse \n           1            1            1            0            0            0 \nsurveyelapse        TIPI1        TIPI2        TIPI3        TIPI4        TIPI5 \n           0            8           10           13            9            9 \n       TIPI6        TIPI7        TIPI8        TIPI9       TIPI10         VCL1 \n           8            9            8            8           13            0 \n        VCL2         VCL3         VCL4         VCL5         VCL6         VCL7 \n           0            0            0            0            0            0 \n        VCL8         VCL9        VCL10        VCL11        VCL12        VCL13 \n           0            0            0            0            0            0 \n       VCL14        VCL15        VCL16    education        urban       gender \n           0            0            0           17           19            7 \n      engnat          age      screenw      screenh         hand     religion \n           2            0            2            2            5           15 \n orientation         race        voted      married   familysize        major \n          25            0           11            6           28          448 \n\n\n\nAchtung: Wenn wir Variablen mit Missings für unsere Analysen nutzen wollen, sollten wir überprüfen, ob die Missings zufällig sind und in Abhängigkeit davon unseren Umgang anpassen, um systematischen Verzerrungen der Analysen entgegenzuwirken.\n\nEinen ausführlichen Überblick zu Missings finden wir im Kapitel Fehlende Werte.\n\n\n8.1.5 Faktorisieren\nWir schauen uns das Faktorisieren exemplarisch an zwei Variablen aus dem Datensatz PWE_data an:\n\nnominalskaliert: gender\n\n“What is your gender?”: 1 = Male, 2 = Female, 3 = Other\n\nordinalskaliert: education\n\n“How much education have you completed?”: 1 = Less than high school, 2 = High school, 3 = University degree, 4 = Graduate degree.\n\n\n\n\n\n\n\nZuerst erstellen wir einen (neuen) unsortierten (d.h. nominalskalierten) Faktor. Dafür benötigen wir nur die Funktion factor(), der wir den zu faktorisierenden Vektor übergeben.\n\n# faktorisieren (unsortiert)\nPWE_data$gender_uf <- factor(PWE_data$gender)\n\nNun erstellen wir einen (neuen) sortierten (d.h. ordinalskalierten) Faktor. Dafür ergänzen wir das Argument ordered=TRUE.\n\n# faktorisieren (sortiert; natürliche Sortierung)\nPWE_data$education_of <- factor(PWE_data$education, ordered=TRUE)\n\nMit dem Argument ordered=TRUE wird eine Variable nach ihrer “natürlichen” Rangfolge sortiert. Bei Zahlen (integer und numeric) bedeutet das, dass größere Zahlen eine höhere Hierarchieebene haben z.B. 1 < 2. Bei einzelnen Buchstaben und Zeichenketten (character) bedeutet das, dass später im Alphabet auftauchende (Anfangs-)Buchstaben eine höhere Hierarchiebene haben z.B. “Hans” < “Rene”.\nManchmal wollen wir diese Sortierung aber nicht übernehmen, sondern eine eigene Hierarchie erstellen, die nicht der natürlichen Rangfolge entspricht. Das können wir machen, indem wir zusätzlich das Argument levels spezifizieren, dem wir einen Vektor mit unserer gewünschten Sortierung übergeben.\n\n# faktorisieren (sortiert; eigene \"non-sense\" Sortierung)\nPWE_data$education_of_s <- factor(PWE_data$education, ordered=TRUE,\n                                levels=c(1,4,2,3))\n\nAbschließend vergleichen wir die ursprüngliche numeric-Variable (education) mit den unsortierten (education_uf) und sortierten (education_of und education_of_s) Faktor-Variablen.\n\nls.str(PWE_data[,c(88, 90, 103:105)])\n\neducation :  num [1:1350] 3 2 2 2 2 4 4 2 1 2 ...\neducation_of :  Ord.factor w/ 4 levels \"1\"<\"2\"<\"3\"<\"4\": 3 2 2 2 2 4 4 2 1 2 ...\neducation_of_s :  Ord.factor w/ 4 levels \"1\"<\"4\"<\"2\"<\"3\": 4 3 3 3 3 2 2 3 1 3 ...\ngender :  num [1:1350] 1 2 2 2 2 2 1 3 2 1 ...\ngender_uf :  Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 2 2 2 2 1 3 2 1 ...\n\n\nWir sehen, dass alle Variablen des gleichen Merkmals zwar die gleichen Werte (gender: 1, 2, 3 und education: 1, 2, 3, 4) haben, aber in unterschiedlichen Datentypen bzw. -strukturen (numeric, factor, Ordered factor) und teils unterschiedlichen Sortierungen vorliegen.\nAußerdem sehen wir, dass die selbst sortierten Faktoren intern eine neue Kodierung bekommen haben (siehe education_of_s). Wir sehen diese nur mit str() bzw. ls.str(). Diese interne Kodierung richtet sich danach, wie die Faktorstufen sortiert sind. Die erste Ausprägung (nach der eigenen Sortierung) beginnt mit 1.\n\n\n8.1.6 Wide- und Long-Format\nIn Abhängigkeit unserer Daten und der Analyse, die wir durchführen wollen, ist es ggf. erforderlich, dass unsere Daten in ein anderes Tabellenformat überführt werden müssen. Es gibt das Wide- und das Long-Format.\nDie Unterscheidung von Wide- und Long-Format ist von Bedeutung, wenn unsere Daten eine genestete Struktur aufweisen, das heisst jeweils mehrere Messungen von derselben Untersuchungseinheit vorliegen (z.B. bei Längsschnitterhebungen, mehrere Ratern oder Schülern in Klassen).\nIm Wide-Format liegen Messungen einer Untersuchungseinheit in einer Zeile vor. Jeder Messzeitpunkt bzw. jede Messung ist eine eigene Variable.\n\nBeispiel 1 Wide-Format: Messzeitpunkte\n\n\n\n\n \n  \n    Untersuchungseinheit \n    t1 \n    t2 \n    t3 \n  \n \n\n  \n    1 \n    4 \n    3 \n    1 \n  \n  \n    2 \n    5 \n    2 \n    3 \n  \n\n\n\n\n\n\n\nUntersuchungseinheit 1\nUntersuchungseinheit 2\n Messwiederholung \nRater\n\nBeispiel 2 Wide-Format: Rater\n\n\n\n\n \n  \n    Untersuchungseinheit \n    self \n    friend \n    parent \n  \n \n\n  \n    1 \n    2 \n    1 \n    3 \n  \n  \n    2 \n    3 \n    4 \n    2 \n  \n\n\n\n\n\nIm Long-Format liegen Messungen einer Untersuchungseinheit in mehreren Zeilen vor. Alle Messzeitpunkte bzw. Messungen von unterschiedlichen Ratern liegen in einer Variable vor und die Messzeitpunkte bzw. Rater werden in einer separaten Variable kodiert.\nBeispiel 1 Long-Format: Messzeitpunkte\n\n\n\n\n \n  \n    Untersuchungseinheit \n    Zeitpunkt \n    Messung \n  \n \n\n  \n    1 \n    1 \n    4 \n  \n  \n    1 \n    2 \n    3 \n  \n  \n    1 \n    3 \n    1 \n  \n  \n    2 \n    1 \n    5 \n  \n  \n    2 \n    2 \n    2 \n  \n  \n    2 \n    3 \n    3 \n  \n\n\n\n\n\nBeispiel 2 Long-Format: Rater\n\n\n\n\n \n  \n    Untersuchungseinheit \n    Rater \n    Messung \n  \n \n\n  \n    1 \n    self \n    2 \n  \n  \n    1 \n    friend \n    1 \n  \n  \n    1 \n    parent \n    3 \n  \n  \n    2 \n    self \n    3 \n  \n  \n    2 \n    friend \n    4 \n  \n  \n    2 \n    parent \n    2 \n  \n\n\n\n\n\nIm Beispieldatensatz PWE_data gibt es keine wiederholte Messungen. Psychometrische und demographische Daten wurden einmalig erhoben. Hierfür gibt es keine Notwendigkeit der Formatierung vom Long- ins Wide-Format oder vice-versa.\nIm Beispieldatensatz airquality gibt es wiederholte Messungen der Untersuchungseinheiten (Ozone, Solar.R, Wind und Temp) zu unterschiedlichen Zeiten, die in Month und Day kodiert werden. Jede dieser Untersuchungseinheiten liegt in mehreren Zeilen vor. Es handelt sich folglich um einen Datensatz im Long-Format. Im Wide-Format hätten wir z.B. die Variablen Ozone_5_1 (Monat 5, Tag 1), Ozone_5_2 (Monat 5, Tag 2), …, Solar.R_5_1 (Monat 5, Tag 1), etc. Je nachdem, wie wir die Daten auswerten wollen, ist es notwendig bzw. nicht notwendig, die Daten umzuformatieren.\nIm Kapitel zum (Wide- und Long-Format) erfahren wir, wie wir beide Formate ineinander überführen können. Hierzu werden jeweils zwei Möglichkeiten vorgestellt: reshape() aus dem Standardpaket stats und spread() bzw. gather() aus dem Paket tidyr."
  },
  {
    "objectID": "Datenvorbereitung.html#datensätze-zusammenführen",
    "href": "Datenvorbereitung.html#datensätze-zusammenführen",
    "title": "8  Datenvorbereitung",
    "section": "8.2 Datensätze zusammenführen",
    "text": "8.2 Datensätze zusammenführen\nSynonyme: Mergen, Fusionieren, Integrieren\n\nNicht immer haben wir das Glück, dass die für uns relevanten Daten in einem gemeinsamen Dataframe vorliegen. Daher schauen wir uns nachfolgend an, wie man Dataframes zusammenführen kann. Es gibt dabei zwei Szenarien, die man unterscheiden kann:\n\ndie selben Variablen von verschiedenen Fällen\n\nz.B. von einer erneuten Aufnahme von Personen in eine Studie\nhier werden die Zeilen “ergänzt”\n\nverschiedene Variablen von den selben Fällen\n\nz.B. wenn einzelne Abschnitte einer Studie (Tests, Fragebögen) in unterschiedlichen Datensätzen gespeichert wurden\nhier werden die Spalten “ergänzt”\n\n\nWir schauen uns wieder Funktionen aus zwei verschiedenen Paketen an: merge() aus dem Basispaket base und bind_rows() bzw. die _join()-Funktionen aus dem Zusatzpaket dplyr.\nWir nutzen dafür die Datensätze vornamen_13 und vornamen_14, in denen die Vornamen der Neugeborenen in München, jeweils für die Jahre 2013 und 2014 enthalten sind. Diese Datensätze eignen sich für beide Szenarien, weil sowohl dieselben Variablen (vorname, anzahl und geschlecht) als auch dieselben Fälle (d.h. Vornamen) in beiden Datensätzen vorkommen (z.B. Maximilian). Die Untersuchungseinheiten sind hier also nicht einzelne Personen, sondern Vornamen. Was bei der Untersuchung einzelner Personen die ID-Variable ist, ist hier die Variable vorname.\n\nEingangs wurde gezeigt, wie wir diese Datensätze herunterladen.\n\n\nAchtung: Es kann sein, dass wir einen Dataframe nach dem Zusammenführen noch in ein anderes Format überführen müssen, um unsere Auswertung durchführen zu können (siehe Wide- und Long-Format).\n\n\n8.2.1 Selbe Variablen, unterschiedliche Fälle\nDie beiden nachfolgend vorgestellten Funktionen unterscheiden sich bezüglich einiger Funktionalitäten.\nEin wichtiger Unterschied ist, dass merge() beim Zusammenführen der Dataframes gleiche Fälle (d.h. Fälle mit gleichen Ausprägungen in den Variablen) nur einmalig übernimmt (d.h. Dopplungen löscht) während bind_rows() alle Fälle übernimmt, auch wenn sich diese doppeln.\nIn Abhängigkeit der geplanten Nutzung der Daten sollten wir individuell entscheiden, welche Funktion wir nutzen wollen.\n\n\nmerge()\n\nMit merge() können wir zwei Dataframes, deren Namen wir der Funktion übergeben, vertikal zusammenführen.\nWir müssen dabei unbedingt all = TRUE spezifizieren, weil der Default (all = FALSE) nur Zeilen behält, die in beiden Dataframes mit genau der gleichen Ausprägung auf den Variablen vorhanden sind (und von diesen jeweils nur eine Version).\nMit all.x = TRUE bzw. all.y = TRUE würden wir, neben den Fällen mit den gleichen Ausprägungen in beiden Datensätzen, auch alle nur im ersten bzw. zweiten Datensatz enthaltenen Fälle behalten.\n\nvornamen_merge_row <- merge(vornamen_13, vornamen_14, all = TRUE)\n\nDie Reihenfolge der Zeilen im gemeinsamen Dataframe richtet sich nach der natürlichen Sortierung der ersten Variable (im zuerst übergebenen Datensatz). Für unser Beispiel sind die Vornamen nach dem Alphabet (beginnend mit “A”) sortiert.\nMit der Funktion dim() können wir überprüfen, wie viele Zeilen und Spalten unser Dataframe beinhaltet.\n\ndim(vornamen_merge_row)\n\n[1] 7496    3\n\n\nHier sehen wir den eingangs erwähnten Unterschied von merge() und bind_rows(). vornamen_13 beinhaltet 4012 und vornamen_14 4032 Zeilen. Insgesamt würden wir also 8044 erwarten. Die Funktion übernimmt aber bei komplett gleichen Fällen (d.h. gleichen Ausprägungen auf allen Variablen) in den beiden Dataframes nur eine Version (so dass es keine Dopplung gibt).\nBeispielsweise kommt folgender Fall in beiden Dataframes vor:\n\n\n     vorname anzahl geschlecht\n1310  Aadhya      1          w\n\n\n\n\n     vorname anzahl geschlecht\n2704  Aadhya      1          w\n\n\nSo verschwindet die gleiche Anzahl an Fällen, die wir mit dem Default-Verhalten der Funktion (all = FALSE) behalten hätten.\n\nAchtung: Wir sollten mit merge() demnach auch nur Dataframes zusammenführen, die genau dasselbe Set an Variablen haben (d.h. ein Dataframe sollte nicht noch eine zusätzliche Variable besitzen) oder wir sollten eine ID-Variable haben, deren IDs nur einmal vorkommen (sowohl innerhalb eines Dataframes als auch zwischen den Dataframes). Sonst kann es zum ungewollten Nicht-Übernehmen von Fällen kommen.\n\nSchauen wir uns das Problem an einem Beispiel an:\nNehmen wir an, dass ein Dataframe x eine zusätzliche Variable a hat. Für die Fälle des anderen Dataframe y würden auf a im zusammengeführten Objekt nur fehlende Werte (NA) stehen. So würde die Funktion solche Fälle (im Vergleich der beiden Dataframes), die bis auf die Variable a die gleichen Ausprägungen haben, nicht übernehmen.\nx und y haben jeweils drei Fälle. x hat drei Variablen; y hat zwei.\n\n\n\n\nx\n\n  c b a\n1 5 5 5\n2 6 6 6\n3 7 7 7\n\n\n\ny\n\n  c b\n1 5 7\n2 7 5\n3 6 6\n\n\n\nmerge_xy <- merge(x, y, all = TRUE) \nmerge_xy\n\n  c b  a\n1 5 5  5\n2 5 7 NA\n3 6 6  6\n4 7 5 NA\n5 7 7  7\n\n\nMan könnte denken, dass 6 Fälle in merge_xy zu finden sind. Weil aber jeweils ein Fall (x: Zeile 2; y Zeile 3) bis auf a die gleichen Ausprägungen in den beiden Dataframes hat, wird dieser nicht mit übernommen.\n\n\n\nbind_rows()\n\n\n\n\nDer Funktion bind_rows() übergeben wir einfach die Dataframes, die wir vertikal aneinander reihen wollen. Die Reihenfolge der übergebenen Dataframes entscheidet dabei auch über die Reihenfolge der Zeilen des zusammengeführten Dataframes (z.B. erst alle Zeilen von vornamen_13, dann von vornamen_14).\nOptional können wir der Funktion das Argument .id übergeben, mit dem wir eine ID-Variable erstellen, welche die Datensätze kodiert (beginnend mit 1).\n\n# library(dplyr)\nvornamen_bind <- bind_rows(vornamen_13, vornamen_14, .id = \"id\")\n\n\n\n\n\n \n  \n      \n    id \n    vorname \n    anzahl \n    geschlecht \n  \n \n\n  \n    1 \n    1 \n    Maximilian \n    166 \n    m \n  \n  \n    4013 \n    2 \n    Maximilian \n    178 \n    m \n  \n\n\n\n\n\nHier sehen wir die ID-Variable. Von den Zeilen 1 bis 4012 ist diese 1; von den Zeilen 4012 bis 8044 ist sie 2.\nEs ist auch möglich, der Funktion mehr als zwei Dataframes zu übergeben, welche in einem gemeinsamen Dataframe gespeichert werden.\nWir können beliebig viele Dataframes mit bind_rows() zusammenführen, solange diese mindestens eine gemeinsame Variable haben.\nZur Überprüfung schauen wir uns mit dim() wieder die Anzahl der Zeilen und Spalten an.\n\ndim(vornamen_bind)\n\n[1] 8044    4\n\n\nDie Anzahl der Zeilen stimmt mit der Summe der Zeilen von vornamen_13 und vornamen_14 überein, d.h. es wurden alle Fälle übernommen.\n\n\n\n8.2.2 Unterschiedliche Variablen, selbe Fälle\nIm Gegensatz zu merge() und bind_rows() unterscheiden sich merge(..., by) und die _join()-Funktionen nicht in ihrer Funktionalität, sondern nur in der Reihenfolge der Fälle im zusammengeführten Dataframe.\nAllerdings kann man die _join()-Funktionen auch alternativ zu bind_rows() nutzen. Dann unterscheidet sich das Ergebnis im Vergleich zu merge() auch lediglich in der Reihenfolge der Fälle. Mehr Infos dazu finden wir im Abschnitt [_join()].\n\n\nmerge(..., by)\n\nDie Funktion merge() können wir auch nutzen, um Spalten aneinander zu heften.\nMit dem Argument by geben wir an, welche ID-Variable die (selben) Fälle kodiert.\nWenn es unterschiedliche Benennungen der gleichen ID-Variablen in den beiden Datensätzen gibt, müssen wir by.x und by.y nutzen. Die Benennung von by.x wird dann übernommen.\nx und y spielen auf die Reihenfolge an, in welcher wir die Datensätze an die Funktion merge() übergeben. x ist der zuerst übergebene Datensatz; y der als zweites übergebene.\nWir wollen auch hier wieder die Daten aus beiden Dataframes übernehmen, und geben das mit all = TRUE an.\nWir könnten aber hier ebenso all.x = TRUE bzw. all.y = TRUE oder all = FALSE nutzen.\n\nvornamen_merge_col <- merge(vornamen_13, vornamen_14, by = \"vorname\", all = TRUE)\n\n\ndim(vornamen_merge_col)\n\n[1] 6371    5\n\n\nNun schauen wir uns einmal die (ersten 6 Fälle der) neu erstellten Variablen an.\n\n\n\n\n \n  \n    vorname \n    anzahl.x \n    geschlecht.x \n    anzahl.y \n    geschlecht.y \n  \n \n\n  \n    + \n    NA \n    NA \n    9 \n    w \n  \n  \n    + \n    NA \n    NA \n    7 \n    m \n  \n  \n    Aadhavan \n    NA \n    NA \n    2 \n    m \n  \n  \n    Aadhya \n    1 \n    w \n    1 \n    w \n  \n  \n    Aahana \n    NA \n    NA \n    1 \n    w \n  \n  \n    Aahel \n    1 \n    m \n    NA \n    NA \n  \n\n\n\n\n\n\n\nWie kommt man auf die Anzahl der Fälle \\(N = 6371\\)?\n\nIn den beiden Dataframes vornamen_13 und vornamen_14 wurden genau die gleichen drei Variablen (vorname, anzahl und geschlecht) erhoben. Wenn wir die beiden zusammenführen, können drei unterschiedliche Szenarien mit Hinblick auf vorname und geschlecht auftreten.\nNachfolgend schauen wir uns jeweils ein Beispiel sowie die Anzahl der Fälle dieser Szenarien an.\n\nvorname und geschlecht sind in beiden Dataframes gleich\n\n\n\n  vorname anzahl.x geschlecht.x anzahl.y geschlecht.y\n6   Aahel        1            m       NA         <NA>\n\n\n\nnrow(vornamen_merge_col[which(\n  vornamen_merge_col$geschlecht.x == vornamen_merge_col$geschlecht.y),])\n\n[1] 1672\n\n\n\ngeschlecht bei vorname unterscheidet sich zwischen den Dataframes\n\n\n\n    vorname anzahl.x geschlecht.x anzahl.y geschlecht.y\n38 Abdullah        7            m        4            m\n\n\n\nnrow(vornamen_merge_col[which(\n  vornamen_merge_col$geschlecht.x != vornamen_merge_col$geschlecht.y),])\n\n[1] 112\n\n\n\nvorname und geschlecht eines Falles sind nur in einem Dataframe enthalten (für die Daten des anderen sind NAs angegeben)\n\n\nvornamen_merge_col[2,]\n\n  vorname anzahl.x geschlecht.x anzahl.y geschlecht.y\n2       +       NA         <NA>        7            m\n\n\n\ncolSums(is.na(vornamen_merge_col)) # spaltenweise Missings gezählt\n\n     vorname     anzahl.x geschlecht.x     anzahl.y geschlecht.y \n           0         2305         2305         2282         2282 \n\n\nWir können uns hier nur die Missings in den einzelnen Spalten anschauen, um die Häufigkeiten für dieses Szenario zu bekommen, weil es vorher in den einzelnen Dataframes keine Missings gab.\nWenn wir nun alle Werte aufsummieren, kommen wir auf die Anzahl der Zeilen im gemeinsamen Dataframe:\n\n1672 + 112 + 2305 + 2282\n\n[1] 6371\n\n\n\n\n\n\n_join()\n\n\n\n\nDie _join()-Funktionen aus dplyr sind danach differenziert, welche Daten wir aus den Datensätzen übernehmen möchten. Diese Unterscheidung ist analog zu dem Argument all in merge().\nFür Daten aus beiden Datensätzen nutzt man full_join(). Analog zu all = TRUE in merge().\nFür Daten aus dem ersten bzw. zweiten Datensatz und den überlappenden Fällen nutzt man left_join() bzw. right_join(). Analog zu all.x = TRUE bzw. all.y = TRUE in merge().\nFür Daten, die in beiden Datensätzen überlappen nutzt man inner_join(). Analog zu all = FALSE in merge().\n\n# library(dplyr)\nvornamen_join <- full_join(vornamen_13, vornamen_14, by=\"vorname\")\n\nWarning in full_join(vornamen_13, vornamen_14, by = \"vorname\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1292 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nNun überprüfen wir wieder die Dimensionen des neu erstellten Dataframes.\n\ndim(vornamen_join)\n\n[1] 6371    5\n\n\nWir sehen, dass der mit full_join(..., by = \"vorname\") zusammengeführte Datensatz genau die gleichen Dimensionen hat wie der mit merge(..., by = \"vorname\", all = TRUE) zusammengeführte. Die beiden Funktionen unterscheiden sich nur in der Sortierung der Fälle (welcher Dataframe zuerst eingegeben wurde vs. natürliche Sortierung der Fälle).\n\n\n\nfull_join() als Alternative zu bind_rows()\n\nMit full_join(x, y) bekommen wir (bis auf die Sortierung der Fälle) das gleiche Ergebnis wie bei merge(x, y, all = TRUE)\n\nvornamen_join_row <- full_join(vornamen_13, vornamen_14)\n\nJoining with `by = join_by(vorname, anzahl, geschlecht)`\n\n\nZur Demonstration der Übereinstimmung schauen wir uns die Dimensionen und den Aufbau des Dataframes (am Beispiel der ersten 6 Zeilen) an.\n\ndim(vornamen_join_row)\n\n[1] 7496    3\n\n\n\n\n\n\n \n  \n    vorname \n    anzahl \n    geschlecht \n  \n \n\n  \n    Maximilian \n    166 \n    m \n  \n  \n    Felix \n    124 \n    m \n  \n  \n    Anna \n    109 \n    w \n  \n  \n    David \n    109 \n    m \n  \n  \n    Sophia \n    108 \n    w \n  \n  \n    Emilia \n    103 \n    w \n  \n\n\n\n\n\n\ndim(vornamen_merge_row)\n\n[1] 7496    3\n\n\n\n\n\n\n \n  \n    vorname \n    anzahl \n    geschlecht \n  \n \n\n  \n    + \n    7 \n    m \n  \n  \n    + \n    9 \n    w \n  \n  \n    Aadhavan \n    2 \n    m \n  \n  \n    Aadhya \n    1 \n    w \n  \n  \n    Aahana \n    1 \n    w \n  \n  \n    Aahel \n    1 \n    m"
  },
  {
    "objectID": "Datenvorbereitung.html#daten-extrahieren",
    "href": "Datenvorbereitung.html#daten-extrahieren",
    "title": "8  Datenvorbereitung",
    "section": "8.3 Daten extrahieren",
    "text": "8.3 Daten extrahieren\nSynonyme: Splitten, Subsetten, Filtern, Selektieren, Extrahieren\n\nManchmal möchten wir nur bestimmte Variablen bzw. bestimmte Fälle aus einem Datensatz betrachten. Generell bietet es sich an, dafür reguläre Ausdrücke (regular expressions z.B. die Metacharactere .*, |, ^ und $) und logische Operatoren (logical operators z.B. >, < und ==) zu nutzen.\nWie wir Variablen (Spalten) und Fälle (Zeilen) selektieren und in einem neuen Dataframe speichern können, schauen wir uns nun an.\n\n\n\n\n8.3.1 Variablen\nWenn wir nur einige Variablen aus einem bzw. aus mehreren Datensätzen benötigen, können wir diese mit verschiedenen Möglichkeiten entnehmen. Im Folgenden schauen wir uns dafür Möglichkeiten aus dem Standardpacket base und dem Zusatzpaket dplyr an.\nUnten befindet sich eine Übersicht, der wir entnehmen können, welche Methode wir wählen sollten in Abhängigkeit davon, ob die Variablen, die wir extrahieren wollen, ähnlich oder unterschiedlich sind.\n\n\n\n\n \nDie Variablennamen sind ...\n  \n    ... sich ähnlich \n    ... unterschiedlich \n  \n \n\n  \n    `grep()` \n    `$`, `select()` \n  \n  \n    z.B. enthalten den Buchstaben 'o':\n`Ozone`, `Solar.R`, `Month` \n    z.B. `Month` und `Day` \n  \n\n\n\n\n\n\n\ngrep()\n\nWenn Variablen eines Datensatzes eine Gemeinsamkeit (z.B. einen gemeinsamen Wortstamm) aufweisen, können wir diese mit der Funktion grep() extrahieren.\ngrep(pattern, names(Datensatz))\nDie Funktion durchsucht die Namen der Variablen eines Dataframes - names(Datensatz) - nach bestimmten Zahlen- oder Zeichenketten (pattern). Diese müssen wir in \" \" angeben (weil Variablennamen als Character gespeichert werden).\n\n\n\nWir wollen beispielshalber alle Variablen extrahieren, die irgendwo ein o im Namen haben.\n\n# Selektion der Namen\nvar_mit_o <- grep(pattern=\"o\", names(airquality)) \n\n# Anwenden der Selektion auf den Dataframe\ndf_var_mit_o <- airquality[var_mit_o]\n\n\n\n\n\n  \n\n\n\nMit dieser Methode haben wir gleich den Vorteil, dass die Namen der Variablen im neuen Datensatz gleich denen im ursprünglichen Datensatz ist.\nWir können unsere Suche mit grep() auch noch spezifischer machen, indem wir die regulären Operatoren nutzen. Mit ^o suchen wir Variablen, die mit einem “o” beginnen; mit o$ jene die mit “o” enden. Mit ^o|o$ suchen wir Variablen, die entweder mit einem “o” beginnen oder enden. Mit ^o.*o$ suchen wir Variablen, die mit einem “o” beginnen und enden.\n\n\n\n$\n\nEinzelne Variablen, die keine Gemeinsamkeit (z.B. einen gemeinsamen Wortstamm) aufweisen, kann man mit dem $-Operator extrahieren.\nDiesen wendet man an, indem man die Form Data Frame$Variable nutzt. Die Variablen können folglich aus unterschiedlichen Datensätzen stammen, da wir jede Variable jeweils neu ansprechen müssen.\nWir entnehmen die Variablen Wind und Ozone und speichern diese in einem neuen Dataframe.\n\ndf_wind_ozone <- data.frame(airquality$Wind, airquality$Ozone)\n\n\n\n\n\n  \n\n\n\nMit dieser Methode haben wir den Nachteil, dass die Variablen im neu erstellten Dataframe nicht mit ihrem ursprünglichen Namen, sondern in der Form Datensatz.Variable benannt sind.\nWir können den Variablen z.B. colnames(Datensatz) <- c(\"Var1\", \"Var2\", ...) wieder ihren ursprünglichen (oder einen neuen) Namen geben.\n\n\n\nselect()\n\nDie Funktion select() kann unterschiedliche Variablen aus dem selben Dataframe extrahieren. Sie ist dabei kompakter zu handhaben als die Extraktion mit $.\nMan übergibt der Funktion zuerst den Dataframe und anschließend die Namen der Variablen, welche man extrahieren möchte. Man kann diese sogar gleich umbenennen.\nWir erstellen einen neuen Dataframe mit den Variablen Month und Day. Die Variable Month werden wir zu Mon umbenennen.\n\n# library(dplyr)\ndf_month_day <- select(airquality, \n                       Mon = Month, # neuer Name = alter Name\n                       Day)\n\n\n\n\n\n  \n\n\n\nWenn wir bis auf einige wenige Variablen alle übernehmen wollen, können wir das realisieren, indem wir jeweils ein - vor die ungewollten Variablennamen setzen. Wenn der ersten Variable, die wir select() übergeben, ein - vorangestellt wurde, übernimmt die Funktion alle Variablen mit Ausnahme jener, die mit - angegeben werden.\nSchauen wir uns das für den Fall an, dass wir Month und Day aus dem Dataframe entfernen wollen.\n\n# library(dplyr)\ndf_without_month_day <- select(airquality, \n                       -Month, \n                       -Day)\n\n\n\n\n\n  \n\n\n\n\n\n8.3.1.1 Datums-Variablen splitten\n\n\n\nFür den Fall, dass wir eine Datums-Variable in unserem Datensatz haben, welche in einem für uns unangemessenen Format vorliegt, können wir diese mit dem Paket lubridate umformatieren. Auf dieser Seite wird der Umgang mit den im Paket enthaltenen Funktionen ymd() und mdy() erklärt.\n\n\n\n8.3.2 Fälle\nWir schauen uns nachfolgend einige Möglichkeiten der Extraktion von Fällen mit spezifischen Ausprägungen (die man z.B. für eine Subgruppenanalyse benötigt) an. Auch hier schauen wir uns wieder sowohl Funktionen aus dem Standardpaket base als auch aus dem Zusatzpaket dplyr an.\nUnten befindet sich eine Übersicht, der wir entnehmen können, welche Methode wir wählen sollten in Abhängigkeit davon, ob die Fälle, die wir extrahieren wollen, ähnlich oder unterschiedlich sind.\n\n\n\n\n \nDie Ausprägungen der Fälle haben ...\n  \n    ... die selben Zeichen \n    ... einen gemeinsamen Wertebereich \n  \n \n\n  \n    `grep()` \n    logische Operatoren, `filter()` \n  \n  \n    z.B. enthalten die Zahl 6: 67, 86, ... \n    z.B. genau 57 oder >= 15 \n  \n\n\n\n\n\n\n\ngrep()\n\n\n\n\nWenn wir Ausprägungen suchen, die sich nicht durch logische Operatoren, sondern durch Ähnlichkeiten (z.B. ein gleiches Zeichen) filtern lassen, dann können wir dafür grep() nutzen.\ngrep(pattern, x, value)\nDie Funktion durchsucht Elemente eines Vektor (x) nach bestimmten Zahlen- oder Zeichenketten (pattern). Mit grep() werden in Abhängigkeit des Arguments value entweder Indizes (FALSE; voreingestellt), oder konkrete Werte (TRUE) ausgegeben. Die Werte schauen wir uns zur Überprüfung an; die Indizes benötigen wir zur Extraktion jener Fälle aus dem Datensatz.\n\n\n\nUnsere gesuchten Zahlen- oder Zeichenketten, die wir an das Argument pattern übergeben, sowie die ausgegeben Werte, werden immer als Character behandelt und von daher in \" \" ausgegeben.\nWenn eine Ausprägung irgendwo eine bestimmten Zahlen- oder Zeichenketten enthalten soll, geben wir diese einfach ein.\nWir durchsuchen die Variable Temp nach den Tagen, an denen eine 6 im Messwert war.\n\n\n\n\ngrep(\"6\", airquality$Temp) # Indizes\n\n [1]   1   4   5   6   7   9  10  12  13  14  16  17  19  20  23  24  28  31  34\n[20]  49  51  53  54  55  85  88  90  96 103 104 110 118 122 135 140 141 142 144\n[39] 147 148 152 153\n\ngrep(\"6\", airquality$Temp, value=TRUE) # Werte\n\n [1] \"67\" \"62\" \"56\" \"66\" \"65\" \"61\" \"69\" \"69\" \"66\" \"68\" \"64\" \"66\" \"68\" \"62\" \"61\"\n[16] \"61\" \"67\" \"76\" \"67\" \"65\" \"76\" \"76\" \"76\" \"76\" \"86\" \"86\" \"86\" \"86\" \"86\" \"86\"\n[31] \"76\" \"86\" \"96\" \"76\" \"67\" \"76\" \"68\" \"64\" \"69\" \"63\" \"76\" \"68\"\n\n\nWenn eine Ausprägung eine bestimmte Zahlen- oder Zeichenketten zu Beginn enthalten soll, setzen wir ein ^ vor diese.\nWenn wir beispielsweise alle Tage suchen, an denen die Temperatur (Temp) im Bereich 60-69°F, dann können wir das folgendermaßen tun:\nDiese Suche könnten wir auch mit den logischen Operatoren durchführen.\n\ngrep(\"^6\", airquality$Temp) # Indizes\n\n [1]   1   4   6   7   9  10  12  13  14  16  17  19  20  23  24  28  34  49 140\n[20] 142 144 147 148 153\n\ngrep(\"^6\", airquality$Temp, value=TRUE) # Werte\n\n [1] \"67\" \"62\" \"66\" \"65\" \"61\" \"69\" \"69\" \"66\" \"68\" \"64\" \"66\" \"68\" \"62\" \"61\" \"61\"\n[16] \"67\" \"67\" \"65\" \"67\" \"68\" \"64\" \"69\" \"63\" \"68\"\n\n\nWenn eine Ausprägung eine bestimmte Zahlen- oder Zeichenketten am Ende enthalten soll, setzen wir ein $ ans Ende.\nWenn wir beispielsweise alle Tage suchen, an denen die Temperaturangabe (Temp) mit einer 6 endet, dann können wir das folgendermaßen tun:\n\ngrep(\"6$\", airquality$Temp) # Indizes\n\n [1]   5   6  13  17  31  51  53  54  55  85  88  90  96 103 104 110 118 122 135\n[20] 141 152\n\ngrep(\"6$\", airquality$Temp, value=TRUE) # Werte\n\n [1] \"56\" \"66\" \"66\" \"66\" \"76\" \"76\" \"76\" \"76\" \"76\" \"86\" \"86\" \"86\" \"86\" \"86\" \"86\"\n[16] \"76\" \"86\" \"96\" \"76\" \"76\" \"76\"\n\n\nWie beim Extrahieren von Variablen können wir auch hier mit grep() verschiedene Bestandteile einer Ausprägung anhand des logischen Operators | suchen.\nAls Beispiel suchen wir Temperaturangaben, die zu Beginn eine 5 enthalten oder mit einer 5 enden.\n\ngrep(\"^5|5$\", airquality$Temp) # Indizes\n\n [1]   5   7   8  15  18  21  25  26  27  36  49  56  63  81  86  97 115 132 151\n\ngrep(\"^5|5$\", airquality$Temp, value=TRUE) # Werte\n\n [1] \"56\" \"65\" \"59\" \"58\" \"57\" \"59\" \"57\" \"58\" \"57\" \"85\" \"65\" \"75\" \"85\" \"85\" \"85\"\n[16] \"85\" \"75\" \"75\" \"75\"\n\n\n\nAchtung: Die Zahlen bzw. Zeichenketten dürfen nicht durch Freizeichen getrennt werden, z.B. würden mit \"6| ^7\" nur Temperaturangaben gefiltert werden, die eine 6 enthalten.\n\nDie Suche mit \"^x|x$\" ergibt gemeinsam die globale Suche nach \"x\".\nWenn wir hingegen mehrere Bedingungen verknüpfen wollen, z.B. \"^x\" und \"x\\$\", dann nutzen wir .*, z.B. \"^x.x\\$\" (für ein Beispiel siehe [mutate(): Zusammenfassung aller Fälle]).\nÄhnlich zum Abschnitt zu Variablen mit grep() extrahieren wenden wir die Selektion mit der Form Datensatz[grep(),] an, und speichern diese in einem neuen Objekt.\nWir können hierfür nur den Indizes-Vektor (value=FALSE; Default) nutzen.\n\ndf_temp5 <-  airquality[grep(\"^5|5$\", airquality$Temp),]\n\n\n\n\n\n  \n\n\n\n\n\n\nLogische Operatoren\n\n\n\n\nWenn wir logische Operatoren auf einzelne Variablen anwenden, können wir Fälle mit bestimmten Ausprägungen filtern.\nHier finden wir eine Einführung zu logischen Operatoren mit Übungsfragen.\nUm nur diese Fälle im gesamten Datensatz zu extrahieren, nutzen wir folgende Syntax:\n\nDatensatz[Variable Operator Ausprägung,]\nWenn wir Fälle (d.h. Zeilen) exrahieren wollen, müssen wir nach den Indizes immer ein Komma angeben z.B. extrahiert df[2,] die zweite Zeile aus df. Das kommt daher, dass in einem zweidimensionalen Objekt immer erst die Zeilen und dann die Variablen angegeben werden z.B. sehen wir das auch bei der Reihenfolge der Dimensionen unserer Objekte im Environment (bei Data).\nBeispielsweise können wir mit dem Gleichheits-Operator == nach exakt einer Ausprägung in einer Variablen suchen.\nWir filtern die Variable Temp (Temperatur in Grad Fahrenheit) nach Fällen mit der Ausprägung 57.\n\ndf_temp57 <- airquality[airquality$Temp == 57,]\n\n\n\n\n\n  \n\n\n\nWir können mittels |(oder) auch mehrere Ausprägungen gleichzeitig auswählen.\nNun wollen wir zusätzlich zu nach Fällen mit der Ausprägung 57 auch jene mit der Ausprägung 66 extrahieren.\n\ndf_temp57_66 <- airquality[airquality$Temp == 57 | airquality$Temp == 66,]\n\n\n\n\n\n  \n\n\n\nWeitere logische Operatoren sind z.B. != (nicht), < (kleiner) und >= (größer gleich).\n\n\n\nfilter()\n\nMit filter() können wir verschiedene Variablen nach bestimmten Kriterien filtern. Dabei greifen wir wieder auf die logischen Operatoren zurück.\nMan übergibt der Funktion zuerst den Dataframe und anschließend die Namen der Variablen mit den Bedingungen, die auf diese jeweils zutreffen sollen.\nWir wollen nur jene Fälle, die in der zweiten Hälfte des Junis erhoben wurden.\n\ndf_month6_day15ff <- dplyr::filter(airquality, Month == 6, Day >= 15)\n\nZu Beginn haben wir erläutert, warum wir manchmal :: nutzen sollten."
  },
  {
    "objectID": "Datenvorbereitung.html#daten-sortieren",
    "href": "Datenvorbereitung.html#daten-sortieren",
    "title": "8  Datenvorbereitung",
    "section": "8.4 Daten sortieren",
    "text": "8.4 Daten sortieren\n\n\n\nFür manche Vorhaben, wie z.B. grafische Darstellungen oder dem Quantifizieren von Heteroskedastizität, benötigt man sortierte Daten.\nWir schauen uns nachfolgend zwei Möglichkeiten an, einen Dataframe nach den Ausprägungen seiner Variablen zu sortieren.\n\n\norder()\n\nNachfolgend sehen wir, wie man mit order() aufsteigend sortiert.\nWeil mit order() nur Zeilenindizes ausgegeben werden, müssen wir diese noch auf den Dataframe anwenden. Das machen wir mit der Form Datensatz[order(Variable),].\n\ndf_ascend_temp <- airquality[order(airquality$Temp),]\n\n\n\n\n\n  \n\n\n\n\nWenn wir Temp-Werte mit der gleichen Ausprägung zusätzlich noch nach der (aufsteigenden) Variable Wind sortieren wollen, können wir diese einfach ergänzen.\n\ndf_ascend_temp_wind <- airquality[order(airquality$Temp, airquality$Wind),]\n\n\n\n\n\n  \n\n\n\nBeispielsweise sehen wir hier, dass die Fälle 27, 25 und 18 (auf Seite 1 in den Zeilen 2, 3 und 4) anders sortiert sind als oben.\nWenn wir nach den absteigenden Werte der Variablen sortieren wollen, hängen wir jeweils ein - vor diese. Alternativ können wir auch das Argument decreasing=TRUE setzen (dann werden aber, im Gegensatz zu unserem Beispiel, alle Variablen absteigend sortiert).\n\ndf_descend_temp_wind_1 <- airquality[order(-airquality$Temp, airquality$Wind),]\n\n\n\n\n\n  \n\n\n\n\n\n\narrange()\n\n\n\n\nDie Funktion arrange() aus dem Paket dplyr hat ein sehr ähnliches Prinzip wie order(). Sie ist dabei in der Handhabung übersichtlicher, weil man der Funktion den Namen des Dataframes einmalig übergibt und nachfolgend nur noch die Variablen angeben muss, nach denen sortiert werden soll. Außerdem muss man den Output nicht zusätzlich auf den Dataframe anwenden, weil arrange() das ohnehin macht.\nStandardmäßig wird hier ebenso wie bei order() aufsteigend sortiert solange man das nicht mit dem Voranstellen eines - ändert.\nSchauen wir uns das für das letzte Beispiel im vorhergehenden Abschnitt an. Wir sortieren den Dataframe absteigend nach Temp und gleiche Werte aufsteigend nach Wind.\n\ndf_ascend_temp_wind_2 <- arrange(airquality, -Temp, Wind)"
  },
  {
    "objectID": "Datenvorbereitung.html#kodierung-ändern",
    "href": "Datenvorbereitung.html#kodierung-ändern",
    "title": "8  Datenvorbereitung",
    "section": "8.5 Kodierung ändern",
    "text": "8.5 Kodierung ändern\nWenn die Daten nicht in einer angemessenen Kodierung vorliegen, muss man diese nachträglich anpassen bzw. neu erstellen. Die Kodierung einer Variablen ist von ihrem Messniveau abhängig.\nFür dieses Kapitel beschränken wir uns auf die Nutzung des Datensatzes PWE_data, welchen wir zu Beginn heruntergeladen haben.\n\n8.5.1 Umkodieren\n\n\n\nWenn wir zum Beispiel Messwerte in Meter zu Messwerten in Zentimeter ändern oder negativ gepolte Items umpolen wollen, spricht man von Umkodieren.\nWir können die Kodierung von Merkmalen in R auf verschiedene Arten ändern. Nachfolgend schauen wir uns zwei Funktionen an, die wir nutzen können.\n\n\nrecode()\n\nIn der Funktion recode() (auch Recode() möglich) aus dem Paket car müssen wir grundsätzlich zwei Argumente spezifizieren: var und recodes. Ersterem übergeben wir die umzukodierende Variable, zweiterem die alte und die neue Kodierung der Variablen.\nWir müssen hierbei einige syntaktische Besonderheiten von recodes beachten:\nrecodes = \"alt_1 = neu_k; alt_2 = neu_k-1; ...; alt_k = neu_1\"\n(Ausprägungen der Kodierung von 1 bis \\(k\\))\n\ndie Input=Output-Parameter müssen gemeinsam als Zeichenkette (d.h. in \" \") vorliegen\ndie verschiedenen Input=Output-Parameter müssen mit Semikolon (;) getrennt werden\n\nWir invertieren im Folgenden die Werte der Variablen Q9A, Q13A und Q15A (numeric). Die Information, dass die Variablen negativ kodiert sind, finden wir in Mirels & Garrett (1971) (nur über HU-VPN zugänglich). Informationen zur Messskala finden wir auch im Codebuch.\nSo sehen die Daten (der ersten 10 Personen) bisher aus:\n\n\n\n\n  \n\n\n\nJetzt invertieren wir die Skalen:\n\nlibrary(car)\nPWE_data$Q9A <- recode(var=PWE_data$Q9A,\n                       recodes=\"1=5; 2=4; 3=3; 4=2; 5=1\")\n\nPWE_data$Q13A <- recode(var=PWE_data$Q13A,\n                       recodes=\"1=5; 2=4; 3=3; 4=2; 5=1\")\n\nPWE_data$Q15A <- recode(var=PWE_data$Q15A,\n                       recodes=\"1=5; 2=4; 3=3; 4=2; 5=1\")\n\nAbschließend überprüfen wir (visuell), ob die Umkodierung geklappt hat:\n\n\n\n\n  \n\n\n\n\n\nWenn die Kodierung aus Zeichenketten (character) besteht, müssen wir diese jeweils noch mit ' ' umschließen.\nSchauen wir uns an, wie man die Ausprägungen von eduaction (1,2,3,4) zu den Beschreibungen (Less than High School,High School,University Degree,Graduate Degree`) ändert.\nMit dem Parameter as.factor legen wir fest, ob die (neue) rekodierte Variable als Faktor gespeichert werden soll.\n\nAchtung: Leider können wir so aber nur ungeordnete (nominalskaliert) und keine geordneten (ordinalskaliert) Faktoren erstellen. Dafür müssten wir auf die Funktion factor(..., ordered = TRUE, levels) zurückgreifen (siehe Abschnitt Faktorisieren).\n\n\n\n# aus Gründen der Darstellung speichern wir die Kodierungen zuerst in einem String:\nrecode <- c(\"1='Less than High School';2='High School';3='University Degree';4='Graduate Degree'\")\nPWE_data$education_new <- recode(var=PWE_data$education,\n                                 as.factor=TRUE,\n                                 recodes=recode)\n\nAbschließend vergleichen wir die Daten (der ersten 10 Personen) von education und education_new:\n\n\n\n\n  \n\n\n\n\n\n\ncase_when()\n\n\n\n\nMit der Funktion case_when() aus dem Paket dplyr können wir für verschiedene Fälle (d.h. Bedingungen) der ursprünglichen Variable angeben, wie diese umkodiert werden soll. Auf die linke Seite schreiben wir eine logische Bedingung (z.B. größer als mit >); auf die rechte Seite die neue Kodierung. Verbunden werden beide mit einer Tilde (~).\nIm Gegensatz zu recode() können wir durch die Verwendung von logischen Operatoren (z.B. >) ganzen Zahlenintervallen dieselbe Kodierung zuweisen.\nAls Beispiel bilden wir Kategorien für das intervallskalierte Merkmal Q1E (mit Frage Q1 verbrachte Zeit in Millisekunden). Den Range des Merkmals erfahren wir mit range(PWE_data$Q1E, na.rm=TRUE) (195 - 181.246). Wir teilen Q1E in vier gleich breite Kategorien ein: \\([195, 45457.75), [45457.75, 90720.5), [90720.5, 135983.2), [135983.2, 181247)\\).\nHinweis: [ heißt inklusive, ) heißt exklusive. Bei der oberen Grenze wird aufgerundet.\n\n# library(dplyr)\n\nPWE_data$Q1E_kat <- case_when(\n  PWE_data$Q1E < 45457.75 ~ 1, # kleiner damit exklusiv \n  PWE_data$Q1E < 90720.5 ~ 2, \n  PWE_data$Q1E < 135983.2 ~ 3, \n  PWE_data$Q1E < 181247 ~ 4) \n\nstr(PWE_data$Q1E_kat) # Überprüfung Datentyp\n\n num [1:1350] 1 1 1 1 1 1 1 1 1 1 ...\n\nunique(PWE_data$Q1E_kat) # um alle Kategorien zu sehen\n\n[1]  1  2  4  3 NA\n\n\nNun haben wir eine neue Variable Q1E_kat erstellt, welche zusammengefasste Informationen aus Q1E enthält. Die neu erstellte Variable liegt als numeric vor, d.h., dass wir diese noch faktorisieren und ordnen müssen, damit sie als ordinalskaliert gehandhabt wird.\n\nPWE_data$Q1E_kat <- factor(PWE_data$Q1E_kat, ordered=TRUE)\n\nstr(PWE_data$Q1E_kat) # Überprüfung Datentyp\n\n Ord.factor w/ 4 levels \"1\"<\"2\"<\"3\"<\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\n8.5.2 Indikatorvariablen: Kodierung nominaler Merkmale\nNominale Merkmale kann man in Form von Dummy-, Effekt- und Kontrastkodierungen repräsentieren. Eine solche Repräsentation ist vor allem im Rahmen des Allgemeinem Linearen Modells (ALM) von Interesse.\nUntenstehende Tabelle gibt einen groben Überblick über die Interpretation der Parameter des ALM in Abhängigkeit der Kodierung.\n\n\n\n\n\n\n\n \n  \n      \n    Intercept b0 \n    Steigung bj \n    mögliche Anwendung \n  \n \n\n  \n    Dummy \n    Mittelwert in der Referenzgruppe \n    Differenz des Mittelwerts der j-ten Gruppe zur Referenzgruppe \n    Vergleich von Experimental- und Kontrollgruppe \n  \n  \n    Effekt \n    Mittelwert der Gruppenmittelwerte (Gesamtmittelwert) \n    Differenz des Mittelwertes der j-ten Gruppe zum Gesamtmittelwert \n    Vergleich von Gruppen in varianzanalytischen Designs \n  \n  \n    Kontrast \n    Mittelwert über die Mittelwerte der Kontrastgruppen \n    lässt sich als Funktion der Kontrastkoeffizienten darstellen, die den jeweiligen Kontrast kodieren \n    Gezielte Einzelvergleiche von (Kombinationen) von Gruppen \n  \n\n\n \nDie Interpretation der Mittelwerte und Differenzen hängt zusätzlich davon ab, ob ein balanciertes oder unbalanciertes Design vorliegt (d.h. ob die Gruppengrößen gleich oder ungleich sind).\n\n\n\n\n\nFür mehr Informationen zu Indikatorvariablen können wir z.B. folgende Quelle nutzen:\n\n\nBortz, J., & Schuster, C. (2010). Allgemeines lineares Modell. In J. Bortz, & C. Schuster (Eds.), Statistik für Sozialwissenschaftler (S.363-384). Heidelberg: Springer\n(für HU-Studierende über ub.hu-berlin.de zugänglich)\n\nIm Folgenden schauen wir uns an, wie man konkret bei der Erstellung der verschiedenen Arten der Indikatorvariablen vorgehen kann.\nDafür nutzen wir die im Abschnitt Faktorisieren erstellte Variable gender_uf aus PWE_data_fac.\nZusätzlich rechnen wir jeweils eine einfache lineare Regression mit den verschiedenen Kodierungen, um die Unterschiede zwischen den Koderierungsarten zu veranschaulichen.\nWir regredieren dafür die Zeit in Sekunden, die die Probanden auf der Instruktionsseite verbracht haben (introelapse), auf ihr Geschlecht (gender_uf).\n\nAchtung: Die nachfolgend vorgestellten Funktionen lassen sich auch auf ordinalskalierte Merkmale (d.h. sortierte Faktoren) anwenden. Bei diesen unterscheidet sich aber die Interpretation der geschätzten Koeffizienten der Regression. Wir bekommen Schätzungen für lineare (L), quadratische (Q) und kubische (C) Trends. Daher behandeln wir im folgenden Abschnitt nur die Kodierung von nominalskalierten Merkmalen.\n\n\n\nDummy-Kodierung\n\nViele Funktionen in R (z.B. lm(), lme() und lmer()) kodieren nominalskalierte Variablen intern automatisch nach der Dummy-Kodierung um, wenn diese vorher als Faktoren deklariert wurden. Dabei wird die erste Kategorie als Referenzkategorie genutzt. Wenn wir eine andere Referenzkategorie haben wollen, können wir dafür die im Folgenden vorgestellten Funktionen (C(..., contr.treatment(n, base)) oder relevel()) nutzen.\n\n\n\nMan benötigt für eine Dummykodierung mit \\(k\\)-Kategorien \\(k-1\\) Indikatorvariablen. Die jeweils interessierende Gruppe wird in der jeweiligen Indikatorvariablen mit 1 kodiert; die anderen mit 0. Als Referenzkategorie (von der die Abweichung berechnet wird) gilt jene, welche in allen Indikatorvariablen mit 0 kodiert wird.\nDie Indikatorvariablen erstellt uns die Funktion contr.treatment() automatisch, wenn wir die Anzahl der Faktorstufen (n) und die Referenz (base) angeben.\nUm die faktorisierte Variable gender_uf, welche 3 Ausprägungen hat, zu kodieren, benötigen wir 2 Dummy-Variablen. Wir wählen die erste Kategorie (1 = Male) als Referenzkategorie.\n\ncontr.treatment(n=3, base=1)\n\n  2 3\n1 0 0\n2 1 0\n3 0 1\n\n\nC(Faktor, contr.treatment(n, base)) setzt die Konstraste für den kategorialen Prädiktor innerhalb von lm():\n\nlm_ct<- lm(introelapse ~ C(gender_uf, contr.treatment(3, 1)), PWE_data) \nsummary(lm_ct)\n\n\nCall:\nlm(formula = introelapse ~ C(gender_uf, contr.treatment(3, 1)), \n    data = PWE_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2248  -2235   -781   -764 961254 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(>|t|)  \n(Intercept)                              2249       1038   2.166   0.0305 *\nC(gender_uf, contr.treatment(3, 1))2    -1466       1496  -0.980   0.3274  \nC(gender_uf, contr.treatment(3, 1))3    -1847       4339  -0.426   0.6703  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26970 on 1340 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.0007689, Adjusted R-squared:  -0.0007224 \nF-statistic: 0.5156 on 2 and 1340 DF,  p-value: 0.5973\n\n\n\n\n\nNun vergleichen wir das Regressionsmodell mit den Dummy-kodierten Indikatorariablen (C(gender_uf, contr.treatment(n=4, base=1)) mit dem Regressionsmodell mit dem unsortierten Faktor (gender_uf), bei dem ebenfalls die erste Kategorie als Referenzkategorie genutzt wird.\n\nlm_uf <- lm(introelapse ~ gender_uf, PWE_data)\nsummary(lm_uf)\n\n\nCall:\nlm(formula = introelapse ~ gender_uf, data = PWE_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2248  -2235   -781   -764 961254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)     2249       1038   2.166   0.0305 *\ngender_uf2     -1466       1496  -0.980   0.3274  \ngender_uf3     -1847       4339  -0.426   0.6703  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26970 on 1340 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.0007689, Adjusted R-squared:  -0.0007224 \nF-statistic: 0.5156 on 2 and 1340 DF,  p-value: 0.5973\n\n\nDa die Referenzkategorie identisch ist, sehen wir, dass wir bei beiden die gleichen Ergebnisse erhalten.\nBei der Dummykodierung entspricht unser Interzept \\(b_0\\) dem ungewichteten Mittelwert in der Referenzkategorie (1 = Male). Die partiellen Steigungsgewichte \\(b_j\\) (C(...)2 und C(...)3 bzw. gender_uf2 und gender_uf3) entsprechen den ungewichteten Mittelwertsunterschieden zwischen der jeweiligen Gruppe (2 = Female bzw. 3 = Other) und der Referenzgruppe (1 = Male).\nAlternativ zu C(..., contr.treatment()) können wir mit der Funktion relevel() die Referenzkategorie eines unsortierten Faktors ändern. Standardmäßig ist immer die erste Gruppe nach der natürlichen Reihenfolge (bei Zahlen aufsteigend und bei Buchstaben alphabetisch) die Referenzkategorie.\nDem Argument ref übergeben wir die derzeitige Position der gewünschten Referenzkategorie.\n\n# Beispiel: Female als Referenzkategorie\nPWE_data$gender_uf_ref <- relevel(PWE_data$gender_uf, ref = 2)\n\nls.str(PWE_data[,c(103, 106)]) # zum Überprüfen\n\neducation_new :  Factor w/ 4 levels \"Graduate Degree\",..: 4 2 2 2 2 1 1 2 3 2 ...\ngender_uf :  Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 2 2 2 2 1 3 2 1 ...\n\n\n\n\n\nEffektkodierung\n\n\n\n\nFür diese Kodierung benötigen wir ebenfalls \\(k-1\\) Indikatorvariablen. Die jeweils zutreffende Gruppe wird in der jeweiligen Indikatorvariablen mit 1 kodiert; die nicht zutreffende mit 0 (ebenso wie bei der Dummy-Kodierung). Die “Referenzkategorie” (in unserem Beispiel No) wird in allen Indikatorvariablen mit -1 kodiert.\nEs gibt eigentlich keine echte Referenzkategorie (wie bei der Dummy-Kodierung). Vielmehr entspricht der Interzept dem Mittelwert über alle Gruppen hinweg.\nAnalog zu contr.treatment(n, ...) bei der Dummy-Kodierung können wir contr.sum(n) nutzen, um eine effektkodierte Matrix eines Faktors zu erstellen.\n\ncontr.sum(n=3)\n\n  [,1] [,2]\n1    1    0\n2    0    1\n3   -1   -1\n\n\nWir müssen lediglich in n spezifizieren, wie viele Faktorstufen es gibt. Auch hier übergeben wir den Output der Funktion an C(), wenn wir z.B. eine lineare Regression mit lm() berechnen wollen.\n\nlm_cs <- lm(introelapse ~ C(gender_uf, contr.sum(3)), PWE_data)\nsummary(lm_cs)\n\n\nCall:\nlm(formula = introelapse ~ C(gender_uf, contr.sum(3)), data = PWE_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2248  -2235   -781   -764 961254 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)\n(Intercept)                   1144.4     1490.2   0.768    0.443\nC(gender_uf, contr.sum(3))1   1104.5     1606.2   0.688    0.492\nC(gender_uf, contr.sum(3))2   -361.5     1614.8  -0.224    0.823\n\nResidual standard error: 26970 on 1340 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.0007689, Adjusted R-squared:  -0.0007224 \nF-statistic: 0.5156 on 2 and 1340 DF,  p-value: 0.5973\n\n\nBei der Effektkodierung entspricht unser Interzept \\(b_0\\) dem Mittelwert der ungewichteten Gruppenmittelwerte. Die partiellen Steigungsgewichte \\(b_j\\) (C(...)1 und C(...)2) entsprechen der Differenz des Mittelwerts der jeweiligen Gruppe (1 = Male bzw. 2 = Female) zum Mittelwert der ungewichteten Gruppenmittelwerte.\nLeider können wir mit contr.sum() nur die erste Faktorstufe als “Referenzkategorie” nutzen. Mit ref() können wir jedoch wieder die Sortierung der Faktorstufen ändern und den umsortierten Faktor dann wieder an C(..., contr.sum(n)) übergeben.\n\n# Beispiel: Female als Referenzkategorie\nPWE_data$gender_uf_ref <- relevel(PWE_data$gender_uf, ref = 2)\n\nls.str(PWE_data[,c(103, 106)]) # zum Überprüfen\n\n\n\n\nKonstrastkodierung\n\n\n\n\n\n\n\nBei der Kontrastkodierung können wir uns eigens gewählte Kontraste zwischen verschiedenen Gruppen anschauen. Die Gewichte \\(c_i\\) eines Kontrastes müssen der Bedingung genügen, dass die Summe der Gewichte über die Anzahl der zu kodierenden Kategorien \\(i\\) null ist, d.h. \\(\\sum\\limits_{i} c_i = 0\\).\nDie jeweilige Kodierung mit 0 in einer Indikatorvariablen sorgt dafür, dass eine Gruppe bzw. ein Fall nicht mit in einen Kontrast eingeht.\nBei multiplen Kontrasten (d.h. mindestens zwei kontrastkodierten Variablen) können wir orthogonale (d.h. unkorrelierte) und nicht orthogonalen (d.h. korrelierte) Kontraste unterscheiden.\nZwei Kontraste \\(j\\) und \\(j'\\) sind orthogonal wenn zusätzlich zur oberen Bedingung gilt: \\(\\sum\\limits_{i} \\, c_{ij} \\cdot c_{ij'} = 0\\)\nIm Folgenden werden wir nur einen Kontrast erstellen. Für mehr Informationen zur Orthogonalität von multiplen Kontrasten können wir z.B. bei Bortz & Schuster (2010) nachschauen.\nWir kontrastieren Männer und Frauen hinsichtlicher der verbrachten Zeit auf der Instruktionsseite (introelapse).\nDafür sortieren wir den Datensatz PWE_data_fac zuerst mit order() nach gender_uf. Die Variable hat eine Zahlen-Kodierung: Männer sind gender_uf = 1, Frauen gender_uf = 2 und Andere gender_uf = 3.\n\nPWE_data <- PWE_data[order(PWE_data$gender_uf),] \n# natürliche (aufsteigende) Sortierung: 1, 2, 3, NA\n\nAnschließend erstellen wir mit rep() die kontrastkodierte Variable.\nAlternativ könnten wir auch recode() oder case_when() nutzen (siehe Umkodieren).\n\n# Anzahl der Fälle in den einzelnen Ausprägungen in Erfahrung bringen:\ntable(PWE_data$gender_uf, useNA = 'ifany')\n\n\n   1    2    3 <NA> \n 675  627   41    7 \n\n# Kontrast erstellen\nPWE_data$gender_kontrast <- c(rep(1/675, 675), # Male (1)\n                                  rep(-1/627, 627), # Female (2)\n                                  rep(0, 48)) # Other (3) & NA; nicht von Interesse\n\nrep(Gewichtung, Gruppengröße): Die jeweilige Gewichtung einer Gruppe (bzw. Kombination von Gruppen) richtet sich nach ihrer Größe.\n\n\n\n\n\nWie genau funktioniert rep()?\n\nFür die manuelle Erstellung von Indikatorvariablen kann man die Funktion rep() nutzen, welche die ihr übergebenen Zahlen bzw. Zahlenfolgen (oder auch Zeichen bzw. Zeichenketten) beliebig häufg wiederholt.\nSchauen wir uns die Funktionsweise der Funktion an einigen Beispielen an.\nDie Zahl 1 wird 10 mal (times) wiederholt:\n\nrep(1, 10) # das gleiche wie: rep(x=1, times=10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\nDie Zahlenfolge 0, 1 wird 10 mal (times) wiederholt:\n\nrep(0:1, 10)\n\n [1] 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n\n\nWenn wir erst 10 mal die 0 und anschließend 10 mal die 1 haben wollen, nutzen wir das Argument each:\n\nrep(0:1, each=10) # das gleiche wie c(rep(0, 10), rep(1, 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n\n\n\nDie Gewichte einer kontrastierten Gruppe (hier: jeweils Männer bzw. Frauen) werden so gewählt, dass sie aufsummiert 1 bzw. -1 ergeben. Wenn wir die gleiche Anzahl an Fällen in den zu kontrastierenden Gruppen haben, können wir die einzelnen Gewichte auch zu 1 bzw. -1 vereinfachen.\n\nAchtung: Wenn wir ungleich große Gruppen haben, wie in unserem Beispiel, dann liegen die einzelnen Gewichtungen als Brüche vor z.B. \\(c_{Männer} = \\frac{1}{675}\\) und \\(c_{Frauen} = -\\frac{1}{627}\\). Wenn wir die Elemente unserer kontrastkodierten Variablen gender_kontrast aufsummieren, erhalten wir -5.5944832^{-17}. Damit genügen wir de facto der Bedingung \\(\\sum\\limits_{i} c_i = 0\\) nicht, aber die Zahl ist so klein (d.h. so nah an 0), dass wir sie vernachlässigen können.\n\nJetzt nehmen wir die kontrastkodierte Variable als Prädiktor in ein neues Regressionsmodell auf.\n\nlm_kontr <- lm(introelapse ~ gender_kontrast, PWE_data)\nsummary(lm_kontr)\n\n\nCall:\nlm(formula = introelapse ~ gender_kontrast, data = PWE_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2464  -2451   -998   -985 961038 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)  \n(Intercept)       1758.6      775.3   2.268   0.0235 *\ngender_kontrast 476517.1   513617.1   0.928   0.3537  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28490 on 1348 degrees of freedom\nMultiple R-squared:  0.0006381, Adjusted R-squared:  -0.0001032 \nF-statistic: 0.8608 on 1 and 1348 DF,  p-value: 0.3537\n\n\n\n\n\nIn unserem Regressionsmodell mit dem Kontrast Männer vs. Frauen entspricht der Interzept \\(b_0\\) dem Mittelwert der Gruppenmittelwerte der betrachteten Gruppen (d.h. Männer und Frauen) und das partiellen Steigungsgewichte \\(b_1\\) dem Unterschied in den Mittelwerten der betrachteten Gruppen."
  },
  {
    "objectID": "Datenvorbereitung.html#summary-variablen",
    "href": "Datenvorbereitung.html#summary-variablen",
    "title": "8  Datenvorbereitung",
    "section": "8.6 Summary-Variablen",
    "text": "8.6 Summary-Variablen\nWenn wir nicht mit den Rohdaten arbeiten wollen, sondern Informationen von mehreren, aggregierten Variablen (z.B. Summenwerte, Mittelwerte) oder anderweitig transformierten Variablen (z.B. standardisierte Werte) auswerten wollen, müssen wir Summary-Variablen erstellen.\nWir nutzen zu diesem Zweck den Datensatz PWE_data, welcher aus einer psychometrischen Erhebung stammt. Zu Beginn des Kapitels haben wir den Datensatz heruntergeladen.\n\nAchtung: Die Werte der Variablen Q9A, Q13A und Q15A, die wir im Folgenden nutzen werden, wurden im Abschnitt Umkodieren rekodiert, weil sie negativ gepolt sind.\n\n\n\nrowSums(), rowMeans() und select(): Summen- und Mittelwerte\n\nWir schauen uns im Folgenden an, wie man Summen- und Mittelwerte von mehreren Variablen erstellt. Dieses Vorgehen ist beispielsweise für die Erstellung von Skalenwerten in der Testkonstruktion von großer Relevanz.\nWir schauen uns den Summen- sowie den Mittelwert über alle Items der Protestant Work Ethic Scale an. Die Fragen wurden auf einer intervallskalierten Skala - von 1 (stimme nicht zu) bis 5 (stimme zu) - beantwortet und in den Variablen, die mit einem Q beginnen und einem A enden gespeichert.\nUm einen Summenwert, d.h. eine Summe einer Person über mehrere Variablen, zu bilden, können wir auf die Funktion rowSums() zurück greifen. Wenn wir fehlende Werte in den Variablen haben, setzen wir das Argument na.rm=TRUE.\nEs ist zusätzlich sinnvoll, mit einer Kombination aus select(), matches() und den regulären Ausdrücken, die relevanten Variablen auszuwählen. Dafür laden wir das Paket dplyr.\nmatches() ist eine select helper Funktion, welcher man reguläre Ausdrücke übergeben kann. Es gibt noch weitere Hilfsfunktionen, die man auch als Alternative zu regulären Ausdrücken nutzen kann z.B. starts_with(x) anstatt ^x. Wenn wir aber mehrere Bedingungen haben, sollten wir matches() und den regulären Ausdruck .* (zur konjunktiven Verknüpfung) nutzen.\n\n# library(dplyr)\nPWE_data$sum <- rowSums(select(PWE_data, matches(\"^Q.*A$\")), na.rm=TRUE)\n\nNun schauen wir uns die neu erstellte Variable (für die die ersten 10 Personen) einmal an:\n\n\n\n\n  \n\n\n\nWenn wir hingegen nicht den Summen- sondern den Mittelwert einer Person über Variablen bilden wollen, nutzen wir rowMeans(). Der Rest bleibt analog zum Vorgehen oben.\n\nPWE_data$mean <- rowMeans(select(PWE_data, matches(\"^Q.*A$\")), na.rm=TRUE)\n\nAbschließend schauen wir uns wieder die neu erstellte Variable (für die die ersten 10 Personen) an:\n\n\n\n\n  \n\n\n\n\n\n\nifelse(): Auswahl bestimmter Fälle\n\nJetzt schauen wir uns an, wie wir eine Variable nur für eine bestimmte Gruppe erstellen können.\nDafür nutzen wir die Funktion ifelse(test, yes, no). Mit dieser testen wir, ob eine oder mehrere Bedingungen (test) zutreffen und geben an, was mit den Fällen passieren soll, auf die die Bedingung(en) zutreffen (yes) und jene, auf die diese nicht zutreffen (no).\nFür unser Beispiel sollen alle Fälle, auf die die Bedingung(en) nicht zutreffen, ein NA auf der neu erstellten Variablen erhalten.\nWenn wir mehrere Bedingungen nutzen wollen, können wir auf die logischen Operatoren zurückgreifen. Mit & geben wir an, dass beide Bedingungen zutreffen sollen; mit | dass eine Bedingung zutreffen soll. Mit runden Klammern können wir Bedingungen noch differenzierter angeben z.B. (A | B) & C heißt, dass entweder A oder B eine (noch festzulegende) Ausprägung erfüllen müssen und zusätzlich noch C. Mehr Informationen zu logischen Operatoren finden wir im gleichnamigen Abschnitt.\nWir nutzen das gleiche Beispiel wie im Abschnitt vorher, nur dass wir jetzt nur den Summenwert für weibliche (gender == 2) Buddhistinnen (religion == 3) bilden wollen.\n\nPWE_data$sum_gr <- ifelse(PWE_data$gender == 2 & PWE_data$religion == 3, # test\n                          rowSums(select(PWE_data, matches(\"^Q.*A$\")),   # yes\n                                  na.rm=TRUE),\n                          NA)                                            # no\n\nSchauen wir uns die neu erstellte Variable sowie die Gruppierungsvariablen (der Personen 687 bis 691 an, unter denen sich 2 von insgesamt 9 weibliche Buddhistinnen befinden) einmal an:\n\n\n\n\n  \n\n\n\n\n\n\nmutate(): Summary-Variablen als Funktion von anderen Variablen erstellen\n\n\n\n\nWenn wir neue Variablen erstellen wollen, die Funktionen von bestehenden Variablen sind, können wir die Funktion mutate() aus dem Paket dplyr nutzen. Besonders nützlich hierbei ist, dass wir mit dem Parameter .keep festlegen können, welche Variablen wir im (neu erstellten) Datensatz behalten wollen.\nNachdem wir schon den Mittelwert der Skale berechnet haben (mean), wollen wir noch eine neue Variable erstellen, die den z-standardisierten Mittelwert der Personen widergibt.\n\n# für z-Standardisierung notwendige Kennwerte berechnen:\nmean_all <- mean(PWE_data$mean, na.rm=TRUE) # Mittelwert über alle Personenmittelwerte\nsd_all <- sd(PWE_data$mean, na.rm=TRUE) # Standardabweichung der Mittelwerte\n# library(dplyr)\nPWE_data_mean <- mutate(PWE_data, \n                        sw_mean = (mean - mean_all) / sd_all,\n                        .keep = \"used\") # alle benutzten und neu erstellen Variablen\n\nAbschließend können wir uns den neu erstellten Datensatz PWE_data_mean (für die ersten 10 Personen) einmal anschauen:"
  },
  {
    "objectID": "Datenvorbereitung.html#weitere-wichtige-hinweise",
    "href": "Datenvorbereitung.html#weitere-wichtige-hinweise",
    "title": "8  Datenvorbereitung",
    "section": "8.7 Weitere wichtige Hinweise",
    "text": "8.7 Weitere wichtige Hinweise\n\n8.7.1 Cheat Sheet dplyr\nWer Gefallen an den tidyverse-Funktionen select(), filter(), mutate(), und summarise() gefunden hat, kann ein Cheat Sheet zur Data Transformation mit dplyr in deutsch oder englisch herunterladen.\n\n\n8.7.2 Stichprobengröße\nEs ist generell sehr wichtig, auch bei der Datenvorbereitung, ein Auge auf die Stichprobengröße zu haben. Teilweise werden bei der Datenvorbereitung einige Untersuchungseinheiten aus der Analyse exkludiert und damit sinkt die Stichprobengröße \\(N\\).\nWenn wir Auswertungen machen, in denen wir Ergebnisobjekte bekommen (z.B. bei der Regression mit lm()), können wir die Information zu \\(N\\) daraus ablesen. Dazu klicken wir auf das Ergebnisobjekt im Environment (z.B. lm_kontr). Unter model sehen wir die Dimensionalität des genutzten Teil des Datensatzes und können anhand der Anzahl der Zeilen \\(N\\) ablesen (z.B. bei lm_kontr: [116 x 3] d.h. 116 Fälle).\n\nIn lm() wird listwise deletion angewendet, d.h. dass jede Zeile, die mindestens ein Missing enthält, aus der Analyse ausgeschlossen wird. Mehr zu Fehlenden Werten im gleichnamigen Kapitel.\n\n\n\n8.7.3 Replizierbarkeit\nWir sollten unsere R-Skripte generell großzügig kommentieren (mit #), damit wir (und ggf. auch Dritte) schnell nachvollziehen können, was wir da eigentlich gemacht haben.\nEs lohnt sich auch, wenn man einen Datensatz (teil-)aufbereitet hat, diesen zu speichern, d.h. als neue Datei außerhalb von R zu exportieren (z.B. wenn man einen Datensatz vom Wide- ins Long-Format gebracht hat).\nAußerdem ist es sinnvoll, alle Schritte der Datenvorbereitung sowie Datenauswertung im gleichen Programm durchzuführen, um möglichen Kompatibilitätsproblemen zwischen verschiedenen Programmen vorzubeugen."
  },
  {
    "objectID": "Datenvorbereitung.html#übung",
    "href": "Datenvorbereitung.html#übung",
    "title": "8  Datenvorbereitung",
    "section": "8.8 Übung",
    "text": "8.8 Übung\nIm Folgenden wollen wir einige Aufgaben bearbeiten, die in den Bereich der Datenvorbereitung fallen. Dazu gehören u.a. das Extrahieren und Sortieren von Daten, die Änderung der Kodierung von Daten sowie das Erstellen von Summary-Variablen. Zuallererst sollten wir uns jedoch immer mit dem genutzten Datensatz vertraut machen.\n\nHier finden wir das Einführungsskript zu Datenvorbereitung.\n\nDazu nutzen wir einen Datensatz, der im Rahmen eines Projektes zur Untersuchung des Zusammenhangs des Bedürfnisses nach Privatsphäre und verschiedenen Persönlichkeitseigenschaften erhoben wurde. Mehr Informationen zum Projekt und zur Publikation finden wir hier.\nDen Datensatz sowie das dazugehörige Codebuch finden wir im Open Science Framework. Mehr Informationen zu OSF, der Replikationskrise und der Open Science Bewegung finden wir hier.\nDen Datensatz können wir, nachdem wir ihn heruntergeladen haben, folgendermaßen in R einlesen:\n\n\n\n\ndata <- read.csv(\"Dateipfad/data.csv\") # hier den eigenen Dateipfad einfügen\n\nSo sollte der Datensatz aussehen:\n\n\n\n\n  \n\n\n\n\n\n8.8.1 Übung 1: Erste Schritte\nZuallererst wollen wir uns mit dem Datensatz vertraut machen. Dazu benötigen wir das Codebuch, welches uns Informationen über die erhobenen Variablen sowie deren Messung gibt. Am besten überfliegen wir das Codebuch und den Datensatz einmal, um uns damit vertraut zu machen, bevor wir die nachfolgenden Aufgaben bearbeiten.\n\nAchtung: Es sind nicht alle Variablen, die im Codebuch auftauchen, auch im Datensatz.\n\n1.) Es gibt zwei Variablen im Datensatz, die nicht im Codebuch zu finden sind. Welche sind das?\n\n\nLösung\n\nDie Variable sex taucht nicht im Datensatz auf. Nur die Variable male, welche die Ausprägungen 0 und 1 besitzt. Schätzungsweise soll mit beiden dieselbe Information koderit werden: das biologische Geschlecht der befragten Personen.\nDie Variable time taucht nicht im Codebuch auf. Möglicherweise ist das die individuelle Bearbeitungszeit für den Fragebogen in Sekunden. Vor der Nutzung der der Variablen time müssten wir deren Bedeutung klären.\n\n\n\n2.) Wie viele Variablen und Beobachtungen enthält der Datensatz?\n\n\nTipp\n\nStandardmäßig sind Variablen die Spalten und Beobachtungen die Zeilen eines Datensatzes.\n\n\n\nLösung\n\nWir finden die Information im R Studio Feld Environment …\n\n\n\n\n\n\n\n\n\n… oder indem wir folgende Funktionen nutzen:\n\nncol(data) # Variablen = Anzahl der Spalten\n\n[1] 70\n\n\n\nnrow(data) # Beobachtungen (Personen; N) = Anzahl der Zeilen\n\n[1] 296\n\n\n\n\n\n3.) Liegen alle Variablen in einem ihrem Messniveau angemessenen Datentyp vor?\n\n\nTipp\n\nIm Codebuch finden wir Informationen zu den Variablen. Über die Funktionen str() bekommen wir Informationen zum Datentyp.\n\n\nLösung\n\n\nstr(data)\n\n'data.frame':   296 obs. of  70 variables:\n $ id           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ male         : int  0 1 0 0 0 0 1 1 0 0 ...\n $ age          : int  19 19 20 19 22 20 20 18 19 19 ...\n $ inc          : int  2 3 1 2 3 2 1 1 1 3 ...\n $ time         : int  2456 1414 828 1043 1806 1133 1625 2319 7129 1343 ...\n $ pri_nee_gen_1: int  6 7 5 6 4 5 6 5 4 4 ...\n $ pri_nee_gen_2: int  6 2 5 3 5 4 6 6 3 4 ...\n $ pri_nee_gen_3: int  6 6 4 6 7 6 5 7 4 6 ...\n $ pri_nee_gen_4: int  7 7 5 6 7 6 7 7 6 6 ...\n $ pri_nee_soc_1: int  2 4 5 2 5 4 5 4 1 6 ...\n $ pri_nee_soc_2: int  4 5 4 2 5 4 6 4 2 6 ...\n $ pri_nee_soc_3: int  3 6 4 3 5 3 4 6 2 6 ...\n $ pri_nee_soc_4: int  1 6 4 2 4 2 5 4 2 4 ...\n $ pri_nee_soc_5: int  1 4 4 5 4 2 6 6 3 6 ...\n $ pri_nee_soc_6: int  1 3 5 5 2 4 4 4 2 2 ...\n $ pri_nee_soc_7: int  1 1 3 3 1 2 2 6 1 2 ...\n $ pri_nee_soc_8: int  1 7 4 2 2 2 2 6 3 2 ...\n $ pri_nee_soc_9: int  1 3 4 2 6 5 6 7 1 6 ...\n $ pri_nee_int_1: int  1 1 4 4 2 4 2 5 2 5 ...\n $ pri_nee_int_2: int  1 6 5 5 1 2 5 5 1 3 ...\n $ pri_nee_int_3: int  7 7 5 5 6 2 4 3 3 6 ...\n $ pri_nee_int_4: int  3 6 2 7 5 5 4 4 4 6 ...\n $ pri_nee_int_5: int  5 6 4 7 2 5 2 3 6 6 ...\n $ pri_nee_int_6: int  3 6 3 5 2 3 3 2 3 3 ...\n $ pri_nee_int_7: int  7 7 5 6 2 5 6 6 6 4 ...\n $ pri_nee_int_8: int  1 7 4 5 5 3 5 5 6 3 ...\n $ pri_nee_int_9: int  4 7 3 3 5 5 5 5 4 3 ...\n $ soc_1        : int  5 5 3 2 2 3 2 5 3 2 ...\n $ soc_2        : int  4 4 6 6 6 6 5 2 6 6 ...\n $ soc_3        : int  4 4 2 3 2 2 5 6 4 6 ...\n $ soc_4        : int  4 6 4 7 6 6 3 5 3 4 ...\n $ soc_5        : int  7 5 2 6 4 2 2 4 2 3 ...\n $ soc_6        : int  4 5 6 5 2 6 2 4 3 4 ...\n $ soc_7        : int  3 2 2 2 2 2 2 5 3 2 ...\n $ soc_8        : int  3 4 6 6 5 4 2 2 7 5 ...\n $ itg_1        : int  1 1 2 3 2 3 1 4 3 1 ...\n $ itg_2        : int  2 1 4 4 4 6 2 4 1 1 ...\n $ itg_3        : int  5 7 4 5 4 5 4 4 7 1 ...\n $ itg_4        : int  2 7 6 2 6 6 7 4 3 7 ...\n $ itg_5        : int  1 1 2 1 2 2 1 4 1 1 ...\n $ itg_6        : int  4 1 2 3 2 2 4 4 2 1 ...\n $ itg_7        : int  1 6 2 2 3 2 1 4 6 1 ...\n $ itg_8        : int  5 2 4 5 4 4 6 4 3 1 ...\n $ itg_9        : int  5 1 1 2 6 5 2 4 7 2 ...\n $ itg_10       : int  5 2 2 7 6 5 7 4 7 6 ...\n $ itg_11       : int  4 4 4 5 5 5 6 4 4 1 ...\n $ anx_1        : int  1 1 3 5 5 2 6 5 3 6 ...\n $ anx_2        : int  5 4 3 1 5 6 3 3 5 5 ...\n $ anx_3        : int  1 1 5 3 3 2 6 3 2 6 ...\n $ anx_4        : int  4 3 3 3 3 3 5 1 7 2 ...\n $ anx_5        : int  6 6 2 5 5 3 6 3 1 2 ...\n $ anx_6        : int  6 7 3 3 5 6 2 5 6 3 ...\n $ anx_7        : int  3 6 5 6 4 2 2 1 2 5 ...\n $ anx_8        : int  6 4 4 3 5 7 4 6 7 2 ...\n $ ria_1        : int  1 7 5 5 6 3 4 3 7 6 ...\n $ ria_2        : int  5 7 5 4 4 6 6 5 6 6 ...\n $ ria_3        : int  1 6 5 6 6 3 3 5 5 3 ...\n $ ria_4        : int  6 3 3 4 5 5 3 4 6 6 ...\n $ ria_5        : int  4 2 5 6 4 2 6 6 5 5 ...\n $ ria_6        : int  5 6 3 3 5 6 2 3 4 5 ...\n $ ria_7        : int  5 7 5 4 5 5 1 7 5 6 ...\n $ ria_8        : int  6 7 5 5 5 6 6 5 4 6 ...\n $ tra_1        : int  5 7 3 3 2 5 3 4 4 5 ...\n $ tra_2        : int  6 2 6 6 6 4 6 4 7 5 ...\n $ tra_3        : int  5 7 4 3 5 5 5 4 5 5 ...\n $ tra_4        : int  7 6 5 7 4 5 6 4 5 2 ...\n $ tra_5        : int  5 7 4 3 5 5 2 4 4 6 ...\n $ tra_6        : int  7 1 4 6 6 5 6 4 6 2 ...\n $ tra_7        : int  4 7 4 3 5 4 2 4 2 6 ...\n $ tra_8        : int  3 7 4 3 4 5 5 4 5 6 ...\n\n\nAlle Variablen liegen als integer vor. Für die Fragebogenitems (pri_nee_, soc_, itg_, anx_, ria_ und tra_), die intervallskaliert sein sollen, ist das korrekt. Die soziodemographischen Variablen male und inc hingegen sind nominal- bzw. ordinalskaliert. Das bedeutet, dass sie noch faktorisiert werden müssen, um in R als solche erkannt zu werden.\n\n# nominalskaliert (unsortierter Faktor):\ndata$male <- factor(data$male)\nstr(data$male)\n\n Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 2 2 1 1 ...\n\n\n\nAchtung: Nicht verwirren lassen: Der Faktor male hat die Kodierungen 0 und 1 (wie schon die integer-Variable vorher), aber die interne Kodierung des Faktors ist 1 und 2.\n\n\n# ordinalskaliert (sortierter Faktor):\ndata$inc <- factor(data$inc, ordered=TRUE)\nstr(data$inc)\n\n Ord.factor w/ 5 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 2 3 1 2 3 2 1 1 1 3 ...\n\nlevels(data$inc)\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\n\nAchtung: Aus dem Codebuch ist leider nicht ersichtlich, welche Kategorie (z.B. < $500) für welche Kodierung steht (z.B. 1). Wir gehen hier davon aus, dass beide aufsteigend gepaart wurden, z.B. < $500 = 1, aber die interne Kodierung eines Faktors beginnt bei 1, d.h. in unserem Fall gibt es 1 und 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.) Wie heißt die Variable, die kodiert, inwieweit die befragte Person gerne viele Menschen um sich herum hat? Welche Antwortoption auf der Messskala (hiermit ist nicht die Kodierung der Daten gemeint) haben die meisten befragten Personen angekreuzt?\n\n\nTipp 1\n\nDen Namen der Variablen sowie deren Messskala finden wir im Codebuch. Die Häufigkeiten der jeweiligen Ausprägungen der Variablen bringen wir in R in Erfahrung.\n\n\n\nTipp 2\n\nMit der Funktion table() können wir uns die Häufigkeiten der Ausprägungen einer Variablen ausgeben lassen. Die Funktion ist auch bereits sehr hilfreich, um einen schnellen Überblick über die möglichen Ausprägungen zu bekommen.\n\n\n\nLösung\n\nDie Variable heißt soc_2 und hat eine Messskala, welche von -3 bis 3 (inklusive 0) geht (Codebuch S.20).\n\ntable(data$soc_2)\n\n\n 1  2  3  4  5  6  7 \n 2 14 36 69 72 61 27 \n\n\nDie Kodierung 5 kommt am häufigsten vor. Die meisten befragten Personen haben damit eine 1 auf der Messskala angegeben.\n\n\n\n5.) Gibt es Werte von Variablen im Datensatz, die unplausibel erscheinen? Wenn ja, entferne die entsprechenden Personen aus dem Datensatz.\n\n\nTipp 1\n\nHiervoll ist es sinnvoll, sich eine Übersicht der Ausprägungen aller Variablen anzuschauen und diese ggf. mit den Angaben im Codebuch zu vergleichen.\n\n\nTipp 2\n\nEs gibt einen Wert einer Variablen der heraussticht.\n\n\nLösung\n\n\nsapply(sapply(data, unique), sort, na.last=TRUE) # sortierte Ausprägungen der Variablen\n\n$id\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n[235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n[253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n[271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n[289] 289 290 291 292 293 294 295 296\n\n$male\n[1] 0    1    <NA>\nLevels: 0 1\n\n$age\n [1]  9 18 19 20 21 22 23 28 29 56 NA\n\n$inc\n[1] 1    2    3    4    5    <NA>\nLevels: 1 < 2 < 3 < 4 < 5\n\n$time\n  [1]      3      5      7      8      9     10     12     13     21     31\n [11]     86    129    165    194    240    259    313    351    369    371\n [21]    384    439    443    455    456    458    484    539    562    585\n [31]    587    593    601    602    606    624    638    639    663    677\n [41]    694    706    725    741    760    778    814    815    828    829\n [51]    840    847    851    852    875    877    890    892    899    910\n [61]    934    986    991    992    999   1035   1039   1040   1043   1052\n [71]   1056   1061   1071   1073   1075   1077   1088   1090   1094   1097\n [81]   1104   1118   1119   1133   1141   1143   1144   1148   1152   1155\n [91]   1157   1158   1166   1167   1176   1177   1187   1197   1200   1209\n[101]   1211   1219   1220   1222   1227   1236   1243   1248   1250   1255\n[111]   1258   1259   1265   1290   1292   1303   1313   1314   1316   1318\n[121]   1319   1332   1343   1345   1349   1363   1367   1372   1376   1387\n[131]   1388   1397   1402   1405   1411   1414   1419   1425   1438   1448\n[141]   1453   1460   1465   1481   1492   1496   1497   1499   1514   1524\n[151]   1525   1560   1581   1595   1596   1602   1606   1625   1628   1633\n[161]   1651   1655   1656   1660   1669   1671   1672   1683   1695   1702\n[171]   1706   1713   1714   1716   1732   1734   1735   1745   1753   1756\n[181]   1772   1778   1780   1803   1806   1815   1820   1831   1845   1847\n[191]   1869   1879   1889   1905   1930   1932   1945   1949   1985   1994\n[201]   2022   2077   2125   2135   2139   2142   2144   2183   2185   2197\n[211]   2248   2255   2319   2342   2348   2360   2373   2393   2420   2447\n[221]   2456   2493   2495   2515   2518   2523   2537   2597   2606   2649\n[231]   2654   2673   2759   2769   2927   3204   3224   3273   3386   3466\n[241]   3739   3803   3808   3858   3913   4084   4436   4702   4748   5031\n[251]   5108   5229   5469   5927   5944   6064   6282   6598   6685   6849\n[261]   7129   7246   8016   8817   9274  12533  13928  17371  17973  22834\n[271]  50047  54973  96391 101191 110358 192301 197593 342508 590589 688498\n\n$pri_nee_gen_1\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_gen_2\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_gen_3\n[1]  2  3  4  5  6  7 NA\n\n$pri_nee_gen_4\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_1\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_2\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_3\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_4\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_5\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_6\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_7\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_8\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_soc_9\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_int_1\n[1]  1  2  3  4  5  6 NA\n\n$pri_nee_int_2\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_int_3\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_int_4\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_int_5\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_int_6\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_int_7\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_int_8\n[1]  1  2  3  4  5  6  7 NA\n\n$pri_nee_int_9\n[1]  1  2  3  4  5  6  7 NA\n\n$soc_1\n[1]  1  2  3  4  5  6  7 NA\n\n$soc_2\n[1]  1  2  3  4  5  6  7 NA\n\n$soc_3\n[1]  1  2  3  4  5  6  7 NA\n\n$soc_4\n[1]  1  2  3  4  5  6  7 NA\n\n$soc_5\n[1]  1  2  3  4  5  6  7 NA\n\n$soc_6\n[1]  1  2  3  4  5  6  7 NA\n\n$soc_7\n[1]  1  2  3  4  5  6  7 NA\n\n$soc_8\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_1\n[1]  1  2  3  4  5  6 NA\n\n$itg_2\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_3\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_4\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_5\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_6\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_7\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_8\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_9\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_10\n[1]  1  2  3  4  5  6  7 NA\n\n$itg_11\n[1]  1  2  3  4  5  6  7 NA\n\n$anx_1\n[1]  1  2  3  4  5  6  7 NA\n\n$anx_2\n[1]  1  2  3  4  5  6  7 NA\n\n$anx_3\n[1]  1  2  3  4  5  6  7 NA\n\n$anx_4\n[1]  1  2  3  4  5  6  7 NA\n\n$anx_5\n[1]  1  2  3  4  5  6  7 NA\n\n$anx_6\n[1]  1  2  3  4  5  6  7 NA\n\n$anx_7\n[1]  1  2  3  4  5  6  7 NA\n\n$anx_8\n[1]  1  2  3  4  5  6  7 NA\n\n$ria_1\n[1]  1  2  3  4  5  6  7 NA\n\n$ria_2\n[1]  1  2  3  4  5  6  7 NA\n\n$ria_3\n[1]  1  2  3  4  5  6  7 NA\n\n$ria_4\n[1]  1  2  3  4  5  6  7 NA\n\n$ria_5\n[1]  1  2  3  4  5  6  7 NA\n\n$ria_6\n[1]  1  2  3  4  5  6  7 NA\n\n$ria_7\n[1]  1  2  3  4  5  6  7 NA\n\n$ria_8\n[1]  1  2  3  4  5  6  7 NA\n\n$tra_1\n[1]  1  2  3  4  5  6  7 NA\n\n$tra_2\n[1]  1  2  3  4  5  6  7 NA\n\n$tra_3\n[1]  1  2  3  4  5  6  7 NA\n\n$tra_4\n[1]  1  2  3  4  5  6  7 NA\n\n$tra_5\n[1]  1  2  3  4  5  6  7 NA\n\n$tra_6\n[1]  1  2  3  4  5  6  7 NA\n\n$tra_7\n[1]  1  2  3  4  5  6  7 NA\n\n$tra_8\n[1]  1  2  3  4  5  6  7 NA\n\nwhich(data$age == 9)\n\n[1] 232\n\n\nDie Person mit der Zeile 232 hat (vermutlich versehentlich) als Alter 9 Jahre angegeben. Wir entfernen diese Person aus dem Datensatz.\n\ndata <- data[-which(data$age == 9),]\n\n\n\n\n6.) Enthält der Datensatz fehlende Werte (“Missings”; NA) und wenn ja, wie viele insgesamt und auf welchen Variablen?\n\n\nTipp\n\nDas Thema “Fehlende Werte” wird im Kapitel zur Datenvorbereitung nur kurz angerissen, weil es ein eigenständiges Kapitel dazu gibt.\n\n\n\nLösung\n\n\nanyNA(data) # prüft ob mind. ein Missing im Datensatz\n\n[1] TRUE\n\n\nJa, data enthält fehlende Werte.\n\ntable(is.na(data)) # absolute Anzahl fehlender Werte\n\n\nFALSE  TRUE \n19337  1313 \n\n\n\ntable(is.na(data))[2]/(table(is.na(data))[1] + table(is.na(data))[2]) \n# relative Anzahl fehlender Werte\n\n      TRUE \n0.06358354 \n\n\n\ncolSums(is.na(data)) # Übersicht über Anzahl Missings für alle Variablen\n\n           id          male           age           inc          time \n            0            24            25            28             0 \npri_nee_gen_1 pri_nee_gen_2 pri_nee_gen_3 pri_nee_gen_4 pri_nee_soc_1 \n           19            19            19            19            19 \npri_nee_soc_2 pri_nee_soc_3 pri_nee_soc_4 pri_nee_soc_5 pri_nee_soc_6 \n           19            19            19            19            19 \npri_nee_soc_7 pri_nee_soc_8 pri_nee_soc_9 pri_nee_int_1 pri_nee_int_2 \n           19            19            19            21            21 \npri_nee_int_3 pri_nee_int_4 pri_nee_int_5 pri_nee_int_6 pri_nee_int_7 \n           21            21            21            21            21 \npri_nee_int_8 pri_nee_int_9         soc_1         soc_2         soc_3 \n           21            21            15            15            15 \n        soc_4         soc_5         soc_6         soc_7         soc_8 \n           15            15            16            15            15 \n        itg_1         itg_2         itg_3         itg_4         itg_5 \n           21            21            21            21            21 \n        itg_6         itg_7         itg_8         itg_9        itg_10 \n           21            21            21            21            21 \n       itg_11         anx_1         anx_2         anx_3         anx_4 \n           21            18            18            18            18 \n        anx_5         anx_6         anx_7         anx_8         ria_1 \n           18            18            18            18            17 \n        ria_2         ria_3         ria_4         ria_5         ria_6 \n           17            17            17            17            17 \n        ria_7         ria_8         tra_1         tra_2         tra_3 \n           17            17            21            21            21 \n        tra_4         tra_5         tra_6         tra_7         tra_8 \n           21            21            21            21            21 \n\n\nJede Variable mit Ausnahme von id und time enthält fehlende Werte.\n\n\n\n7.) Was ist die höchste Anzahl an fehlenden Werten von einer Person und welche Person hat bzw. welche Personen haben die meisten fehlenden Werte?\n\n\nTipp\n\nIn der letzten Aufgabe haben wir uns variablenweise Missings angeschaut mit colSums(). Nun wollen wir uns zeilenweise Missings anschauen.\n\n\n\nLösung\n\n\nmax(rowSums(is.na(data))) # maximale Anzahl Missings (absoluter Wert)\n\n[1] 68\n\n\n\nmax(rowSums(is.na(data))) / ncol(data)\n# Max-Anzahl Missings einer Person / Anzahl aller Variablen (relativer Wert)\n\n[1] 0.9714286\n\n\nFehlende Werte auf 68 Variablen ist die Höchstzahl an fehlenden Werten pro Person. Das entspricht einem Prozentsatz von ca. 97% fehlenden Werten.\n\n# Personen mit diesen Zeilen haben die meisten Missings:\nnames(which(rowSums(is.na(data)) == max(rowSums(is.na(data)))))\n\n [1] \"63\"  \"66\"  \"78\"  \"133\" \"134\" \"136\" \"139\" \"180\" \"199\" \"206\" \"262\" \"267\"\n[13] \"268\" \"281\" \"283\"\n\n\nWir müssen hier names() nutzen, damit wir die Zeilennamen ausgegeben bekommen da wir sonst einen benannten Vektor ausgegeben bekommen.\nMan sollte überlegen, wie mit den Daten dieser 15 Personen bei etwaigen Analysen umzugehen wäre. Wenn die Daten MCAR sind, könnten wir diese Personen aus den Analysen entfernen. Mehr Infos zu Fehlenden Werten im gleichnamigen Kapitel.\n\n\n\n\n\n8.8.2 Übung 2: Extrahieren von Daten\nNun wollen wir einzelne Daten aus dem Datensatz extrahieren. Dabei interessieren uns entweder Variablen mit bestimmten Ausprägungen (von Personen) oder Personen mit bestimmten Ausprägungen (auf Variablen).\n\nAchtung: Achte darauf, immer auch die id-Variable mit zu extrahieren, um Beobachtungseinheiten identifizieren zu können, auch wenn das nicht jedes mal erneut in der Aufgabenbeschreibung steht.\n\n1.) Extrahiere alle demographischen Variablen aus data.\n\n\nTipp 1\n\nIm Codebuch (S. 26ff) steht, welche Variablen zu den demographischen Angaben zählen.\n\nAchtung: Im Codebuch steht die Variable sex, welche im Datensatz nicht enthalten ist. Alternativ gibt es die Variable male.\n\n\n\n\nTipp 2\n\nEs gibt drei demographische Variablen in data (aber mehr im Codebuch).\n\n\n\nLösung\n\n\nlibrary(dplyr)\nselect(data, id, male, age, inc)\n\n\n\n\n\n  \n\n\n\n\n\n\n2.) Extrahiere alle Variablen, die grundlegende Bedürfnisse (General Needs) erfassen.\n\n\nTipp 1\n\nDie Variablen haben denselben Wortstamm: gen.\n\n\n\nTipp 2\n\nEs gibt vier Items zu grundlegenden Bedürfnissen.\n\n\n\nLösung\n\n\ndata[,c(1, # id\n        grep(\"gen\", names(data)))] # general needs\n\n\n\n\n\n  \n\n\n\n\n\n\n3.) Extrahiere alle Variablen, die gesellschaftliche (Societal Needs) und interpersonelle (Interpersonal Needs) Bedürfnisse erfassen.\n\n\nTipp 1\n\nDie Variablen haben zwar denselben Wortstamm – nee – aber den haben die Variablen zu grundlegenden Bedürfnissen (General Needs) auch.\n\n\n\nTipp 2\n\nEs gibt insgesamt 18 Items zu gesellschaftlichen und interpersonellen Bedürfnissen (jeweils 9).\n\n\n\nLösung\n\n\n\n\n\n# Vorauswahl von Bedürfnis-Variablen mittels des gemeinsamen Wortstammes:\nall_needs <- data[,c(1, # id\n                     grep(\"nee\", names(data)))] # alle Bedürfnis-Variablen\n# Auswahl der gewollten Variablen aus den Bedürfnis Variablen (neben \"id\") ..\n# .. solche, die \"c\" oder \"t\" im Namen haben (trifft nur auf General needs nicht zu):\nall_needs[grep(\"id|c|t\", names(all_needs))]\n\n\n\n\n\n  \n\n\n\n\n\n\n4.) Extrahiere alle Personen, die weniger als $500 oder mehr als $5.000 monatlich zur Verfügung haben.\n\n\nTipp\n\nUm beide Bedingungen (< $500 und > $5.000) abzufragen, können wir den logischen Operator | (“oder”) nutzen.\n\n\n\nLösung\n\nZuerst vergleichen wir die Angaben zu den Ausprägungen von inc im Codebuch (S. 28) und der Kodierung in R. Laut Codebuch ist < $500 die erste Ausprägung; > $5.000 die letzte. Nun schauen wir, mit welchen Kodierungen diese korrespondieren.\n\ntable(data$inc)\n\n\n  1   2   3   4   5 \n148  74  24  15   6 \n\n\nDIe beiden werden mit 1 und 5 kodiert. Anschließend wenden wir dieses Wissen auf unsere Selektion an.\n\nfilter(data, inc == 1 | inc == 5)\n\n\n\n\n\n  \n\n\n\n\n\n\n5.) Extrahiere die “Extremkreuzer” (Personen, die nur die niedrigste oder höchste Ausprägung einer Frage ankreuzen) aus dem Geselligkeits-Fragebogen (Sociability).\n\n\nTipp 1\n\nWir können wieder den logischen Operator | (“oder”) nutzen, um jeweils Personen mit der niedrigste oder höchsten Ausprägung eines Items auszuwählen.\n\n\n\nTipp 2\n\nEs gibt insgesamt zwei Extremkreuzer.\n\n\n\nLösung\n\nIm Codebuch (S. 20) sehen wir, dass für alle Items des Geselligkeits-Fragebogen dieselbe Skale, welche von -3 bis 3 geht, genutzt wurde. Nun schauen wir uns am Beispiel eines Items an, wie diese in R kodiert werden.\n\n# niedrigste und höchste Ausprägung in Erfahrung bringen:\ntable(data$soc_1)\n\n\n 1  2  3  4  5  6  7 \n23 71 54 55 52 21  4 \n\n\n\n# Fall-Selektion anwenden:\nfilter(data, soc_1 == 1 | soc_1 == 7,\n             soc_2 == 1 | soc_2 == 7,\n             soc_3 == 1 | soc_3 == 7,\n             soc_4 == 1 | soc_4 == 7,\n             soc_5 == 1 | soc_5 == 7,\n             soc_6 == 1 | soc_6 == 7,\n             soc_7 == 1 | soc_7 == 7,\n             soc_8 == 1 | soc_8 == 7)\n\n\n\n\n\n  \n\n\n\nAuf den Seiten 6 bis 7 sehen wir die Items des Geselligkeits-Fragebogens (oben rechts ist der Pfeil).\n\n\n\n6.) Extrahiere die Items des Fragebogens zu Integrität (Integrity) für alle Personen, die 18 Jahre alt sind.\n\n\nTipp 1\n\nDie Reihenfolge der Auswahl, d.h. ob zuerst Variablen oder zuerst Fälle selektiert werden, ist eigentlich irrelevant, aber wenn wir erst die Fälle selektieren und dann die Variablen müssen wir die Variable age nicht wieder aus dem finalen Datensatz entfernen.\n\n\n\nTipp 2\n\nDer finale Datensatz besteht aus 12 Variablen (davon sind 11 die Integritäts-Items) von 54 Personen.\n\n\n\nLösung\n\n\n# Fälle selektieren:\ndata_18 <- filter(data, age == 18)\n# Variablen selektietren:\nall_itg_items <- data_18[,c(1, # id\n                         grep(\"itg\", names(data_18)))] # Integrität Items\n\n\n\n\n\n  \n\n\n\n\n\n\n7.) Extrahiere alle Personen, die im Fragebogen zu interpersonellen Bedürfnissen (Needs, Interpersonal) über dem Gesamtmittelwert (Mittelwert über alle Personen im Datensatz) liegen. Entferne Personen mit mindestens einem fehlenden Wert auf diesen Items aus der Analyse.\n\nHier muss zusätzlich eine Summary-Variable (Übung 5) erstellt werden.\n\n\nAchtung: Unser Vorgehen mit fehlenden Wert hier (casewise deletion) ist stark vereinfacht und sollte in echten Analysen nicht ohne Belege für MCAR durchgeführt werden.\n\n\n\nTipp 1\n\nWir müssen\n1) alle Items des Fragebogens extrahieren,\n2) alle Personen mit mind. einem Missing entfernen,\n3) die Items jeweils für jede Person aufsummieren (d.h. individuelle Scores bilden),\n4) diese individuellen Scores über alle Personen aufsummieren, um den Gesamtmittelwert zu berechnen,\n5) die individuellen Scores mit dem Gesamtmittelwert vergleichen, um nur überdurchschnittliche Personen in unserer finalen Auswahl zu haben.\n\n\n\n\nTipp 2\n\nIm finalen Datensatz befinden sich 140 (von initial 296) Personen.\n\n\n\nLösung\n\n\n# 1) Items extrahieren:\nall_int_items <- data[,c(1, # id\n                         grep(\"int\", names(data)))] # Int. Bedürfnisse Items\n# 2) Personen mit mind. einem Missing entfernen:\nall_int_items <- na.omit(all_int_items) # 21 Personen entfernt\n# 3) individuelle Scores bilden:\nlibrary(dplyr)\nall_int_items <- mutate(all_int_items, score = rowSums(all_int_items))\n# 4) Mittelwert individueller Scores berechnen:\nmean_score <- mean(all_int_items$score)\nmean_score\n\n[1] 182.6387\n\n# 5) überdurchschnittliche Personen extrahieren:\nfinal_data <- filter(all_int_items, score > mean_score) # 135 Personen entfernt\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n8.8.3 Übung 3: Sortieren von Daten\nIm Folgenden wollen wir unseren Datensatz nach den aufsteigenden bzw. absteigenden Ausprägungen auf einer der enthaltenen Variablen sortieren.\n1.) Sortiere data (primär) nach den aufsteigenden Ausprägungen in age und (sekundär) nach dem Geschlecht (Frauen zuerst).\n\nAchtung: Da die Kodierung der Variablen male im Codebuch nicht geklärt wird, gehen wir hier standarmäßig davon aus, dass 0 für nein und 1 für ja steht.\n\n\n\nLösung\n\nDa Frauen zuerst in der sekundären Sortierung vorkommen sollen, können wir beide Variablen aufsteigend sortieren.\n\ndata[order(data$age, data$male),] # Default: aufsteigende Sortierung\n\n\n\n\n\n  \n\n\n\n\n\n\n2.) Sortiere data (primär) nach absteigendem Einkommen und (sekundär, tertiär, ect.) nach den aufsteigenden Ausprägungen auf den Traditionalismus-Items (Traditionalism) in ihrer natürlichen Reihenfolge (beginnend bei _1, endend bei _8).\n\n\nTipp 1\n\nMit der Funktion order() können wir nur jeweils auf- oder absteigend für alle angegebenen Variablen sortieren. Daher sollten wir für diese Aufgabe auf andere Funktionen, z.B. arrange() aus dem Paket dplyr zurückgreifen.\n\n\n\nTipp 2\n\nUm besser beurteilen zu können, ob die Sortierung erfolgreich war, ist es sinnvoll, zuerst einen neuen Datensatz nur aus den relevanten Variablen (inklusive id) zu erstellen.\n\n\n\nLösung\n\n\nlibrary(dplyr)\n# neuer Datensatz nur mit relevanten Variablen:\ndata_tra <- data[,c(1, # id\n                    4, # inc\n                    grep(\"tra\", names(data)))]\n# Sortierung:\narrange(data_tra, -inc, tra_1, tra_2, tra_3, tra_4, tra_5, tra_6, tra_7, tra_8)\n# mit einem \"-\" sortieren wir absteigend\n\n\n\nWarning: There was 1 warning in `arrange()`.\nℹ In argument: `..1 = -inc`.\nCaused by warning in `Ops.ordered()`:\n! '-' is not meaningful for ordered factors\n\n\n\n\n  \n\n\n\n\n\n\n\n\n8.8.4 Übung 4: Änderung der Kodierung von Daten\nNun wollen wir die Kodierungen einiger Variablen ändern bzw. Variablen mit neuen Kodierungen erstellen.\n1.) Erstelle eine neue Variable income, die das monatliche Einkommen der befragten Personen kodiert, und dafür die Kategorien aus dem Codebuch (S.28) nutzt.\n\n\nTipp\n\nUm besser beurteilen zu können, ob die Kodierung erfolgreich war, ist es sinnvoll, zuerst einen neuen Datensatz bestehend aus inc und id zu erstellen.\n\n\n\nLösung\n\n\n# neuen Datensatz erstellen:\nlibrary(dplyr)\ndata_inc <- select(data, id, inc)\n# neue Variable income erstellen:\nrec <- c(\"1='<$500'; 2='$500-$1000'; 3='$1000-$2000'; 4='$2000-$4000'; 5='>$5000'\")\nlibrary(car)\ndata_inc$income <- recode(data_inc$inc, recodes=rec)\n\nDie Variable rec, welche die Überführung der bestehenden in die neue Kodierung enthält, wurde nur aus darstellerischen Gründen erstellt. Der Inhalt kann auch direkt dem Parameter recodes übergeben werden.\n\n\n\n\n  \n\n\n\n\n\n\n2.) Erstelle eine neue Variable sex, die analog zur gleichnamigen Variablen im Codebuch das biologische Geschlecht kodiert. Gehe für diese Aufgabe davon aus, dass alle Personen, die einen fehlender Wert auf male haben, einer Kategorie other (nicht-binäres Geschlecht) zugeordnet werden.\n\n\nTipp\n\nUm besser beurteilen zu können, ob die Kodierung erfolgreich war, ist es sinnvoll, zuerst einen neuen Datensatz bestehend aus male und id zu erstellen.\n\n\n\nLösung\n\n\nlibrary(dplyr)\n# neuen Datensatz erstellen:\ndata_sex <- select(data, id, male)\n# neue Variable sex erstellen:\ndata_sex$sex <- case_when(data_sex$male == 0 ~ \"female\",\n                          data_sex$male == 1 ~ \"male\",\n                          is.na(data_sex$male) ~ \"other\")\n\n\n\n\n\n  \n\n\n\n\n\n\n3.) Rekodiere die negativ kodierten Items des Fragebogens zu Risikovermeidung (Risk Avoidance).\n\n\nTipp 1\n\nDie negativ kodierten Items sind im Codebuch jeweils mit einem * markiert.\n\n\n\nTipp 2\n\nEs ist sinnvoll zur Überprüfung der Aufgabe, einen neuen Datensatz bestehend aus den negativ kodierten Items und ihrem rekodierten Pendant (und natürlich id) zu erstellen.\n\n\n\nLösung\n\n\n# neuen Datensatz erstellen:\nlibrary(dplyr)\ndata_ria <- select(data, id, \n                   ria_1, ria_3, ria_5) # negativ kodierte Items\n# neue rekodierte Variablen erstellen:\nlibrary(car)\n\ndata_ria$ria_1_rec <- recode(data_ria$ria_1, recodes=\"1=7; 2=6; 3=5; 4=4; 5=3; 6=2; 7=1\")\ndata_ria$ria_3_rec <- recode(data_ria$ria_3, recodes=\"1=7; 2=6; 3=5; 4=4; 5=3; 6=2; 7=1\")\ndata_ria$ria_5_rec <- recode(data_ria$ria_5, recodes=\"1=7; 2=6; 3=5; 4=4; 5=3; 6=2; 7=1\")\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n8.8.5 Übung 5: Erstellen von Summary-Variablen\nNachfolgend wollen wir neue Variablen erstellen, die Informationen aus mehreren Variablen des Datensatzes zusammenfassen.\n1.) Erstelle für die verschiedenen Bereiche von Bedürfnissen – Allgemein (General), Gesellschaftlich (Societal) und Interpersonell (Interpersonal) – jeweils separate Summenwerte und einen Gesamtsummenwert (über alle drei Bereiche).\n\n\nTipp\n\nDie Items aller drei Bedürfnis-Bereiche haben denselben Wortstamm: nee.\n\n\n\nLösung\n\n\n# neuer Datensatz mit relevanten Variablen:\ndata_need <- data[,c(1, # id\n                     grep(\"nee\", names(data)))] # Bedürfnis-Items\n\n# Summenwerte der drei Bedürfnis-Bereiche und Gesamtsummenwert:\nlibrary(dplyr)\ndata_need$score_gen <- rowSums(select(data_need, matches(\"nee_gen\")))\ndata_need$score_soc <- rowSums(select(data_need, matches(\"nee_soc\")))\ndata_need$score_int <- rowSums(select(data_need, matches(\"nee_int\")))\ndata_need$score_all <- rowSums(select(data_need, matches(\"nee\")))\n\n\n\n\n\n  \n\n\n\n\n\n\n2.) Erstelle die personenspezifischen Summenwerte der Items, die interpersonelle Bedürfnisse erfassen (Needs, Interpersonal), für alle Personen, die ein monatliches Einkommen von mehr als $1.000 haben. Bei allen Personen, die weniger zur Verfügung haben, soll ein \"/\" in der Summenwert-Variablen stehen.\n\n\nTipp\n\nZur Bearbeitung der Aufgabe können wir mutate() mit ifelse() kombinieren, um den beiden Bedingungen (monatliches Einkommen von mehr bzw. weniger als $1.000) unterschiedliche Werte zuzuweisen.\n\n\n\nLösung\n\n\n# neuer Datensatz mit relevanten Variablen:\ndata_int <- data[,c(1, # id\n                    4, # inc\n                     grep(\"int\", names(data)))] # Int. Bedürfnisse Items\n\nNun vergleichen wir die Angaben zu den Ausprägungen von inc im Codebuch (S. 28) und der Kodierung in R. Es gibt insgesamt 5 Ausprägungen. Auf unsere Bedingung (mehr als $1.000) treffen drei Ausprägungen zu. Daher ist es codesparender, wenn wir die zwei nicht zutreffenden Ausprägungen (< $500 und $500 - $1000) negieren (!=). Laut Codebuch ist < $500 die erste Ausprägung; $500 - $1000 die zweite. Nun schauen wir, mit welchen Kodierungen diese korrespondieren.\n\ntable(data$inc)\n\n\n  1   2   3   4   5 \n148  74  24  15   6 \n\n\nSie haben die Kodierungen 1 und 2. Diese Information können wir nun anwenden.\n\n# neue Variable mit Summenwert bzw. \"/\" erstellen\nlibrary(dplyr)\ndata_int$score <- ifelse(data_int$inc != 1 & data_int$inc != 2,\n                         rowSums(select(data_need, matches(\"int\"))),\n                         \"/\") # ifelse(Bedingung, trifft zu, trifft nicht zu)\n\n\n\n\n\n  \n\n\n\n\n\n\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] car_3.1-2        carData_3.0-5    rmarkdown_2.21   knitr_1.42      \n[5] readr_2.1.4      kableExtra_1.3.4 dplyr_1.1.2     \n\nloaded via a namespace (and not attached):\n [1] jsonlite_1.8.4    highr_0.10        compiler_4.3.0    crayon_1.5.2     \n [5] webshot_0.5.4     tidyselect_1.2.0  xml2_1.3.3        stringr_1.5.0    \n [9] systemfonts_1.0.4 scales_1.2.1      yaml_2.3.7        fastmap_1.1.1    \n[13] R6_2.5.1          generics_0.1.3    htmlwidgets_1.6.2 tibble_3.2.1     \n[17] munsell_0.5.0     svglite_2.1.1     pillar_1.9.0      tzdb_0.3.0       \n[21] rlang_1.1.0       utf8_1.2.3        stringi_1.7.12    xfun_0.39        \n[25] viridisLite_0.4.1 cli_3.6.1         withr_2.5.0       magrittr_2.0.3   \n[29] digest_0.6.31     rvest_1.0.3       rstudioapi_0.14   hms_1.1.3        \n[33] lifecycle_1.0.3   vctrs_0.6.2       evaluate_0.20     glue_1.6.2       \n[37] abind_1.4-5       fansi_1.0.4       colorspace_2.1-0  httr_1.4.5       \n[41] tools_4.3.0       pkgconfig_2.0.3   htmltools_0.5.5  \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Fehlende-Werte.html#sind-die-missings-einheitlich-kodiert",
    "href": "Fehlende-Werte.html#sind-die-missings-einheitlich-kodiert",
    "title": "9  Fehlende Werte",
    "section": "9.1 Sind die Missings einheitlich kodiert?",
    "text": "9.1 Sind die Missings einheitlich kodiert?\nIn manchen Anwendungen werden Missings nicht mit NA sondern anderweitig kodiert (z.B. bei Unipark mit 99 oder -99). Daher müssen wir dafür sorgen, dass R Missings auch als solche erkennt. Das gewährleistet man, indem man alle Missings auf NA setzt.\n\nWenn man weiß, wie Missings im Datensatz kodiert sind, kann man gleich zu Wie kann ich die Missings auf NA setzen? springen. Wenn man weiß, dass alle Missings einheitlich mit NA kodiert sind, kann man den ganzen Abschnitt auslassen.\n\n9.1.1 Wie kann ich prüfen, ob die Missings einheitlich kodiert sind?\nUm herauszufinden, ob auch alle Missings einer Variablen korrekterweise mit NA kodiert sind, vergleichen wir die angegebenen Werte der Variablen in unseren Daten mit den möglichen Ausprägungen der Variablen (die wir zumeist im Codebuch finden; in unserem Fall steht diese Information in der Einleitung).\nDazu kombinieren wir die Funktion unique(), die uns alle Werte eines Vektors (einmalig) ausgibt, mit der Funktion sort(), die uns die Werte noch sortiert (standarmäßig aufsteigend). Mit na.last = TRUE gibt uns sort() sogar die Information zum Vorhandensein von Missings am Ende aus.\n\nsort(unique(daten$Var_3), na.last=TRUE)\n\n[1]  0  1  3 99 NA\n\n\nDa wir wissen, dass Var_3 nur die Ausprägungen \\(0,1,2,3\\) annehmen kann, können wir schließen, dass \\(99\\) auch ein Missing sein muss.\nWenn man also alle möglichen Ausprägungen der Variablen kennt, kann man auf diese Weise einfach herausfinden, ob noch anderweitig kodierte Missings im Datensatz vorliegen.\nWenn der Datensatz sehr groß ist, ist der oben gezeigte Ansatz allerdings sehr mühsam. Dann können wir die Funktion sapply() integrieren, um unique() und sort() auf jede Variable im Datensatz anzuwenden. Diese hat die Form sapply(Daten, Funktion).\nDa wir zwei Funktionen auf den Datensatz anwenden wollen - unique() und sort() - müssen wir sapply() zweimal anwenden:\n\nsapply(sapply(daten, unique), sort, na.last=TRUE)\n\n$Var_1\n[1] -99   0   1   2   3\n\n$Var_2\n[1] 0 1 2 3\n\n$Var_3\n[1]  0  1  3 99 NA\n\n$Var_4\n[1] 1 2 3\n\n$Var_5\n[1]  0  2  3 99\n\n\n\nDie hier durchgeführte Überprüfung ist analog zum [Plausibilitätscheck][Plausibilitätscheck] im Kapitel zur Datenvorbereitung.\n\nIn Var_1 gibt es die Ausprägung -99 und in Var_5 die Ausprägung 99, welche keine möglichen Ausprägungen sind. Es ist davon auszugehen, dass das ebenso Kodierungen für fehlende Werte sind.\n\n\n9.1.2 Wie kann ich die Missings auf NA setzen?\nNun wollen wir diese Missings umkodieren. Vorher wollen wir uns noch einmal anschauen, was passiert, wenn man das nicht macht.\nWenn man Missings im Datensatz nicht einheitlich auf NA kodiert, nimmt R an, dass es sich um gültige Werte handelt. Das führt dann zu falschen Ergebnissen. Das schauen wir uns exemplarisch einmal am Mittelwert der Spalte Var_3 an.\n\n\n      Var_1 Var_2 Var_3 Var_4 Var_5\nVpn_1   -99     1    NA     1     2\nVpn_2     0     2     1     3     0\nVpn_3     1     3     3     3     2\nVpn_4     3     2    99     1    99\nVpn_5     2     0     0     2     3\n\n\n\n# Mittelwert vor Umkodierung\nmean(daten$Var_3, na.rm=TRUE)\n\n[1] 25.75\n\n\nNun kodieren wir die Missings in Var_3 einheitlich um …\n\n# Umkodierung für einzelne Variablen\ndaten$Var_3[daten$Var_3 == 99] <- NA\n\n\n\n== heißt “ist genau”\n\nDer Befehl ersetzt in daten Elemente der Spalte Var_3, welche die Ausprägung 99 besitzen, mit NA.\n\n\n      Var_1 Var_2 Var_3 Var_4 Var_5\nVpn_1   -99     1    NA     1     2\nVpn_2     0     2     1     3     0\nVpn_3     1     3     3     3     2\nVpn_4     3     2    NA     1    99\nVpn_5     2     0     0     2     3\n\n\n… und schauen uns den Mittelwert von Var_3 wieder an.\n\n# Mittelwert nach Umkodierung\nmean(daten$Var_3, na.rm=T)\n\n[1] 1.333333\n\n\nHätten wir die Missings nicht einheitlich auf NA kodiert, hätten wir errechnet, dass der Mittelwert von Var_3 25.75 anstatt ~1.33 betragen würde.\nWir sehen also, dass es sehr wichtig ist, in Erfahrung zu bringen, ob im Datensatz alle Missings einheitlich auf NA gesetzt sind, und wenn nicht, diese einheitlich zu kodieren, da man sonst falsche Ergebnisse erhält.\nJetzt enthält die Spalte Var_3 schon keine Elemente mit der Ausprägung 99 mehr, aber in Var_1 gibt es noch ein -99 und in Var_5 noch ein 99.\nUm nicht einzeln Spalten und Ausprägungen ansprechen zu müssen, kann man alles in einem Befehl kombinieren.\n\n# Umkodierung für den gesamten Datensatz\ndaten[daten == 99 | daten == -99] <- NA\n\n\n\n| heißt “oder”\n\nHiermit werden im gesamten daten jene Elemente, welche die Ausprägung 99 oder -99 besitzen, durch NA ersetzt."
  },
  {
    "objectID": "Fehlende-Werte.html#enthält-ein-datensatz-missings",
    "href": "Fehlende-Werte.html#enthält-ein-datensatz-missings",
    "title": "9  Fehlende Werte",
    "section": "9.2 Enthält ein Datensatz Missings?",
    "text": "9.2 Enthält ein Datensatz Missings?\nWenn wir wissen wie unsere fehlenden Werte kodiert sind, wollen wir in einem nächsten Schritt natürlich wissen, ob ein Datensatz überhaupt Missings enthält. Es gibt zahlreiche Ansätze, um das herauszufinden. Einige davon schauen wir uns einmal genauer an.\nBei kleineren Datensätzen ist eine visuelle Inspektion möglich. Dafür nutzt man entweder View() (Großbuchstabe am Anfang beachten!) oder man klickt auf den Datensatz im Environment (oberes rechtes Panel).\n\n\n      Var_1 Var_2 Var_3 Var_4 Var_5\nVpn_1    NA     1    NA     1     2\nVpn_2     0     2     1     3     0\nVpn_3     1     3     3     3     2\nVpn_4     3     2    NA     1    NA\nVpn_5     2     0     0     2     3\n\n\nUm zu überprüfen, ob ein Datensatz mindestens einen fehlenden Wert enthält, kann man anyNA() nutzen. Man bekommt ein TRUE (d.h. ja, mindestens ein Missing enthalten) oder FALSE (d.h. nein, keine Missings enthalten) ausgegeben.\n\nanyNA(daten)\n\n[1] TRUE\n\n\nUm einen groben Eindruck davon zu bekommen, welche Elemente fehlen, kann man is.na() nutzen. Der Output besteht aus FALSE oder TRUE für jedes Element des Datensatzes. TRUE bedeutet dabei, dass an dieser Stelle ein Missing ist.\n\nis.na(daten) \n\n      Var_1 Var_2 Var_3 Var_4 Var_5\nVpn_1  TRUE FALSE  TRUE FALSE FALSE\nVpn_2 FALSE FALSE FALSE FALSE FALSE\nVpn_3 FALSE FALSE FALSE FALSE FALSE\nVpn_4 FALSE FALSE  TRUE FALSE  TRUE\nVpn_5 FALSE FALSE FALSE FALSE FALSE\n\n\n\nAchtung: Bei is.na() und anyNA() wird auch NaN (Not a number; entsteht bei unlösbaren Rechnungen) mitgezählt. Da Zweitere aber wesentlich seltener vorkommen, konzentrieren wir uns nur auf NA.\n\nDen logischen Vektor, den is.na() erzeugt, kann man mit which() kombinieren, um sich die Positionen der Missings ausgeben zu lassen. Mithilfe des Arguments arr.ind = TRUE lässt man sich die Reihe und die Spalte dieser ausgeben.\nOhne arr.ind = TRUE würde man nur die Indizes ausgegeben bekommen. Für Matrizen sind diese weniger leicht zu nutzen, weil die Nummerierung fortlaufend spaltenweise vorliegt. In unserem Fall einer 5 x 5 Matrix heißt das z.B., dass das Element in der 1. Zeile der 3. Spalte (also der eine fehlende Wert) den Index 11 trägt.\nBei Vektoren kann man arr.ind = TRUE weglassen, da diese entweder nur aus einer Spalte oder einer Zeile bestehen.\n\nwhich(is.na(daten), arr.ind = TRUE)\n\n      row col\nVpn_1   1   1\nVpn_1   1   3\nVpn_4   4   3\nVpn_4   4   5"
  },
  {
    "objectID": "Fehlende-Werte.html#wie-kann-man-die-missings-zählen-und-verorten",
    "href": "Fehlende-Werte.html#wie-kann-man-die-missings-zählen-und-verorten",
    "title": "9  Fehlende Werte",
    "section": "9.3 Wie kann man die Missings zählen (und verorten)?",
    "text": "9.3 Wie kann man die Missings zählen (und verorten)?\nDie genaue Anzahl der Missings zu kennen ist wichtig, um ein Gefühl dafür zu kriegen, wie vollständig ein Datensatz ist. Dazu kombinieren wir die is.na()-Funktion mit anderen Funktionen, die FALSE (d.h. vorhandenen Werte) und TRUE (d.h. fehlenden Werte) zählen.\n\n9.3.1 Alle Missings eines Datensatzes\nZuerst schauen wir uns die Gesamtanzahl der Missings aller Elemente im Datensatz an.\n\ntable(is.na(daten))\n\n\nFALSE  TRUE \n   21     4 \n\n\n\n\n9.3.2 Missings in einzelnen Spalten oder Zeilen\nSpaltenweises Zählen der Missings gibt Informationen über mögliche Probleme mit bestimmten Variablen. Zeilenweises Zählen der Missings gibt beispielsweise Informationen über Teilnehmende, die die Fragen nicht vollständig beantwortet haben.\nEs ist daher wichtig, sich einen Überblick darüber zu machen, ob sich bei bestimmten Variablen oder bei bestimmten Personen besonders viele Missings häufen. Wenn das der Fall sein sollte, muss man überlegen, wie man damit umgeht (dazu mehr im späteren Verlauf).\nWenn wir eine bestimmte Spalte oder Zeile betrachten möchten, können wir die is.na()-Funktion mit der table()-Funktion kombinieren. Zweiteres sorgt dafür, dass wir eine Häufigkeitstabelle von TRUE und FALSE ausgegeben bekommen.\n\n\n\n\n\n\n\nWir können auf verschiedenem Wege auf eine Spalte bzw. Zeile eines Datensatzes referenzieren.\n\ntable(is.na(daten$Var_1)) # Datensatz$Spaltenname\n\n\nFALSE  TRUE \n    4     1 \n\ntable(is.na(daten[\"Var_1\"])) # Datensatz[\"Spaltenname\"]\n\n\nFALSE  TRUE \n    4     1 \n\ntable(is.na(daten[\"Vpn_1\",])) # Datensatz[\"Zeilenname\"]\n\n\nFALSE  TRUE \n    3     2 \n\ntable(is.na(daten[,1])) # Datensatz[,Spaltenindex]\n\n\nFALSE  TRUE \n    4     1 \n\ntable(is.na(daten[1,])) # Datensatz[,Zeilenindex]\n\n\nFALSE  TRUE \n    3     2 \n\n\n\nAchtung: Die ersten drei vorgestellten Möglichkeiten, $ und Datensatz[\"Spalten- bzw. Zeilenname\"], funktionieren nur bei Dataframes, und nicht bei Matrizen. Die Möglichkeit der Indexierung können wir auch bei Matrizen nutzen.\n\n\nMehr Informationen zu Datenstrukturen finden wir im Kapitel Einführung in R.\n\n\n\n9.3.3 Missings in allen Spalten oder Zeilen\nWenn man sich einen Überblick über die Missings in allen Variablen bzw. bei allen Personen verschaffen möchte, kann man dafür colSums() bzw. rowSums() mit dem is.na()-Befehl kombinieren. Damit werden spalten- bzw. zeilenweise Summen von TRUE (d.h. den Missings) gebildet.\nUm die Größenordnung der Missings besser beurteilen zu können, sollte man sich der maximal möglichen Anzahl der Elemente in einer Spalte bzw. Zeile bewusst sein. Diese können wir mit nrow() bzw. ncol() in Erfahrung bringen.\n\n# Übersicht der Missings in allen Variablen (Spalten)\ncolSums(is.na(daten))\n\nVar_1 Var_2 Var_3 Var_4 Var_5 \n    1     0     2     0     1 \n\n# ... im Vergleich zur maximalen Anzahl an Beantwortungen\nnrow(daten)\n\n[1] 5\n\n\n\n# Übersicht der Missings in allen Personen (Zeilen)\nrowSums(is.na(daten))\n\nVpn_1 Vpn_2 Vpn_3 Vpn_4 Vpn_5 \n    2     0     0     2     0 \n\n# ... im Vergleich zur maximalen Anzahl der beantwortbaren Fragen\nncol(daten)\n\n[1] 5\n\n\n\n\n\n\n\n9.3.3.1 Visualisierung der Missings\nMit der Funktion aggr() aus dem Paket VIM kann man sich zwei Plots ausgeben lassen, die den relativen Anteil von Missings in den einzelnen Variablen und die Anzahl an Missings in bestimmten Kombinationen von Variablen (d.h. in den Zeilen) ausgeben.\nWenn man summary(aggr()) nutzt, bekommt man sowohl die grafische Visualisierung als auch eine Übersicht der Häufigkeiten.\n\n# install.packages(\"VIM\")\nlibrary(VIM)\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\nsummary(aggr(daten))\n\n\n\n\n\n Missings per variable: \n Variable Count\n    Var_1     1\n    Var_2     0\n    Var_3     2\n    Var_4     0\n    Var_5     1\n\n Missings in combinations of variables: \n Combinations Count Percent\n    0:0:0:0:0     3      60\n    0:0:1:0:1     1      20\n    1:0:1:0:0     1      20\n\n\nIm linken Plot sehen wir, dass nur Missings in Var_1, Var_3 und Var_5 vorhanden sind. Außerdem sehen wir auf der \\(y\\)-Achse den relativen Anteil an Fällen in den Variablen. In der Übersicht unter der Tabelle sehen wir die absoluten Häufigkeiten für alle Variablen (Missings per variable). Im rechten Plot sehen wir die vorhanden Kombinationen von Missings in den Variablen. Blau zeigt an, dass kein Missing vorhanden ist; rot zeigt an, dass ein Missing vorhanden ist. Beispielsweise zeigt die unterste Reihe (die komplett blau ist) eine Kombination, in der keine Missings in Variablen vorhanden sind. Rechts daneben sieht man einen Balken, der den Anteil dieser Kombination im Verhältnis zu den anderen Kombinationen darstellt. Der Balken in der untersten Reihe ist der größte, d.h. dass diese Kombination am häufigsten vorkommt und somit die meisten Fälle (Zeilen) im Datensatz keine Missings enthalten. Leider bekommen wir hier keine Häufigkeiten dafür angezeigt. Dazu können wir aber in die unten stehende Übersicht schauen. (Missings in combinations of variables), in der wir absolute und relative Häufigkeiten ausgegeben bekommen."
  },
  {
    "objectID": "Fehlende-Werte.html#sind-die-missings-zufällig",
    "href": "Fehlende-Werte.html#sind-die-missings-zufällig",
    "title": "9  Fehlende Werte",
    "section": "9.4 Sind die Missings zufällig?",
    "text": "9.4 Sind die Missings zufällig?\nAm Anfang des Kapitels wurde bereits erwähnt, dass systematische Missings eine Auswertung verzerren können. Was es aber genau bedeutet, wenn Missings zufällig oder nicht zufällig sind und wie man das überprüfen kann, beleuchten wir in diesem Abschnitt.\n\n9.4.1 Arten von Missings\n\n\n\n\n\n\nEs gibt grundlegend drei Mechanismen, die zur Entstehung von fehlenden Werten führen können: Missing Completely at Random (MCAR), Missing at Random (MAR) und Missing not at Random (MNAR). Im folgenden schauen wir uns die Definition dieser Arten an.\n\nMissing Completely at Random (MCAR)\n\nMissings in einer Variable sind völlig zufällig, wenn sie unabhängig von allen anderen Variablen und dem Missing selbst (d.h. der eigentlichen Ausprägung in dieser Variable, die nicht angegeben wurde) sind. Das heißt, dass fehlende Werte zufällig über alle Beobachtungen verteilt sind. Es gibt somit keine systematischen Missing-Muster.\nMan kann zwar nicht testen, ob ein Missing auf einer Variable aufgrund der eigentlichen Ausprägung in dieser Variablen fehlt (da wir keine Informationen über diese haben), aber man kann testen, ob ein Missing in einer Variable mit den anderen Variablen zusammen hängt. Streng genommen ist also nur ein Teil der Annahme testbar.\n\nMissing at Random (MAR)\n\nMissings in einer Variable sind zufällig, wenn sie durch andere Variablen erklärt werden können. Es gibt somit systematische Missing-Muster. Das heißt, dass Missings häufiger in einem oder mehreren Teilstichproben des Datensatzes vorkommen können. Nach der Kontrolle für die anderen Variablen hängt die Wahrscheinlichkeit für diese Missings aber nicht mehr von ihren eigentlichen (fehlenden) Ausprägungen ab.\nFiktives Beispiel: Männer füllen mit geringerer Wahrscheinlichkeit einen Depressionsfragebogen aus. Das hat aber, nach Kontrolle für Geschlecht, nichts mit ihren Angaben in dem Depressionsfragebogen zu tun.\nDie MAR-Annahme ist nicht direkt testbar, weil man nicht ausschließen kann, dass die Missings nach Kontrolle für die anderen Variablen nicht mehr von ihren eigentlichen Ausprägungen abhängen (da wir keine Informationen über diese haben). Man kann dafür indirekt kontrollieren, in dem man sich beispielsweise Variablen anschaut, die mit der Variable, in der die Missings sind, hoch korrelieren.\n\nMissing not at Random (MNAR)\n\nMissings sind nicht zufällig verteilt und können nicht durch andere Variablen erklärt werden. Dass bedeutet, dass die Ausprägung in der Variable, die fehlt, der Grund dafür ist, das sie fehlt.\nFiktives Beispiel: Männer füllen einen Depressionsfragebogen aufgrund der mit dem Fragebogen zu erfassenden Höhe der Depressivität nicht aus (z.B. bei besonders hoher Depressivität werden Fragen nicht beantwortet).\n\n\n\n\nWas bedeutet “(alle) anderen Variablen”?\n\nDie Auffassung darüber, von welchen “anderen Variablen” die Missings in einer Variable unabhängig sein sollen, unterscheidet sich zwischen verschiedenen AutorInnen und ist nicht immer eindeutig. Während einige grob von beobachtbaren Variablen (Vgl. Schafer & Graham, 2002), verfügbaren Variablen (Vgl. Cohen, Cohen, West & Aiken, 2003) oder Variablen im Datensatz, die analysiert werden (Vgl. Little, 1988) sprechen, grenzen Andere diese mehr ein z.B. Variablen, die im Modell spezifiziert sind (Vgl. Allison, 2002). Letztere Definition erleichtert die Überprüfung der Zufälligkeit der Missings (d.h. ob diese MCAR, MAR, oder MNAR sind).\n\nÜbersicht der Arten von Missings\n\n\n\n\n \n  \n    Fehlende Werte sind unabhängig von ... \n    MCAR \n    MAR \n    MNAR \n  \n \n\n  \n    ... allen andere Variablen \n    X \n     \n     \n  \n  \n    ... ihren eigentlichen (fehlenden) Ausprägungen \n    X \n    X \n     \n  \n\n\n\n\n\nMCAR ist eine strengere Annahme als MAR. Wenn die Daten MCAR sind, dann sind sie auch MAR. Bei MAR und MNAR kann es zu Parameterverzerrungen kommen, wenn man Methoden nutzt, welche die strengere Annahme MCAR voraussetzen.\nBsp.1: Das Löschen von Fällen wenn die Daten nicht MCAR sind.\nBsp.2: Die Nutzung der Maximum Likelihood Schätzung oder der multiplen Imputation wenn die Daten nicht (mindestens) MAR sind.\nSchematisch könnte unser Vorgehen bei der Exploration der Zufälligkeit von Missings folgendermaßen aussehen:\n\n\n\n\n\n\n\n\nBeispiel für Test auf MCAR\n\nExemplarisch wollen wir uns einen möglichen Test anschauen, der überprüft, ob die Annahme von MCAR verletzt ist.\n\\(\\chi^2\\)-Test für multivariate Daten von Little (1988):\nDieser überprüft, ob es signifikante Unterschiede zwischen den Mittelwerten der Muster von fehlenden Werten gibt. Die Nullhypothese (\\(H_0\\)) besagt, dass die Mittelwerte der Variablen (Spalten) nicht in Abhängigkeit der Missingmuster variieren (MCAR). Die Alternativhypothese (\\(H_1\\)) besagt, dass die Mittelwerte sich zwischen den verschiedenen Mustern von Missings unterscheiden (MAR oder MNAR).\nAuch hier muss man sich vorher überlegen, wo man das Signifikanzniveau \\(\\alpha\\) setzt. Für unser Beispiel legen wir es entsprechend der gängigen Konventionen auf \\(\\alpha= 0.05\\) fest.\nZur Durchführung des Tests in R greifen wir auf die Funktion mcar_test() aus dem Paket naniar zu. Das laden wir uns über Github herunter (wofür wir wiederum das Paket remotes benötigen).\n\nAchtung: Die Funktion mcar_test() kann wir nur mit (quantitativen) Daten des Typs numeric (integer und double), logical und factor umgehen.\n\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"njtierney/naniar\")\nlibrary(naniar)\nmcar_test(daten)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      <dbl> <dbl>   <dbl>            <int>\n1      5.88     6   0.437                3\n\n\nAls Output bekommen wir den \\(\\chi^2\\)-Wert (statistic), die Anzahl der Freiheitsgrade (df), den p-Wert (p-value), sowie die Anzahl der Missing-Muster (missing.patterns).\nDer \\(p\\)-Wert für den MCAR-Test für unseren Datensatz ist größer als die Irrtumswahrscheinlichkeit \\(\\alpha\\). Dies bedeutet, dass wir die \\(H_0\\) beibehalten können und (bis auf weiteres) davon ausgehen, dass die MCAR-Annahme erfüllt ist.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFür mehr Informationen zu diesem Test siehe Little (1988).\nIm Paket naniar gibt es noch viele weitere Funktionen zur Zusammenfassung, Visualisierung und Manipulation von fehlenden Werten. Auf der Github-Seite finden wir eine Übersicht einiger dieser Funktionen."
  },
  {
    "objectID": "Fehlende-Werte.html#wie-kann-man-mit-missings-umgehen",
    "href": "Fehlende-Werte.html#wie-kann-man-mit-missings-umgehen",
    "title": "9  Fehlende Werte",
    "section": "9.5 Wie kann man mit Missings umgehen?",
    "text": "9.5 Wie kann man mit Missings umgehen?\nEs gibt verschiedene Möglichkeiten, um mit unvollständigen Datensätzen umzugehen. Diese sind mehr oder weniger geeignet in Abhängigkeit davon, welche Annahme (MCAR, MAR, MNAR) die Missings erfüllen. Es gibt aber keine einheitlichen Richtlinien darüber, wie man mit Missings umgehen sollte. Das liegt u.a. auch daran, dass schon die Überprüfung der Zufälligkeit von Missings schwierig ist. Wichtig ist grundsätzlich, dass man sich mit den Missings eines Datensatzes auseinander setzt und einen Weg findet, mit ihnen umzugehen, ohne dass dies die Ergebnisse verzerren könnte. Man muss also für die eigene Fragestellung und Auswertungsmethode einen geeigneten Weg finden.\n\n\nWeitere Methoden zum Umgang mit Missings finden wir auch in den Quellen, die wir unter Literaturempfehlungen finden.\n\nIm Folgenden wollen wir uns darauf beschränken, uns einige gängige Methoden anzuschauen, die man nutzen kann, wenn die fehlenden Werte MCAR sind. Wenn diese Annahme nicht erfüllt ist, können bei Nutzung der folgenden Methoden verzerrte Parameterschätzungen resultieren.\nZwei grundlegende Möglichkeiten sind entweder ganze Fälle mit Missings zu exkludieren (listwise/casewise deletion) oder vorhandene Elemente von Fällen für einen Teil der Analysen zu nutzen (pairwise deletion). In vielen Funktionen in R können wir zwischen beiden Möglichkeiten entscheiden.\nNachteil von beiden (d.h. kompletter bzw. partieller Ausschluss von Zeilen) ist generell, dass die Stichprobengröße \\(N\\) sinkt und damit einhergehend größere Standardfehler und eine geringere Power resultieren. Ein weiteres Problem bei pairwise deletion ist außerdem, dass sich die Stichprobengröße \\(N\\) sowie die Zusammensetzung der Stichproben für unterschiedliche Analysen unterscheiden wird.\nEine weitere Möglichkeit mit fehlenden Werten umzugehen ist diese zu imputieren (d.h. diese “vorherzusagen”). Da Imputation aber ein vielschichtiges Thema mit vielen verschiedenen Methodiken ist, gehen wir im Weiteren nicht darauf ein.\n\n9.5.1 Neuen Datensatz erstellen, der keine Missings enthält\nMit na.omit() löscht man listwise/casewise, d.h. Fälle, die mindestens ein Missing aufweisen werden komplett gelöscht. Es ist ratsam, den damit neu erstellten Datensatz als ein neues Objekt zu speichern (anstatt den originalen Satensatz zu überschreiben):\n\ndaten_cw <- na.omit(daten)\n\n\n\n      Var_1 Var_2 Var_3 Var_4 Var_5\nVpn_2     0     2     1     3     0\nVpn_3     1     3     3     3     2\nVpn_5     2     0     0     2     3\n\n\n\n\n\n\nWir haben jetzt leider den Nachteil, dass unser Datensatz von fünf auf drei Personen geschrumpft ist (weil zwei Personen mindestens auf einer Variablen ein Missing hatten).\nAbhängig von der eigenen Auswertung möchte man das vielleicht eher nicht so machen, sondern die vorhandenen Werte in den hier gelöschten Zeilen noch anderweitig nutzen.\n\n\n9.5.2 Festlegen, wie Funktionen mit Missings umgehen sollen\nAnstatt die Daten in einem ersten Schritt hinsichtlich der fehlenden Werte zu bereinigen, erlauben viele Funktionen den Umgang mit fehlenden Werten direkt mittels zusätzlicher Argumente zu spezifizieren. Um zu erfahren, welche Argumente eine Funktion nutzen kann, können wir im unteren rechten Panel bei Help nachschauen.\nDazu schauen wir uns exemplarisch drei verschiedene Funktionen und einige ihrer Möglichkeiten im Umgang mit fehlenden Werten an.\n\n9.5.2.1 lm(…, na.action)\nMit lm() können wir eine (einfache oder multiple) lineare Regression durchführen. Mit dem Parameter na.action können wir über den Umgang mit den Missings bestimmen. Ein mögliches Argument dafür ist na.omit. Dabei wird bei Vorhandenseins eines Missings in einer Zeile die komplette Zeile aus der Berechnung genommen (listwise/casewise deletion). Das ist der Default dieser Funktion.\nIn manchen Situationen ist es wichtig, Informationen darüber zu haben, wo Missings in einer Zeile sind (z.B. bei der Prüfung der Annahmen der Unabhängigkeit der Fehlerterme in der Multiplen Linearen Regression). Wenn ich beispielsweise einen Boxplot der Residuen einer linearen Regression in einer bestimmten Gruppe erstellen möchte, benötige ich einen Vektor der Residuen, in dem noch die Information darüber enthalten ist, in welcher Zeile Werte fehlen. Wenn der Residuenvektor und der Gruppenvektor unterschiedliche Zeilenanzahlen - in Abhängigkeit der Missings - haben, kann ich den Boxplot sonst nicht erstellen. Dafür nutzen wir das Argument na.exclude. Hierbei werden die Indizes der Missings nicht einfach gelöscht (und dadurch die Zeilenanzahl reduziert) sondern gespeichert. Ansonsten ist die Berechnung äquivalent zu na.omit (d.h. auch listwise/casewise deletion). Mittels residuals(lm_Ergebnisobjekt) können wir dann den Residuenvektor extrahieren.\n\n\n\n\nHier gehts zur Überprüfung der Unabhängigkeit der Fehlerterme mittels Boxplots -Section 14.4.1.2.2 aus dem Kapitel zu Annahmen der Regression.\n\n\n\n9.5.2.2 mean(…, na.rm)\nDie Funktion mean() enthält den Parameter na.rm, welcher festlegt, ob einzelne fehlende Elemente vor der Ausführung der Funktion entfernt werden sollen. Mit TRUE entfernen wir die Missings; mit FALSE behalten wir sie. Bei vielen Funktionen ist letzteres voreingestellt, was häufig aber eine Durchführung der Funktion verhindert.\n\nmean(daten$Var_1) # kann nicht berechnet werden weil Default na.rm = FALSE\n\n[1] NA\n\nmean(daten$Var_1, na.rm = TRUE)\n\n[1] 1.5\n\n\n\n\n9.5.2.3 colMeans(…, na.rm)\nDie Funktion colMeans(), mit der wir Spaltenmittelwerte von mehrdimensionalen Datenstrukturen (z.B. Matrizen oder Dataframes) berechnen können, besitzt ebenfalls den Parameter na.rm. TRUE lässt uns hier (direkt) pairwise deletion anwenden. Schauen wir uns das einmal genauer an.\n\nExemplarisch begrenzen wir uns auf die ersten drei Spalten von daten mittels [, 1:3].\n\nmean(daten[, 1], na.rm = TRUE)\nmean(daten[, 2]) # na.rm=TRUE nicht notwendig\nmean(daten[, 3], na.rm = TRUE)\n\n[1] 1.5\n[1] 1.6\n[1] 1.333333\n\n\n\ncolMeans(daten[, 1:3], na.rm = TRUE)\n\n   Var_1    Var_2    Var_3 \n1.500000 1.600000 1.333333 \n\n\nWie wir sehen, bekommen wir die gleichen Ergebnisse bei mean() und colMeans(). Beide nutzen (quasi) pairwise deletion. Allerdings sprechen wir nur im Fall von colMeans() davon, weil es bei mean() keine andere Möglichkeit gibt, als jeweils die fehlende Werte eines Vektors (eindimensionale Datenstruktur) zu entfernen oder eben nicht.\n\ncolMeans(daten_cw[, 1:3])\n# daten_cw sind die mit na.omit() bereinigten Daten (listwise deletion)\n\n   Var_1    Var_2    Var_3 \n1.000000 1.666667 1.333333 \n\n\nVergleichen wir diese Ergebnisse nun mit denen von oben, sehen wir, dass nur der Mittelwert von Var_3 gleich. Bei den Mittelwerten der beiden anderen Spalten unterscheiden sich die Ergebnisse.\n\n\n      Var_1 Var_2 Var_3\nVpn_1    NA     1    NA\nVpn_2     0     2     1\nVpn_3     1     3     3\nVpn_4     3     2    NA\nVpn_5     2     0     0\n\n\nDie unterschiedlichen Spaltenmittelwerte kommen daher zustande, dass na.omit() für alle Berechnungen Vpn_1 und Vpn_4 ausschließt (listwise/casewise deletion), wohingegen na.rm die Missings nur in den Spalten ausschließt, die gerade zur Berechnung benötigt werden (pairwise deletion), z.B. Vpn_4 bei der Berechnung des Mittelwerts von Var_3, aber nicht bei denen von Var_1 und Var_2.\n\n\n9.5.2.4 cor(…, use)\nMit cor() können wir Korrelationstabellen berechnen. Dabei können wir mit dem Parameter use festlegen, wie mit Missings umgegangen werden sollen. Wir beschränken uns hier auf zwei Möglichkeiten von use. Mit complete.obs nutzen wir listwise/casewise deletion; mit pairwise.complete.obs nutzen wir pairwise deletion.\nUm den Unterschied zwischen beiden Möglichkeiten besser zu verstehen, schauen wir uns die jeweiligen Korrelationstabellen (der ersten drei Variablen) an.\nDa die Korrelationsmatrizen symmetrisch sind (d.h. ober- und unterhalb der Diagonalen gleich sind) wird jeweils die obere Diagonale für die Tabellen ausgeblendet.\n\ncor_co <- cor(daten[, 1:3], use = \"complete.obs\")\ncor_co <- round(cor_co, 3) \n\n\n\n\nKorrelation mit complete.obs\n \n  \n      \n    Var_1 \n    Var_2 \n    Var_3 \n  \n \n\n  \n    Var_1 \n    1 \n     \n     \n  \n  \n    Var_2 \n    -0.655 \n    1 \n     \n  \n  \n    Var_3 \n    -0.327 \n    0.929 \n    1 \n  \n\n\n\n\n\ncor_pco <- cor(daten[, 1:3],  use = \"pairwise.complete.obs\")\ncor_pco <- round(cor_pco, 3)\n\n\n\n\nKorrelation mit pairwise.complete.obs\n \n  \n      \n    Var_1 \n    Var_2 \n    Var_3 \n  \n \n\n  \n    Var_1 \n    1 \n     \n     \n  \n  \n    Var_2 \n    -0.308 \n    1 \n     \n  \n  \n    Var_3 \n    -0.327 \n    0.929 \n    1 \n  \n\n\n\n\nWenn man die beiden Korrelationstabellen vergleicht, sieht man, dass sich die Korrelation zwischen Var_1 und Var_2 unterscheidet. Das liegt daran, dass Vpn_4 in allen Berechnungen mit complete.obs ausgeschlossen wurde, weil Var_3 dort ein Missing enthält, während pairwise.complete.obs diese Zeile bei der Korrelation von Var_1 und Var_2 miteinbezogen hat.\n\n\n      Var_1 Var_2 Var_3\nVpn_1    NA     1    NA\nVpn_2     0     2     1\nVpn_3     1     3     3\nVpn_4     3     2    NA\nVpn_5     2     0     0\n\n\n\nAchtung: Im Gegensatz zu complete.obs basieren die verschiedenen Korrelationen bei pairwise.complete.obs auf Werten aus unterschiedlichen Zeilen (d.h. von unterschiedlichen Personen)."
  },
  {
    "objectID": "Fehlende-Werte.html#literaturempfehlungen",
    "href": "Fehlende-Werte.html#literaturempfehlungen",
    "title": "9  Fehlende Werte",
    "section": "9.6 Literaturempfehlungen",
    "text": "9.6 Literaturempfehlungen\nFür ein tiefergehenden Einblick empfehlen wir Euch die folgenden Arbeiten:\n\nAllison, P. D. (2002). Missing Data. In P. D. Allison (Ed.), The Sage Handbook of Quantitative Methods in Psychology (pp.72-89). Thousand Oaks, CA: Sage Publications Ltd. Abgerufen über http://www.statisticalhorizons.com/wp-content/uploads/2012/01/Milsap-Allison.pdf\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Missing Data. In J. Cohen, P. Cohen, S. G. West, & L. S. Aiken (Eds.), Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences (pp. 431-451)*. Hillsdale, NJ: Erlbaum.\n(für HU-Studierende über ub.hu-berlin.de zugänglich)\n\n\n\nLehrbuch der Master-Vorlesung “Multivariate Verfahren”\n\n\nLittle, R. J. A. (1988). A test of missing completely at random for multivariate data with missing values. Journal of the American Statistical Association, 83(404), 1198–1202.\n(für HU-Studierende über ub.hu-berlin.de zugänglich)\n\n\nSchafer, J. L., & Graham, J. W. (2002). Missing Data: Our View of the State of the Art. Psychological Methods, 7(2), 147-177. https://psycnet.apa.org/doi/10.1037/1082-989X.7.2.147"
  },
  {
    "objectID": "Fehlende-Werte.html#faq",
    "href": "Fehlende-Werte.html#faq",
    "title": "9  Fehlende Werte",
    "section": "9.7 FAQ",
    "text": "9.7 FAQ\nWir haben fehlende Werte (sog. Missings) in unserem Datensatz und wissen nicht, wie wir damit umgehen sollen? In diesem Abschnitt bekommen wir eine kurze Antwort darauf.\n\nWenn dieser Abschnitt nicht ausreicht, oder wir mehr zu fehlenden Daten wissen möchtest, können wir uns das detaillierte Einführungskapitel dazu anschauen.\n\nAchtung: Wenn wir Variablen, die Missings enthalten, für eine Analyse nutzen wollen, sollten wir immer daran denken, dass sich damit auch die Stichprobengröße \\(N\\) für diese spezifische Auswertung ändert.\n\n\n\n9.7.1 Erkennt R deine Missings?\nGenerell werden Missings in verschiedenen Anwendungen (z.B. Unipark, SPSS) häufig anders kodiert als in R. In R werden fehlende Werte mit NA gekennzeichnet. Wenn das in deinem Datensatz nicht (einheitlich) so ist, musst du die Missings erst auf NA kodieren, damit R diese auch als Missings erkennt.\nWenn du nicht weißt, ob die Missings in deinem Datensatz auch anders kodiert sein könnten, kannst du das mit einer Häufigkeitstabelle der einzelnen Ausprägung der Variablen (d.h. Spalten) überprüfen. Dazu musst du nur wissen, welche möglichen Ausprägungen es geben kann (z.B. wenn du eine Intervallskala von 1-5 hast dann sollte es nur diese Werte geben), um Abweichungen davon festzustellen.\ntable(daten$Var, useNA='ifany')\nWenn die Missings z.B. mit 99 kodiert sind, können wir sie folgendermaßen auf NA setzen:\n\ndaten[daten == 99] <- NA\n\n\n9.7.2 Wie können Funktionen mit Missings umgehen?\nBei vielen Funktionen muss man festlegen, wie diese mit Missings umgehen sollen. Exemplarisch schauen wir uns das einmal an zwei Funktionen an.\nWenn du wissen möchtest, wie du in anderen Funktionen mit Missings umgehen kannst, schau dir entweder die R-Dokumentation dazu an (unteres rechtes Panel bei Help oder alternativ ?mean) oder suche im Internet. In unserem Kapitel zu Fehlermeldungen findest du sowohl einen Abschnitt zum Aufbau der [R-Dokumentation][R-Dokumentation] sowie einen Abschnitt zum [Suchen im Internet][Suchen im Internet].\n\n\nmean( )\n\nBei der Berechnung des Mittelwerts eines Vektors kann man Missings rausschmeißen, indem man das Argument na.rm nutzt: mean(daten, na.rm=TRUE)\n\n\nlm( )\n\nBei der Regressionsrechnung ist voreingestellt (“defaulted”), dass Personen mit mindestens einem Missing auf irgendeiner Variable aus der Rechnung ausgeschlossen werden (“listwise deletion”; manchmal auch “casewise deletion” genannt). Andere Optionen kann man mit dem Argument na.action festlegen. Um zu sehen, welche anderen Optionen es gibt, schaue in der Hilfe nach z.B. mit ?lm."
  },
  {
    "objectID": "Fehlende-Werte.html#übung",
    "href": "Fehlende-Werte.html#übung",
    "title": "9  Fehlende Werte",
    "section": "9.8 Übung",
    "text": "9.8 Übung\nIm Folgenden wollen wir einen Datensatz hinsichtlich der fehlenden Werte (Missings) beurteilen. Dazu schauen wir, ob die fehlenden Werte korrekt kodiert sind, wie viele und auf welchen Variablen bzw. in welchen Fällen diese vorhanden sind, ob sie zufällig sind und wie wir mit ihnen umgehen können.\n\nHier finden wir das Einführungsskript zu Fehlenden Werten.\n\n\nAchtung: Die Aufgabenstellungen hier überschneiden sich teilweise mit denen aus der [Übung zur Datenvorbereitung][Übung Datenvorbereitung]. Wir arbeiten hier aber mit anderen Datensätzen.\n\n\n\nDatensatz A: Normed Causality Statements\n\nIn dieser querschnittlichen Studie untersuchten Hussey & De Houwer inwieweit Personen normativ unmissverständlichen kausalen Aussagen zustimmen (z.B. X ruft Y hervor: Witze rufen Gelächter hervor).\nMehr Informationen zur Studie befinden sich auf der OSF-Seite. Den Datensatz finden wir hier; ein Codebuch dazu hier.\n\nAchtung: Das Codebuch enthält nicht zu allen Variablen Informationen, da es für den aufbereiteten Datensatz erstellt wurde und wir uns aber die Rohdaten anschauen. Einen Großteil der Variablen, die nicht im Codebuch zu finden sind, entfernen wir noch.\n\nNach dem Herunterladen, können wir den Datensatz folgendermaßen in R einlesen:\n\n\n\n\ndata_a <- read.csv(\"Dateipfad/group_a.csv\") # hier den eigenen Dateipfad einfügen\n\nWir entfernen noch einige für uns irrelevante Informationen zur Erhebung:\n\ndata_a <- data_a[,-c(2:7, 9, 145:153)]\n\n\n\n\n\n  \n\n\n\n\nAchtung: Wir gehen im Folgenden davon aus, dass die Variablen statements..c1., statements..c2.. statements..c3. und statements..c4. aus dem Datensatz den Variablen catch_1, catch_2, catch_3 und catch_4 entsprechen.\n\n\n\n\nDatensatz B: Affective Forecasting and Social Anxiety\n\nIn der Studie untersuchen Glenn & Teachman, inwiefern sich Menschen mit geringer und starker Sozialangst bezüglich ihrer Bewertung von zukünftigen emotionalen Situationen unterscheiden.\nDen Datensatz finden wir hier; ein detailliertes Codebuch mit weiteren Informationen zur Studie hier.\n\nAchtung: Das Codebuch enthält leider keine Informationen zu den demographischen Variablen. Einige werden wir uns dennoch anschauen, da sie eindeutig interpretierbar erscheinen.\n\nDen Datensatz können wir, nachdem wir ihn heruntergeladen haben, folgendermaßen in R einlesen:\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nfigures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav: Very long string\nrecord(s) found (record type 7, subtype 14), each will be imported in\nconsecutive separate variables\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias1\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias2\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias3\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias4\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias5\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias6\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias7\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias8\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias9\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias10\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias11\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias12\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias13\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias14\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias15\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias16\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias17\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias18\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias19\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias20\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 4 added in variable: pre_outcome_old\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs1\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs2\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs3\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs4\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs5\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs6\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs7\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs8\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs9\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs10\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs11\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs12\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs13\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs14\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs15\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs16\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs17\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs18\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs19\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs20\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs21\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs22\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq1\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq2\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq3\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq4\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq5\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq17\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq16\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq15\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq8\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq9\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq10\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq14\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq13\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq6\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq7\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq11\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq12\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq18\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq19\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq20\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq21\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq28\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq27\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq26\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq24\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq25\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq22\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq23\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq32\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq31\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq29\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq30\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq33\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq34\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq39\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq38\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq37\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq35\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq36\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes1\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes2\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes3\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes4\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes5\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes6\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes7\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes8\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes9\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes10\n\n\n\n# install.packages(\"foreign\")\nlibrary(foreign)\ndata_b <- read.spss(\"Dateipfad/AffectiveForecasting_0707017.sav\", to.data.frame = TRUE) \n# noch den eigenen Dateipfad einfügen\n\nDa der Datensatz aus 479 Variablen besteht, wollen wir unsere Auswahl etwas eingrenzen. Wir schauen uns nur folgende Variablen an:\n\n# nur Daten aus dem Pretest\ndata_b <- data_b[, c(1:4, 6:7, # soziodemographische Variablen\n                    12:13, 24:27, 30:49, 75:94)] # Pretest Variables\n# wir schauen uns nur die umkodierten Pretest Variablen an\n# d.h. jene ohne \"_orig\"\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n9.8.1 Übung 1: (Korrekte) Kodierung\nBevor wir uns die fehlenden Werte genauer anschauen können, ist es sinnvoll, einen Plausibilitätscheck durchzuführen. Damit überprüfen wir, ob fehlende Werte auch korrekt kodiert sind, d.h. mit NA.\n1.) Gibt es fehlende Werte im Datensatz, die nicht mit NA kodiert sind?\n\n\nTipp\n\nHier vergleichen wir die möglichen Ausprägungen der Variablen, die wir im Codebuch finden, mit den tatsächlichen Ausprägungen der Variablen, die wir uns in R anschauen können.\n\n\n\nLösung A\n\n\n# sortierte Ausprägungen der Variablen inklusive NAs anzeigen:\nsapply(sapply(data_a, unique), sort, na.last=TRUE)\n\n$id\n [1]   1  11  21  31  41  51  61  71  81  91 101 111 121 131 141 151 161 171 181\n[20] 191 201 211 221 231 241 251 261 271 281 291 301 311 321 331 341 351 361 371\n[39] 381 391 401 411 421 431 441 451 461 471 481 491 501 511 521 531 541 551 561\n[58] 571 581 591 601 611 621 631 641 651 661 671 681 691 701\n\n$consent\n[1] \"\"  \"Y\"\n\n$age\n [1] 20 21 22 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 41 42 44 46 49 51\n[26] 53 54 56 61 NA\n\n$gender\n[1] \"\"  \"f\" \"m\"\n\n$gender...comment\n[1] \"\"                                                                                                                     \n[2] \"Comment? Thanks for the opportunity? Unsure why there is a comment box before the task is completed, but that's okay.\"\n[3] \"have a good day\"                                                                                                      \n[4] \"N/A\"                                                                                                                  \n\n$statements..1.\n[1]  1  2  3  5 NA\n\n$statements..2.\n[1]  1  2  3  4 NA\n\n$statements..3.\n[1]  1  2  3  4  5 NA\n\n$statements..4.\n[1]  1  2  3  4 NA\n\n$statements..5.\n[1]  1  2  3  5 NA\n\n$statements..6.\n[1]  1  2  3  4 NA\n\n$statements..7.\n[1]  1  2  3  4  5 NA\n\n$statements..8.\n[1]  3  4  5 NA\n\n$statements..9.\n[1]  1  2  3  4 NA\n\n$statements..10.\n[1]  3  4  5 NA\n\n$statements..11.\n[1]  1  2  3  4  5 NA\n\n$statements..12.\n[1]  1  2  3  4 NA\n\n$statements..13.\n[1]  1  2  3  4  5 NA\n\n$statements..14.\n[1]  1  2  3  4 NA\n\n$statements..15.\n[1]  1  2  3  4 NA\n\n$statements..16.\n[1]  2  4  5 NA\n\n$statements..17.\n[1]  1  2  3  4  5 NA\n\n$statements..18.\n[1]  1  2  3  4 NA\n\n$statements..19.\n[1]  1  2  3  4  5 NA\n\n$statements..20.\n[1]  1  2  3  4  5 NA\n\n$statements..21.\n[1]  1  2  3  4  5 NA\n\n$statements..22.\n[1]  1  2  4 NA\n\n$statements..23.\n[1]  1  2  3  4  5 NA\n\n$statements..24.\n[1]  1  2  3 NA\n\n$statements..25.\n[1]  1  2  3  4 NA\n\n$statements..26.\n[1]  1  2 NA\n\n$statements..27.\n[1]  1  2  3  4  5 NA\n\n$statements..28.\n[1]  1  3  4  5 NA\n\n$statements..29.\n[1]  1  2  3  4 NA\n\n$statements..30.\n[1]  1  2  3  4 NA\n\n$statements..31.\n[1]  1  2  5 NA\n\n$statements..32.\n[1]  1  2  3  4  5 NA\n\n$statements..33.\n[1]  1  2  3  4 NA\n\n$statements..34.\n[1]  1  2  3  4 NA\n\n$statements..35.\n[1]  1  2  3  4  5 NA\n\n$statements..36.\n[1]  1  2  3 NA\n\n$statements..37.\n[1]  1  2  3  4  5 NA\n\n$statements..38.\n[1]  1  2  3  4  5 NA\n\n$statements..39.\n[1]  1  2  3  4  5 NA\n\n$statements..40.\n[1]  1  2  3  4  5 NA\n\n$statements..41.\n[1]  1  2  3  4 NA\n\n$statements..42.\n[1]  1  2  3  4  5 NA\n\n$statements..43.\n[1]  1  2  3  4  5 NA\n\n$statements..44.\n[1]  1  2  3  4  5 NA\n\n$statements..45.\n[1]  1  2  3  4  5 NA\n\n$statements..46.\n[1]  1  2  3  4  5 NA\n\n$statements..47.\n[1]  1  2  3  4  5 NA\n\n$statements..48.\n[1]  1  2  3  4  5 NA\n\n$statements..49.\n[1]  1  2  3  4  5 NA\n\n$statements..50.\n[1]  1  2  3  4  5 NA\n\n$statements..51.\n[1]  1  2  3  4  5 NA\n\n$statements..52.\n[1]  1  2  3  4  5 NA\n\n$statements..53.\n[1]  3  4  5 NA\n\n$statements..54.\n[1]  1  2  3  4  5 NA\n\n$statements..55.\n[1]  1  2  3  4  5 NA\n\n$statements..56.\n[1]  1  2  3  4  5 NA\n\n$statements..57.\n[1]  1  2  3  4  5 NA\n\n$statements..58.\n[1]  1  2  3  4  5 NA\n\n$statements..59.\n[1]  1  2  3  4 NA\n\n$statements..60.\n[1]  1  2  3  4  5 NA\n\n$statements..61.\n[1]  1  2  3  4  5 NA\n\n$statements..62.\n[1]  1  2  3  4  5 NA\n\n$statements..63.\n[1]  1  2  3 NA\n\n$statements..64.\n[1]  2  3  4  5 NA\n\n$statements..65.\n[1]  1  2  3  4  5 NA\n\n$statements..66.\n[1]  1  2  3  4  5 NA\n\n$statements..67.\n[1]  1  3  4  5 NA\n\n$statements..68.\n[1]  1  2  3  4  5 NA\n\n$statements..69.\n[1]  1  2  3  4  5 NA\n\n$statements..70.\n[1]  1  2  3  4  5 NA\n\n$statements..71.\n[1]  1  2  3  4  5 NA\n\n$statements..72.\n[1]  1  2  3  4 NA\n\n$statements..73.\n[1]  1  2  3  4 NA\n\n$statements..74.\n[1]  1  2  3  4  5 NA\n\n$statements..75.\n[1]  1  2  3  4  5 NA\n\n$statements..76.\n[1]  1  2  3  4  5 NA\n\n$statements..77.\n[1]  1  2  3  4 NA\n\n$statements..78.\n[1]  1  2  3  4  5 NA\n\n$statements..79.\n[1]  1  2  3  4  5 NA\n\n$statements..80.\n[1]  1  2  3  4  5 NA\n\n$statements..81.\n[1]  1  2  3  4  5 NA\n\n$statements..82.\n[1]  1  2  3  4  5 NA\n\n$statements..83.\n[1]  1  2  3  4  5 NA\n\n$statements..84.\n[1]  1  2  3  4 NA\n\n$statements..85.\n[1]  1  2  3  4 NA\n\n$statements..86.\n[1]  1  2  3  4  5 NA\n\n$statements..87.\n[1]  2  3  4  5 NA\n\n$statements..88.\n[1]  1  2  3  4  5 NA\n\n$statements..89.\n[1]  1  2  4  5 NA\n\n$statements..90.\n[1]  1  2  3  4  5 NA\n\n$statements..91.\n[1]  1  2  3  5 NA\n\n$statements..92.\n[1]  1  2  3  4  5 NA\n\n$statements..93.\n[1]  1  2  3  4  5 NA\n\n$statements..94.\n[1]  1  2  3  4  5 NA\n\n$statements..95.\n[1]  3  4  5 NA\n\n$statements..96.\n[1]  1  2  3  4 NA\n\n$statements..97.\n[1]  1  2  3  4 NA\n\n$statements..98.\n[1]  1  2  3  4  5 NA\n\n$statements..99.\n[1]  1  2  3  4 NA\n\n$statements..100.\n[1]  2  3  4  5 NA\n\n$statements..101.\n[1]  1  2  3  4  5 NA\n\n$statements..102.\n[1]  1  2  3  4 NA\n\n$statements..103.\n[1]  4  5 NA\n\n$statements..104.\n[1]  1  2  3  4  5 NA\n\n$statements..105.\n[1]  3  4  5 NA\n\n$statements..106.\n[1]  1  2  3  5 NA\n\n$statements..107.\n[1]  1  2  3  4  5 NA\n\n$statements..108.\n[1]  1  2  3  4 NA\n\n$statements..109.\n[1]  1  2  3  4 NA\n\n$statements..110.\n[1]  3  4  5 NA\n\n$statements..111.\n[1]  1  2  3  4 NA\n\n$statements..112.\n[1]  2  3  4  5 NA\n\n$statements..113.\n[1]  1  2  3  4  5 NA\n\n$statements..114.\n[1]  1  2  3  4  5 NA\n\n$statements..115.\n[1]  1  2  3  4 NA\n\n$statements..116.\n[1]  1  2  3  4  5 NA\n\n$statements..117.\n[1]  2  3  4  5 NA\n\n$statements..118.\n[1]  1  2  3  4 NA\n\n$statements..119.\n[1]  2  3  4  5 NA\n\n$statements..120.\n[1]  1  2  3  5 NA\n\n$statements..121.\n[1]  1  2  3  4  5 NA\n\n$statements..122.\n[1]  1  2  3  4 NA\n\n$statements..123.\n[1]  1  3  4  5 NA\n\n$statements..124.\n[1]  1  2  3  4  5 NA\n\n$statements..125.\n[1]  1  2  3  4  5 NA\n\n$statements..126.\n[1]  1  2  3  4 NA\n\n$statements..127.\n[1]  1  2  3  5 NA\n\n$statements..128.\n[1]  1  2  3  4 NA\n\n$statements..c1.\n[1]  4 NA\n\n$statements..c2.\n[1]  4 NA\n\n$statements..c3.\n[1]  1  2  4 NA\n\n$statements..c4.\n[1]  1  2  4 NA\n\n\nIn den Variablen consent, gender, gender...comment kommt eine leere Ausprägung (\"\") vor. Außerdem hat hat eine Person in der Variablen gender...comment den Text N/A angegeben. Dieser String wird allerdings nicht als korrekte NA-Kodierung erkannt. Wir können in beiden Fällen davon ausgehen, dass dies Missings sind, die nicht richtig kodiert wurden.\n\n\n\nLösung B\n\n\n# sortierte Ausprägungen der Variablen inklusive NAs anzeigen:\nsapply(sapply(data_b, unique), sort, na.last=TRUE) \n\n$P_ID\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  61  63  64  65  66  67  68  69  70  71  72  73  74\n [73]  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92\n [91]  93  94  95  96  98  99 100 101 102 103 104 105 107 108 109 110 111 112\n[109] 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n[127] 131 132 133 134 135 136 137 138 139 141 142 143 144 145 146 147 148 149\n[145] 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167\n[163] 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n[181] 186 187 188 189 190 191 192\n\n$age\n[1] 17 18 19 20 21 22 24 25 38\n\n$gender\n[1] Male   Female\nLevels: Male Female Other/Prefer not to answer\n\n$race_white_yn\n[1]  0  1 NA\n\n$ethn\n[1] Hispanic/Latino      Not Hispanic/Latino  Prefer not to answer\nLevels: Hispanic/Latino Not Hispanic/Latino Prefer not to answer\n\n$race\n[1] Asian                     Black or African American\n[3] White                     More than one race       \n[5] Other                     Prefer not to answer     \n8 Levels: American Indian/Alaskan Native Asian ... Prefer not to answer\n\n$group\n[1] Low SA  High SA\nLevels: Low SA High SA\n\n$cond\n[1] Negative speech evaluation Positive speech evaluation\nLevels: Negative speech evaluation Positive speech evaluation\n\n$preselect_curr_e1\n [1] -100  -67  -65  -62  -61  -54  -51  -50  -47  -42  -36  -34  -32  -31  -25\n[16]  -23  -22  -21  -20  -19  -17  -16  -15  -14  -13  -12  -11  -10   -9   -8\n[31]   -7   -1    0    4    7    9   10   11   12   13   16   18   19   20   21\n[46]   22   23   25   26   28   29   30   31   32   33   34   35   36   38   39\n[61]   40   41   42   43   44   45   46   47   48   49   50   51   52   53   54\n[76]   57   59   60   61   62   64   67   68   69   70   71   72   77   79   83\n[91]   84   89   90   92   98  100   NA\n\n$preselect_curr_e2\n [1] -97 -67 -64 -57 -49 -44 -43 -32 -30 -27 -24 -23 -22 -21 -19 -17 -16 -15 -13\n[20] -12  -9  -8  -7  -2  -1   0   3   4   5   6   9  10  11  12  13  14  15  16\n[39]  17  19  20  21  22  23  25  26  27  28  29  30  31  32  33  34  35  37  38\n[58]  41  42  43  45  46  47  48  49  52  53  54  55  57  61  62  63  64  66  67\n[77]  68  69  72  74  82  83  86 100  NA\n\n$preselect_curr_e3\n [1] -100  -78  -69  -61  -58  -57  -50  -47  -45  -42  -39  -35  -31  -30  -29\n[16]  -28  -27  -26  -25  -24  -23  -21  -20  -19  -18  -17  -16  -15  -14  -12\n[31]  -11  -10   -9   -8   -7   -6   -4   -1    0    5    7    8    9   10   12\n[46]   13   14   15   16   18   19   20   21   23   24   25   26   27   28   30\n[61]   31   33   34   35   36   37   39   40   41   42   44   45   46   49   50\n[76]   51   52   53   54   55   59   60   62   63   65   72   80   84   85   87\n[91]   91   93   95   97  100   NA\n\n$preselect_curr_e4\n [1] -100  -67  -52  -45  -44  -43  -41  -37  -36  -35  -31  -30  -25  -22  -21\n[16]  -17  -16  -12  -11  -10   -8   -7   -6   -2   -1    0    1    2    3    4\n[31]    5    7    8    9   10   11   12   14   15   16   17   18   20   21   22\n[46]   23   24   25   26   27   28   29   30   31   34   36   37   38   41   42\n[61]   43   44   45   46   47   48   49   50   51   52   54   55   57   59   61\n[76]   64   67   71   72   73   74   75   78   80   81   85   86   87   88   90\n[91]   95  100   NA\n\n$preselect_ave_e1\n [1] -100  -75  -69  -67  -51  -49  -48  -44  -42  -41  -40  -37  -36  -35  -34\n[16]  -33  -32  -31  -30  -29  -28  -27  -26  -24  -23  -22  -21  -20  -19  -18\n[31]  -17  -16  -15  -14  -13  -12  -11  -10   -9   -8   -5   -4   -3   -1    0\n[46]    1    2    3    4    5    6    7    8    9   10   11   12   13   14   16\n[61]   17   18   19   21   22   23   25   26   27   28   29   30   33   35   36\n[76]   38   45   49   52   53   55   57   83   NA\n\n$preselect_ave_e2\n [1] -100  -56  -54  -52  -51  -48  -47  -44  -43  -41  -40  -39  -38  -37  -34\n[16]  -32  -31  -29  -28  -27  -25  -22  -20  -19  -18  -17  -16  -15  -14  -13\n[31]  -12  -11  -10   -9   -7   -6   -5   -3   -1    0    3    5    6    7    8\n[46]    9   11   12   13   16   18   19   20   21   22   24   25   27   30   31\n[61]   32   33   38   39   43   47   49   50   51   59   62   85  100   NA\n\n$preselect_ave_e3\n [1] -100  -69  -65  -63  -54  -53  -47  -45  -40  -35  -33  -32  -28  -27  -26\n[16]  -25  -22  -21  -20  -19  -18  -17  -16  -14  -13  -11  -10   -9   -8   -7\n[31]   -6   -5   -4   -3   -1    0    2    4    5    6    7    8    9   10   11\n[46]   12   15   16   17   19   20   21   22   23   24   25   27   30   31   32\n[61]   37   38   39   42   47   49   50   52   53   72   81   86   92  100   NA\n\n$preselect_ave_e4\n [1] -100  -91  -80  -72  -69  -63  -56  -53  -49  -47  -41  -40  -37  -36  -35\n[16]  -34  -32  -31  -30  -29  -26  -25  -22  -21  -20  -19  -18  -17  -16  -15\n[31]  -14  -13  -12  -11  -10   -9   -8   -7   -6   -5   -4   -1    1    2    3\n[46]    4    5    7    8    9   10   11   12   13   14   16   19   21   22   23\n[61]   25   27   28   29   30   31   34   35   36   38   40   43   45   52   55\n[76]   57   65   86   87  100   NA\n\n$preselect_e1\n  [1] -100  -97  -91  -90  -86  -85  -84  -81  -80  -78  -77  -74  -73  -71  -70\n [16]  -68  -66  -63  -62  -61  -58  -57  -56  -55  -54  -53  -52  -51  -50  -49\n [31]  -48  -47  -45  -44  -43  -41  -39  -38  -37  -36  -35  -34  -33  -32  -31\n [46]  -29  -28  -27  -25  -24  -23  -22  -20  -19  -18  -17  -16   -9   -1    4\n [61]   10   11   13   15   21   22   24   26   27   28   31   37   41   42   45\n [76]   46   49   50   52   54   55   56   58   59   60   61   63   64   67   68\n [91]   71   74   75   76   78   79   81   82   83   85   87   88   91   93   97\n[106]  100   NA\n\n$preselect_e2\n  [1] -100  -86  -81  -80  -74  -73  -72  -71  -70  -65  -64  -62  -57  -56  -54\n [16]  -51  -50  -48  -47  -46  -45  -44  -39  -37  -35  -34  -33  -32  -31  -30\n [31]  -29  -28  -27  -26  -25  -24  -22  -21  -20  -19  -17  -16  -15   -9   -8\n [46]   -7   -6   -3   -1    4   10   12   13   14   18   19   20   22   23   24\n [61]   26   27   28   33   34   37   38   41   42   43   45   46   48   50   52\n [76]   53   54   55   56   57   58   59   61   62   63   65   66   67   71   72\n [91]   73   74   75   78   79   82   85   86   88   89   91   92   98   99  100\n[106]   NA\n\n$preselect_e3\n [1] -99 -87 -86 -81 -76 -75 -73 -71 -60 -59 -54 -53 -50 -48 -47 -43 -42 -40 -39\n[20] -38 -37 -35 -32 -29 -28 -27 -26 -25 -23 -22 -18 -17 -16 -15 -14 -11 -10  -8\n[39]  -7  -6  -5  -3  -1   0   1   2   3   5   6   7   9  12  13  16  18  19  20\n[58]  21  23  24  25  28  29  34  36  37  38  42  43  44  46  47  48  49  50  52\n[77]  53  55  57  59  61  63  64  70  72  87  88  91 100  NA\n\n$preselect_e4\n [1] -100  -95  -87  -85  -83  -82  -80  -79  -74  -72  -71  -69  -68  -66  -63\n[16]  -62  -60  -59  -57  -51  -49  -48  -42  -40  -39  -35  -32  -31  -29  -26\n[31]  -25  -24  -22  -21  -20  -18  -15  -13  -12  -10   -9   -8   -6   -1    3\n[46]    4    8   10   13   16   17   22   24   25   26   28   29   32   33   34\n[61]   35   37   42   43   44   46   48   49   51   53   54   57   60   61   63\n[76]   64   66   67   68   69   70   71   72   73   74   77   78   79   84   85\n[91]   86   87   92   98  100   NA\n\n$preselect_neg_e1\n [1] -100  -99  -97  -91  -90  -86  -85  -84  -81  -80  -79  -78  -77  -76  -75\n[16]  -74  -73  -71  -70  -69  -68  -66  -65  -64  -63  -62  -61  -60  -59  -58\n[31]  -57  -56  -55  -54  -53  -52  -51  -50  -49  -48  -47  -45  -44  -43  -41\n[46]  -39  -38  -36  -35  -34  -33  -32  -31  -29  -28  -27  -25  -24  -23  -22\n[61]  -21  -20  -19  -18  -17  -16  -15  -14  -12  -10   -9   -8   -1    4   13\n[76]   26   40   NA\n\n$preselect_neg_e2\n [1] -100  -98  -95  -94  -87  -86  -81  -80  -76  -74  -73  -72  -71  -70  -66\n[16]  -65  -64  -62  -59  -57  -56  -54  -53  -51  -50  -49  -48  -47  -46  -45\n[31]  -44  -43  -42  -40  -39  -38  -37  -36  -35  -34  -33  -32  -31  -30  -29\n[46]  -28  -27  -26  -25  -24  -23  -22  -21  -20  -19  -17  -16  -15  -14  -13\n[61]  -12  -10   -9   -8   -7   -6   -4   -3   -1    4   19   NA\n\n$preselect_neg_e3\n [1] -100  -99  -87  -86  -83  -81  -76  -75  -74  -73  -71  -70  -67  -60  -59\n[16]  -58  -57  -55  -54  -53  -52  -50  -48  -47  -46  -44  -43  -42  -40  -39\n[31]  -38  -37  -36  -35  -34  -33  -32  -31  -30  -29  -28  -27  -26  -25  -24\n[46]  -23  -22  -20  -18  -17  -15  -14  -12  -11  -10   -9   -8   -7   -6   -5\n[61]   -1    0    8    9   11   13   19   20   21   23   25   32   34   36   40\n[76]   41   45   50   54  100   NA\n\n$preselect_neg_e4\n [1] -100  -98  -95  -88  -87  -85  -84  -83  -82  -80  -79  -78  -74  -72  -71\n[16]  -69  -68  -67  -66  -65  -64  -63  -62  -61  -60  -59  -58  -57  -55  -54\n[31]  -52  -51  -49  -48  -46  -45  -43  -42  -41  -40  -39  -38  -37  -35  -34\n[46]  -33  -32  -31  -30  -29  -26  -25  -24  -22  -21  -20  -18  -17  -16  -15\n[61]  -13  -12  -11  -10   -9   -8   -6   -5   -4   -3   -1    4   10   17   25\n[76]   29   56   57   NA\n\n$preselect_pos_e1\n [1] -37 -17   0   6   7  10  11  13  15  18  20  21  22  23  24  27  28  29  30\n[20]  31  32  35  36  37  39  40  41  42  43  45  46  49  50  51  52  54  55  56\n[39]  58  59  60  61  62  63  64  65  66  67  68  70  71  73  74  75  76  77  78\n[58]  79  80  81  82  83  84  85  86  87  88  90  91  92  93  94  97 100  NA\n\n$preselect_pos_e2\n [1]   4   5  10  11  12  13  14  15  16  17  18  20  21  22  23  24  25  26  27\n[20]  28  29  30  31  32  33  34  35  36  37  38  40  41  42  43  45  46  48  50\n[39]  51  52  53  54  55  56  57  58  59  61  62  63  64  65  66  67  68  69  71\n[58]  72  73  74  75  76  78  79  80  81  82  85  86  88  89  91  92  93  97  98\n[77]  99 100  NA\n\n$preselect_pos_e3\n [1] -75 -73 -60 -50 -40 -35 -33 -29 -27 -16 -15 -12 -11  -8  -7  -3  -1   0   1\n[20]   2   3   5   6   7   9  11  12  13  15  16  17  18  19  21  22  23  24  25\n[39]  26  27  28  29  32  33  34  37  38  42  43  44  46  47  48  49  50  52  53\n[58]  55  57  59  61  62  63  64  66  70  72  75  77  81  82  87  88  91 100  NA\n\n$preselect_pos_e4\n [1] -14  -1   3   5   8  11  13  14  16  17  21  22  23  24  25  26  27  28  29\n[20]  32  33  34  35  36  37  38  39  40  42  43  44  45  46  48  49  51  52  53\n[39]  54  57  59  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n[58]  77  78  79  80  81  82  83  84  85  86  87  88  89  90  92  95  98 100  NA\n\n$preselect_peak_e1\n [1] -100  -96  -88  -79  -77  -75  -73  -72  -66  -64  -62  -58  -56  -53  -52\n[16]  -50  -48  -47  -45  -43  -42  -40  -39  -38  -37  -36  -34  -33  -31  -29\n[31]  -28  -25  -24  -23  -22  -20  -17  -16  -15  -14  -12  -11  -10   -9   -6\n[46]   -4   -1    4    5    9   10   11   12   13   14   15   16   18   19   20\n[61]   21   22   23   25   26   27   28   29   30   31   32   33   34   36   38\n[76]   40   41   43   46   47   50   51   54   56   58   61   65   67   73   75\n[91]   88   91  100   NA\n\n$preselect_peak_e2\n [1] -78 -69 -55 -53 -31 -30 -25 -21 -20 -19 -18 -17 -15 -14  -7  -5  -3  -2  -1\n[20]   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18  20\n[39]  21  22  25  29  30  31  33  34  36  40  41  43  44  47  50  52  54  55  60\n[58]  61  64  66  68  69  75  79  85  94 100  NA\n\n$preselect_peak_e3\n [1] -100  -99  -96  -94  -92  -91  -90  -89  -88  -87  -86  -85  -84  -83  -79\n[16]  -76  -74  -73  -72  -71  -70  -69  -67  -66  -65  -64  -63  -60  -59  -58\n[31]  -57  -56  -55  -54  -53  -52  -50  -49  -48  -47  -46  -45  -44  -42  -41\n[46]  -39  -38  -37  -36  -35  -34  -33  -32  -31  -30  -29  -28  -27  -26  -25\n[61]  -24  -21  -20  -19  -18  -17  -16  -14  -13  -12   -9   -8   -7   -5   -1\n[76]    0    3    8   10   15   16   20   26   31   32   33   36   37   39   42\n[91]   46   51   53   70  100   NA\n\n$preselect_peak_e4\n [1] -100  -92  -70  -65  -60  -57  -54  -50  -49  -48  -41  -40  -39  -38  -36\n[16]  -35  -33  -32  -31  -30  -29  -27  -25  -23  -22  -21  -20  -19  -18  -17\n[31]  -16  -15  -14  -13  -12  -10   -8   -7   -1    1    2    3    4    7    8\n[46]    9   10   11   12   13   15   16   18   19   20   21   23   24   25   26\n[61]   27   28   29   30   32   33   34   35   36   40   43   44   47   48   49\n[76]   50   59   60   64   65   67   69   70   74   76  100   NA\n\n$sias1\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias2\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias3\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me       <NA>                              \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias4\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias5\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias6\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me       <NA>                              \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias7\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias8\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias9\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias10\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me       <NA>                              \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias11\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me       <NA>                              \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias12\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me       <NA>                              \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias13\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias14\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias15\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias16\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias17\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias18\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias19\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me      \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n$sias20\n[1] 0                                  0= Not at all characteristic of me\n[3] 1= Slightly characteristic of me   2= Moderately characteristic of me\n[5] 4= Very characteristic of me       <NA>                              \n6 Levels: 0 ... 5= Extremely characteristic of me\n\n\nLeider sehen wir nicht alle Ausprägungen von race. Diese können wir uns auch separat mit levels() anschauen.\n\nlevels(data_b$race)\n\n[1] \"American Indian/Alaskan Native\"           \n[2] \"Asian\"                                    \n[3] \"Black or African American\"                \n[4] \"Native Hawaiian or Other Pacific Islander\"\n[5] \"White\"                                    \n[6] \"More than one race\"                       \n[7] \"Other\"                                    \n[8] \"Prefer not to answer\"                     \n\n\nIn race scheint es keine falsch kodierten Missings zu geben.\n\nAchtung: Scheinbar wird das Messniveau der SIAS-Items (sias...) als nominalskaliert, und nicht wie sonst üblich als intervallskaliert, angenommen. Zumindest liegen die betreffenden Variables als ungeordnete Faktoren vor.\n\nEs fällt außerdem auf, dass die Items des SIAS, welche eine 5-stufige Skala haben sollten, die von 0-4 geht, eine merkwürdige Kodierung der Daten aufweisen:\n\nes gibt 6 Ausprägungen der Kodierung\ndie Ausprägung 3=..., welche es laut Codebuch geben sollte, scheint nicht vorhanden zu sein\nes gibt eine Ausprägung 5=..., welche laut Codebuch nicht vorliegen sollte\nes gibt zwei Ausprägungen, die die 0 beinhalten\n\nDas sollten wir noch weiter explorieren. Dazu schauen wir uns die Häufigkeiten der Ausprägungen genauer an:\n\nsapply(data_b[, grep(\"sias\", colnames(data_b))], table, useNA=\"always\")\n\n                                   sias1 sias2 sias3 sias4 sias5 sias6 sias7\n0                                     41    94    46    98    20    91    82\n0= Not at all characteristic of me    47    46    53    35    16    46    46\n1= Slightly characteristic of me      35    26    37    29    36    29    21\n2= Moderately characteristic of me    42    12    31    21    71    17    27\n4= Very characteristic of me          22     9    19     4    44     3    11\n5= Extremely characteristic of me      0     0     0     0     0     0     0\n<NA>                                   0     0     1     0     0     1     0\n                                   sias8 sias9 sias10 sias11 sias12 sias13\n0                                     93    29    107     34     53     85\n0= Not at all characteristic of me    50    31     31     45     45     46\n1= Slightly characteristic of me      23    35     26     33     28     33\n2= Moderately characteristic of me    15    49     13     51     30     21\n4= Very characteristic of me           6    43      9     23     30      2\n5= Extremely characteristic of me      0     0      0      0      0      0\n<NA>                                   0     0      1      1      1      0\n                                   sias14 sias15 sias16 sias17 sias18 sias19\n0                                      71     57     50     68     64     85\n0= Not at all characteristic of me     53     47     50     39     39     25\n1= Slightly characteristic of me       22     22     26     29     25     33\n2= Moderately characteristic of me     19     39     32     38     33     30\n4= Very characteristic of me           22     22     29     13     26     14\n5= Extremely characteristic of me       0      0      0      0      0      0\n<NA>                                    0      0      0      0      0      0\n                                   sias20\n0                                      47\n0= Not at all characteristic of me     44\n1= Slightly characteristic of me       31\n2= Moderately characteristic of me     37\n4= Very characteristic of me           27\n5= Extremely characteristic of me       0\n<NA>                                    1\n\n\nEs fällt auf, dass Die Ausprägung 5=... in keiner der Variablen vorkommt (Häufigkeit \\(0\\)). Um zu überprüfen, wie die Kodierung im Datensatz zur Skala im Codebuch in Bezug zu setzen ist, schauen wir uns ein Item an, welches im Originaldatensatz eine rekodierte Version enthält: sias5 und sias5_RS. Wir speichern beide Variablen in einem neuen Datensatz.\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nfigures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav: Very long string\nrecord(s) found (record type 7, subtype 14), each will be imported in\nconsecutive separate variables\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias1\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias2\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias3\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias4\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias5\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias6\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias7\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias8\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias9\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias10\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias11\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias12\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias13\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias14\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias15\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias16\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias17\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias18\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias19\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 0 added in variable: sias20\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 4 added in variable: pre_outcome_old\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs1\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs2\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs3\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs4\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs5\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs6\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs7\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs8\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs9\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs10\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs11\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs12\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs13\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs14\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs15\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs16\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs17\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs18\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs19\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs20\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs21\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: rrs22\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq1\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq2\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq3\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq4\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq5\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq17\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq16\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq15\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq8\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq9\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq10\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq14\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq13\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq6\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq7\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq11\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq12\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq18\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq19\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq20\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq21\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq28\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq27\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq26\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq24\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq25\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq22\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq23\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq32\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq31\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq29\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq30\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq33\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq34\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq39\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq38\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq37\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq35\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: ffmq36\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes1\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes2\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes3\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes4\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes5\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes6\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes7\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes8\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes9\n\n\nWarning in\nread.spss(\"figures/Fehlende-Werte/Daten/AffectiveForecasting_0707017.sav\", :\nUndeclared level(s) 999 added in variable: fpes10\n\n\n\ndata_b <- read.spss(\"Dateipfad/AffectiveForecasting_0707017.sav\", to.data.frame = TRUE) \n# noch den eigenen Dateipfad einfügen\ndata_b_test <- data_b_test[, c(79, 95)] # sias5, sias5_RS\n\n\n\n\n\n  \n\n\n\n\nAchtung: Wir müssen hier umdenken, da sias5_RS die rekodierte Version von sias5 widergibt. Wir wollen allerdings die allgemeingültige Zuordnung von Daten und Angaben im Codebuch verstehen.\n\nDie Zuordnung der Kodierung in den Daten (links) und der im Codebuch enthaltenen Skala (rechts) scheint wie folgt:\n\n0 \\(\\rightarrow\\) 0\n0=... \\(\\rightarrow\\) 1\n1=... \\(\\rightarrow\\) 2\n2=... \\(\\rightarrow\\) 3\n4=... \\(\\rightarrow\\) 4\n\nDie Ausprägung 5=... wird, wie oben bereits festgestellt, gar nicht genutzt. Sie wurde dennoch in den Faktorstufen der sias...-Items vermerkt.\nWenn wir tiefgehender mit den Daten arbeiten würden (z.B. Datenanalyse), würde es sich anbieten, die Kodierung der Items anzupassen, sodass die Ausprägung auf der Skala auch aus der Kodierung ersichtlich wird. Man könnte dann auch darüber nachdenken, die Variablen als intervallskaliert zu behandeln (wie es der Standard ist).\nAnsonsten scheinen alle fehlenden Werte korrekt kodiert zu sein.\n\n\n\n2.) Kodiere ggf. inkorrekt kodierte Missings zu NA um.\n\n\nTipp\n\nWir kodieren mittels <- NA jene Ausprägungen um, die falsch kodierte fehlende Werte zeigen.\n\n\n\nLösung A\n\nIn der vorhergehenden Aufgabe haben wir jene Ausprägungen von Variablen identifiziert, die auch fehlende Werte kodieren sollen: \"\" und N/A. Diese kodieren wir nun um.\n\ndata_a[data_a == \"\" | data_a == \"N/A\" ] <- NA \n# Überprüfung:\nsapply(sapply(data_a[c(2, 4 ,5)], unique), sort, na.last=TRUE)\n# data_a[c(2, 4 ,5)] ist die Auswahl der Variablen, die die falschen Kodierungen enthielten\n\n\n\n$consent\n[1] \"Y\" NA \n\n$gender\n[1] \"f\" \"m\" NA \n\n$gender...comment\n[1] \"Comment? Thanks for the opportunity? Unsure why there is a comment box before the task is completed, but that's okay.\"\n[2] \"have a good day\"                                                                                                      \n[3] NA                                                                                                                     \n\n\nJetzt sind alle fehlenden Werte mit NA gekennzeichnet.\n\n\n\n\n\n9.8.2 Übung 2: Verortung\nNun wollen wir uns ein paar deskriptive Statistiken der fehlenden Werte anschauen.\n\nAchtung: Datensatz A: Die character-Variable gender...comment stellt einen Kommentar zu der Variablen gender dar. Sie kodiert demnach qualitative Daten, die wir uns im Folgenden nicht weiter anschauen wollen. Daher entfernen wir die Variable nun aus unserem (Analyse-)Datensatz:\n\n\ndata_a <- data_a[,-5]\n\n1.) Wie viele Missings gibt es insgesamt im Datensatz (in absoluten und relativen Zahlen)?\n\n\nTipp\n\nMit der table()-Funktion können wir uns Häufigkeitstabellen ausgeben lassen. Jetzt müssen unsere Daten nur noch dichotom in fehlend und nicht fehlend eingeteilt werden.\n\n\n\nLösung A\n\nabsolute Anzahl: TRUE\n\ntable(is.na(data_a))\n\n\nFALSE  TRUE \n 7912  1744 \n\n\nrelative Anzahl: TRUE / (FALSE + TRUE)\n\ntable(is.na(data_a))[2] / ( table(is.na(data_a))[1] + table(is.na(data_a))[2] ) \n\n     TRUE \n0.1806131 \n\n\nEs gibt 1744 fehlende Werte. Das sind ca. 18.06% aller Werte im Datensatz.\n\n\n\nLösung B\n\nabsolute Anzahl: TRUE\n\ntable(is.na(data_b))\n\n\nFALSE  TRUE \n 9065   659 \n\n\nrelative Anzahl: TRUE / (FALSE + TRUE)\n\ntable(is.na(data_b))[2] / ( table(is.na(data_b))[1] + table(is.na(data_b))[2] ) \n\n      TRUE \n0.06777046 \n\n\nEs gibt 659 fehlende Werte. Das sind ca. 6.78% aller Werte im Datensatz.\n\n\n\n2.) Welche Variable enthält die meisten Missings? Wie viele Missings sind das (in absoluten und relativen Zahlen)\n\n\nTipp: Verortung Missings\n\nMit colSums() werden die Summen der Spalten eines Datensatzes angegeben. Nun müssen wir unsere Daten nur wieder dichotom in fehlend und nicht fehlend einteilen.\n\n\n\nTipp: Namen der Variablen\n\nMit colnames() können wir uns Namen von Variablen ausgeben lassen. Wir müssen nur noch die relevanten auswählen.\n\n\n\nLösung A\n\n\n# maximale Anzahl an Missings pro Variable:\nmax(colSums(is.na(data_a))) \n\n[1] 13\n\n# relative Anzahl an Missings pro Variable:\nmax(colSums(is.na(data_a))) / nrow(data_a) \n\n[1] 0.1830986\n\n# Spaltennamen der Variablen mit den meisten Missings:\ncolnames(data_a[colSums(is.na(data_a)) == max(colSums(is.na(data_a)))]) \n\n  [1] \"statements..1.\"   \"statements..2.\"   \"statements..3.\"  \n  [4] \"statements..4.\"   \"statements..5.\"   \"statements..6.\"  \n  [7] \"statements..7.\"   \"statements..8.\"   \"statements..9.\"  \n [10] \"statements..10.\"  \"statements..11.\"  \"statements..12.\" \n [13] \"statements..13.\"  \"statements..14.\"  \"statements..15.\" \n [16] \"statements..16.\"  \"statements..17.\"  \"statements..18.\" \n [19] \"statements..19.\"  \"statements..20.\"  \"statements..21.\" \n [22] \"statements..22.\"  \"statements..23.\"  \"statements..24.\" \n [25] \"statements..25.\"  \"statements..26.\"  \"statements..27.\" \n [28] \"statements..28.\"  \"statements..29.\"  \"statements..30.\" \n [31] \"statements..31.\"  \"statements..32.\"  \"statements..33.\" \n [34] \"statements..34.\"  \"statements..35.\"  \"statements..36.\" \n [37] \"statements..37.\"  \"statements..38.\"  \"statements..39.\" \n [40] \"statements..40.\"  \"statements..41.\"  \"statements..42.\" \n [43] \"statements..43.\"  \"statements..44.\"  \"statements..45.\" \n [46] \"statements..46.\"  \"statements..47.\"  \"statements..48.\" \n [49] \"statements..49.\"  \"statements..50.\"  \"statements..51.\" \n [52] \"statements..52.\"  \"statements..53.\"  \"statements..54.\" \n [55] \"statements..55.\"  \"statements..56.\"  \"statements..57.\" \n [58] \"statements..58.\"  \"statements..59.\"  \"statements..60.\" \n [61] \"statements..61.\"  \"statements..62.\"  \"statements..63.\" \n [64] \"statements..64.\"  \"statements..65.\"  \"statements..66.\" \n [67] \"statements..67.\"  \"statements..68.\"  \"statements..69.\" \n [70] \"statements..70.\"  \"statements..71.\"  \"statements..72.\" \n [73] \"statements..73.\"  \"statements..74.\"  \"statements..75.\" \n [76] \"statements..76.\"  \"statements..77.\"  \"statements..78.\" \n [79] \"statements..79.\"  \"statements..80.\"  \"statements..81.\" \n [82] \"statements..82.\"  \"statements..83.\"  \"statements..84.\" \n [85] \"statements..85.\"  \"statements..86.\"  \"statements..87.\" \n [88] \"statements..88.\"  \"statements..89.\"  \"statements..90.\" \n [91] \"statements..91.\"  \"statements..92.\"  \"statements..93.\" \n [94] \"statements..94.\"  \"statements..95.\"  \"statements..96.\" \n [97] \"statements..97.\"  \"statements..98.\"  \"statements..99.\" \n[100] \"statements..100.\" \"statements..101.\" \"statements..102.\"\n[103] \"statements..103.\" \"statements..104.\" \"statements..105.\"\n[106] \"statements..106.\" \"statements..107.\" \"statements..108.\"\n[109] \"statements..109.\" \"statements..110.\" \"statements..111.\"\n[112] \"statements..112.\" \"statements..113.\" \"statements..114.\"\n[115] \"statements..115.\" \"statements..116.\" \"statements..117.\"\n[118] \"statements..118.\" \"statements..119.\" \"statements..120.\"\n[121] \"statements..121.\" \"statements..122.\" \"statements..123.\"\n[124] \"statements..124.\" \"statements..125.\" \"statements..126.\"\n[127] \"statements..127.\" \"statements..128.\" \"statements..c1.\" \n[130] \"statements..c2.\"  \"statements..c3.\"  \"statements..c4.\" \n\n\nDie meisten Missings sind in den statement-Variablen mit je einer absoluten Anzahl von 13 fehlenden Werten. Das entspricht ca. 18.31%.\n\n\n\nLösung B\n\n\n# maximale Anzahl an Missings pro Variable:\nmax(colSums(is.na(data_b))) \n\n[1] 72\n\n# relative Anzahl an Missings pro Variable:\nmax(colSums(is.na(data_b))) / nrow(data_b) \n\n[1] 0.3850267\n\n# Spaltennamen der Variablen mit den meisten Missings:\ncolnames(data_b[colSums(is.na(data_b)) == max(colSums(is.na(data_b)))]) \n\n[1] \"preselect_peak_e2\"\n\n\nDie meisten Missings hat die Variable preselect_peak_e2 mit 72 fehlenden Werten. Das entspricht ca. 38.5%.\n\n\n\n3.) Welche Person hat die meisten Missings? Wie viele Missings sind das (in absoluten und relativen Zahlen)?\n\n\nTipp: Verortung Missings\n\nMit rowSums() werden die Summen der Zeilen eines Datensatzes angegeben. Zusätzlich müssen wir unsere Daten nur wieder dichotom in fehlend und nicht fehlend einteilen.\n\n\n\nTipp: Zeilenindizes der Fälle\n\nMit which() können wir uns die Zeilenindizes von Fällen ausgeben lassen. Wir müssen nur noch die relevanten auswählen.\n\n\n\nLösung A\n\n\n# maximale Anzahl an Missings pro Person:\nmax(rowSums(is.na(data_a)))\n\n[1] 135\n\n# relative Anzahl an Missings pro Person:\nmax(rowSums(is.na(data_a))) / ncol(data_a)\n\n[1] 0.9926471\n\n# welche Personen haben die meisten Missings:\nwhich(rowSums(is.na(data_a)) == max(rowSums(is.na(data_a))))\n\n[1]  1  3  4 27 32 41 49 51\n\n\nDie meisten Missings haben die Personen 1, 3, 4, 27, 32, 41, 49, 51 mit jeweils 135 fehlenden Werten. Das entspricht ca. 99.26%.\n\n\n\nLösung B\n\n\n# maximale Anzahl an Missings pro Person:\nmax(rowSums(is.na(data_b)))\n\n[1] 20\n\n# relative Anzahl an Missings pro Person:\nmax(rowSums(is.na(data_b))) / ncol(data_b)\n\n[1] 0.3846154\n\n# welche Personen haben die meisten Missings:\nwhich(rowSums(is.na(data_b)) == max(rowSums(is.na(data_b))))\n\n[1] 183 185 186 187\n\n\nDie meisten Missings haben die Personen 183, 185, 186, 187 mit jeweils 20 fehlenden Werten. Das sind ca. 38.5% fehlende Werte.\n\n\n\n4.) Welche bzw. wie viele Patterns mit Missings (Missings in bestimmten Kombinationen von Variablen) gibt es? Visualisiere sie. (Für Datensatz B reicht es, wenn das am häufigsten auftretende Pattern mit Missings beschrieben wird.)\n\n\nTipp\n\nIm Paket VIM gibt es die Funktion aggr(), die beim Visualisieren von Missings helfen kann.\n\n\n\nLösung A\n\nDer Output von summary(aggr()) gibt uns verschiedene Informationen über die Missings und Missing-Patterns. Neben der Zusammensetzung der Patterns bekommen wir Auskunft über die absoluten und relativen Häufigkeiten der variablenweisen Missings und der Missing-Patterns. In den Grafiken steht blau für vorhandene Werte; rot für Missings. Wir sehen in der rechten Grafik, dass ein Missing-Pattern keine Missings enthält.\n\n#install.packages (\"VIM\")\nlibrary(VIM)\nsummary(aggr(data_a))\n\n\n\n\n\n Missings per variable: \n         Variable Count\n               id     0\n          consent     8\n              age    10\n           gender    10\n   statements..1.    13\n   statements..2.    13\n   statements..3.    13\n   statements..4.    13\n   statements..5.    13\n   statements..6.    13\n   statements..7.    13\n   statements..8.    13\n   statements..9.    13\n  statements..10.    13\n  statements..11.    13\n  statements..12.    13\n  statements..13.    13\n  statements..14.    13\n  statements..15.    13\n  statements..16.    13\n  statements..17.    13\n  statements..18.    13\n  statements..19.    13\n  statements..20.    13\n  statements..21.    13\n  statements..22.    13\n  statements..23.    13\n  statements..24.    13\n  statements..25.    13\n  statements..26.    13\n  statements..27.    13\n  statements..28.    13\n  statements..29.    13\n  statements..30.    13\n  statements..31.    13\n  statements..32.    13\n  statements..33.    13\n  statements..34.    13\n  statements..35.    13\n  statements..36.    13\n  statements..37.    13\n  statements..38.    13\n  statements..39.    13\n  statements..40.    13\n  statements..41.    13\n  statements..42.    13\n  statements..43.    13\n  statements..44.    13\n  statements..45.    13\n  statements..46.    13\n  statements..47.    13\n  statements..48.    13\n  statements..49.    13\n  statements..50.    13\n  statements..51.    13\n  statements..52.    13\n  statements..53.    13\n  statements..54.    13\n  statements..55.    13\n  statements..56.    13\n  statements..57.    13\n  statements..58.    13\n  statements..59.    13\n  statements..60.    13\n  statements..61.    13\n  statements..62.    13\n  statements..63.    13\n  statements..64.    13\n  statements..65.    13\n  statements..66.    13\n  statements..67.    13\n  statements..68.    13\n  statements..69.    13\n  statements..70.    13\n  statements..71.    13\n  statements..72.    13\n  statements..73.    13\n  statements..74.    13\n  statements..75.    13\n  statements..76.    13\n  statements..77.    13\n  statements..78.    13\n  statements..79.    13\n  statements..80.    13\n  statements..81.    13\n  statements..82.    13\n  statements..83.    13\n  statements..84.    13\n  statements..85.    13\n  statements..86.    13\n  statements..87.    13\n  statements..88.    13\n  statements..89.    13\n  statements..90.    13\n  statements..91.    13\n  statements..92.    13\n  statements..93.    13\n  statements..94.    13\n  statements..95.    13\n  statements..96.    13\n  statements..97.    13\n  statements..98.    13\n  statements..99.    13\n statements..100.    13\n statements..101.    13\n statements..102.    13\n statements..103.    13\n statements..104.    13\n statements..105.    13\n statements..106.    13\n statements..107.    13\n statements..108.    13\n statements..109.    13\n statements..110.    13\n statements..111.    13\n statements..112.    13\n statements..113.    13\n statements..114.    13\n statements..115.    13\n statements..116.    13\n statements..117.    13\n statements..118.    13\n statements..119.    13\n statements..120.    13\n statements..121.    13\n statements..122.    13\n statements..123.    13\n statements..124.    13\n statements..125.    13\n statements..126.    13\n statements..127.    13\n statements..128.    13\n  statements..c1.    13\n  statements..c2.    13\n  statements..c3.    13\n  statements..c4.    13\n\n Missings in combinations of variables: \n                                                                                                                                                                                                                                                                    Combinations\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1\n 0:0:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1\n 0:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1\n Count   Percent\n    58 81.690141\n     3  4.225352\n     2  2.816901\n     8 11.267606\n\n\nEs gibt drei Patterns mit Missings. Generell fehlen in allen drei Missing-Patterns alle statements-variablen (Spaltennummern 5:131). Das erste (nur aus diesen fehlenden Werten bestehende) Pattern kommt 3 mal vor. Im zweiten Pattern fehlen zusätzlich noch die Variablen age und gender (Spaltennummern 3 und 4) und es kommt 2 mal vor. Im dritte Pattern fehlt zusätzlich noch die Variable consent (Spaltennummer 2) und es kommt 8 mal vor.\nDie Fälle, welche ins letzte Pattern fallen, haben demnach auf allen Variablen, außer id, fehlende Werte. Zufälligerweise gibt es auch genau 8 fehlende Fälle auf der Variablen consent. Es ist hier naheliegend zu vermuten, dass diese 8 Personen den Fragebogen gar nicht ausgefüllt haben (d.h. die Erhebung abgebrochen haben). Das sind wahrscheinlich auch dieselben Personen, die wir in 2.3 gefunden haben, als wir die Fälle mit den meisten Missings identifiziert haben.\n\n# Überprüfung ob Fälle mit max-Anzahl Missings == Fälle mit Missings auf consent\nwhich(rowSums(is.na(data_a)) == max(rowSums(is.na(data_a)))) == which(is.na(data_a$consent))\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nEs handelt sich um dieselben Fälle. Wir entfernen diese 8 Personen nun aus dem Datensatz.\n\ndata_a <- data_a[which(!is.na(data_a$consent)),]\n# which(!is.na(data_a$consent)) selektiert Fälle mit vorhandenen Werten in consent\n\n\n\n\nLösung B\n\nWir bekommen von summary(aggr()) verschiedene Informationen über die Missings und Missing-Patterns. Neben der Zusammensetzung der Patterns bekommen wir Auskunft über die absoluten und relativen Häufigkeiten der variablenweisen Missings und der Missing-Patterns. In den Grafiken steht blau für vorhandene Werte; rot für Missings. Wir sehen in der rechten Grafik, dass ein Missing-Pattern keine Missings enthält.\n\n#install.packages (\"VIM\")\nlibrary(VIM)\nsummary(aggr(data_b))\n\n\n\n\n\n Missings per variable: \n          Variable Count\n              P_ID     0\n               age     0\n            gender     0\n     race_white_yn     1\n              ethn     0\n              race     0\n             group     0\n              cond     0\n preselect_curr_e1    19\n preselect_curr_e2    22\n preselect_curr_e3    13\n preselect_curr_e4    31\n  preselect_ave_e1    37\n  preselect_ave_e2    44\n  preselect_ave_e3    59\n  preselect_ave_e4    49\n      preselect_e1    10\n      preselect_e2    14\n      preselect_e3    34\n      preselect_e4    16\n  preselect_neg_e1    13\n  preselect_neg_e2    24\n  preselect_neg_e3    37\n  preselect_neg_e4    25\n  preselect_pos_e1     6\n  preselect_pos_e2     7\n  preselect_pos_e3    34\n  preselect_pos_e4     5\n preselect_peak_e1    24\n preselect_peak_e2    72\n preselect_peak_e3     6\n preselect_peak_e4    51\n             sias1     0\n             sias2     0\n             sias3     1\n             sias4     0\n             sias5     0\n             sias6     1\n             sias7     0\n             sias8     0\n             sias9     0\n            sias10     1\n            sias11     1\n            sias12     1\n            sias13     0\n            sias14     0\n            sias15     0\n            sias16     0\n            sias17     0\n            sias18     0\n            sias19     0\n            sias20     1\n\n Missings in combinations of variables: \n                                                                                            Combinations\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:1:1:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:1:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:1:1:0:0:1:1:0:0:1:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:1:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:1:0:0:0:1:1:0:0:1:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:1:0:0:0:1:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:1:1:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:1:0:0:0:1:0:0:0:0:1:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:1:0:0:0:0:0:0:0:0:0:0:1:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:1:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:1:1:0:0:0:0:0:1:0:1:0:0:0:0:1:0:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:0:1:1:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:0:0:0:0:0:0:0:1:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:0:0:1:0:0:0:1:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:0:1:0:0:1:1:1:0:0:1:0:0:0:1:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:0:0:1:1:1:1:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:0:1:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:0:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:1:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:0:1:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:1:0:0:0:0:0:1:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:0:0:0:0:0:1:1:0:0:1:0:0:0:1:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:0:0:1:0:0:1:1:0:0:0:0:0:0:0:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:0:0:1:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:1:1:0:1:1:1:1:0:1:1:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:0:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:0:1:1:1:1:1:0:0:1:1:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:1:0:1:1:1:0:0:0:1:0:0:0:0:1:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:1:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:0:0:1:0:0:1:1:0:0:0:1:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n 0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0\n Count    Percent\n    51 27.2727273\n     1  0.5347594\n     1  0.5347594\n     6  3.2085561\n     6  3.2085561\n     4  2.1390374\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     3  1.6042781\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     2  1.0695187\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     2  1.0695187\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     4  2.1390374\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     2  1.0695187\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     2  1.0695187\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n     1  0.5347594\n\n\n\n# Anzahl der Patterns (minus eins, weil das nur vorhandene Werte enthält):\nlength(summary(aggr(data_b))[[\"combinations\"]][[\"Combinations\"]]) - 1\n\n\n\n[1] 114\n\n\nEs gibt 114 Patterns mit Missings. Die Missing-Pattern, welche am häufigsten vorkommen, mit jeweils 6 mal, haben nur einen fehlenden Wert auf der Variablen preselect_peak_e4 (Spaltennummer 32) bzw. preselect_peak_e2 (Spaltennummer 30).\n\n\n\n\n\n9.8.3 Übung 3: Zufälligkeit\nNachdem wir uns im vorhergehenden Abschnitt deskriptiv-statistische Kennwerte der Missings und Missing-Patterns angeschaut haben, wollen wir uns im Folgenden auch inferenz-statistisch absichern. Dazu überprüfen wir die Zufälligkeit der Missing-Patterns. Ob fehlende Werte zufällig sind ist eine essentielle Frage, die wir vor der Nutzung der Daten klären müssen.\n1.) Sind die fehlenden Werte MCAR? Was hätte es für Konsequenzen für die weitere Datenverarbeitung, wenn die fehlenden Werte MAR oder MNAR wären?\n\n\nAuffrischung: MCAR, MAR und MNAR\n\nMCAR: Missings hängen weder von der (fehlenden) Ausprägung auf der Variablen selbst noch von anderen Variablen und deren fehlenden Werten ab. Sie sind also komplett zufällig.\nMAR: Missings hängen nicht von der (fehlenden) Ausprägung auf der Variablen selbst, aber von anderen Variablen und deren Missings ab. Die Missings sind also zufällig, sobald für diese andere Variablen kontrolliert wird.\nMNAR: Missings hängen von der (fehlenden) Ausprägung auf der Variablen selbst ab. Sie sind also selbst dann nicht zufällig, wenn für die anderen Variablen kontrolliert wird.\n\n\n\nTipp\n\nZur Überprüfung der Zufälligkeit können wir den Test von Little (1988)1 verwenden. Dieser überprüft, ob es signifikante Unterschiede zwischen den variablenweisen Mittelwerten der Missing Patterns gibt. Die \\(H0\\), dass es keine überzufälligen Unterschiede der Mittelwerte in Abhängigkeit der Missing Patterns gibt (= MCAR), ist unsere Wunschhypothese. Der Test ist in der Funktion mcar_test() aus dem Paket naniar implementiert.\n\nAchtung: Die Funktion mcar_test() kann wir nur mit Variablen des Typs numeric (integer und double), logical und factor umgehen.\n\nAußerdem können keine qualitativen Daten ausgewertet werden. Es gab eine qualitative Variable in Datensatz A, gender...comment, welche wir aber zu Beginn des letzten Abschnitts bereits aus dem Datensatz entfernt haben.\n\n\n\nLösung A\n\nDie Funktion mcar_test() kann nicht mit Daten vom Typ character umgehen. Daher müssen wir die nominalskalierten Variablen gender und consent zuerst faktorisieren:\n\ndata_a$gender <- factor(data_a$gender)\ndata_a$consent <- factor(data_a$consent)\n\nDas Signifikanzniveau für den Test legen wir auf \\(\\alpha = 0.05\\) fest.\n\n# install.packages(\"remotes\") \n# remotes::install_github(\"njtierney/naniar\") \nlibrary(naniar)\nmcar_test(data_a) \n\nWarning in norm::prelim.norm(data): NAs introduced by coercion to integer range\n\n\nError in `dplyr::mutate()`:\nℹ In argument: `d2 = purrr::pmap_dbl(...)`.\nℹ In group 1: `miss_pattern = 1`.\nCaused by error in `purrr::pmap_dbl()`:\nℹ In index: 1.\nCaused by error in `solve.default()`:\n! Lapack routine dgesv: system is exactly singular: U[2,2] = 0\n\n\nWir bekommen hier nur eine Fehlermeldung ausgegeben. Das liegt daran, dass die Funktion mit unseren Daten nicht klar kommt. Genauer gesagt, wird nur ein Missing-Pattern mittels mcar_test() erkannt, obwohl es nach aggr() 4 gibt (bzw. nun nur noch 3, da wir alle Fälle mit fehlenden Werten auf consent entfernt haben und damit ein Missing-Pattern weniger haben). Leider gibt es keine alternative Implementierung des Tests von Little (für die aktuelle R-Version), und daher können wir nicht weiter an diesen Daten arbeiten.\n\n\n\nLösung B\n\nDas Signifikanzniveau legen wir auf \\(\\alpha = 0.05\\) fest.\n\n# install.packages(\"remotes\") \n# remotes::install_github(\"njtierney/naniar\") \nlibrary(naniar)\nmcar_test(data_b)\n\nWarning in norm::prelim.norm(data): NAs introduced by coercion to integer range\n\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      <dbl> <dbl>   <dbl>            <int>\n1     3572.  5355       1              115\n\n\nDa der \\(p\\)-Wert über unserem á priori festgelegtem Signifikanzniveau \\(\\alpha = 0.05\\) liegt, haben wir Evidenz dafür, dass die Daten MCAR sind.\n\nAchtung: Das hier ein \\(p\\)-Wert von \\(1\\) ausgegeben wird ist formal nicht korrekt. Das liegt wahrscheinlich an der Rundung, denn korrekterweise wäre es wohl \\(p=0.99\\).\n\n\n\n\nKonsequenzen der Verarbeitung von Daten, die MAR oder MNAR sind\n\nWenn die Missings MAR oder MNAR sind, kann es zu Parameterverzerrungen kommen, wenn wir Methoden nutzen würden, welche die strengere Annahme MCAR annehmen.\nBeispiele: Wir können keine Fälle löschen wenn die Daten MAR oder MNAR sind. Wenn die Daten MNAR sind, können wir auch die Maximum Likelihood Schätzung sowie die multiple Imputation nicht nutzen.\n\n\n\n2.) Wie gehen wir weiter vor (mit Hinblick auf die Ergebnisse des Tests nach Little)?\n\n\nLösung\n\nWie bereits oben beschrieben können wir nicht (einfach) weiter an Datensatz A arbeiten, solange wir nicht testen konnten, ob die Daten MCAR sind. Eine Möglichkeit wäre es, die Daten mit einer anderen Statistiksoftware auszuwerten, in der es einen implementierten MCAR-Test gibt. Es ist aber nicht ausgeschlossen, dass dieser auf dem gleichen Algorithmus wie mcar_test() beruht und wir das gleiche Problem haben würden.\n\n\n\nBezüglich Datensatz B können wir alle Verfahren anwenden, die MCAR oder MAR fordern, weil wir bei der Durchführung des Test nach Little Evidenz für die \\(H_0\\) (d.h. Daten sind MCAR) gefunden haben.\n\n\n\n\n\n9.8.4 Übung 4: Umgang\n\nAchtung: Die folgenden Aufgaben beziehen sich nur auf den Datensatz B.\n\n1.) Berechne die Mittelwerte aller numerischen, mindestens intervallskalierten Variablen jeweils mit listwise und pairwise deletion. Vergleiche die jeweilige Anzahl der für die Berechnung genutzten Datenpunkte. Der Mittelwert welcher Variablen unterscheidet sich am meisten, welcher am wenigsten?\n\n\nTipp: listwise und pairwise deletion\n\nlistwise/casewise deletion: Fälle mit mind. einem Missing in einer Variablen werden aus allen Mittelwertsberechnungen ausgeschlossen.\npairwise deletion: Fälle mit Missings werden nur aus den jeweiligen Mittelwertsberechnungen derjenigen Variablen ausgeschlossen, in denen der Wert fehlt.\n\n\n\nTipp: numerisch\n\nMit is.numeric() können wir überprüfen, ob ein Vektor numerisch ist. Das müssen wir nur noch auf alle Spalten des Datensatzes anwenden.\n\n\n\nTipp: Anzahl Beobachtungen\n\nBei listwise deletion gehen für die Berechnung der Mittelwerte in jede Variable die gleichen Datenpunkte ein. Wir können einfach mit nrow() die Anzahl herausfinden.\nBei pairwise deletion gehen für die Berechnung der Mittelwerte unterschiedliche Datenpunkte ein. Mit colSums() zählen wir Spaltensummen; jetzt müssen wir nur noch die nicht fehlenden Werte angeben.\n\n\n\nTipp: Unterschiedlichkeit der Mittelwerte\n\nDazu berechnen wir die Differenz der Mittelwerte einer Variablen, die mit listwise und pairwise deletion erstellt wurden. Sinnvoll ist es hier, mit abs() die absolute Differenz zu erhalten.\n\n\n\nLösung\n\n\n# herausfinden, welche Variablen numerisch sind:\nwhich_num <- c()\nfor (i in 1:ncol(data_b)){\n  if (is.numeric(data_b[, i]) == TRUE) {\n    which_num <- append(which_num, i)\n  }\n}\n\n# Selektion auf Datensatz anwenden:\ndata_b_num <- data_b[, which_num]\n\n# Variablen, die nicht (mind.) intervallskaliert sind, aussortieren;\n# dazu vergleichen wir die Angaben im Codebuch und den Datentyp der Variablen\ndata_b_num <- data_b_num[, -c(1, # P_ID (nominalskaliert)\n                              3 # race_white_yn (nominalskaliert)\n                              )]\n\n# Mittelwerte berechnen:\nlistwise_mean <- colMeans(na.omit(data_b_num)) \npairwise_mean <- colMeans(data_b_num, na.rm=TRUE) \n\n# Anzahl genutzter Datenpunkte herausfinden:\nlistwise_n <- nrow(na.omit(data_b)) # gleich für alle Variablen\npairwise_n <- colSums(!is.na(data_b_num)) # verschieden für jede Variable\n\n# Vergleich der jeweiligen Mittelwerte\nvergleich <- data.frame(listwise_mean, pairwise_mean, listwise_n, pairwise_n)\nfor (i in 1:nrow(vergleich)){\n vergleich$diff[i] <- abs(vergleich[i, 1] - vergleich[i, 2])\n}\n\n\n\n\n\n  \n\n\n\nDie Anzahl der in die Berechnungen eingegangenen Datenpunkte unterscheidet sich stark. Während bei pairwise deletion zwischen 115 und 187 Beobachtungen für die Berechnungen genutzt wurden, wurden bei listwise deletion bei der Berechnung der Mittelwerte aller Variablen (die gleichen) 51 Beobachtungen genutzt.\n\nvergleich[which(vergleich$diff == min(vergleich$diff)), ]  # ähnlichster Mittelwert\n\n    listwise_mean pairwise_mean listwise_n pairwise_n       diff\nage      19.14815      19.24064         51        187 0.09249356\n\nvergleich[which(vergleich$diff == max(vergleich$diff)), ]  # unterschiedlichster Mittelwert\n\n                  listwise_mean pairwise_mean listwise_n pairwise_n     diff\npreselect_peak_e3     -26.31481     -44.12155         51        181 17.80673\n\n\nDie Variable mit dem geringsten Unterschied in den mit listwise und pairwise deletion berechneten Mittelwerten ist age; die Variable mit dem größten Unterschied ist preselect_peak_e3.\n\n\n\n2.) Berechne die Korrelationen aller numerischen, mindestens intervallskalierten Variablen jeweils mit listwise und pairwise deletion.  Die Korrelation welcher Variablen unterscheidet sich am meisten zwischen listwise und pairwise deletion, welcher am wenigsten?\n\n\nTipp: listwise und pairwise deletion\n\nlistwise/casewise deletion: Fälle mit mind. einem Missing in einer Variablen werden aus allen Mittelwertsberechnungen ausgeschlossen.\npairwise deletion: Fälle mit Missings werden nur aus der jeweiligen Korrelationsberechnungen derjenigen Variablen ausgeschlossen, in denen der Wert fehlt.\n\n\n\nTipp: numerisch\n\nMit is.numeric() können wir überprüfen, ob ein Vektor numerisch ist. Das müssen wir nur noch auf alle Spalten des Datensatzes anwenden.\n\n\n\nTipp: Unterschiedlichkeit der Korrelationen\n\nDazu berechnen wir die Differenz der Korrelationen jeder Variablen, die mit listwise und pairwise deletion erstellt wurden. Die Korrelationen von beiden Methoden sind je in einer Matrix gespeichert, d.h. für den Vergleich lohnt es sich, eine Schleife, die über alle Zeilen und Spalten geht, zu erstellen. Sinnvoll ist außerdem, mit abs() die absolute Differenz zu berechnen.\n\n\n\nTipp: Korrelationen der Variablen mit den extremsten Unterschieden finden\n\nMit which(..., arr.ind = TRUE) können wir uns Zeilenname, Zeilen- und Spaltenindizes ausgeben lassen.\n\n\n\nLösung\n\n\n# herausfinden, welche Variablen numerisch sind:\nwhich_num <- c()\nfor (i in 1:ncol(data_b)){\n  if (is.numeric(data_b[, i]) == TRUE) {\n    which_num <- append(which_num, i)\n  }\n}\n\n# Selektion auf Datensatz anwenden:\ndata_b_num <- data_b[, which_num]\n\n# Variablen, die nicht (mind.) intervallskaliert sind, aussortieren;\n# dazu vergleichen wir die Angaben im Codebuch und den Datentyp der Variablen\ndata_b_num <- data_b_num[, -c(1, # P_ID (nominalskaliert)\n                              3 # race_white_yn (nominalskaliert)\n                              )]\n\n# Korrelationen berechnen:\nlistwise_corr <- cor(data_b_num, use=\"complete.obs\") \n\n\n\n\n\n  \n\n\n\n\npairwise_corr <- cor(data_b_num, use=\"pairwise.complete.obs\") \n\n\n\n\n\n  \n\n\n\n\n# Matrix für die Differenzen der Korrelationen vorbereiten:\ndiff_corr <- matrix(nrow=nrow(listwise_corr), # Matrix erstellen\n                    ncol=ncol(listwise_corr))\ndiff_corr <- data.frame(diff_corr, # zu Dataframe umwandeln\n                        row.names = colnames(data_b_num)) # Zeilennamen ergänzen\ncolnames(diff_corr) <- colnames(data_b_num) # Spaltennamen ergänzen\n# Differenzen der Korrelationen berechnen:\nfor (i in 1:nrow(listwise_corr)){ # über alle Zeilen ...\n  for (j in 1:ncol(listwise_corr)){ # ... und über alle Spalten ...\n    diff_corr[i,j] <- abs(listwise_corr[i,j] - pairwise_corr[i,j])\n  } # ... absolute Abweichungen berechnen\n}\n\n\n\n\n\n  \n\n\n\n\n# (zuerst) doppelte Elemente und Diagonalelemente löschen:\ndiff_corr[upper.tri(diff_corr, diag = TRUE)] <- NA\n\n# Korrelation mit größtem Unterschied:\nmax(diff_corr, na.rm = TRUE)\n\n[1] 0.3696347\n\nwhich(diff_corr == max(diff_corr, na.rm = TRUE), \n      arr.ind = T) # Zeilnenname, Zeilen- und Spaltennummer\n\n                  row col\npreselect_peak_e3  24   4\n\ncolnames(diff_corr[4]) # Spaltenname\n\n[1] \"preselect_curr_e3\"\n\n# Korrelation mit kleinstem Unterschied:\nmin(diff_corr, na.rm = TRUE)\n\n[1] 0.0001259926\n\nwhich(diff_corr == min(diff_corr, na.rm = TRUE), \n      arr.ind = T) # Zeilenname, Zeilen- und Spaltennummer\n\n                 row col\npreselect_neg_e2  15   3\n\ncolnames(diff_corr[3]) # Spaltenname\n\n[1] \"preselect_curr_e2\"\n\n\nDie Korrelation mit dem größten Unterschied zwischen der listwise und der pairwise deletion Methode ist jene zwischen preselect_peak_e3 und preselect_curr_e3 (0.37); die Korrelation mit dem kleinsten Unterschied jene zwischen preselect_neg_e2 und preselect_curr_e2 (1.3^{-4}).\n\n\n\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n[1] foreign_0.8-84   naniar_1.0.0     kableExtra_1.3.4 knitr_1.42      \n[5] VIM_6.2.2        colorspace_2.1-0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3      xfun_0.39         ggplot2_3.4.2     htmlwidgets_1.6.2\n [5] visdat_0.6.0      lattice_0.21-8    vctrs_0.6.2       tools_4.3.0      \n [9] generics_0.1.3    tibble_3.2.1      proxy_0.4-27      fansi_1.0.4      \n[13] highr_0.10        DEoptimR_1.0-12   pkgconfig_2.0.3   Matrix_1.5-4     \n[17] data.table_1.14.8 webshot_0.5.4     lifecycle_1.0.3   compiler_4.3.0   \n[21] stringr_1.5.0     munsell_0.5.0     carData_3.0-5     htmltools_0.5.5  \n[25] class_7.3-21      yaml_2.3.7        pillar_1.9.0      car_3.1-2        \n[29] tidyr_1.3.0       MASS_7.3-58.4     boot_1.3-28.1     abind_1.4-5      \n[33] robustbase_0.95-1 tidyselect_1.2.0  rvest_1.0.3       digest_0.6.31    \n[37] stringi_1.7.12    dplyr_1.1.2       purrr_1.0.1       fastmap_1.1.1    \n[41] cli_3.6.1         magrittr_2.0.3    vcd_1.4-11        utf8_1.2.3       \n[45] e1071_1.7-13      withr_2.5.0       scales_1.2.1      sp_1.6-0         \n[49] rmarkdown_2.21    httr_1.4.5        nnet_7.3-18       norm_1.0-10.0    \n[53] ranger_0.15.1     zoo_1.8-12        evaluate_0.20     laeken_0.5.2     \n[57] lmtest_0.9-40     viridisLite_0.4.1 rlang_1.1.0       Rcpp_1.0.10      \n[61] glue_1.6.2        xml2_1.3.3        svglite_2.1.1     rstudioapi_0.14  \n[65] jsonlite_1.8.4    R6_2.5.1          systemfonts_1.0.4\n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Grafiken.html#bevor-es-losgeht",
    "href": "Grafiken.html#bevor-es-losgeht",
    "title": "10  Grafiken",
    "section": "10.1 Bevor es losgeht",
    "text": "10.1 Bevor es losgeht\nBevor wir uns in ggplot2 vertiefen, wollen wir überprüfen, ob unsere Daten in der benötigten Datenstruktur vorliegen. Außerdem lernen wir einige Webseiten kennen, die uns dabei helfen, uns für eine Art der Visualisierung unserer Daten zu entscheiden.\n\n10.1.1 Richtiges Datenformat\nBevor es losgehen kann, müssen wir erst einmal sicherstellen, dass unsere Daten auch in adäquater Form vorliegen. Um Grafiken mit ggplot() erzeugen zu können, müssen alle genutzen Variablen in einem gemeinsamen Dataframe vorliegen.\nDas können wir folgendermaßen überprüfen:\n\nis.data.frame(ChickWeight)\n## [1] TRUE\n\nFalls unser Datensatz nicht als Dataframe vorliegt, könnten wir ihn so umwandeln:\n\nChickWeight <- as.data.frame(ChickWeight)\n\nIn Abhängigkeit der eigenen Fragestellung müssen die Daten im Long- bzw. Wide-Format vorliegen. Schauen wir uns den Unterschied einmal an der Fragestellung, wie sich das Gewicht der Küken (weight) zu unterschiedlichen Zeitpunkten (Time) verändert, an.\nWenn wir Time als Prädiktor für weight aufnehmen möchten, müssen die Daten im Long-Format vorliegen, damit Time auch als eigene Variable kodiert ist.\nWenn wir uns hingegen für eine Veränderung des Gewichts von Tag 0 (Time 0) zu Tag 2 (Time 2) interessieren, müssen die Daten im Wide-Format vorliegen, damit die Gewichtsdaten zu den Messzeitpunkten in einzelnen Spalten (Variablen) vorliegen.\nWenn wir wissen wollen, wie wir Daten vom Wide- ins Long-Format (oder vice versa) bekommen, dann können wir unser Kapitel dazu anschauen.\n\nAußerdem kann es Probleme geben, wenn nominal- oder ordinalskalierte Variablen (z.B. Diet) im Datensatz nicht als Faktor vorliegen. Mit str() können wir uns anschauen, in welchem Daten- bzw. Objekttyp die Variablen eines Dataframes vorliegen.\n\nstr(ChickWeight) # überprüfen\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  578 obs. of  4 variables:\n $ weight: num  42 51 59 64 76 93 106 125 149 171 ...\n $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...\n $ Chick : Ord.factor w/ 50 levels \"18\"<\"16\"<\"15\"<..: 15 15 15 15 15 15 15 15 15 15 ...\n $ Diet  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"formula\")=Class 'formula'  language weight ~ Time | Chick\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n - attr(*, \"outer\")=Class 'formula'  language ~Diet\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Time\"\n  ..$ y: chr \"Body weight\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(days)\"\n  ..$ y: chr \"(gm)\"\n\n\nMit factor() können wir einzelne Spalten faktorisieren.\n\nChickWeight$Diet <- factor(ChickWeight$Diet) # umwandeln\n# Diet lag bereits vorher als ungeordneter Faktor vor\n\n\n\n10.1.2 Welche Grafik sollte ich für meine Daten nehmen?\nFalls wir noch auf der Suche nach einer informativen und ansprechenden Grafik für unsere Daten seid, können wir uns einen Überblick über geeignete Grafiken auf from Data to Viz verschaffen.\nFür mehr Inspiration (sowie teilweise auch den R-Code) können wir auf den folgenden zwei Seiten nachschauen:\nAuf R Graph Gallery finden wir verschiedene Grafiken nach Oberbegriffen (z.B. Zusammenhänge, Korrelation) sortiert und unterteilt in bestimmte Arten von Grafiken (z.B. Histogramm, Korrelogramm). Wenn wir auf die jeweilige Grafik klicken, kommen wir auf eine Seite, auf der es eine Definition und mehrere Beispielgrafiken, teils mit R-Code, gibt. Wir finden hier auch nicht nur Grafiken, die mit ggplot erstellt wurden.\nAuf Top 50 ggplot2 Visualizations - The Master List finden wir auch eine gute Übersicht möglicher ggplot-Grafiken für verschiedene Anliegen (z.B. Korrelation, Variation, Veränderung) mit den dazugehörigen R-Codes. Wir finden hier auch sehr einzigartige Visualisierungen wie z.B. Dendrogramme."
  },
  {
    "objectID": "Grafiken.html#grundlegender-aufbau-von-ggplot",
    "href": "Grafiken.html#grundlegender-aufbau-von-ggplot",
    "title": "10  Grafiken",
    "section": "10.2 Grundlegender Aufbau von ggplot()",
    "text": "10.2 Grundlegender Aufbau von ggplot()\nDas gg in ggplot() steht für grammar of graphics. ggplot strebt einen intuitiven Ansatz zur Erstellung von Grafiken an. Die essenziellen drei Komponenten, die wir zur Erstellung jeder Grafik benötigen, sind:\n\nDatensatz\nDiesen übergeben wir an das Argument data.\nästhetische Mappings (aesthetics)\nDiese werden in aes() festgelegt. Dazu gehören u.a. die x- und y-Dimensionen, Farben und Größe.\ngeometrische Objekte (geoms)\nDiese werden in geom() festgelegt. Die Formen, mit denen die Daten dargestellt werden, sind z.B. Punkte, Linien, Balken.\n\nDie Besonderheit an ggplot ist, dass wir verschiedene Ebenen übereinander legen. Diese Ebenen verbinden wir syntaktisch jeweils mit einem +. Das Grundgerüst ist dabei das Koordinatensystem (1. Ebene) und die Art der Grafik, die wir mittels geometrischer Objekt festlegen (2. Ebene). Zusätzlich können wir diese beiden Ebenen modifizieren (z.B. Farben der geometrischen Objekte ändern) und ebenso neue Ebenen ergänzen (z.B. Beschriftung).\nDie Ebenen können wir auch als Teilfunktionen begreifen, die eine Grafik konstituieren. Was wir z.B. bei plot(..., main=\"...\") in einer Funktion realisieren können (das Koordinatensystem, die Art der Grafik - ein Streudiagramm - und die Beschriftung) machen wir in ggplot mit mehreren Teilfunktionen (ggplot() + geom_point() + ggtitle()).\n\n10.2.1 Ebene 1: Grundlegendes Koordinatensystem\nDas Koordinatensystem spezifizieren wir mit ggplot(). Dem Parameter data übergeben wir den Datensatz (1). Das ästhetische Mapping (2) legen wir mit aes() fest. Diesem geben wir die Namen der Variablen, die auf der x- und y-Achse dargestellt werden sollen.\n\nggplot(data=ChickWeight, aes(x=Time, y=weight))\n\n\n\n\n\n\nWir können aes() aber nicht nur in ggplot(), d.h. dem grundlegenden Koordinatensystem, sondern auch in anderen Ebenen (z.B. geom_point()) nutzen.\n\nWie bei anderen Funktionen können wir die Parameterbezeichnung data, x und y weglassen und nur die Argumente (d.h. die Namen des Datensatzes bzw. der Variablen) angeben. Bei x und y müssen wir dann aber unbedingt die Reihenfolge einhalten: ggplot(ChickWeight, aes(Time, weight)).\n\n\n10.2.2 Ebene 2: Art der Grafik\nBisher wurde nur das Koordinatensystem mit den jeweiligen Achsengrenzen der Variablen erstellt. Wenn wir die Daten nun plotten möchten, müssen wir noch festlegen, wie die Daten dargestellt werden sollen (z.B. die Häufigkeit als Balken, die bivariate Verteilung als Punkte). Dazu nutzten wir geometrische Objekte(3), die wir mit den geom-Funktionen festlegen (z.B. geom_line() für Linien oder geom_bar() für Balken). Damit ergänzen wir den Plot um eine neue Ebene.\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point()\n\n\n\n\n\n\nHier sehen wir, wie die 1. Ebene, das grundlegende Koordinatensystem ggplot(...), mit der 2. Ebene, den geplotteten Daten geom_point(...) durch das + verknüpft wurden.\n\nNachdem wir uns nun mit dem grundlegenden Aufbau von ggplot() vertraut gemacht haben, schauen wir uns nun an, wie wir spezifische Arten von Grafiken erstellen."
  },
  {
    "objectID": "Grafiken.html#eine-variable",
    "href": "Grafiken.html#eine-variable",
    "title": "10  Grafiken",
    "section": "10.3 Eine Variable",
    "text": "10.3 Eine Variable\nZuerst schauen wir uns an, wie wir die Häufigkeitsverteilung einer Variablen visualisieren können.\n\n10.3.1 Kategorial\nHierfür nehmen wir die Variable Diet. Diese kodiert die Gruppen, in denen das Futter der Küken variiert wurde.\nSpäter erfahren wir noch, wie wir die Reihenfolge und Benennung von kategorialen Variablen ändern können.\n\n10.3.1.1 Balkendiagramm\nUm ein Balkendiagramm zu erzeugen, nutzen wir geom_bar(). Standardmäßig werden mit geom_bar() absolute Häufigkeiten (count) geplottet.\n\nggplot(data=ChickWeight, aes(x=Diet)) +\n  geom_bar()\n\n\n\n\nWir können folgendermaßen auch die relativen Häufigkeiten (prob) plotten:\n\nggplot(data=ChickWeight, aes(x=Diet)) +\n  geom_bar(aes(y = after_stat(prop), group=1))\n\n\n\n\nMit + coord_flip() können wir das Diagramm um 90° nach rechts kippen. Jetzt ist auf der x-Achse die Häufigkeit und auf der y-Achse die Gruppierung abgebildet.\n\nggplot(data=ChickWeight, aes(x=Diet)) + \n  geom_bar() + \n  coord_flip()\n\n\n\n\nWir können auch sogenannte gestapelte Balkendiagramme (stacked bar plots) anfertigen.\nDazu müssen erst einen neuen Dataframe erzeugen, in welchem die Gruppennamen sowie die (absoluten) Häufigkeiten der Variablen je als neue Variable gespeichert sind.\n\n# Häufigkeitstabelle von Diet erstellen und in Dataframe konvertieren:\nfreq_Diet <-as.data.frame(table(ChickWeight$Diet))\n# (optional) Spalten umbenennen:\ncolnames(freq_Diet) <- c(\"Futtergruppe\", \"Häufigkeit\")\n\nMit diesem Dataframe erstellen wir nun ein gestapeltes Balkendiagramm:\n\nggplot(data=freq_Diet, aes(x=\"\", y=Häufigkeit, fill=Futtergruppe)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\nAuf die Integration von Farben, hier durch den Parameter fill, gehen wir später im Abschnitt zu Farbe noch ausführlicher ein.\n\nMehr Informationen zur Erstellung von gestapelten sowie auch gruppierten Balkendiagrammen finden wir hier. Beide Grafiken eignen sich auch gut, um Gruppierungskombinationen mit noch einer weitere kategorialen Variablen darzustellen.\n\n\n10.3.1.2 Kreisdiagramm\nUm ein Kreisdiagramm zu erstellen, müssen wir, analog zur Erstellung von gestapelten Balkendiagrammen, zuerst einen neuen Dataframe erzeugen, in welchem die Namen und (absoluten) Häufigkeiten der Gruppen der Variablen als neue Variablen vorhanden sind.\n\n# Häufigkeitstabelle von Diet erstellen und in Dataframe konvertieren:\nfreq_Diet <-as.data.frame(table(ChickWeight$Diet))\n# (optional) Spalten umbenennen:\ncolnames(freq_Diet) <- c(\"Futtergruppe\", \"Häufigkeit\")\n\nAuch das weitere Vorgehen überschneidet sich weitest mit dem der Erstellung eines gestapelten Balkendiagrammes. Wir müssen lediglich noch + coord_polar(\"y\") ergänzen, damit die Anteile der Häufigkeiten in Polarkoordinaten überführt werden.\n\nggplot(data=freq_Diet, aes(x=\"\", y=Häufigkeit, fill=Futtergruppe)) +\n  geom_bar(stat = \"identity\") + \n  coord_polar(\"y\")\n\n\n\n\n\n\nAuf Farben gehen wir im gleichnamigen Abschnitt später noch ausführlicher ein.\n\nAuf dieser Seite finden wir weitere Hilfe zur Erstellung und Modifikation von Kreisdiagrammen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Metrisch\nBeispielhaft schauen wir uns hierfür das Gewicht der Küken (weight) an.\n\n10.3.2.1 Histogramm\nMit Hilfe von Histogrammen können wir uns die Verteilung einer metrischen Variablen anschauen. Um ein Histogramm zu erstellen, nutzen wir geom_histogram().\n\n\nRichtlinien zur Kategorisierung einer metrischen Variablen\n\nBevor wir eine metrische Variable zum Zwecke der grafischen Darstellung kategorisieren, sollten wir uns Gedanken über die Anzahl und Breite der Kategorien machen, in die wir die Daten einteilen möchten. Es ist meist nicht sinnvoll, die Häufigkeiten der Rohwerte von metrischen Variablen zu plotten, weil identische Messwerte selten vorliegen. Daher ist es sinnvoll, die Rohdaten zu gruppieren. Dazu müssen wir uns Gedanken über die Intervallgröße dieser Kategorien machen (“Problem der Kategorisierung”). Durch Vergrößerung gehen (relevante) Informationen verloren; durch Verkleinerung bleiben (zu viele) idiosynkratische Merkmale erhalten.\nPrinzipiell gilt:\n\nJe größer eine Stichprobe ist, desto schmaler können die einzelnen Kategorien sein (d.h. mehr Kategorien).\n\nAls Orientierung können wir hierfür die Sturges Regel nutzen:\n\\(m ≈ 1 + 3.32 \\cdot lg(N)\\) (\\(m\\) = Anzahl der Kategorien, \\(N\\) = Stichprobengröße, \\(lg\\) = dekadischer Logarithmus)\n\n\nSchauen wir uns die ungefähre Anzahl der Kategorien nach der Sturges Regel an. Dazu nutzen wir die Funktion nclass.Sturges(1:Stichprobengröße).\n\n# Gibt es fehlende Werte?\nanyNA(ChickWeight$weight) \n\n[1] FALSE\n\n## wenn nein:\nnclass.Sturges(1:nrow(ChickWeight))\n\n[1] 11\n\n## wenn ja:\n# nclass.Sturges(1:table(is.na(ChickWeight$weight))[[2]])\n# table(is.na(ChickWeight$weight))[[2]] gibt uns die Anzahl der vorhanden werte\n\nNach Sturges Regel sollten wir für weight in etwa 11 Kategorien bilden.\n\nJe größer die Streuung der Variablen, desto breiter können die einzelnen Kategorien sein (d.h. weniger Kategorien).\n\n\nmin(ChickWeight$weight) # kleinster Wert\n\n[1] 35\n\nmax(ChickWeight$weight) # größter Wert\n\n[1] 373\n\n# Variationsbreite (Streuung) = größter minus kleinster Wert:\nmax(ChickWeight$weight) - min(ChickWeight$weight) \n\n[1] 338\n\n# bei Vorhandensein von fehlenden Werten:\n# min(..., na.rm=TRUE) bzw. max(..., na.rm=TRUE)\n\nEs ist sinnvoll, zuerst nach Sturges Regel eine Anzahl an Bins zu errechnen, und diese dann ggf. in Abhängigkeit der Variationsbreite des Merkmals zu verändern.\nGenerell sollten wir nicht mehr als 20 Kategorien bilden. Diese sollten im Regelfall auch ungefähr die gleiche Breite aufweisen.\n\nWir erstellen ein Histogramm von weight mit den empfohlenen 11 Kategorien, welche wir mit dem Argument bins erstellen.\n\nggplot(data=ChickWeight, aes(x=weight)) + \n  geom_histogram(bins=11)\n\n\n\n\n\n\nProblem der korrekten Darstellung der Variationsbreite\n\nGrundsätzlich ist es, sowohl in base graphics als auch in ggplot, problematisch, den wahren Range (d.h. die Variationsbreite) einer Variablen darzustellen. Ohne explizite Eingabe der Grenzen werden diese leider nicht berücksichtigt. Das kann zu einem falschen Eindruck der Variationsbreite der Variablen führen.\nDieses Problem demonstrieren wir einmal an obigem Beispiel.\nMit geom_vline(xintercept) erstellen wir zwei vertikale Linien, die den kleinsten bzw. größten beobachteten Wert von weight markieren. Diese nutzen wir zur Veranschaulichung der Problematik.\n\nggplot(data=ChickWeight, aes(x=weight)) + \n  geom_histogram(bins=11) + \n  geom_vline(xintercept=min(ChickWeight$weight)) + # kleinster Wert\n  geom_vline(xintercept=max(ChickWeight$weight)) # größter Wert\n\n\n\n\nWie wir sehen haben wir zwar die gewünschten 11 Kategorien, aber diese sind leider so eingeteilt, dass die äußeren Kategorien über den kleinsten und größten beobachteten Wert von weight hinausgehen (d.h. sie sind zu groß).\nUm das Problem in den Griff zu bekommen, nutzen wir das Argument breaks, mit dem wir die Kategoriengrenzen manuell festlegen. Diesem übergeben wir die Funktion seq(from, to, by), die uns eine reguläre Sequenz erstellt (damit wir nicht alle Grenzen einzeln eingeben müssen). Auch hier wollen wir wieder die empfohlenen 11 Kategorien haben, daher teilen wir den Range (373-35) von weight durch 11.\n\nggplot(data=ChickWeight, aes(x=weight)) + \n  geom_histogram(breaks=seq(35, 373, (373-35)/11)) + \n  geom_vline(xintercept=min(ChickWeight$weight)) + # kleinster Wert\n  geom_vline(xintercept=max(ChickWeight$weight)) # größter Wert\n\n\n\n\nWie wir an den vertikalen Linien sehen, hält die Breite der Kategorien insgesamt jetzt den Range von weight ein.\n\nWeiterführend finden wir zusätzliche Modifikationen für Histogramme auf dieser Seite.\n\n\n10.3.2.2 Dichtefunktion\nDie Dichteverteilung einer kontinuierlichen Variablen können wir mit geom_density() einzeichnen. Per Default wird dafür die Gaußsche Dichtefunktion (kernel = \"gaussian\") genutzt.\n\nggplot(data=ChickWeight, aes(x=weight)) + geom_density()"
  },
  {
    "objectID": "Grafiken.html#zwei-variablen",
    "href": "Grafiken.html#zwei-variablen",
    "title": "10  Grafiken",
    "section": "10.4 Zwei Variablen",
    "text": "10.4 Zwei Variablen\nNun schauen wir uns die grafische Darstellung von Zusammenhängen zwischen zwei Variablen an.\n\n10.4.1 X kategorial, Y metrisch\nSpäter erfahren wir noch, wie wir die Reihenfolge und Benennung von kategorialen Variablen ändern können.\n\n10.4.1.1 Boxplot\n\n\n\nWenn wir uns die Merkmalsverteilung einer metrischen Variable in Abhängigkeit einer kategorialen Variablen anschauen möchten, können wir Boxplots nutzen.\n\n\nWelche Kennwerte werden in Boxplots dargestellt?\n\nIn Boxplots werden mehrere deskriptiv-statistische Kennwerte dargestellt:\n\nzentrale Tendenz:\n\nMedian: dargestellt durch die dicken, horizontalen Linien in den Boxen\n\nVariabilität:\n\nInterquartilsrange (IQR): mittlere 50% der Verteilung, dargestellt durch die Boxen\nWhisker: größter bzw. kleinster beobachteter Wert innerhalb der oberen bzw. unteren Ausreißergrenzen, dargestellt durch die vertikale Linien an den Boxen\nAusreißer: einzelne Beobachtungen außerhalb der Whisker, dargestellt durch Punkte\n\n\n\nEinen Boxplot erhalten wir mit geom_boxplot(). Wenn wir fehlende Werte in unseren Daten haben, müssen wir noch das Argument na.rm = TRUE ergänzen, welches diese (aus der Grafik) entfernt.\nWir schauen uns das Gewicht der Küken (weight) zum zweiten Messzeitpunkt (Time 2) in Abhängigkeit der Fütterung (Diet) an.\n\nggplot(data=ChickWeight[ChickWeight$Time == 2,], aes(x=Diet, y=weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nErweiterte Indexierung: Wie können wir nur bestimmte Daten (z.B. nur Daten zum zweiten Messzeitpunkt) für ggplot() auswählen?\n\nWenn wir nicht alle Daten, sondern nur solche, auf die bestimmte Bedingungen zutreffen, auswählen wollen, können wir dazu die Indexierung mittels der eckigen Klammern [] nutzen.\nFür unser Beispiel wollen wir uns nur die Daten des zweiten Messzeitpunkts (Time 2) anschauen. Formal würde (nur) die Bedingung so aussehen: ChickWeight$Time == 2. Diese wollen wir noch auf unseren Dataframe anwenden; dazu nutzen wir die eckigen Klammern: ChickWeight[ChickWeight$Time == 2,].\nMit dem Komma legen wir fest, welche der beiden Dimensionen unseres Dataframes wir meinen (Zeile, Spalte). Weil wir Fälle mit bestimmten Ausprägungen (Bedingungen) auswählen wollen, setzen wir das Komma am Ende.\nZu beachten ist außerdem, dass wir die Indexierung bereits in der grundlegenden Ebene, d.h. in ggplot(data, ...), festlegen müssen. Zu Beginn haben wir gelernt, dass wir durch die Übergabe des Dataframes an ggplot(data, ...) das grundlegende Koordinatensystem erstellen und damit die Anzahl der Datenpunkte (Beobachtungen) festlegen. Folglich würde es nicht funktionieren, die Selektion erst in einer späteren Ebene vorzunehmen, weil die an Anzahl der Datenpunkte sich unterscheiden würde.\n\n\n\n10.4.1.2 Violinenplot\nDer Violinenplot ist eine Variante von Boxplots, in dem die Dichtefunktion eines metrischen Merkmals grafisch dargestellt wird. Diese wird an der Senkrechten zur x-Achse gespiegelt.\nUm die Grafiken mit dem Boxplot aus dem letzten Abschnitt vergleichen zu können, schauen wir uns hier ebenfalls das Gewicht der Küken (weight) zum zweiten Messzeitpunkt (Time 2) in Abhängigkeit der Diät (Diet) an.\n\nHier wird das Indexieren der Daten (zum zweiten Messzeitpunkt) erklärt.\n\nGenerell können wir den Violinenplot mit geom_violin() erstellen. Zusätzlich gibt es hier einen sehr wichtigen Parameter: scale. Mit diesem legen wir fest, ob die einzelnen “Violinen” in der gleichen Größe (area; voreingestellt) oder in Größen proportional zur Anzahl der Beobachtungen in jeder Gruppe (count) dargestellt werden sollen.\nWir schauen uns unsere Daten einmal in beiden Modi an.\n\nggplot(data= ChickWeight[ChickWeight$Time == 2,], aes(x=Diet, y=weight)) +\n  geom_violin(scale = \"area\")\n\nggplot(data=ChickWeight[ChickWeight$Time == 2,], aes(x=Diet, y=weight)) + \n  geom_violin(scale = \"count\")\n\n\n\n\n\n\n\nWir sehen, dass die Größen derselben “Violinen” sich in den beiden Grafiken unterscheiden. Das liegt daran, dass bei ersterer Grafik (scale=area) die Anzahl der Beobachtungen pro Gruppe unbeachtet bleibt, während sie in zweiter Grafik (scale=count) über die relative Größe der “Violinen” visualisiert wird. Wenn wir gleich große Gruppen hätten, würden wir keinen Unterschied zwischen den beiden Grafiken erkennen.\n\n\n10.4.1.3 Gruppenmittelwerte visualisieren\n\n\n\nIm Folgenden schauen wir uns die finalen (Time 21) Mittelwerte des Gewichts der Küken (weight) in den einzelnen Futtergruppen (Diet) an. Dabei wollen wir außerdem die 95%-Konfidenzintervalle für die einzelnen Mittelwerte einzeichnen.\n\nHier wird das Indexieren der Daten (zum finalen Messzeitpunkt) erklärt.\n\nWenn wir aggregierte Daten einer statistischen Analyse (z.B. Gruppenmittelwerte bei einem \\(t\\)-Test) darstellen möchten, ist ein weit verbeiteter Ansatz, Balkendiagramme mit Fehlerbalken zu nutzen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegativbeispiel\n\n\nAchtung: Das Problem bei diesem Ansatz ist, dass Personen dazu neigen, die Werte (innerhalb des 95% Konfidenzintervalls), die innerhalb der Balken liegen, als statistisch wahrscheinlicher wahrzunehmen als jene, die außerhalb der Balken liegen. Dieses Phänomen ist als Within-the-bar Bias bekannt. Es ist daher sinnvoll, für die Darstellung von Mittelwerten keine Balkendiagramme zu nutzen, sondern auf geeignetere Visualisierungen zurückzugreifen.\n\nBeispielsweise vermitteln einzelne Punkte (für die Mittelwerte) mit Fehlerbalken die relevanten Informationen besser.\nGenerell um statistische Kennwerte zu ergänzen, nutzen wir die Funktion stat_summary() (mehr dazu hier). Mit fun.data=\"mean_cl_normal\" bekommen wir Mittelwerte mit 95% Konfindenzintervallen.\n\nggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + \n  stat_summary(fun.data=\"mean_cl_normal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositivbeispiel\n\nWenn wir zusätzlich noch unsere Informationsdichte erhöhen wollen, d.h. nicht nur den Mittelwert, sondern die Verteilung in den einzelnen Gruppen visualisieren wollen, können wir Violinenplots integrieren.\n\nggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) +\n  geom_violin(scale=\"count\") + stat_summary(fun.data=\"mean_cl_normal\")\n# Reihenfolge beachten! erst Violinenplot, dann Mittelwerte darüber\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositivbeispiel\n\n\n\n\n10.4.2 X und Y metrisch\n\n10.4.2.1 Streudiagramm\nMit Streudiagrammen (Scatterplots) können wir Wertepaare zweier kontinuierlicher Variablen grafisch darstellen. Das machen wir mit geom_point().\nWir schauen uns im Folgenden den Zusammenhang von Zeit seit dem Schlüpfen (Time) und dem Gewicht in Gramm (weight) der Küken an.\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point()\n\n\n\n\n\nWie wir die Datenpunkte in Abhängigkeit einer weiteren kategorialen Variablen einfärben können, erfahren wir im Abschnitt Farben.\n\n\n\n10.4.2.2 Overplotting vermeiden\nWenn wir sehr große Datensätze haben, könnten wir das Problem haben, dass wir individuelle Daten optisch nicht mehr gut unterscheiden können, weil viele Datenpunkte übereinander liegen. Dieses Problem nennt man auch Overplotting. Im Folgenden schauen wir uns einige Möglichkeiten an, wie wir das Problem beheben können.\nHierfür schauen wir uns wieder den Zusammenhang von Zeit seit dem Schlüpfen (Time) und dem Gewicht in Gramm (weight) der Küken an.\n\n10.4.2.2.1 Punkte modifizieren\nWir können die Formen verkleinern mit dem Parameter size (Breite der Linie in mm), …\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point(size = 0.1)\n\n\n\n\n… die Form ändern mit dem Parameter shape, …\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point(shape = 1)\n\n\n\n\n… oder die Form transparent machen mit dem Parameter alpha (0 < alpha < 1).\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point(alpha = 0.1)\n\n\n\n\n\n\n10.4.2.2.2 Jittern\n“Jittern” heißt, dass wir etwas zufälliges Rauschen einfügen, damit die Datenpunkte etwas voneinander abweichen.\n\nAchtung: Weil wir hiermit aber einen falschen Eindruck von den Daten vermitteln könnten, sollten wir die Verwendung von gejitterten Daten immer kennzeichnen.\n\nPer default werden die Punkte in 80% der Breite der implizierten Bins (z.B. der Bin von Time 0) geplottet, sodass die Bins optisch noch gut voneinander zu trennen sind. Mit dem Parameter width können wir die Breite anpassen.\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_jitter()\n\n\n\n\n\n# mehr (aber zu viel) Variation:\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_jitter(width=1) \n\n\n\n\nEs gibt noch weitere Möglichkeiten Overplotting zu vermeiden. Auf dieser Seite finden wir noch andere Beispiele sowie ihre Umsetzung in R.\n\n\n\n10.4.2.3 Regressionsgerade einzeichnen\nWir können unsere bivariaten Daten zusätzlich noch durch eine Funktion beschreiben lassen. Häufig nutzen wir dafür Regressionsmodelle.\nDie Regressionsgerade eines einfachen linearen Regressionsmodells können wir mit geom_smooth(method=\"lm\") über unser Streudiagramm legen.\nPer default wird nicht nur eine Regressionsgerade eingezeichnet, sondern auch das 95%-Konfidenzintervall um die Gerade gelegt.\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point() +\n  geom_smooth(formula = y ~ x, method=\"lm\")\n\n\n\n\n\n\nDas Konfidenzintervall um die Regressionsgerade könnten wir mit se=FALSE weglassen. Generell ist es aber sinnvoll, die Genauigkeit unseres Regressionsmodells in die Grafik miteinzubeziehen.\n\nWenn wir die Datenpunkte in einem Streudiagramm mit Regressionsgerade einfärben wollen ohne gruppenspezifische Regressionsgeraden zu erhalten, müssen wir aes(col) in geom_point(), und nicht in ggplot(), spezifizieren.\n\n\n\n\nPaket ggiraphextra: Interaktive Plots und weitere Darstellungen von Regressionsmodellen\n\nMit der Funktion ggPredict() aus dem Paket ggiraphExtra können wir interaktive Streudiagramme mit Regressionsgeraden (von einfachen und multiplen linearen Regressionsmodellen) erstellen. Hierfür benötigen wir die Basisfunktion ggplot() gar nicht (aber ggiraphExtra baut auf ggplot2 auf). Allerdings müssen wir zuerst manuell mit lm() die Regression rechnen und das Ergebnisobjekt an ggPredict() übergeben.\nIm Folgenden schauen wir uns an, wie wir interaktive Streudiagramme mit einer Regressionsgeraden (eines einfachen linearen Regressionsmodells) erstellen können.\nWeitere Hilfe zu ggPredict(), z.B. wie wir gruppenspezifische Ergebnisse einer multiplen linearen Regression mit Interaktion oder einer logistischen Regression visualisieren können, findet wir hier.\nInteraktiv bedeutet, dass wir in den Plot reinzoomen, uns die Funktion der Regressionsgerade sowie die ID und die Prädiktor- und Kriteriumswerte der Personen anzeigen lassen können. Um den Plot interaktiv zu machen, müssen wir interactive=TRUE festlegen.\nAuch hier können wir ein 95%-iges Konfidenzintervall um die Regressionsgerade legen, indem wir se=TRUE spezifizieren. Per Default ist hier (im Gegensatz zu geom_smooth) kein Konfidenzintervall eingezeichnet. Die Visualisierung der (Un)Genauigkeit unseres Regressionsmodells ist allerdings zu empfehlen.\n\n# install.packages(\"ggiraphExtra\", dependencies=TRUE)\n\n# Regression rechnen:\nreg_ggP <- lm(formula = weight ~ Time, data = ChickWeight)\n\n# Ergebnisobjekt plotten\nlibrary(ggiraphExtra)\nggPredict(reg_ggP, se = TRUE, interactive = TRUE)\n\n\n\n\n\n\nAchtung: Leider ist die Nutzung von ggPredict() teilweise noch fehleranfällig, weil von ggiraphExtra erst eine Beta-Version vorliegt (Stand: Version 0.2.9). Es kann u.a. zu Problemen kommen, wenn man ordinalskalierte (ordered factors) oder nominalskalierte Variablen (factors) ins Modell aufnimmt. Trotzdem ist das interaktive Streudiagramm ein sinnvolles Feature, z.B. für Ergebnispräsentation in html-Dokumenten. Man sollte die Weiterentwicklung des Pakets ggiraphExtra also verfolgen."
  },
  {
    "objectID": "Grafiken.html#mehr-als-zwei-variablen",
    "href": "Grafiken.html#mehr-als-zwei-variablen",
    "title": "10  Grafiken",
    "section": "10.5 Mehr als zwei Variablen",
    "text": "10.5 Mehr als zwei Variablen\nWie wir bis hier gesehen haben, können wir bis zu zwei Variablen sehr gut in einer Grafik visualisieren. Wenn wir aber mehr (kategoriale) Variablen aufnehmen wollen, müssen wir darauf achten, dass die zusätzlichen Informationen nicht zu Lasten der Verständlichkeit der Grafik sind.\nMit Facetten erstellen wir eine Matrix aus (Unter)Grafiken einer Art für verschiedene Gruppen. Mit einem Alluvial Plot können wir die Häufigkeiten der Zugehörigkeit zu verschiedenen Gruppen visualisieren.\nBeide Grafiken können genutzt werden, wenn wir Variablen visualisieren wollen, von denen mindestens eine kategorial ist. Wir können mit ihnen mehr als drei Variablen visualisieren, wenn alle (Alluvial Plot) bzw. alle bis auf maximal zwei (Facetten Plots) Variablen kategorial sind. Werte kontinuierliche Variablen können wir auch in Kategorien einteilen um sie hier zu nutzen.\nIm Folgenden nutzen wir auch den Datensatz mpg, weil dieser mehr kategoriale Varialen enhält als ChickWeight.\n\n10.5.1 Mindestens eine kategoriale Variable\n\n10.5.1.1 Facetten (Facet Grids)\nWenn wir uns eine Art von Grafik (z.B. Streudiagramm) in Abhängigkeit einer kategorialen Variablen (d.h. für verschiedene Gruppen) separat anschauen wollen, können wir + facet_grid() nutzen. Mit dieser Funktion bekommen wir “Facetten”, d.h. (Unter)Grafiken für jede Ausprägung der kategorialen Variablen.\nWir können außerdem entscheiden, wie die Facetten angeordnet sein sollen:\n\nfacet_grid(Variable ~ .)\nDie Variable wird zeilenweise angeordnet, indem wir sie vor die Tilde schreiben und einen Punkt dahinter setzen.\nfacet_grid( ~ Variable)\nDie Variable wird spaltennweise angeordnet, indem wir sie nach der Tilde schreiben.\n\n\nDie Anordnung von Zeile und Spalte in facet_grid() ist analog zum Indexieren von zweidimensionalen Datenstrukturen (z.B. Matrizen, Dataframes): Zuerst die Zeile, dann die Spalte.\n\nWir schauen uns die Gewichtszunahme (weight) über die Zeit (Time) getrennt für die einzelnen Experimentalgruppen (Diet) an.\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point() +\n  facet_grid(Diet ~ .) # Zeilen\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point() +\n  facet_grid( ~ Diet) # Spalten\n\n\n\n\n\n\n\nWir können auch Facetten für mehrere kategoriale Variablen erstellen. Das Produkt der Anzahl der Ausprägungen der Variablen gibt die Menge der (Unter)Grafiken an (z.B. drei Variablen mit jeweils zwei Ausprägungen: 2 x 2 x 2 = 8 Grafiken). Wenn wir mehr als zwei Variablen haben, werden Variablen, die zusammen in einer Zeile bzw. einer Spalte gestaffelt werden, mit einem + verbunden. Damit die einzelnen Grafiken aber noch übersichtlich bleiben, sollten wir ihre Anzahl tendenziell gering halten.\nWir schauen uns aus dem Datensatz mpg Histogramme des Hubraums (displ) der Autos an. Die Histogramme sind nach Jahr (year), Antrieb (drv) und Brennstofftrieb (fl) gestaffelt.\n\nggplot(data=mpg, aes(x=displ)) + \n  geom_histogram() +\n  facet_grid(fl ~ year + drv) # Zeilen ~ Spalten\n\n\n\n\n\n\nIdealerweise würde man vor der Verarbeitung nominal- und ordinalskalierte Variablen, die als numerische Vektoren vorliegen (wie drv und fl), noch faktorisieren. Mehr dazu auch im Kapitel zu Datenvorbereitung\n\nWeitere Informationen zu facet_grid(), z.B. wie man hier die Achsengrenzen und -beschriftung anpassen kann, befinden sich auf dem Cheat Sheet.\nMehr Informationen dazu, wie wir nur Plots für einige Gruppen erstellen, erhalten wir später.\n\n\n\n10.5.2 Nur kategoriale Variablen\n\n10.5.2.1 Alluvial Plot\nAlluvial Plots sind eine Art von Flussdiagrammen, die die Zugehörigkeit zu mehreren Gruppierungen visualisieren. Sie stellen somit auch eine grafische Alternative zu Kontingenztabellen dar.\nWir wollen das Gewicht der Küken zum ersten und letzten Messzeitpunkt (Time) in Abhängigkeit der Fütterung (Diet) visualisieren. Dazu werden wir die Werte der metrischen Variablen weight in zwei Gruppen aufteilen: größer bzw. kleiner als der Mittelwert.\nFür die Visualisierung längsschittlicher Daten müssen wir die folgenden Schritte durchlaufen: Zuerst wählen wir die relevanten Fälle, d.h. jene zum ersten und letzten Messzeitpunkt, aus und speichern sie in einem neuen Dataframe (optional; nur wenn wir nicht alle Zeitpunkte visualisieren wollen). Diesen wandeln wir dann ins Wide-Format um. Danach erstellen wir eine neue Variable, die kodiert, ob das Gewicht eines Kükens zu einem gewissen Zeitpunkt kleiner oder größer als der Mittelwert ist. Zuletzt entfernen die nicht länger benötigten Variablen aus dem Dataframe.\n\nDiese Schritte werden im folgenden nur knapp erklärt. Mehr Hilfe finden wir im Kapitel zu Datenvorbereitung in den Abschnitten Daten extrahieren und Kodierung ändern.\n\n\n\nlibrary(dplyr)\n# neuer Datensatz nur mit Werten von erster und letzter Messung:\ncopy <- filter(ChickWeight, Time == 0 | Time == 21)\n\n# ins Wide-Format bringen:\nlibrary(tidyr)\ncont_Chick <- spread(copy, key=\"Time\", value=\"weight\")\n  # Die Variablen 0 und 21 wurden erzeugt. Diese müssen wir bei der Indexierung \n  # immer mit \"..\" umschließen, da sie sonst nicht als solche erkannt werden.\n\n# für beide Messzeitpunkte eine neue Variable erstellen:\ncont_Chick$t0 <- case_when(cont_Chick$\"0\" < mean(cont_Chick$\"0\", na.rm=TRUE) ~ \"kleiner\",\n                           cont_Chick$\"0\" > mean(cont_Chick$\"0\", na.rm=TRUE) ~ \"größer\")\ncont_Chick$t21 <- case_when(cont_Chick$\"21\" < mean(cont_Chick$\"21\", na.rm=TRUE) ~ \"kleiner\",\n                           cont_Chick$\"21\" > mean(cont_Chick$\"21\", na.rm=TRUE) ~ \"größer\")\n\n# nicht mehr benötigte Variablen raus:\ncont_Chick <- select(cont_Chick, -\"0\", -\"21\", -Chick)\n  # Die ID-Variable \"Chick\" benötigen wir nicht mehr und sie würde uns die\n  # im Anschluss erstellte Kontingenztabelle unnötig erweitern.\n\n\n\n\nFür jeden Alluvial Plot (egal ob längsschnittliche oder querschnittliche Daten genutzt wurden) müssen wir die Daten in eine Kontingenztabelle überführen (damit die Häufigkeiten der Gruppierungskombinationen explizit in einer neuen Spalte gespeichert werden) und anschließend wieder in einen Dataframe umwandeln.\n\n# Kontingenztabelle erstellen\ncont_Chick <- table(cont_Chick)\n# in Dataframe umwandeln\ncont_Chick <- as.data.frame(cont_Chick)\n\nUm besser zu verstehen, warum das sein muss, können wir uns den Datensatz vor und nach der Umwandlung anschauen.\n\n\n\nVergleich unseres Dataframes vor und nach der Umwandlung\n\nVor der Umwandlung steht jede Zeile für eine Beobachtung (d.h. ein Küken).\n\n\n\n\n \n  \n    Diet \n    t0 \n    t21 \n  \n \n\n  \n    1 \n    größer \n    kleiner \n  \n  \n    1 \n    kleiner \n    kleiner \n  \n  \n    1 \n    größer \n    kleiner \n  \n  \n    1 \n    größer \n    kleiner \n  \n  \n    1 \n    kleiner \n    größer \n  \n  \n    1 \n    kleiner \n    kleiner \n  \n  \n    1 \n    kleiner \n    größer \n  \n  \n    1 \n    größer \n    NA \n  \n  \n    1 \n    größer \n    kleiner \n  \n  \n    1 \n    kleiner \n    kleiner \n  \n  \n    1 \n    größer \n    kleiner \n  \n  \n    1 \n    kleiner \n    kleiner \n  \n  \n    1 \n    kleiner \n    kleiner \n  \n  \n    1 \n    kleiner \n    größer \n  \n  \n    1 \n    kleiner \n    NA \n  \n  \n    1 \n    kleiner \n    NA \n  \n  \n    1 \n    größer \n    kleiner \n  \n  \n    1 \n    kleiner \n    NA \n  \n  \n    1 \n    größer \n    kleiner \n  \n  \n    1 \n    kleiner \n    kleiner \n  \n  \n    2 \n    kleiner \n    größer \n  \n  \n    2 \n    kleiner \n    kleiner \n  \n  \n    2 \n    größer \n    kleiner \n  \n  \n    2 \n    größer \n    kleiner \n  \n  \n    2 \n    kleiner \n    größer \n  \n  \n    2 \n    größer \n    größer \n  \n  \n    2 \n    kleiner \n    kleiner \n  \n  \n    2 \n    kleiner \n    größer \n  \n  \n    2 \n    kleiner \n    größer \n  \n  \n    2 \n    größer \n    kleiner \n  \n  \n    3 \n    größer \n    größer \n  \n  \n    3 \n    kleiner \n    größer \n  \n  \n    3 \n    kleiner \n    kleiner \n  \n  \n    3 \n    kleiner \n    größer \n  \n  \n    3 \n    kleiner \n    größer \n  \n  \n    3 \n    kleiner \n    größer \n  \n  \n    3 \n    kleiner \n    kleiner \n  \n  \n    3 \n    kleiner \n    größer \n  \n  \n    3 \n    größer \n    größer \n  \n  \n    3 \n    kleiner \n    größer \n  \n  \n    4 \n    größer \n    kleiner \n  \n  \n    4 \n    größer \n    größer \n  \n  \n    4 \n    größer \n    kleiner \n  \n  \n    4 \n    größer \n    NA \n  \n  \n    4 \n    kleiner \n    kleiner \n  \n  \n    4 \n    kleiner \n    größer \n  \n  \n    4 \n    kleiner \n    kleiner \n  \n  \n    4 \n    kleiner \n    größer \n  \n  \n    4 \n    kleiner \n    größer \n  \n  \n    4 \n    kleiner \n    größer \n  \n\n\n\n\n\nNach der Umwandlung (in eine Kontingenztabelle und wieder zurück in einen Dataframe) steht in jeder Zeile eine Kombination (welche Futtergruppe Diet und jeweils kleiner oder größer zum ersten und zum zweiten Messzeitpunkt) und die Anzahl der Beobachtungen.\n\n\n\n\n \n  \n    Diet \n    t0 \n    t21 \n    Freq \n  \n \n\n  \n    1 \n    größer \n    größer \n    0 \n  \n  \n    2 \n    größer \n    größer \n    1 \n  \n  \n    3 \n    größer \n    größer \n    2 \n  \n  \n    4 \n    größer \n    größer \n    1 \n  \n  \n    1 \n    kleiner \n    größer \n    3 \n  \n  \n    2 \n    kleiner \n    größer \n    4 \n  \n  \n    3 \n    kleiner \n    größer \n    6 \n  \n  \n    4 \n    kleiner \n    größer \n    4 \n  \n  \n    1 \n    größer \n    kleiner \n    7 \n  \n  \n    2 \n    größer \n    kleiner \n    3 \n  \n  \n    3 \n    größer \n    kleiner \n    0 \n  \n  \n    4 \n    größer \n    kleiner \n    2 \n  \n  \n    1 \n    kleiner \n    kleiner \n    6 \n  \n  \n    2 \n    kleiner \n    kleiner \n    2 \n  \n  \n    3 \n    kleiner \n    kleiner \n    2 \n  \n  \n    4 \n    kleiner \n    kleiner \n    2 \n  \n\n\n\n\n\n\nBevor wir den Alluvial Plot erstellen, laden wir das benötigte Paket ggalluvial und überprüfen, ob unser neu erstellter Dataframe auch wirklich im korrekten Alluvial Format vorliegt.\n\nlibrary(ggalluvial)\nis_alluvia_form(cont_Chick)\n\n[1] TRUE\n\n\nEs ist zu empfehlen, den nachfolgenden Code für die eigenen Daten größtenteils zu kopieren. Lediglich in Zeilen (a) und (d) müssen die Argumente von fill bzw. limit angepasst werden.\n\nggplot(data=cont_Chick, aes(y=Freq, axis1=t0, axis2=t21)) + \n  # (a) grundlegender Alluvial Plot:\n  geom_alluvium(aes(fill=Diet)) +\n  # (b) Balken zur Visualisierung der Häufigkeiten in t0 und t21:\n  geom_stratum(fill=\"white\", width=1/12) +\n  # (c) Einfügen der Gruppennamen von t0 und t21:\n  geom_label(stat = \"stratum\", aes(label=after_stat(stratum))) +\n  # (d) Einfügen bzw. Ändern der Benennung der Variablen t0 und 21:\n  scale_x_discrete(limits = c(\"erste Messung\", \"letzte Messung\"),\n  # (e) Verringern der Fläche außerhalb der (äußersten) Strata:\n                   expand = c(.05, .05))\n\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes.\n\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes.\n\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes.\n\n\n\n\n\nUm den Plot (und den Code) besser zu verstehen, werden wir noch einige wichtige Begriffe erläutern:\n\naxis: vertikale Dimension einer Variablen auf der x-Achse (weiße Balken von t0 und t21)\nstratum: Gruppierung von axis (für t0 und t21 jeweils größer und kleiner)\nalluvium: horizontale “Strömungen” im Hintergrund, die die kombinierten Gruppenzugehörigkeiten beschrieben (z.B. größer zur ersten Messung und kleiner zur letzten Messung) und nach einer weiteren Gruppe farblich kodiert sind (Diet)\n\nDie verschiedenen Strata zeigen uns, wie groß die Anteile an Küken sind, die ihr Gewicht gehalten oder verändert haben (jeweils im Vergleich zum Mittelwert zum jeweiligen Zeitpunkt). Wir können auch vergleichen, ob sich die Trends in den Gruppen von Diet unterscheiden z.B. hat Gruppe 3 den größten Zuwachs bekommen (erste Messung: kleiner; letzte Messung: größer).\nMehr Informationen und weitere Beispiele für Alluvial Plots finden wir auch in der Dokumentation des Pakets.\nIm folgenden Abschnitt werden noch weitere Erweiterungen vorgestellt z.B. wie wir einen Titel und andee Achsenbeschriftungen einfügen können."
  },
  {
    "objectID": "Grafiken.html#modifikationen",
    "href": "Grafiken.html#modifikationen",
    "title": "10  Grafiken",
    "section": "10.6 Modifikationen",
    "text": "10.6 Modifikationen\nNachfolgend schauen wir uns einige Möglichkeiten der Modifikation von Grafiken an. Als Beispiele nutzen wir dafür die bisher erstellten Grafiken.\n\nAchtung: Wenn wir in der R-Dokumentation nach ggplot2-specs suchen, finden wir eine Übersicht der ästhetischen Spezifikationen wie z.B. Farben, Linientypen, Punktformen, Schriftarten, Textausrichtung etc.\n\n\n10.6.1 Farbe\nNachfolgend schauen wir an, wie wir bestimmte Elemente bzw. Teile von Elementen einer Grafik farblich hervorheben können und welche Möglichkeiten es gibt (einzelne) Farben und Farbpaletten zu nutzen.\nUm unsere Farbwahl auch für Menschen mit verschiedenen Sehschwächen geeignet zu gestalten, können wir verschiedene Farbwahrnehmungen mit Coblis simulieren.\n\n\n\n\n10.6.1.1 Grundlegendes\nEs gibt zwei Parameter, mit denen wir jeweils festlegen können, ob Elemente farblich umrandet und/oder komplett ausgefüllt werden sollen.\n\ncol, color oder colour: farbliche Umrandung eines Elements\nIn verschiedenen Funktionen ist das Argument zur farblichen Umrandung unterschiedlich benannt (d.h. col, color oder colour). Es gibt aber auch viele Funktionen, in denen alle Parameter funktionieren (und dasselbe bewirken). Genauere Informationen erhalten wir in der Dokumentation der jeweiligen Funktion.\nfill: farbliche Füllung eines Elements\n\nWeil col und fill separate Parameter sind, können wir auch beide gleichzeitig nutzen, um sowohl die Umrandung als auch die Ausfüllung eines Elements (geom) verschieden einzufärben.\n\nAchtung: Allerdings gibt es Elemente (geoms), die davon ausgenommen sind. Bei Linien (z.B. geom_line()) und Text (z.B. geom_text()) können wir nur col nutzen. Bei Punkten (z.B. geom_point()) gibt es einige Formen (21-24, siehe unten), die col und fill nutzen können. Alle anderen Formen können auch nur col nutzen. Die Punktform legen wir mit dem Argument shape fest.\n\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point(shape=23, fill=\"red\", col=\"blue\")\n\n\n\n\n\n\nAbbildung aus der Dokumentation von “ggplot2-specs”\n\n\n\n\n\n\n\nZusäzlich können wir zwei Farbmodi unterscheiden:\n\nstatische Farben\n\ngelten für alle (spezifizierten) Elemente\naußerhalb von aes()\n\nvariable Farben\n\nunterschiedliche Farben für verschiedene Ausprägungen einer Variablen\nes wird automatisch eine Legende erstellt\ninnerhalb von aes()\n\n\n\n\nNachfolgend finden wir eine Übersicht, in der die jeweiligen Unterschiede zwischen col und fill sowie statischen und variablen Farben verdeutlicht werden sollen. Der Code zur Erstellung der jeweiligen Grafik befindet sich über den Abbildungen.\n\n\n\n\n\n\nstatisch\n\n\nVariablenausprägung\n\n\n\n\nfill\n\n\n\n\n\n\n\n\n\n\ncol\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.6.1.2 (Einzelne) Farben\nWir können in R standardmäßig implementierte (“built-in”) Farben sowie Hex-Farben nutzen.\nAuf dieser Seite finden wir eine Übersicht der built-in Farben. Diese können wir einfach mit ihrem Namen auswählen z.B. \"red\" oder \"seagreen1\".\n\nAchtung: Diese built-in Farben sind nicht mit allen Anwendungen in R kompatibel. Beispielsweise können wir \"seagreen\", aber nicht \"seagreen1\" in der Funktion kable() aus dem Paket knitr, welche eine Tabelle erstellt, nutzen.\n\nDarüber hinaus können wir auch Hex-Farben (mit dem Hexadezimal-Zahlensystem kodierte Faben) nutzen, die aus einem # und einer 6-stelligen Zeichenfolge bestehen, z.B. \"#53FD9F\". Dabei werden jeweils zwei Zahlen genutzt um rot, blau und grün zu kodieren (RGB). Beispielsweise können wir hier eigene Farben erstellen und uns den Hex-Code kopieren.\n\n\nGrafische Elemente transparent machen\n\nWir können einzelne Elemente einer Grafik transparent machen. Wenn wir die Grafik exportieren wollen, müssen wir nicht nur in der Erstellung der Grafik sondern auch beim Export angeben, dass ein bestimmtes Element transparent sein soll. Nur Grafiken im PNG-Format können transparent sein.\nFolgendermaßen können wir beispielsweise den Hintergrund der Grafik (plot.background) transparent machen:\n\nggplot(ChickWeight[ChickWeight$Time == 8,], aes(Diet, weight)) + \n  geom_violin(scale = \"count\") +\n  theme(plot.background = element_rect(fill = \"transparent\")) # Erstellung\n\nggsave(\"Grafik_transp.png\", bg = \"transparent\") # Export\n\n\n\n\n\n\n\n\n\n\nUm zu sehen, dass die sonst weiße Hintergrundfläche der Grafik nun transparent ist, müssten wir die Grafik runterladen, weil der Hintegrund hier auch weiß ist.\n\n\n\n10.6.1.3 Farbpaletten\nEs ist zu empfehlen bei der farblichen Kodierung von Variablenausprägungen auf Farbpaletten zurückgreifen, da diese bereits gut durchdachte Farbkombinationen enthalten. Die Idee dahinter ist bei kategorialen Variablen verschiedene Farben und bei metrischen Variablen ähnliche Farben für ähnliche Ausprägungen zu nutzen.\nNeben den voreingestellten Farbpaletten, die wir automatisch nutzen, wenn wir col oder fill innerhalb von aes() spezifizieren, können wir auch weitere Farbpaletten nutzen.\n\nAuf colorbrewer2.org finden wir jeweils angemessene Farbkombinationen zur Kodierung von kategorialen Daten. Teilweise sind diese Farbpaletten bereits in ggplot2 implementiert z.B. in der Funktion scale_colour_brewer() bzw. scale_fill_brewer(), mit der wir Farbanpassungen vornehmen können.\nFür kategoriale Daten ist type=\"qual\" (qualitative) am besten geeignet, da es Paletten mit sehr unterschiedliche Farben nutzt. Es gibt noch \"seq\" (sequential) und \"div\" (diverging), mit denen wir eine Reihenfolge bzw. eine Grenze farblich darstellen können. Für mehr Informationen lohnt es sich auf der Webseite nachzuschauen. Dort sehen wir z.B. auch die verschiedenen Paletten, die wir mit dem Parameter palette ändern können.\n\nggplot(ChickWeight[ChickWeight$Time == 8,], aes(Diet, weight)) + \n  geom_violin(scale = \"count\", aes(fill=Diet)) + \n  scale_fill_brewer(type=\"qual\")\n\n\n\n\n\n\n\n10.6.2 Legenden modifizieren\nWenn wir Farben, Punktformen o.ä. auf Variablen(ausprägungen) anwenden (d.h. diese in aes() spezifizieren), wird automatisch eine Legende erstellt.\nEs gibt verschiedene Aspekte von Legenden, die wir ändern können. Im Folgenden schauen wir uns Text und Positionierung an. Weitere Modifikationen finden wir hier.\n\n10.6.2.1 Text\nWir können sowohl den Titel als auch die einzelnen Elemente einer Legende anders benennen.\nWenn es sich um eine kategoriale Variable handelt, die wir mit fill farblich kodiert haben, können wir die Änderungen mit scale_fill_discrete(name, labels) vornehmen.\nDem Parameter name übergeben wir den Titel der Legende, labels einen Vektor mit den Namen der Bezeichnung der Ausprägungen.\n\nggplot(data=ChickWeight, aes(x=Diet)) +\n  geom_bar(aes(fill = Diet)) +                # fill ...\n  scale_fill_discrete(name = \"Fütterung\",     # ... deswegen _fill_\n                      labels = c(\"Diät 1\", \"Diät 2\", \n                                 \"Diät 3\", \"Diät 4\"))\n\n\n\n\nAnalog dazu nutzen wir scale_color_discrete(), wenn wir das Argument color (zur farblichen Umrandung) genutzt haben.\nWenn wir eine metrische Variable farblich kodiert haben, nutzen wir scale_fill_continuous bzw. scale_colour_continuous ().\n\n\n10.6.2.2 Position\nDie Position der Legende können wir mit theme(legend.position) ändern. Wir können zwischen oben (\"top\"), unten (\"bottom\"), links (\"left\") und rechts (\"right\") wählen. Mit \"none\" können wir die Legende entfernen.\n\nggplot(data=ChickWeight, aes(x=Diet)) + geom_bar() + geom_bar(aes(fill = Diet)) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n10.6.3 Beschriftung\nIm Folgenden schauen wir uns an, wie wir Titel und Achsenbeschriftung, sowie Beschriftungen in der Grafik ändern bzw. hinzufügen können.\n\n10.6.3.1 Überschrift\nUm dem Plot (nur) eine Überschrift zu verpassen nutzen wir ggtitle().\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + \n  ggtitle(\"Gewichtsveränderung über die Zeit\")\n\n\n\n\n\n\n10.6.3.2 Achsenbeschriftung\nWenn wir (nur) die Beschriftung der x- oder y-Achse ändern möchten (z.B. weil diese anders benannt werden sollen als die Variablen im Datensatz), können wir xlab() bzw. ylab() nutzen.\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() +\n  xlab(\"Zeit in Tagen\") + ylab(\"Gewicht in Gramm\")\n\n\n\n\n\n\n10.6.3.3 Überschrift, Achsenbeschriftung und Bildunterschrift\nWir können sowohl eine Überschrift als auch Achsenbeschriftungen ergänzen, indem wir labs(title, x, y) nutzen. Außerdem können wir der Grafik mit caption noch eine Bildunterschrift verpassen.\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + \n  labs(title =\"Gewichtsveränderung über die Zeit\", \n       x = \"Zeit in Tagen\", y = \"Gewicht in Gramm\", caption=\"Weitere Notizen\")\n\n\n\n\n\n\n\n10.6.4 Beschriftung in der Grafik\n\n10.6.4.1 Ausprägung von Variablen\nMit geom_text() (Bsp. 1 und 2) oder geom_label() (Bsp. 3 und 4) können wir Ausprägungen einer Variablen als Text in unsere Grafik einfügen.\n\nAchtung: Allerdings können wir die Funktionen nicht bei Facetten anwenden.\n\nBeide Funktionen machen fast das Gleiche. Optisch unterscheiden sie sich dadurch, dass geom_label() den Text zusätzlich mit einem weißen Feld hinterlegt.\nUnseren Text übergeben wir an aes(label), welches sowohl in ggplot() (Bsp. 1 und 3), als auch in geom_text() bzw. geom_label() (Bsp. 2 und 4) spezifiziert werden kann.\nZusätzlich können wir die Ausrichtung des Texts anpassen. Dazu nutzen wir hjust (horizontal) und vjust (vertikale) (Bsp. 2 und 4), welchen wir einen Wert zwischen 0 und 1 übergeben (Default: hjust=0.5 und vjust=0.5). Wir können auch Werte außerhalb von 0 bis 1 spezifizieren; allerdings wird in der Dokumentation davon abgeraten (ohne weitere Begründung; mehr Informationen dazu in der Dokumentation von ggplot2-specs).\nBeispiel 1 und 2 zeigen die Gewichtsveränderung (x-Achse: Time, y-Achse: weight) über die Zeit von Küken 1 (ChickWeight$Chick == 1). Wir beschriften die Datenpunkte mit den jeweiligen Messzeitpunkten (aes(label=Time)).\n\nggplot(data=ChickWeight[ChickWeight$Chick == 1,], \n       aes(x=Time, y=weight, label=Time)) + \n  geom_point() + \n  geom_text()\n\nggplot(data=ChickWeight[ChickWeight$Chick == 1,], \n       aes(x=Time, y=weight)) + \n  geom_point() + \n  geom_text(aes(label=Time), vjust=1)\n\n\n\n\n\n\n\nBeispiel 3 und 4 zeigen die Gewichtsverteilung der Küken in den Futtergruppen (x-Achse: Diet, y-Achse: weight) zum letzten Messzeitpunkt (Time 21). Wir beschriften die Datenpunkte mit den jeweiligen Nummern der Küken (aes(label=Chick)).\n\nggplot(data=ChickWeight[ChickWeight$Time == 21,], \n       aes(x=Diet, y=weight, label=Chick)) + \n  geom_point() + geom_label()\n\nggplot(data=ChickWeight[ChickWeight$Time == 21,], \n       aes(x=Diet, y=weight)) + \n  geom_point() + geom_label(aes(label=Chick), hjust=1.2)\n\n\n\n\n\n\n\nEs gibt noch weitere Parameter, mit denen wir u.a. das Padding des Textes, die Rundung des Textfeldes, die Schriftgröße oder -farbe ändern können.\nWeitere Hilfe zur Nutzung und Modifikation von geom_text() und geom_label() finden wir hier.\n\n\n10.6.4.2 Beliebiger Text an beliebiger Stelle\nWenn wir einen eigenen Text in eine Grafik schreiben wollen, können wir annotate(geom = \"text\") und annotate(geom = \"label\") nutzen. Analog zu den Funktionen im vorhergehenden Abschnitt sind beide geom-Optionen gleich, außer dass der Text bei \"label\" mit einem weißen Feld hinterlegt wird.\nAnders als bei den Funktionen im vorhergehenden Abschnitt müssen wir eine Position festlegen (weil unser Text nicht zu bestehenden Datenpunkten gematcht wird). Dafür können wir x und y nutzen. Diese entsprechen den Maßen der Einheiten der jeweiligen Achsen.\nBeispiel 1 zeigt die Gewichtsveränderung (x-Achse: Time, y-Achse: weight) über die Zeit von Küken 1 (ChickWeight$Chick == 1).\nBeispiel 2 zeigt die Gewichtsverteilung der Küken in den Futtergruppen (x-Achse: Diet, y-Achse: weight) zum letzten Messzeitpunkt (Time 21).\n\nggplot(data=ChickWeight[ChickWeight$Chick == 1,], \n       aes(x=Time, y=weight)) + \n  geom_point() + \n  annotate(geom = \"text\", label=\"Küken 1\", x=5, y=200)\n\nggplot(data=ChickWeight[ChickWeight$Time == 21,], \n       aes(x=Diet, y=weight)) + \n  geom_point() + \n  annotate(geom=\"label\", label=\"Messzeitpunkt 21\", x=1.7, y=370)\n\n\n\n\n\n\n\n\n\n\nWir können die Darstellung des Textes (wie im vorhergehenden Abschnitt) auch anpassen. Mehr Informationen dazu in der Dokumentation von annotate()."
  },
  {
    "objectID": "Grafiken.html#kategoriale-variablen-benennung-und-reihenfolge-der-ausprägungen",
    "href": "Grafiken.html#kategoriale-variablen-benennung-und-reihenfolge-der-ausprägungen",
    "title": "10  Grafiken",
    "section": "10.7 Kategoriale Variablen: Benennung und Reihenfolge der Ausprägungen",
    "text": "10.7 Kategoriale Variablen: Benennung und Reihenfolge der Ausprägungen\n\n10.7.1 Benennung\nDer Einfachheit halber nutzen wir zur Kodierung von Gruppen häufig Zahlen, wie z.B. bei Diet: 1, 2, 3, 4. In einer Grafik hingegen ist es aber sinnvoller, Begriffe zu nutzen.\nMit levels() können wir uns nicht nur die verschiedenen Stufen eines Faktors ausgeben lassen. Wir können die Funktion außerdem nutzen, um die Benennung der Stufen zu ändern.\nIm Folgenden ändern wir die Benennungen der Gruppen von Diet.\n\nChickWeight$Diet_name <- ChickWeight$Diet # Kopie von `Diet` als neue Spalte\nlevels(ChickWeight$Diet_name) <-  c(\"Diät 1\", \"Diät 2\", \n                                    \"Diät 3\", \"Diät 4\")\n\nggplot(data=ChickWeight, aes(x=Diet_name, y=weight)) + geom_boxplot()\n\n\n\n\n\n\n10.7.2 Reihenfolge\nDie Reihenfolge von Faktorstufen ist bei Wortketten standardmäßig alphabetisch und bei Zahlen aufsteigend. Wenn wir das ändern möchten, müssen wir die Sortierung des Faktors ändern (dadurch ändert sich nicht die Sortierung der Variablen im Datensatz).\nDie Sortierung des Faktors können wir mit der Funktion factor() ändern. Dem Parameter x übergeben wir die Variable; levels einen character-Vektor mit der neuen Sortierung.\nWir ändern die Reihenfolge der Gruppen von Diet.\n\n# Kopie von `Diet` erstellen (optional):\nChickWeight$Diet_sort <- ChickWeight$Diet \n# Änderung der Reihenfolge:\nChickWeight$Diet_sort <- factor(x=ChickWeight$Diet_sort, levels=c(\"3\", \"4\", \"2\", \"1\"))\n\n\nggplot(data=ChickWeight, aes(x=Diet, y=weight)) +\n  geom_boxplot() + \n  ggtitle(\"unbearbeitet\")\n\nggplot(data=ChickWeight, aes(x=Diet_sort, y=weight)) +\n  geom_boxplot() + \n  ggtitle(\"bearbeitet\")"
  },
  {
    "objectID": "Grafiken.html#grafiken-einzelner-gruppen-und-die-anpassung-von-achsengrenzen",
    "href": "Grafiken.html#grafiken-einzelner-gruppen-und-die-anpassung-von-achsengrenzen",
    "title": "10  Grafiken",
    "section": "10.8 Grafiken einzelner Gruppen und die Anpassung von Achsengrenzen",
    "text": "10.8 Grafiken einzelner Gruppen und die Anpassung von Achsengrenzen\nManchmal ist es sinnvoll, die Achsengrenzen von x- und y-manuell zu ändern, beispielsweise wenn wir Grafiken einzelner Gruppen vergleichen möchten.\nIm Abschnitt zu Facetten haben wir gelernt, wie wir einzelne Grafiken für jede Ausprägung einer kategorialen Variablen (bzw. für jede Kombination von Ausprägungen mehrerer kategorialer Veriablen) erstellen können. Diese werden jedoch kleiner dargestellt, je mehr Gruppierungen es gibt. Wenn wir ohnehin nur spezifische Gruppen vergleich wollen, können wir für diese manuell Grafiken erstellen.\nWenn wir uns für die Gewichtszunahme (weight) in den Gruppen (Diet) zu Beginn (Time 0) und zum Ende (Time 21) des Experiments interessieren, dann können manuell beide Grafiken erstellen.\n\nHier wird das Indexieren der Daten erklärt.\n\n\nggplot(data=ChickWeight[ChickWeight$Time == 0,], aes(x=Diet, y=weight)) +\n  geom_point()\nggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDas Problem hierbei ist, dass sich die Streubreiten von weight, und damit die Achsengrenzen von y, zu beiden Zeitpunkten stark unterscheiden. Dadurch könnten wir einen falschen Eindruck von den Daten bekommen.\nUm das zu verhindern, können wir die Achsengrenzen anpassen. Das machen wir mit xlim() und ylim(). Diesen übergeben wir jeweils einen numerischen Vektor, der den unteren und den oberen Grenzwert der Achse enthält.\n\nggplot(data=ChickWeight[ChickWeight$Time == 0,], aes(x=Diet, y=weight)) +\n  geom_point() + ylim(c(0,375))\n\nggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) +\n  geom_point() + ylim(c(0,375))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBei genauer Betrachtung fällt weiterhin auf, dass unsere beiden y-Achsen nicht bei 0 beginnen, obwohl wir das mit ylim(c(0,...)) scheinbar so festgelegt haben.\nStandardmäßig werden Achsen (kontinuierlicher Variablen) etwas erweitert. Dieses Verhalten können wir z.B. mit scale_y_continuous(expand = c(0, 0)) auf der y-Achse (und scale_x_coninuous() auf der x-Achse) ändern.\n\nAchtung: Etwaige Änderungen der Achsengrenzen müssen wir dann aber auch in scale_y_continuous(limits) machen.\n\n\nggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) +\n  geom_point() + scale_y_continuous(limits=c(0,375), expand = c(0, 0))"
  },
  {
    "objectID": "Grafiken.html#geraden-einzeichnen",
    "href": "Grafiken.html#geraden-einzeichnen",
    "title": "10  Grafiken",
    "section": "10.9 Geraden einzeichnen",
    "text": "10.9 Geraden einzeichnen\nWir können auch Geraden einzeichnen (z.B. alternativ zu Achsengrenzen oder um manuell Regressionsgeraden einzuzeichnen).\nVertikale Gerade können wir mit geom_vline(xintercept) einfügen; horizontale mit geom_hline(yintercept).\nDabei können wir den Parametern xintercept und yintercept auch mehrere Werte übergeben. Dazu packen wir diese in einen Vektor mit c().\nWir schauen uns dazu eines der Beispiele aus dem Abschnitt vorher an: die Gewichtszunahme (weight) in den Gruppen (Diet) zum Ende (Time 21) des Experiments. Wir zeichnen jeweils den kleinsten und größten Wert, sowie den Mittelwert (über alle Diet-Gruppen) von weight als (horizontale) Gerade ein.\n\nHier wird das Indexieren der Daten erklärt.\n\n\n# Maxima von weight (zu Time 21) herausfinden:\nmin <- min(ChickWeight$weight, na.rm=TRUE)\nmax <- min(ChickWeight$weight, na.rm=TRUE)\n\nggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + \n  geom_point() + \n  # Min & Max:\n  geom_hline(yintercept=c(min, max)) +\n  # Mittelwert:\n  geom_hline(yintercept=mean(ChickWeight$weight[ChickWeight$Time == 21]), col=\"red\")\n\n\n\n\nSo können wir die Verteilungen von weight in den Diet-Gruppen zu den zwei Zeitpunkten besser vergleichen. Wir können nicht nur sehen, dass die Verteilung an Tag 21 wesentlich breiter gestreut ist als an Tag 0 (weil dadurch die Achsengrenzen angepasst werden), sondern auch …"
  },
  {
    "objectID": "Grafiken.html#weitere-statistische-kennwerte-ergänzen",
    "href": "Grafiken.html#weitere-statistische-kennwerte-ergänzen",
    "title": "10  Grafiken",
    "section": "10.10 Weitere statistische Kennwerte ergänzen",
    "text": "10.10 Weitere statistische Kennwerte ergänzen\nManchmal möchten wir in einer bestehende Grafik noch zusätzliche statistische Kennwerte einfügen. Dazu können wir stat_summary() nutzen. Damit können wir verschiedenen Kennwerte von y (z.B. den Mittelwert, den Median, oder den minimalen und maximalen Wert) in eine Grafik einzeichnen.\nAls Beispiel schauen wir uns ein Streudiagramm des Gewichts der Küken (weight) in Abhängigkeit ihrer Diät (Diet) am 10. Tag (Time 10) an.\nPer Default wird mit stat_summary() (ohne Spezifikation von Argumenten) der Mittelwert (als Punkt), umschlossen vom Standardfehler des Mittelwerts (als Linie), eingezeichnet.\nWir können dem Parameter fun aber auch die Argumente \"median\" oder \"mean\" übergeben. Letzteres zeichnet ebenfalls den Mittelwert ein (aber ohne Standardfehler). Wenn wir die Extrema einfügen wollen, nutzen wir fun.min = \"min\" und fun.max = \"max\".\n\nggplot(data=ChickWeight[ChickWeight$Time == 10,], aes(x=Diet, y=weight)) + \n  geom_point() + stat_summary(col=\"blue\") + # Mittelwert + SE: blau\n  stat_summary(fun = \"median\", col=\"red\", pch=3) # Median: rotes Kreuz\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\nWarning: Removed 4 rows containing missing values (`geom_segment()`).\n\n\n\n\n\n\nggplot(data=ChickWeight[ChickWeight$Time == 10,], aes(x=Diet, y=weight)) + \n  stat_summary(fun = \"mean\", fun.min = \"min\", fun.max = \"max\")\n\n\n\n\nMehr Informationen zur Funktion finden wir in der R-Dokumentation von stat_summary() bzw. eine ausführlichere Version davon mit Beispielabbildungen hier."
  },
  {
    "objectID": "Grafiken.html#motive-themes",
    "href": "Grafiken.html#motive-themes",
    "title": "10  Grafiken",
    "section": "10.11 Motive (Themes)",
    "text": "10.11 Motive (Themes)\nEs gibt einige Optionen, den Hintergrund des Koordinatensystems zu verändern.Standarmäßig haben wir einen grauen Hintergrund mit weißen Rasterlinien; das entspricht der Einstellung theme_grey().\nWenn wir beispielsweise einen weißen Hintergrund mit grauen Rasterlinien haben möchten, nutzen wir dafür theme_minimal().\n\nggplot(data=ChickWeight, aes(x=Time, y=weight)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWir können unsere Palette möglicher Motiven mit dem Paket ggthemes erweitern. In diesem finden wir u.a. die Funktion theme_tufte(), mit der Grafiken nach dem Vorbild von Edward Tufte auf das Notwendige reduziert werden (z.B. weißer Hintergrund, keine unnötigen Rasterlinien eingezeichnet etc.)."
  },
  {
    "objectID": "Grafiken.html#grafiken-exportieren",
    "href": "Grafiken.html#grafiken-exportieren",
    "title": "10  Grafiken",
    "section": "10.12 Grafiken exportieren",
    "text": "10.12 Grafiken exportieren\nWenn wir unsere Grafik erstellt haben, müssen wir diese noch exportieren, um sie außerhalb von R nutzen zu können.\nGrundsätzlich gibt es zwei Möglichkeiten dafür:\n\nüber das untere rechte Panel bei Plots > Export (nur in RStudio möglich)\nmit Hilfe von Funktionen\n\nIm Folgenden schauen wir uns an, wie wir den Export-Button und die Funktion ggsave(), mit der wir mit ggplot2 erstellte Grafiken exportieren können, nutzen.\nEine Übersicht über das Exportieren von Grafiken mit Funktionen aus dem Basispaket grDevices, finden wir hier. Mit diesen Funktionen können wir Grafiken, die mit einem beliebigen R-Paket erstellt wurden, exportieren.\n\nAchtung: Wenn wir mehrere Garfiken exportieren wollen, müssen wir darauf achten, dass wir diesen unterschiedliche Namen zuweisen. Sowohl innerhalb von R als auch beim Export werden bestehende Objekte mit dem gleichen Namen überschrieben.\n\n\nAchtung: Hinweis für Windows-NutzerInnen: Manchmal kann es zu Probleme mit Aliasing kommen. Das ist, wenn Grafikelemente und/oder Text nach dem Export “krisselig” dargestellt werden.\n\nBeispiel für Aliasing\n\n\n\n\n\n\n\n\n\nBildquelle: https://www.r-bloggers.com/exporting-nice-plots-in-r/\nUm das zu verhindern, können wir das Paket Cairo, mit dessen gleichnamiger Funktion, nutzen. Mehr Informationen zur Anwendung können wir auf R-bloggers finden.\n\n10.12.1 Export-Button\nWenn wir die Grafik mittels der grafischen Benutzeroberfläche exportieren möchten, klicken wir auf Export im unteren rechten Panel. Wenn wir diese als Bild exportieren wollen, wählen wir Save as Image… aus.\n\n\n\n\n\n\n\n\n\n\n\n\nWenn wir mehrere Grafiken geplottet haben können wir mit den Pfeiltasten (ganz links unter dem Reiter Files) auswählen, welche Grafik wir exportieren wollen.\n\nDaraufhin öffnet sich ein neues Fenster, in dem wir einige Einstellungen tätigen können.\n\n\n\n\n\n\n\n\n\nBildformat\nHier können wir wählen, in welchem Bildformat wir unsere Grafik abspeichern wollen. Standardmäßig wird sie als PNG gespeichert. Weitere Optionen sind JPEG, TIFF, BMP, SVG und EPS.\nVerzeichnis\nWenn wir auf den Button Directory… klicken, können wir festlegen, wo die Grafik gespeichert werden soll. Standardmäßig wird sie in der obersten Ordnerstruktur, bzw. wenn wir ein Workind Directory gesetzt haben in diesem, gespeichert.\nName der Datei\nHier können wir unserer Datei einen Namen geben. Standardmäßig wird als Name Rplot vorgeschlagen, wenn wir in dem aktuellen Verzeichnis keine andere Datei (im gleichen Format) besitzen, die den gleichen Namen trägt. Wenn es bereits eine solche Datei gibt, wird als Name für die neue Datei automatisch Rplot01 (bzw. Rplot02 usw.) vorgeschlagen.\nGröße bearbeiten\nFür die Bearbeitung der Größe der Grafik haben wir zwei Möglichkeiten:\n\nAngeben der Maße für Breite und Höhe\nGroßziehen der Grafik\n\nBei beiden Möglichkeiten können wir entweder willkürlich die Maße anpassen oder das Verhältnis zwischen Breite und Höhe beibehalten indem wir ein Häkchen bei Maintain aspect ratio setzen.\n\nHier geben wir jeweils die Maße für Breite und Höhe in px (Pixel) an. Wenn wir das Häkchen bei Maintain aspect ratio gesetzt haben, müssen wir nur ein Maß angeben; das andere wird automatisch berechnet. Danach können wir auf Update Preview klicken, um uns eine Vorschau der Grafik anzuschauen (aber das geht nur, solange die Maße nicht zu groß für die Vorschau sind).\n\n\nDie Standardmaße 398 x 375 px entsprechen 14,04 x 13,23 cm (B x H).\n\n\nAnalog können wir die Größe der Grafik auch mit der Maus verändern. Dazu ziehen wir einfach an der unteren rechten Ecke der Grafikvorschau.\n\nWenn wir eine gewisse Größe in cm haben wollen, können wir die px-Angaben z.B. auf dieser Seite umrechnen lassen.\n\nAchtung: Die Auflösung der Grafik können wir in dem Export-Fenster nicht ändern. Standardmäßig beträgt diese 72 dpi.\n\n\n\n10.12.2 ggsave()\nMit ggplot2 erstellte Grafiken können wir mit ggsave() exportieren. Schauen wir uns einmal an, was wir in der Funktion spezifizieren können.\n\nggsave(\n  filename = \"Grafik.png\", # Name und Bildformat der zu exportierenden Grafik\n  path = \"/Users/...\",\n  plot = last_plot(), # letzter Plot als Default oder alternativ Name der Grafik in R\n  width = 12, # Breite\n  height = 9, # Höhe\n  units = \"cm\", # in welcher Einheit angegeben\n  dpi = 72 # Default; äquivalent zu: dpi = \"screen\"\n)\n\nDas gewünschte Bildformat können wir einfach als Endung an den Namen der Grafik anhängen. Beides übergeben wir in \" \" an das Argument filename. Wir haben hier mehr Auswahl an möglichen Outputformaten. Es gibt z.B. PNG, PDF, JPEG, TIFF, EPS, PS, TEX und SVG.\nWir legen mit dem Argument plot fest, welche Grafik wird exportieren wollen. Das funktioniert nur wenn wir diese vorher in einer Variablen gespeichert haben (z.B. name <- ggplot(...)). Dann erscheint der Name auch im Global Environment (oberer rechtes Panel in R). Standarmäßig wird der zuletzt erstellte Plot exportiert, wenn wir das Argument plot weglassen (oder plot=last_plot() eingeben).\nDas Verzeichnis, in dem die Grafik abgelegt werden soll, können wir mit dem Argument path festlegen. Diesem übergeben wir den Ordnerpfad, welchen wir auch in \" \" setzen müssen. Standardmäßig wird die Grafik in der obersten Ordnersturktur bzw. im derzeitigen Working Directory (wenn eins gesetzt wurde) abgelegt.\nDie Größe der zu exportierenden Grafik können wir mit width und height ändern. Mit units legen wir dabei fest, in welcher Einheit die Maße sein sollen. Wenn wir width, height und units weglassen, wird die Größe des current graphics device genutzt. Diese ist von unseren jeweiligen globalen Computer-Einstellungen abhängig.\n\nManchmal müssen wir mit den Maßen ein bisschen herumspielen, bis wir die perfekte Größe gefunden haben.\n\nIm Gegensatz zur Möglichkeiten des Exportierens via Export-Button können wir in ggsave() mit dem Argument dpi die Bildauflösung verändern. Zusäzlich zu der Standardeinstellung von 72 dpi (screen), können wir 300 dpi (print) und 320 dpi (retina) nutzen. Wir können sowohl die Zahl, als auch die Bezeichnung in \" \" angeben."
  },
  {
    "objectID": "Grafiken.html#weiterführende-hilfen",
    "href": "Grafiken.html#weiterführende-hilfen",
    "title": "10  Grafiken",
    "section": "10.13 Weiterführende Hilfen",
    "text": "10.13 Weiterführende Hilfen\n\n10.13.1 Eine einfache ggplot-Funktion: qplot()\nEs gibt im Paket ggplot2 auch eine einfacher handzuhabende Variante zu ggplot() - das ist qplot() (“quick plot”). Diese ist an den Aufbau der base graphics Funktion plot() angelehnt. Mit ihr können wir verschiedenen Grafiken erstellen, aber sie ist weniger flexibel und modifizierbar als ggplot().\nWir erstellen exemplarisch ein Streudiagramm vom Gewicht der Küken (weight) in Abhängigkeit der Zeit (Time) und ergänzen eine Überschrift sowie Achsenbeschriftungen.\n\nqplot(x=Time, y=weight, data=ChickWeight, geom=\"point\", \n      main=\"Gewichtsveränderung über die Zeit\",\n      xlab=\"Zeit in Tagen\",\n      ylab=\"Gewicht in Gramm\")\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nDas gleiche Beispiel haben wir auch im Abschnitt Beschriftung.\n\nFür mehr Informationen zu qplot() (vor allem zu den möglichen Argumenten) können wir hier nachschauen.\n\n\n10.13.2 Mehrere Plots zusammenführen\nWenn wir mehrere Grafiken haben, die wir in einer gemeinsamen Datei speichern wollen, dann müssen wir dafür auf zusätzliche Pakete zurückgreifen. Diese Seite gibt einen guten Überblick darüber, welche Funktionen wir dafür nutzen können. Am Ende der Seite unter Alternative options befindet sich eine kleine Tabelle, in der die Pakete mit ihren Funktionen hinsichtlich ihrer ggsave()-Kompatibilität und Möglichkeit zur Anordnung der Plots verglichen werden.\n\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggalluvial_0.12.5  ggiraphExtra_0.3.0 lsr_0.5.2          knitr_1.42        \n[5] kableExtra_1.3.4   dplyr_1.1.2        ggplot2_3.4.2     \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3       mycor_0.1.1        xfun_0.39          htmlwidgets_1.6.2 \n [5] insight_0.19.1     lattice_0.21-8     vctrs_0.6.2        tools_4.3.0       \n [9] generics_0.1.3     tibble_3.2.1       fansi_1.0.4        cluster_2.1.4     \n[13] pkgconfig_2.0.3    Matrix_1.5-4       data.table_1.14.8  checkmate_2.1.0   \n[17] RColorBrewer_1.1-3 uuid_1.1-0         webshot_0.5.4      lifecycle_1.0.3   \n[21] compiler_4.3.0     farver_2.1.1       stringr_1.5.0      sjmisc_2.8.9      \n[25] munsell_0.5.0      htmltools_0.5.5    yaml_2.3.7         htmlTable_2.4.1   \n[29] Formula_1.2-5      tidyr_1.3.0        pillar_1.9.0       ellipsis_0.3.2    \n[33] MASS_7.3-58.4      Hmisc_5.0-1        rpart_4.1.19       nlme_3.1-162      \n[37] sjlabelled_1.2.0   tidyselect_1.2.0   rvest_1.0.3        digest_0.6.31     \n[41] stringi_1.7.12     reshape2_1.4.4     purrr_1.0.1        labeling_0.4.2    \n[45] splines_4.3.0      fastmap_1.1.1      grid_4.3.0         colorspace_2.1-0  \n[49] cli_3.6.1          magrittr_2.0.3     base64enc_0.1-3    utf8_1.2.3        \n[53] foreign_0.8-84     withr_2.5.0        scales_1.2.1       backports_1.4.1   \n[57] rmarkdown_2.21     httr_1.4.5         nnet_7.3-18        gridExtra_2.3     \n[61] evaluate_0.20      viridisLite_0.4.1  mgcv_1.8-42        rlang_1.1.0       \n[65] ggiraph_0.8.7      Rcpp_1.0.10        glue_1.6.2         ppcor_1.1         \n[69] xml2_1.3.3         svglite_2.1.1      rstudioapi_0.14    jsonlite_1.8.4    \n[73] plyr_1.8.8         R6_2.5.1           systemfonts_1.0.4 \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Wide-and-Long-Format.html#vom-long--ins-wide-format",
    "href": "Wide-and-Long-Format.html#vom-long--ins-wide-format",
    "title": "11  Wide- & Long-Format",
    "section": "11.1 Vom Long- ins Wide-Format",
    "text": "11.1 Vom Long- ins Wide-Format\nHierbei wird eine messwiederholte Variable (weight) in Abhängigkeit der Ausprägungen der Zeitvariable (Time) in mehrere Spalten aufgeteilt.\nIm Wide-Format gibt es mehr Spalten und weniger Zeilen (als im Long-Format). Die Anzahl der Zeilen entspricht der Anzahl der Untersuchungseinheiten.\n\nIn unserem Datensatz sind die Untersuchungseinheiten Küken.\n\nSowohl reshape() als auch spread() benötigen zur Formatierung von Long nach Wide eine ID-Variable im Datensatz (auch wenn man diese bei spread() nicht an ein Argument übergeben muss). Eine ID-Variable bezeichnet die Untersuchungsobjekte. Im Eingangsbeispiel wäre dies die Variable Untersuchungseinheit.\n\n11.1.1 stats: reshape()\nMit dem reshape()-Befehl kann man Daten sowohl vom Long- ins Wide-Format als auch vom Wide- ins Long-Format bringen.\nBei der Formatierung von Long in Wide sind folgende 5 Argumente wichtig:\n\ndata: Name des Datensatzes\nv.names: messwiederholte Variable, die im Long-Format in einer Spalte vorliegt und im Wide-Format auf mehrere Spalten ausgedehnt werden soll\ntimevar: Zeitvariable, die im Long-Format die Modi kodiert\nidvar: ID-Variable, die unterschiedliche Untersuchungseinheiten kodiert\ndirection: vom Long- ins Wide-Format\n\nZusätzlich kann man mit drop=\"Variable\" eine bzw. mit drop=c(\"Variable_1\", \"Variable_2\", ..., \"Variable_x\") mehrere Variablen aus dem umformatierten Datensatz entfernen.\n\nreshape_wide <- reshape(data=ChickWeight, # Name des Datensatzes\n                    v.names=\"weight\", \n                      # messwiederholte Variable (in einer Spalte)\n                    timevar=\"Time\", \n                      # (bestehende) Zeitvariable\n                    idvar=\"Chick\", \n                      # (bestehende) ID-Variable\n                    direction=\"wide\") # vom Long- ins Wide-Format\n\n\n\n\n\n  \n\n\n\n\nAchtung:  Ganz links sehen wir die “alte” Zeilennummerierung von ChickWeight. Die Zeilennummerierung ändert sich in reshape_wide, weil alle Beobachtungen einer Untersuchungseinheit zu unterschiedlichen Messzeitpunkten in einer Zeile vorliegen. Das erkennt man auch daran, dass die Zeilennummerierung von ChickWeight (meistens) in 12-er Schritten vorliegt und es genau 12 Messzeitpunkte gibt. Da bei einigen Küken weniger als 12 Messzeitpunkte vorhanden sind, gilt das aber nicht für den ganzen Datensatz reshape_wide.\n\n\n\n11.1.2 tidyr: spread()\nHier sind nur drei Argumente wichtig:\n\ndata: Name des Datensatzes\nkey: Zeitvariable, die im Long-Format unterschiedliche Modi kodiert\nvalue: messwiederholte Variable, die im Long-Format in einer Spalte vorliegt und im Wide-Format auf mehrere Spalten ausgedehnt werden soll\n\n\nlibrary(tidyr) # Laden des Pakets\n\nspread_wide <-  spread(data=ChickWeight, # Name des Datensatzes\n                  key=\"Time\", #  Zeitvariable\n                  value=\"weight\") # messwiederholte Variable\n\n\n\n\n\n  \n\n\n\n\n\n11.1.3 Unterschied zwischen reshape() und spread()\nBei reshape() werden Zeilennummerierungen auf Basis des übergebenen Datensatzes erzeugt (siehe Hinweis im reshape()-Abschnitt).\nWide-Format: Argumente von reshape() und spread() gegenübergestellt\n\n\n\n\n \n  \n    reshape \n    spread \n  \n \n\n  \n    data \n    data \n  \n  \n    v.names \n    value \n  \n  \n    timevar \n    key \n  \n  \n    idvar \n    *kein explizites Argument,\naber benötigt* \n  \n\n\n\n\n\n\nWährend wir in reshape() mit dem Parameter direction fetslegen, welches Format wir erstellen wollen, nutzen wir in tidyr entweder spread() oder gather().\n\nDie Benennung der Spalten der messwiederholten Variable unterscheidet sich in den beiden Ansätzen. Während bei reshape() eine Kombination aus dem Namen der messwiederholten Variablen und der Ausprägung der Zeitvariablen genutzt wird (z.B. weight.0), wird bei spread() nur die Ausprägung der Zeitvariablen genutzt (z.B. 0)."
  },
  {
    "objectID": "Wide-and-Long-Format.html#vom-wide--ins-long-format",
    "href": "Wide-and-Long-Format.html#vom-wide--ins-long-format",
    "title": "11  Wide- & Long-Format",
    "section": "11.2 Vom Wide- ins Long-Format",
    "text": "11.2 Vom Wide- ins Long-Format\nHierbei wird eine messwiederholte Variable (weight), die in mehreren Spalten vorliegt, zu einer Spalte zusammengefasst und es wird eine dazugehörige Zeitvariable (Time) erstellt.\nIm Long-Format gibt es weniger Spalten und mehr Zeilen (als im Wide-Format). Die Anzahl der Zeilen entspricht nicht der Anzahl der Untersuchungseinheiten, sondern der Anzahl der Untersuchungseinheiten x Anzahl der (jeweiligen) Messwiederholungen. Deswegen ist es im Long-Format besonders wichtig, eine ID-Variable zu haben, um einzelne Untersuchungseinheiten differenzieren zu können.\n\nZur Erinnerung: In unserem Beispiel sind die Untersuchungseinheiten Küken.\n\n\nAchtung: Manchmal werden nicht alle Untersuchungseinheiten zu jedem Zeitpunkt untersucht. In unserem Datensatz ChickWeight ist das der Fall.\n\nWir können uns folgendermaßen anschauen, wie häufig jede Untersuchungseinheit beobachtet wurde:\n\ntable(ChickWeight$Chick)\n\n\n18 16 15 13  9 20 10  8 17 19  4  6 11  3  1 12  2  5 14  7 24 30 22 23 27 28 \n 2  7  8 12 12 12 12 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 \n26 25 29 21 33 37 36 31 39 38 32 40 34 35 44 45 43 41 47 49 46 50 42 48 \n12 12 12 12 12 12 12 12 12 12 12 12 12 12 10 12 12 12 12 12 12 12 12 12 \n\n\n\nDie ungeraden Zeilen (1,3, und 5) zeigen die ID-Variable Chick; die geraden (2,4, und 6) die Häufigkeit der Messung.\nKüken 18, 16, 15 und 44 wurden weniger als 12 mal untersucht.\n\nDeswegen gibt es auch einen Unterschied zwischen der Anzahl der Zeilen im ursprünglichen Long-Format von ChickWeight und den nachfolgend erstellten Long-Formaten des Datensatzes. Im ursprünglichen Long-Format ChickWeight werden die Zeilen der Küken, die zu bestimmten Messzeitpunkten nicht beobachtet wurden, weggelassen. Mit reshape() und gather() werden diese Zeilen erstellt und die Beobachtung der messwiederholten Variable mit NA (= fehlender Wert) versehen.\n\n11.2.1 stats: reshape()\nBei der Formatierung von Wide in Long sind folgende 6 Argumente wichtig:\n\ndata: Name des Datensatzes\nvarying: messwiederholte Variable, die im Wide-Format auf mehrere Spalten ausgedehnt ist und im Long-Format in einer Spalte vorliegen soll\ntimevar: Zeitvariable, die unterschiedliche Modi kodiert, und die neu im Long-Format erstellt werden soll\ntimes: Ausprägungen der Zeitvariable, d.h. die Kodierung der Modi\nidvar: ID-Variable, die unterschiedliche Untersuchungseinheiten kodiert\ndirection: vom Wide- ins Long-Format\n\nMit v.names kann man die messwiederholte Variable, welche im Wide-Format in mehreren Spalten vorliegt und nun im Long-Format zu einer Spalte zusammengefasst wird, umbenennen. Wenn alle Spaltennamen den gleichen Stamm haben (bei uns weight.0, weight.2, …), dann wird dieser als Name der zusammengefassten Variable genutzt. Wenn die Spalten unterschiedliche Namen haben, sollte man hier einen gemeinsamen Namen angeben.\nAuch hier kann man einzelne Variablen mit drop=\"Variable\" bzw. mehrere Variablen mit drop=c(\"Variable_1\", \"Variable_2\", ..., \"Variable_x\") aus dem umformatierten Datensatz entfernen.\n\nreshape_long <- reshape(data=reshape_wide, # Name des Datensatzes\n                    varying=3:14, \n                      # messwiederholte Variable (mehrere Spalten) \n                    timevar=\"Time\", \n                      # (neu erstellte) Zeitvariable \n                    times=c(seq(0, 20, 2), 21), \n                      # Ausprägungen von timevar (Messzeitpunkte)\n                    idvar=\"Chick\", \n                      # (bestehende) ID-Variable\n                    direction=\"long\") # vom Wide- ins Long-Format\n\n\n\n\n\n  \n\n\n\n\nAchtung:  Ganz links sehen wir die neue Zeilennummerierung von reshape_long. Die Zeilennummerierung hat hier Nachkommastellen (z.B. 1.0), weil hier ID-Variable und Zeitvariable kombiniert wurden. Die Zahl vor dem Punkt steht für die Untersuchungseinheit (Chick); die Zahl nach dem Komma für den Messzeitpunkt (Time).\n\n\n\n\nWas macht varying=3:14?\n\nMan kann bestehende Variablen auch mit ihren Indizes (d.h. hier: ihren Spaltennamen) ansprechen. Das ist häufig weniger schreibintensiv, als alle Spaltennamen in einem Vektor zu speichern z.B. varying=c(\"weight.0\", \"weight.2\", ..., \"weight.21\"). So könnte man z.B. auch idvar=1 anstatt idvar=\"Chick\" schreiben.\nMit den Variablen, die man im Rahmen der Umformatierung erst neu erstellt (z.B. timevar), geht das aber nicht, da diese noch gar nicht existieren und von daher auch keine Indizes haben.\n\n\n\nWie können wir verschiedene messwiederholte Variablen jeweils zusammenfassen? bzw. Wie nutzen wir varying=list() und v.names?\n\nWenn man mehrere Variablen zu den verschiedenen Messzeitpunkten erhoben hat, kann man diese mit varying=list() vom Wide- ins Long-Format bringen. Dabei muss man darauf achten, dass alle messwiederholten Variablen die gleiche Anzahl an Spalten besitzen (d.h. alle zu jedem Messzeitpunkt erhoben wurden).\nFür unseren Datensatz generieren wir ein fiktives Beispiel, in dem wir die 12 messwiederholten weight-Spalten duplizieren und dann jeweils zu einer Variablen zusammenfügen. Deswegen schreiben wir varying=list(3:14, 15:26) in reshape().\nZusätzlich kann man die messwiederholten Variablen mit v.names=c(\"...\", \"...\") umbenennen. Das funktioniert auch für einzelne messwiederholte Variablen: Dann benötigt man keinen Namensvektor c(\"...\", \"...\") mehr, sondern nur v.names=\"...\".\nWir nennen die zwei fiktiven Variablen in unserem Beispiel AV_1 und AV_2.\n\n# messwiderholte Variablen duplizieren:\nreshape_dup <- cbind(reshape_wide, reshape_wide[3:14])\n\nreshape_dup_long <- reshape(data=reshape_dup,\n                    varying=list(3:14, 15:26), \n                      # zwei messwiederholte Variable ...\n                      # ... mit jeweils mehreren Spalten\n                    v.names=c(\"AV_1\", \"AV_2\"),\n                      # Benennung der zwei messwiederholten Variablen\n                      # optional\n                    timevar=\"Time\", \n                    times=c(seq(0, 20, 2), 21),\n                      # Ausprägungen von timevar (Messzeitpunkte)\n                    idvar=\"Chick\", \n                    direction=\"long\")\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n11.2.2 tidyr: gather()\nHierfür benötigt man 5 Argumente:\n\ndata: Name des Datensatzes\nkey: Zeitvariable, die unterschiedliche Modi kodiert und die neu im Long-Format erstellt werden soll\nvalue: Benennung der neu zu erstellenden messwiederholten Variable\n...: messwiederholte Variable, die im Wide-Format auf mehrere Spalten ausgedehnt ist und im Long-Format in einer Spalte vorliegen soll\nfactor_key: ob die neu erstellte Zeitvariable als Faktor (TRUE) oder als Character (FALSE; Default) gehandhabt werden soll\n\n\nlibrary(tidyr) # Laden des Pakets\n\ngather_long <- gather(data=spread_wide, # Name des Datensatzes\n                  key=\"Time\", # (neu erstellte) Zeitvariable \n                  value=\"weight\", # (neu erstellte) messwiederholte Variable \n                  3:14, # messwiederholte Variable (mehrere Spalten) \n                  factor_key=TRUE) # ob die Zeitvariable ein Faktor sein soll\n\n\n\n\n\n  \n\n\n\n\n\n11.2.3 Unterschied zwischen reshape() und gather()\nBei reshape() werden Zeilennummerierungen auf Basis des übergebenen Datensatzes erzeugt (siehe Hinweis im reshape()-Abschnitt).\nLong-Format: Argumente von reshape() und gather() gegenübergestellt\n\n\n\n\n \n  \n    reshape \n    gather \n  \n \n\n  \n    data \n    data \n  \n  \n    varying \n    ... \n  \n  \n    timevar \n    key \n  \n  \n    times \n     \n  \n  \n    idvar \n     \n  \n  \n    v.names \n    value \n  \n  \n    drop \n     \n  \n  \n     \n    factor_key \n  \n\n\n\n\n\n\nDas Argument direction wurde nicht mit in die Übersicht aufgenommen. Während man bei reshape() angeben muss, welches Format man erstellen will, nutzt man in tidyr entweder gather() oder spread().\n\nBei gather() werden die Ausprägungen der Zeitvariable (Modi) so benannt, wie vorher die einzelnen Spalten der messwiederholten Variablen hießen. Wenn wir uns das anschauen wollen, könnten wir einfach reshape_wide (anstatt spread_wide) mit gather() bearbeiten. Im Gegensatz dazu kann man bei reshape() mit times eine eigene Benennung der Ausprägungen festlegen.\nBei reshape() wird mit idvar eine neue ID-Variable erstellt. Dazu muss ich dem Argument nur einen Namen geben. Wenn es bereits eine ID-Variable gibt, sollte man den Namen dieser angeben, weil ansonsten noch eine neue ID-Variable erstellt wird.\nMit drop kann man zusätzlich bestimmte Variablen aus dem neu formatierten Datensatz entfernen.\nBei gather() kann man mit mit factor_key entscheiden, ob die neu erstellte Zeitvariable als Faktor oder als Character behandelt werden sollen. Mit reshape() ist der Datentyp immer numerisch."
  },
  {
    "objectID": "Wide-and-Long-Format.html#andere-funktionen-zum-umformatieren",
    "href": "Wide-and-Long-Format.html#andere-funktionen-zum-umformatieren",
    "title": "11  Wide- & Long-Format",
    "section": "11.3 Andere Funktionen zum Umformatieren",
    "text": "11.3 Andere Funktionen zum Umformatieren\nEs gibt noch andere Funktionen, mit denen man Datensätze vom Long- ins Wide-Format oder umgekehrt umformatieren kann.\nIn gewisser Hinsicht sind pivot_longer() und pivot_wider() aus dem Paket tidyr die Nachfolger von gather() und spread(). Neben der intuitiveren Benennung enthalten sie einige Neuerungen wie z.B. die Möglichkeit, mit mehreren messwiederholten Variablen mit verschiedenen Datentypen zu arbeiten (dieses Feature wurde von melt() und dcast() übernommen). Außerdem wurde der Support für gather() und spread() eingestellt, d.h. bestehende Probleme mit diesen beiden Funktionen werden nicht mehr behoben werden. Hier finden wir eine Vignette, in der die Handhabung von pivot_longer() und pivot_wider() erklärt wird.\nWenn wir wissen wollen, wie man melt() (Wide zu Long) und dcast() (Long zu Wide) aus dem Paket reshape2 nutzt, können wir dafür hier nachschauen.\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] kableExtra_1.3.4 dplyr_1.1.2      tidyr_1.3.0      rmarkdown_2.21  \n\nloaded via a namespace (and not attached):\n [1] jsonlite_1.8.4    highr_0.10        compiler_4.3.0    webshot_0.5.4    \n [5] tidyselect_1.2.0  xml2_1.3.3        stringr_1.5.0     systemfonts_1.0.4\n [9] scales_1.2.1      yaml_2.3.7        fastmap_1.1.1     R6_2.5.1         \n[13] generics_0.1.3    knitr_1.42        htmlwidgets_1.6.2 tibble_3.2.1     \n[17] munsell_0.5.0     svglite_2.1.1     pillar_1.9.0      rlang_1.1.0      \n[21] utf8_1.2.3        stringi_1.7.12    xfun_0.39         viridisLite_0.4.1\n[25] cli_3.6.1         withr_2.5.0       magrittr_2.0.3    digest_0.6.31    \n[29] rvest_1.0.3       rstudioapi_0.14   lifecycle_1.0.3   vctrs_0.6.2      \n[33] evaluate_0.20     glue_1.6.2        fansi_1.0.4       colorspace_2.1-0 \n[37] purrr_1.0.1       httr_1.4.5        tools_4.3.0       pkgconfig_2.0.3  \n[41] htmltools_0.5.5  \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Markdown.html#neues-r-markdown-dokument-anlegen",
    "href": "Markdown.html#neues-r-markdown-dokument-anlegen",
    "title": "12  R Markdown",
    "section": "12.1 Neues R Markdown Dokument anlegen",
    "text": "12.1 Neues R Markdown Dokument anlegen\nNachfolgend sehen wir, wie wir ein neues R Markdown Dokument (Rmd) anlegen können.\n\n\n\n\n\n\n\n\n\nWir können zwischen verschiedenen Outputformaten, u.a. html, pdf und Word, wählen.\n\n\n\n\n\n\n\n\n\n\nhtml (hypertext markup language) ist auch eine Auszeichnungsprache und die Basis von Inhalten im Internet. Unser Webbrowser (z.B. Firefox, Chrome, Safari) stellt die Inhalte des html dar. Die Inhalte der Lernplattform sind ebenfalls html Dokumente.\n\nSo sieht das neu geöffnete Rmd aus.\n\n\n\n\n\n\n\n\n\n\n\nHier wurde als Outputformat html festgelegt. Das sehen wir ganz oben in output: html_document.\n\nMit dem Knit-Button können wir unser Output-Dokument erstellen.\nAlternativ können wir zum Knitten einen Kurzbefehl nutzen:\n\nWindows:  Shift  +  Ctrl  +  K \nMac:  Shift  +  Cmd  +  K \n\nFür die Outputformate html und pdf bekommen wir in einem neuen Fenster in R eine Vorschau davon. Für Word Dokumente öffnet sich (ggf.) Microsoft Word.\n\n\n\n\n\n\n\n\n\n\n\n\nhtml\n\n\n\n\n\n\n\n\n\n\n\n\n\npdf\n\n\n\n\n\n\n\n\n\n\n\n\n\nWord\nDies ist nur die erste Seite des Word Dokuments. Die anderen Inhalte sind auf die zweite Seite gerutscht.\n\n\n12.1.1 YAML\nGehen wir nochmal einen Schritt zurück und schauen uns das Rmd genauer an. Der Code ganz oben im Dokument ist der YAML Header. In diesem legen wir globale Dokument-Optionen fest. Nachfolgend sehen wir einige.\n\n\n\n\n\n\n\n\n\n\nAchtung: Standardmäßig steht hier output: html-document (bzw. output: pdf_document oder output: word_document) hintereinander in einer Zeile. Um weitere YAML Optionen ergänzen zu können, müssen wir html_document (bzw. pdf_document oder word_document) in eine neue Zeile schreiben und einen Doppelpunkt dahinter ergänzen. Die YAML Optionen kommen jeweils auch in eine eigene Zeile.\n\nMit toc: true können wir eine Gliederung (table of contents) einfügen.\n\n\n\n\n\n\n\n\n\nDiese können wir permanent anzeigen lassen, sodass sie trotz Scrollen immer an der gleichen Stelle (links) bleibt. Dazu ergänzen wir toc_float: true.\n\nAchtung: Diese Option funktioniert nur in html.\n\n\n\n\n\n\n\n\n\n\nWir können unsere Überschriften auch automatisch nummerieren lassen indem wir number_sections: true nutzen.\n\n\n\n\n\n\n\n\n\nDie Gestaltung können wir mit der Option theme anpassen. Hier wurde das Theme journal genutzt. Hier finden wir eine Übersicht über alle bestehenden Möglichkeiten.\n\nAchtung: Diese Option funktioniert nur in html.\n\n\n\n\n\n\n\n\n\n\nEinen Überblick über bestehende YAML Optionen finden wir hier."
  },
  {
    "objectID": "Markdown.html#text",
    "href": "Markdown.html#text",
    "title": "12  R Markdown",
    "section": "12.2 Text",
    "text": "12.2 Text\nNormaler Text wird in R Markdown einfach geschrieben. Im Gegensatz dazu konnten wir im R Skript nur Anmerkungen machen indem wir ein # vor unseren Text gesetzt haben.\nNachfolgend schauen wir uns Möglichkeiten der Gestaltung von Text (d.h. die nutzbaren Auszeichnungssprachen) an. Das sind Markdown, html und LaTex.\nEine kurze Übersicht über Textgestaltung mit Markdown können wir auch in RStudio angezeigt bekommen, wenn wir in der oberen Taskleiste in RStudio auf Help und dann auf Markdown Quick Reference klicken.\n\n\n\n\n12.2.1 Überschriften\nUm Überschriften zu setzen, schreiben wir ein bzw. mehrere # an den Anfang einer Zeile. Die Überschrift wird dann im Rmd farblich hervorgehoben. Je mehr # wir setzen, desto kleiner wird die Überschrift. Wie die einzelnen Überschriften aussehen, hängt von unserem Design (theme im YAML Header oder Template) ab.\n\n\nMarkdown\n\n\n\n\n\n\n\n\n\n\n\n\nDie Überschriften erscheinen auch in der Gliederung, wenn wir im YAML Header eine festgelegt haben (toc: true). Mit der Option toc_depth können wir festlegen, bis zu welcher Stufe die Überschriften inkludiert werden sollen.\n\n\nAchtung: Im R-Skript konnten wir # zum Kommentieren nutzen. Das funktioniert hier auch, aber nur in den Code Chunks. Um Textpassagen auszukommentieren, können wir den html-Kommentar-Tag <!-- --> nutzen. Mit folgendem Kurzbefehl können wir sowohl Text als auch Code auskommentieren (funktioniert in html und pdf):\n\n\nWindows/Linux:  Ctrl  +  Shift  +  C \nMac:  Cmd  +  Shift  +  C \n\n\n\n12.2.2 Hervorhebung\nWir können unseren Text auch fett und kursiv schreiben.\n\n\nMarkdown\n\n\n\n\n\n\n\n\n\n\n\nEs gibt zwei verschiedenen Optionen (* oder _) damit wir beide kombinieren können, d.h. fett und kursiv schreiben können. Dabei ist Reihenfolge (ob zuerst fett oder kursiv) egal. Wir müssen nur darauf achten, dass das Zeichen, welches zuletzt eingefügt wurde, auch zuerst beendet wird (z.B. **_eine Möglichkeit_**).\n\n\n12.2.3 Zeilenumbruch\n\n\n\nEs gibt mehrere Möglichkeiten, einen Zeilenumbruch herbeizuführen. Entweder wir machen mindestens zwei Leerzeichen ans Ende einer Zeile oder wir nutzen einen Backslash (\\).\n\n\nMarkdown\n\n\n\n\n\n\n\n\n\n\n12.2.4 Verlinkung\nWir können in unserem Output-Dokument auch auf externe (Web-)Quellen verlinken. Dazu nutzen wir [verlinkter Text](http://example.com).\nWenn unser Output-Dokument ein html ist, ist es benutzerfreundlicher, wenn sie die Webseite in einem neuen Fenster öffnet. Dazu nutzen wir den html Tag <a href=\"http://example.com\" target=\"_blank\">verlinkter Text</a>.\nIn html haben wir zusätzlich die Möglichkeit, innerhalb eines Dokuments zu verlinken. Am einfachsten ist es, auf eine Überschrift zu verlinken. Dazu nutzen wir einfach eckige Klammern, in die wir den Namen einer Überschrift schreiben: [Überschrift].\n\n\n12.2.5 Formeln\n\n\n\nWir können Formeln und mathematische Symbole mittels LaTex integrieren. Bei der Installation von dem Paket rmarkdown wird (standarmäßig) automatisch das Paket tinytex heruntergeladen. Das ermöglicht uns die Nutzung von LaTex. Das machen wir mit $ und speziellem LaTex-Code. Einen guten Überblick darüber, wie wir wichtige mathematische Symbole in LaTex schreiben, gibt uns diese Seite.\nUm Formeln im Fließtext zu schreiben (sog. Inline Formeln), nutzen wir einfache Dollarzeichen z.B. $\\alpha=0.05$ wird zu \\(\\alpha=0.05\\).\nUm Formeln als (zentrierten) Blocksatz zu schreiben nutzen wir doppelte Dollarzeichen z.B.\n$$ \\sigma^2= \\sum_{i =1}^{n} \\frac{(x_i - \\bar{x}) ^2}{n-1} $$ wird zu \\[ \\sigma^2= \\sum _{i =1} ^{n} \\frac{(x_i - \\bar{x}) ^2} {n-1} \\]\n\nAchtung: Leerzeichen haben im LaTex Code keinen Einfluss. Sie sind hier nur der Übersichtlichkeit halber eingefügt. Wenn wir in den Formeln Leerzeichen ahben wollen, müssen wir dafür speziellen LaTex Code nutzen."
  },
  {
    "objectID": "Markdown.html#r-code",
    "href": "Markdown.html#r-code",
    "title": "12  R Markdown",
    "section": "12.3 R-Code",
    "text": "12.3 R-Code\nNun schauen wir uns an, welche Möglichkeiten es gibt, R-Code und/oder deren Outputs in unser Dokument zu integrieren.\n\n12.3.1 Code Chunk\nAllen Code, denn wir sonst in ein R-Skript geschrieben haben, und der dann in der Konsole ausgeführt wurde (und ggf. im Viewer angezeigt wurde, d.h. Plots), schreiben wir in sogenannte Code Chunks.\nWir können Code Chunks einfügen indem wir auf Insert > R klicken …\n\n\n\n\n\n\n\n\n\n… oder einen Kurzbefehl nutzen:\n\nWindows:  Ctrl  +  Alt  +  I \nMac:  Alt  +  Cmd  +  I \n\n\n\nIm Rmd sind die Code Chunks (in der Standardeinstellung) grau hinterlegt.\n\n\n\n\n\n\n\n\n\nWenn wir auf den Pfeil ganz rechts klicken, können wir den Code in dem Chunk ausführen. Darunter erscheint ggf. der Output des Codes.\nZum Ausführen des Code Chunks gibt es können wir auch einen Kurzbefehl nutzen:\n\nWindows:  Shift  +  Ctrl  +  Enter \nMac:  Shift  +  Cmd  +  Ctrl Enter\n\n\n\n\nAchtung: Wenn wir beim Knitten Fehlermeldungen angezeigt bekommen, die auf den Code zurückzuführen sind, sollten wir diesen erstmal in einem “normalen” R-Skript ausführen. So können wir schauen, ob das Problem direkt im Code oder in der Einbettung in R Markdown liegt.\n\nIm Output-Dokument werden standarmäßig auch der Code und ggf. der Output angezeigt.\n\n1+2+3\n\n[1] 6\n\nvektor <- c(1, 2, 3)\n\n\nvektor\n\n[1] 1 2 3\n\nmean(vektor)\n\n[1] 2\n\n\nEs gibt Optionen zur Einstellung des Verhaltens der Code Chunks, die wir oben im Code Chunk innerhalb der geschweiften Klammern {r, ...} festlegen. Zwei davon werden wir häufiger brauchen: echo und eval.\nMit echo legen wir fest, ob wir den Code im Output-Dokument anzeigen wollen ({r, echo=TRUE}) oder nicht ({r, echo=FALSE}). Mit eval legen wir fest, ob wir den Code ausführen wollen ({r, eval=TRUE}) oder nicht ({r, eval=FALSE}).\nEs gibt auch die Möglichkeit, globale Optionen für das Verhalten der Chunks festzulegen. Diese werden auf jeden Code Chunk angewendet (außer wenn wir in einem Code Chunk eine andere Option spezifizieren) und verhindern dadurch überflüssige Wiederholungen. Globale Optionen legen wir mit knitr::opts_chunk$set(...) fest. Die Funktion kommt direkt in den Code Chunk (und nicht wie bei den Chunk-spezifischen Optionen in {r}).\n\n\n\n\n\n\n\n\n\ninclude=FALSE führt den Code aus, zeigt den Output des Codes aber nicht im Output-Dokument an. Das ist eine kürzere Alternative zu {r, echo=FALSE, eval=TRUE}, wenn der Output aus Zeichen besteht (z.B. das Ergebnis einer Rechnung; der Inhalt eines Objektes) bzw. {r, echo=FALSE, fig.show='hide'} wenn der Output eine Grafik ist.\nCode Chunks können auch benannt werden, indem in den geschweiften Klammern nach einem Leerzeichen hinter dem r der gewünschte Name eingetragen wird, z.b. {r, ...} für den Namen setup. Das hilft uns bei der Orientierung, weil wir die Namen in der Dokumentübersicht in RStudio angezeigt bekommen. Wir müssen allerdings darauf achten, denselben Namen nicht mehrmals zuzuweisen. Sonst bekommen wir eine Fehlermeldung.\n\nEigentlich benennen wir einen Chunk mit einem Tag: {r label=setup, …}. Diesen können wir aber weglassen, solange wir den Namen an die erste Stelle schreiben.\n\n\n\n\n\n\n\n\n\n\n\n\n12.3.2 Inline Code\nWenn es sich bei dem Output (von Code) um Zeichen handelt (und nicht um z.B. Tabellen und Grafiken), können wir diesen auch direkt in den Fließtext integrieren. Dazu nutzen wir r ....\nWir können damit z.B. Inhalte eines Objektes oder das Ergebnis einer (mathematischen) Funktion (ohne den Code) ausgeben lassen. Der Output wird außerdem so formatiert wie der Fließtext.\nIm vorhergehenden Abschnitt haben wir beispielsweise das Objekt vektor in einem Code Chunk definiert. Dessen Inhalte können wir mit r vector ausgeben lassen: 1, 2, 3\nWir können auch den Mittelwert von vektor berechnen mit r mean(vector): 2\nNur mit den `...` (ohne r) können wir außerdem Text hervorheben.\n\n\n12.3.3 Grafiken, Bilder und Tabellen\nWir können mit R-Code in den Code Chunks außerdem auch Grafiken und Tabellen erstellen und ausgeben, sowie auch Bilder, die nicht im Rmd erstellt wurden, anzeigen.\nBei Grafiken müssen wir nichts weiter beachten. Wir können denselben R-Code nutzen wie auch in einem R-Skript (.R).\nWenn wir lokal gespeicherte Bilder integrieren wollen, können wir die Funktion include_graphics(\"dateipfad\") aus dem Paket knitr nutzen. Dieses wurde (standardmäßig) automatisch heruntergeladen, als wir rmarkdown installiert haben.\n\n\nknitr ist das Paket, welches unseren R-Code im Rmd ausführt.\n\n\nlibrary(knitr)\ninclude_graphics(\"dateipfad\")\n\n\n\n\n\nSchauen wir uns noch Code Chunk Optionen an, die wir für Grafiken und Bilder nutzen können. Diese schreiben wir wieder oben in den Code Chunk in {r, ...}.\nMit out.width (Breite) und out.height (Höhe) können wir die Größe der Abbildungen im Outputdokument kontrollieren. Wir nehmen dafür am besten ‘…px’ (pixel). Wenn wir wollen, dass das Seitenverhältnis beibehalten wird, sollten wir immer nur die Breite oder die Höhe ändern.\n\nBeispiel:\n{r, out.width='300px'}\n\nMit fig.align können wir die Ausrichtung beeinflussen. Gültige Optionen sind: 'left', 'right' and 'center'.\n\nBeispiel:\n{r, fig.align='center'}\n\nUm Tabellen zu erstellen, gibt es verschiedene Funktionen (die nur in Rmd funktionieren). Wir schauen uns im Folgenden eine an (und zwar dieselbe, die auch für die Inhalte der R-Lernplattform genutzt wird).\n\nZuerst benötigen wir einen Beispieldatensatz. Wir nutzen den Datensatz women, der im Basispaket datasets enthalten ist und den wir mit der Funktion data() in unser Environment bekommen.\n\ndata(\"women\")\nwomen\n\n   height weight\n1      58    115\n2      59    117\n3      60    120\n4      61    123\n5      62    126\n6      63    129\n7      64    132\n8      65    135\n9      66    139\n10     67    142\n11     68    146\n12     69    150\n13     70    154\n14     71    159\n15     72    164\n\n\nSo sieht es aus, wenn wir den Datensatz einfach ausgeben. Schönere Tabellen erhalten wir mit der Funktion kable() aus dem Paket kableExtra.\n\ninstall.packages(\"kableExtra\")\nlibrary(kableExtra)\nkable(women)\n\n\n\n\n\n \n  \n    height \n    weight \n  \n \n\n  \n    58 \n    115 \n  \n  \n    59 \n    117 \n  \n  \n    60 \n    120 \n  \n  \n    61 \n    123 \n  \n  \n    62 \n    126 \n  \n  \n    63 \n    129 \n  \n  \n    64 \n    132 \n  \n  \n    65 \n    135 \n  \n  \n    66 \n    139 \n  \n  \n    67 \n    142 \n  \n  \n    68 \n    146 \n  \n  \n    69 \n    150 \n  \n  \n    70 \n    154 \n  \n  \n    71 \n    159 \n  \n  \n    72 \n    164 \n  \n\n\n\n\n\n\n\n\nDie gelbe Hintergrundfarbe sowie die grüne Schriftfarbe, die erscheinen, wenn man mit der Maus über die Zeilen geht, sind zusätzlich eingestellt für diese Seite. Wenn ihr kable() bei euch ausführt, ist das nicht enthalten.\n\nEs gibt noch diverse weitere Optionen, Tabellen zu verschönern. Eine ausführliche Dokumentation dazu gibt es auf dieser Seite."
  },
  {
    "objectID": "Markdown.html#weiterführend",
    "href": "Markdown.html#weiterführend",
    "title": "12  R Markdown",
    "section": "12.4 Weiterführend",
    "text": "12.4 Weiterführend\n\n12.4.1 Templates\nTemplates (Vorlagen) besitzen ein vorgefertigtes Design und ggf. sogar weitere Funktionalitäten für unser Output-Dokument.\nWenn wir Templates nutzen, sind in diesen ggf. nicht alle bzw. noch zusätzliche YAML Optionen nutzbar.\nWir wollen nun noch zwei Templates vorstellen, die für Projektdokumentation bzw. Veröffentlichungen hilfreich sein können. Für beide müssen wir jeweils ein zusätzliches Paket herunterladen.\nNach der Installation der Pakete finden wir die Templates hier:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.1.1 distill\nMit dem Template Distill Article aus dem Paket distill können wir html Dokumente erstellen.\n\n\n\n\n\n\n\n\n\nDie Inhalte der R Lernplattform basieren auch auf diesem Template. Allerdings wurden einige Designänderungen vorgenommen und zusätzliche Funktionen eingefügt.\n\ninstall.packages(\"distill\")\n\nIn diesem haben wir u.a. folgende zusätzliche (bzw. geänderte) Funktionalitäten:\n\nSeitennotizen (am rechten Rand)\nBilder und Grafiken können über die Seitenränder hinaus gehen (siehe Beispiel unten)\nFußnoten werden angezeigt, wenn man mit der Maus über sie fährt2\nGliederung (nur) ganz oben im Dokument\nDie YAML Option toc_float bewirkt in distill etwas anderes. Mit true wird die Gliederung am linken Rand angezeigt, wenn das Fenster, in dem das Outputdokument geöffnet wird (zumeist der Brwoser), größer gleich 1000px ist. Mit false wird die Gliederung immer mittig angezeigt.\nAppendix (wenn z.B Fußnoten oder Referenzen eingefügt)\n\n\nSeitennotiz\n\nEine ausführliche Dokumentation zu den Funktionalitäten des Templates und ihrer Nutzung finden wir hier.\n\n\n\n\n\nBildquelle: https://psyteachr.github.io/msc-data-skills/images/memes/better_graphs.png\n\n\n\n\n\n\n12.4.1.2 papaja\nMit dem Paket papaja (Preparing APA Journal Articles) bekommen wir ein Template, um Artikel ganz einfach im APA-Style zu schreiben. Das Outputformat ist pdf.\n\n\n\n\n\n\n\n\n\n\nAchtung: Bevor wir das Paket herunterladen, müssen wir schauen, ob die folgenden zwei Voraussetzungen erfüllt sind:\n\n\nRStudio Version größer gleich 0.98.932\nTeX Distribution (neuer als 2013; Links zu den Tex Distributions sind z.B. auf der Entwicklerseite, welche unten verlinkt ist, zu finden)\n\n\nUnsere R Studio Version finden wir z.B. in R Studio unter Help > About RStudio heraus.\n\nWeil das Paket noch nicht über den CRAN Mirror verfügbar ist, müssen wir es über die Github-Seite des Entwicklers herunterladen. Dazu benötigen wir wiederum das Paket devtools.\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_github(\"crsh/papaja\")\n\nIn diesem Video wird die Nutzung des Templates erklärt.\nWenn wir das kostenlose Literaturverwaltungsprogramm Zotero nutzen, können wir zusätzlich direkt daraus unsere Zitationen importieren. Damit wird auch automatisch eine Bibliographie erstellt. Um Zotero und R Makrdown gemeinsam nutzen zu können, müssen wir das Paket citr (in R) sowie die Software Better BibTex (auf unserem Rechner) installieren.\n\n# Installation von citr:\ninstall.packages(\"citr\")\n# Installation von Better BibTex siehe Link\n\nWeitere Informationen finden wir auf der Githubseite des Entwicklers.\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nAchtung: Es ist sinnvoll, die nachfolgende Funktion in einen Code Chunk im eigenen Rmd einzubetten.\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] kableExtra_1.3.4\n\nloaded via a namespace (and not attached):\n [1] httr_1.4.5        svglite_2.1.1     cli_3.6.1         knitr_1.42       \n [5] rlang_1.1.0       xfun_0.39         highr_0.10        stringi_1.7.12   \n [9] jsonlite_1.8.4    glue_1.6.2        colorspace_2.1-0  htmltools_0.5.5  \n[13] scales_1.2.1      rmarkdown_2.21    evaluate_0.20     munsell_0.5.0    \n[17] fastmap_1.1.1     yaml_2.3.7        lifecycle_1.0.3   stringr_1.5.0    \n[21] compiler_4.3.0    rvest_1.0.3       htmlwidgets_1.6.2 rstudioapi_0.14  \n[25] systemfonts_1.0.4 digest_0.6.31     viridisLite_0.4.1 R6_2.5.1         \n[29] magrittr_2.0.3    webshot_0.5.4     tools_4.3.0       xml2_1.3.3       \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Git.html#git",
    "href": "Git.html#git",
    "title": "13  Git und GitLab",
    "section": "13.1 Git",
    "text": "13.1 Git\nGit ist ein Programm zur Versionsverwaltung von Dateien. Das bedeutet, dass wir damit nachverfolgen können, wie sich Dateien über die Zeit verändert haben. Außerdem können wir auch sehen, wer sie verändert hat und wir können zu früheren Zuständen des Dokuments zurückkehren. Es gibt viele unterschiedliche Systeme zur Versionsverwaltung, aber git ist das am weiten verbreitetste und auch die Grundlage für zum Beispiel gitHub und gitLab. Git ist kostenlos für Windows, Mac und Linux verfügbar. Weitere Informationen zur Installation gibt es hier."
  },
  {
    "objectID": "Git.html#einführung-in-git",
    "href": "Git.html#einführung-in-git",
    "title": "13  Git und GitLab",
    "section": "13.2 Einführung in Git",
    "text": "13.2 Einführung in Git\nWir werden im Folgenden alle Befehle in der command line durchführen. Es gibt sehr viele unterschiedliche grafische Oberflächen für git, die einem das Leben erleichtern können. Aber wenn wir online nach Hilfe für git suchen, werden wir immer auf die command line Befehle stoßen und daher wollen wir uns hier auch nur mit diesen beschäftigen.\n\n13.2.1 Schritt 0\nWir wollen git als erstes sagen, wer wir sind. Das ist wichtig, damit später jeder nachvollziehen kann, wer was am Dokument verändert hat. Hierfür nutzen wir den git config Befehl.\n\ngit config --global user.name Mein Name\ngit config --global user.email MeinName@mail.com\n\n\n\n13.2.2 git init\nDer erste Schritt ist, git zu sagen, dass wir eine Versionskontrolle starten wollen. Hierfür gehen wir zuerst in den Ordner, in dem die Dateien liegen, die wir “nachverfolgen” wollen. Dort starten wir dann die Kommandozeile und geben den Befehl git init ein (init steht für initialize). Git erstellt nun einen .git-Ordner, der aber nur sichtbar ist, wenn wir verborgene Ordner im Betriebssystem anzeigen lassen. Diesen brauchen wir aber im Folgenden nicht; es macht also nichts, wenn ihr ihn bei euch nicht sehen könnt.\n\n\n13.2.3 git add\nSobald wir git init ausgeführt haben, können wir Dateien zur Versionskontrolle hinzufügen. Hierfür geben wir den Befehl “git add” gefolgt vom Name der Datei ein. Z.B. git add gitTutorial.tex. Die Datei “gitTutorial.tex” wird nun in eine Art “Warteraum” aufgenommen. Man spricht hier von “staged”. Wichtig ist: Wir haben die Datei so noch nicht in git zur Nachverfolgung hinzugefügt! Dateien aus Unterordnern können wir über die relativen Pfadnamen hinzufügen (z.B. figures/git/gitLogo.png). Mit git add . werden alle Dateien im Ordner sowie in allen Unterordnern hinzugefügt. Wir können auch eine .gitignore-Datei anlegen, in der wir festlegen, welche Dateien oder Dateitypen nicht zur Nachverfolgung hinzugefügt werden sollen. Diese können wir in einem Texteditor erstellen und dann unter .gitignore im Ordner speichern\n\nHinweis: Wenn wir versehentlich eine Datei in den “Warteraum” aufgenommen haben, können wir sie mit z.B. mit git rm --cached gitTutorial.tex wieder entfernen\n\nBeispiel für Inhalte einer .gitignore Datei:\n# Dateien mit folgenden Dateiendungen werden nicht nachverfolgt:\n.log\n.aux\n.pdf\n.gz\n.out\n# Dateien aus dem Unterordner archive werden nicht nachverfolgt\n/archive\n\n\n13.2.4 git status\nWenn wir git status aufrufen, sehen wir nun unter “Changes to be committed” welche Dateien beim nächsten commit gespeichert werden. Das ist der “Warteraum”. Darunter sehen wir außerdem, welche Dateien nicht hinzugefügt wurden. So können wir noch einmal kontrollieren, ob wir alles wichtige mit add aufgenommen haben.\n\n\n\n13.2.5 git commit\nDer wichtigste Befehl ist git commit: Mit commit halten wir die aktuelle Version aller Dateien aus dem “Warteraum” fest. Man macht hier also im Prinzip einen Schnappschuss der Dateien im aktuellen Zustand. Zu diesen Zuständen kann man dann immer wieder zurückkehren, wenn man einen Fehler gemacht hat oder schauen möchte, was sich seither verändert hat. Wenn man git commit eingibt, wird man dazu aufgefordert, auch eine commit message zu schreiben. Hier sollte eine Beschreibung der Veränderungen seit dem letzten commit eingetragen werden. Die Beschreibung speichert man mit [Strg + S] und schließt das Fenster mit [Strg + X].\n\nHinweis: Unter Windows muss man zuerst i klicken, bevor man eine commit message schreiben kann. Nach dem Eingeben der commit message mit Esc den Bearbeitungsmodus schließen und :wq eingeben, um das Fenster zu schließen.\nalternativ kann man auch git commit -m Dies ist meine commit message eingeben; dann öffnet sich kein neues Fenster für die commit message.\n\n\n\n\n13.2.6 git log\nMit git log erhaltet ihr eine Übersicht aller commits. Hier können wir auch den “Namen” des commits sehen (die sogenannten Hash-Werte).\nZ.B. d3e1166c891c21ee78dcaf6c7484eaf443957435. Diese brauchen wir, wenn wir zu einem früheren Zustand zurück kehren wollen.\n\n\n\n13.2.7 Zu einem früheren Zustand zurückkehren\nUm zu einem früheren Zustand zurückzukehren, müssen wir zuerst im log den Hash-Wert des Commits heraussuchen, zu dem wir zurück wollen (z.B. d3e1166c891c21ee78dcaf6c7484eaf443957435). Mit git checkout d3e1166c891c21ee78dcaf6c7484eaf443957435 können wir dann zu diesem Commit zurückkehren. Dabei werden alle Dateien auf den Zustand zurück gesetzt, den sie im Commit d3e1166c891c21ee78dcaf6c7484eaf443957435 hatten. Wichtig: Dabei werden die neueren Commits nicht überschrieben! Wir können immer wieder zur aktuellsten Version zurückkehren (mit git checkout master).\n\n\n13.2.8 Branches\nBranches sind dann wichtig, wenn wir verschiedene Versionen eines Dokumentes ausprobieren wollen. Zur Zeit befinden wir uns auf dem “master” branch und alle Veränderungen, die wir commiten, landen in diesem master branch. Wenn wir größere Veränderungen an einem R-Skript oder einem Dokument vornehmen, das wir auch mit anderen teilen, kann das aber problematisch sein. Damit wir uns nicht immer den master “zerschießen” können wir branches (also Ableger) nutzen. Um einen solchen branch zu erstellen, geben wir git branch Branchname ein. Wir können zwischen dem master und dem neuen branch mit git checkout Branchname wechseln. Wenn wir wieder zum master zurück wollen, geben wir git checkout master ein.\nHinweise:\n\nWenn man einfach nur git branch eingibt, bekommt man eine Übersicht über alle existierenden branches. Git markiert außerdem, welcher Branch gerade angezeigt wird.\nWenn man ein Dokument verändert, aber nicht commitet hat, kann man den branch nicht wechseln. Andernfalls würde man alle Veränderungen verlieren\n\n\n\n13.2.9 Mergen\nNachdem wir alle Änderungen in einem branch vorgenommen haben, wollen wir nun unseren neuen branch und den master vereinen (merge). Hierfür gehen wir wieder in den master (git checkout master). Dort geben wir nun git merge Branchname ein. Das kann z.B. auch genutzt werden, wenn eine andere Person an unserem Manuskript weiter geschrieben hat und wir nun diese Veränderungen in unser Dokument übernehmen wollen. Dabei schaut Git auch, ob mehrere Personen die selben Textstellen verändert haben. In solchen Fällen kommt es zu einem merge conflict. Git wird versuchen, diese Konflikte zu lösen, aber manchmal müssen wir auch selbst Hand anlegen. Weiter unten wird erklärt, wie man mit merge Konflikten in GitLab umgeht."
  },
  {
    "objectID": "Git.html#github-gitlab-gitbucket",
    "href": "Git.html#github-gitlab-gitbucket",
    "title": "13  Git und GitLab",
    "section": "13.3 GitHub, GitLab, GitBucket, …",
    "text": "13.3 GitHub, GitLab, GitBucket, …\nGitHub, GitLab, GitBucket und ähnliche Plattformen sind Webseiten, auf denen wir unser git-Projekt hochladen können, um es anderen zur Verfügung zu stellen oder ein Backup zu haben. Wir werden im Folgenden die HU-eigene GitLab Instanz nutzen. Wichtig: In GitLab können wir Projekte auch so speichern, dass nur wir selbst oder von uns ausgewählte Personen auf die Dokumente Zugriff haben!\n\n13.3.1 Ein neues Projekt erstellen\nAuf GitLab können wir unter “New Project” eine neues Projekt erstellen. Hier können wir auch einstellen, ob ein Projekt nur für uns oder für alle sichtbar sein soll.\n\nNachdem wir ein neues Projekt erstellt haben, können wir unser bestehendes git-Projekt hochladen. Die hierfür notwendigen Schritte werden nach dem Erstellen eines neuen Projektes direkt in gitLab angezeigt. Sie sind: git remote add origin https://scm.cms.hu-berlin.de/Nutzername/Projektname.git git push -u origin master\n\nremote steht für ein repository, das nicht auf dem eigenen Rechner liegt. git remote add origin fügt also ein neues remote repository hinzu, das origin heißt und in https://scm.cms.hu-berlin.de/Nutzername/Projektname.git liegt. Nachfolgend müssen wir nicht mehr die ganze Web-Adresse schreiben, sondern es reicht der Name des remote repository (origin). Wir können das remote repository auch anders nennen (z.B. git remote add GitLabRemote https://scm.cms.hu-berlin.de/Nutzername/Projektname.git). Aber origin hat sich als Standard durchgesetzt.\ngit push -u origin master lässt sich übersetzen als: lade (git push) den lokalen master in das remote repository mit dem Namen origin. Dabei wird im remote repository ebenfalls der master überschrieben.\nWenn wir ein gemeinsames Projekt erstellen, können wir andere Personen auf GitLab gezielt zum Projekt hinzu fügen und ihnen bestimmte Rollen zuordnen. Mehr Infos zu den Rollen und ihrer Bedeutung gibt es hier.\n\n\n\n13.3.2 git clone\nHäufig wollen wir an einem Projekt einer anderen Person weiterarbeiten. Hierfür laden wir das Projekt erst einmal herunter. Zuerst gehen wir in den Ordner, in dem wir das git-Projekt speichern wollen. Dann führen wir dort in der command-line den folgenden Befehl aus: git clone https://scm.cms.hu-berlin.de/orzekjan/git-einfuehrung.git\nGit lädt nun das Projekt “git-einfuehrung” herunter und wir können es lokal bearbeiten. Dabei sollten wir zuerst einen neuen Branch erstellen. So können wir sicherstellen, dass wir Veränderungen im Projekt, die von einer anderen Person im origin vorgenommen werden, leichter mit unseren Veränderungen integrieren können.\n\nUm den Link zum Herunterladen eines Projekts zu finden, kann man auf der Projektseite auf Clone drücken und den HTTPS Link kopieren.\nden lokalen branch zur Bearbeitung erstellen wir wie oben: z.B. mit git branch revisionJO.\n\n\n\n\n\n13.3.3 git push\nNachdem wir ein Repository herunter geladen und verändert haben, können wir es mit git push wieder auf den Server laden. Wenn es sich um ein gemeinsames Projekt handelt, sollte man nicht direkt in den master pushen; sonst überschreibt man eventuell Veränderungen, die andere vorgenommen haben. Stattdessen sollte man den eigenen branch (im Beispiel den branch revisionJO) in einen neuen remote branch pushen. Dies geht mit dem Befehl:\ngit push origin revisionJO\nDer lokale branch revisionJO wird so in das online-Projekt kopiert. Dort gibt es nun ebenfalls einen branch mit dem Namen revisionJO. Wenn der Branch mit diesem Namen bereits vorher existierte, wird er nur aktualisiert.\n\nBei unseren eigenen Projekten können wir auch direkt in den Master pushen. Das geht mit git push origin master.\nIn GitLab können wir nun zu unserem branch wechseln und mit “Create merge request” ein Zusammenführen des masters und des revisionJO-branches anfragen.\n\nHier können wir auch noch einen ausführlicheren Kommentar hinterlassen, warum wir eine Veränderung vorgenommen haben.\nAlle Veränderungen werden im Reiter “Changes” detailliert aufgeführt:\n\nIm Reiter “Overview” können Projektadministrator_innen nun den Merge annehmen und unsere vorgeschlagenen Veränderungen damit übernehmen oder ablehnen. Außerdem können dort die Veränderungen auch noch einmal diskutiert werden.\n\n\n\n13.3.4 git pull\nMit pull können wir unser git repository vom Server auf den Rechner aktualisieren. Wir können dabei festlegen, welchen branch wir herunterladen wollen. Z.B. laden wir mit\ngit pull origin master\nden branch master aus dem Online-Projekt origin herunter. So können wir nach dem merge sicherstellen, dass wir auch wieder die aktuelle Version des master haben.\n\n\n13.3.5 Branches archivieren und löschen\nNachdem man den branch revisionJO in den master gemerged haben, ist üblich, den alten branch zu löschen. So vermeidet man, am Ende sehr viele branches zu haben, die alle nicht mehr genutzt werden. Den branch revisionJO löscht man mit:\ngit branch -d revisionJO\nWenn man den branch vorher archivieren möchte, geht dies mit:\ngit tag archive/revisionJO revisionJO\nSo wird ein Ordner archive erstellt, in dem der branch revisionJO gespeichert ist. Wir können den branch nun löschen, aber er ist weiterhin im Ordner archive hinterlegt. Um alle tags anzuzeigen, können wir einfach git tag eingeben. Dort sehen wir nun auch den neu angelegten archive Ordner.\nUm den branch revisionJO wieder herzustellen, können wir git checkout -b revisionJO archive/revisionJO eingeben."
  },
  {
    "objectID": "Git.html#weitere-ressourcen",
    "href": "Git.html#weitere-ressourcen",
    "title": "13  Git und GitLab",
    "section": "13.4 Weitere Ressourcen",
    "text": "13.4 Weitere Ressourcen\nEinfache Einführung: Git & GitHub Crash Course For Beginners"
  },
  {
    "objectID": "Voraussetzungspruefung.html#linearität",
    "href": "Voraussetzungspruefung.html#linearität",
    "title": "14  Annahmen der Multiplen Linearen Regression",
    "section": "14.1 Linearität",
    "text": "14.1 Linearität\n\nMit Linearität ist die korrekte Spezifikation der Form des Zusammenhangs zwischen Kriterium und Prädiktoren gemeint. Genauer gesagt, meint die Annahme, dass der Erwartungswert des Kriteriums sich als Linearkombination der Prädiktoren darstellen lässt.\n\n\nAchtung: Dies bedeutet jedoch nicht notwendigerweise, dass der Zusammenhang der Variablen linear sein muss.\n\nEs muss sich lediglich um eine linear additive Verknüpfung der Regressionsterme handeln. Beispielsweise spezifiziert die folgende Regressionsgleichung \\(y=b_0 + b_1x^2 + e\\) einen quadratischen Zusammenhang zwischen \\(Y\\) und \\(X\\) mittels einer linear additiven Verknüpfung der Regressionsterme (hier nur ein einziger Prädiktor \\(X\\)). Siehe auch den Abschnitt zum Umgang mit Nicht-Linearität.\nWenn die Form des Zusammenhangs zwischen Kriterium und Prädiktoren nicht richtig spezifiziert wurde, können ernsthafte Probleme auftreten. Dies wäre zum Beispiel dann der Fall, wenn es zwischen Prädiktoren und Kriterium in Wirklichkeit einen quadratischen Zusammenhang gibt, wir in unserem Regressionsmodell aber nur einen linearen Zusammenhang spezifiziert haben. Sowohl die Regressionskoeffizienten als auch die Standardfehler könnten in einem solchen Fall verzerrt sein.\n\n14.1.1 Überprüfung\n\n14.1.1.1 Bivariate Streudiagramme\nIn einem ersten Schritt schauen wir uns bivariate Streudiagramme an. Das heißt, wir schauen uns nicht das gesamte Modelle (mit mehreren Prädiktoren) an, sondern nur Zusammenhänge zwischen einzelnen Prädiktoren und dem Kriterium.\n\nAchtung: Auch wenn die bivariaten Streudiagramme auf Linearität hinweisen, sollten wir nicht vergessen, dass auch Interkationen zwischen Prädiktoren zu nicht-linearen Zusammenhängen führen können. Die Nutzung von bivariaten Streudiagramen zur Überprüfung der Annahme der Linearität ist weder eine notwendige, noch eine hinreichende Bedingung. Sie sind daher mit Vorsicht zu beurteilen.\n\n\n# Lebenszufriedenheit - Zufriedenheit mit Studieninhalten\nplot(daten$zufr_inhalt, daten$leb_zufr)\nlz_inh <- lm(daten$leb_zufr ~ daten$zufr_inhalt, na.action='na.exclude') # Einfache Regression\nabline(lz_inh) # Einzeichnen Regressionsgerade\n\n\n\n\nDer Plot spricht für einen linearen Zusammenhang zwischen Lebenszufriedenheit (Kriterium) und Zufriedenheit mit Studieninhalten (Prädiktor).\n\n# Lebenszufriedenheit - Zufriedenheit mit Studienbedingungen\nplot(daten$zufr_beding, daten$leb_zufr)\nlz_bed <- lm(daten$leb_zufr ~ daten$zufr_beding, na.action='na.exclude') # Einfache Regression\nabline(lz_bed) # Einzeichnen Regressionsgerade\n\n\n\n\nDer Plot von Lebenszufriedenheit (Kriterium) und Zufriedenheit mit Studienbedingungen (Prädiktor) weist auf einen linearen Zusammenhang hin.\n\n\n14.1.1.2 Residualplot\nDas wichtigste Werkzeug zur Prüfung der Linearitätsannahme ist der Residualplot. In einem Residualplot werden die vorhergesagten Werte \\(\\hat y_i\\) (auf der \\(x\\)-Achse) gegen die Residuen \\(\\hat e_i = y_i - \\hat y_i\\) (auf der \\(y\\)-Achse) abgetragen.\n\nplot(lm_lz, which = 1) \n# erster Plot der plot()-Funktion für ein lm-Objekt ist der Residualplot\n\n\n\n\n\n\nDie gestrichelte Linie bei \\(y = 0\\) zeigt den Erwartungswert der Residuen. Diese ist immer null und die Residuen sollten sich ohne erkennbares Muster, um diese Linie verteilen.\nDie rote Linie ist die Lowess Fit Line. Diese sollte sich der gestrichelten Linie annähern, wenn der Zusammenhang zwischen Prädiktoren und Kriterium linear ist.\nIn unserem Beispiel legt der Residualplot nahe, dass der Zusammenhang zwischen Lebenszufriedenheit und Zufriedenheit mit Studieninhalten und -bedingungen weitgehend linear ist.\nDie Annahme der Linearität wäre z.B. verletzt, wenn die Residuen einen U-förmigen Zusammenhang mit den vorhergesagten Werten aufweisen würden. Das würde nahelegen, dass ein quadratischer Zusammenhang zwischen dem Kriterium und den Prädiktoren besteht, der nicht adäquat modelliert wurde.\n\n\n\n\n\n\n\n\n\n\n\n\n14.1.2 Umgang\nUm einen angemessen Weg zu finden, um mit nicht-linearen Zusammenhängen zwischen den Variablen umzugehen, können wir die folgenden vier Fragen zur Eingrenzung nutzen:\n\nGibt es eine Theorie, die einen spezifischen nicht-linearen Zusammenhang zwischen den integrierten Variablen vorhersagt?\n\nWir sollten eine Regressionsgleichung aufstellen, die dem theoretisch implizierten mathematischen Zusammenhang widerspiegelt.\n\nWie sieht der beobachtete bivariate Zusammenhang zwischen den Variablen aus?\n\nWenn der Zusammenhang zwischen dem Kriterium und einzelnen Prädiktoren oder zwischen einzelnen Prädiktoren untereinander nicht-linear ist, können die Prädiktoren transformiert werden (z.B. \\(log X_1\\)). Die Art der Transformation hängt von der Art des Zusammenhangs ab.\n\nWie sieht die Lowess Line in den originalen Daten aus? und\n\nBleibt die Varianz der Residuen über den Bereich des Kriteriums hinweg konstant? (siehe Homoskedastizität)\n\nHomoskedastizität: Hierbei könnten wir einzelne Regressionsterme in Polynomen höherer Ordnung darstellen (z.B. \\(X_1^2\\), \\(X_2^3\\)). In einigen Fällen könnten einfache Potenzfunktionen nicht ausreichend sein. Dann könnten nichtparametrische Funktionen die bessere Lösung sein, um den Zusammenhang zwischen Kriterium und Prädiktoren zu spezifizieren.\nHeteroskedastizität: Wir könnten das Kriterium durch eine nichtlineare mathematische Funktion von \\(Y\\) ersetzen (z.B. \\(log Y\\)). Mit dieser Transformation entsprechen die Residuen nun den beobachteten \\(log Y\\) minus den vorhergesagten \\(log \\hat Y\\)."
  },
  {
    "objectID": "Voraussetzungspruefung.html#exogenität-der-prädiktoren",
    "href": "Voraussetzungspruefung.html#exogenität-der-prädiktoren",
    "title": "14  Annahmen der Multiplen Linearen Regression",
    "section": "14.2 Exogenität der Prädiktoren",
    "text": "14.2 Exogenität der Prädiktoren\n\nDie Prädiktoren \\(X\\) sind unabhängig vom Fehlerterm der Regressionsgleichung \\(e\\): \\(E(e|X)=0\\).\n\nDas impliziert z.B. perfekte Reliabilität und das alle relevanten Variablen im Modell aufgenommen sind, das heißt, dass es keine konfundierenden Variablen gibt. Das ist ein zentrales Anliegen in der Wissenschaft, jedoch ist Exogenität nicht leicht nachzuweisen.\nFür unser Beispiel der Regression von ‘Zufriedenheit mit Studieninhalten’ und ‘Zufriedenheit mit Studienbedingungen’ auf ‘Lebenszufriedenheit’ müssten wir überlegen, ob noch andere Variablen einen Einfluss haben könnten. Beispielsweise könnten auch verschiedene Persönlichkeitsfaktoren mit ‘Lebenszufriedenheit’ zusammenhängen. Das würde sich dann darin äußern, dass die Prädiktoren noch systematische Varianz mit dem Fehlerterm teilen.\nWenn nicht alle relevanten Prädiktoren im Modell spezifiziert sind oder enthaltene Prädiktoren messfehlerbehaftet sind, können daraus verzerrte Regressionskoeffizienten und Standardfehler resultieren.\n\n14.2.1 Überprüfung\nVor der Erhebung müssen wir uns sorgfältig Gedanken darüber machen, welche Prädiktoren relevant sind. Diese müssen vollständig in das Modell integriert werden. Beispielsweise sollten wir stets eine Literaturrecherche durchführen, um uns über den derzeitigen Stand der Forschung in einem Themenbereich zu informieren.\nZur Überprüfung der Exogenität könnten wir außerdem eine hierarchische Regression durchführen, in der wir schrittweise weitere Variablen aufnehmen. Wenn sich die Regressionsgewichte bei Aufnahme eines neuen Prädiktors ändern, war die Exogenitätsannahme der ursprünglichen Prädiktoren wahrscheinlich nicht erfüllt. Solche Modellvergleiche helfen bei der Beurteilung der Exogenität.\nWir sollten uns zusätzlich theoretisch überlegen, ob die gemessenen Prädiktoren messfehlerbehaftet sein könnten. Direkt beobachtbare Variablen (z.B. Alter, höchster Bildungsabschluss oder Körpergröße) stehen weniger im Verdacht, messfehlerbehaftet zu sein. Nicht direkt beobachtbare (latente) Variablen hingegen (z.B. Berufserfolg, Kreativität oder Wohlbefinden) können mit größerer Wahrscheinlichkeit messfehlerbehaftet sein.\nUnsere Variablen ‘Lebenszufriedenheit’, ‘Zufriedenheit mit Studieninhalten’ und ‘Zufriedenheit mit Studienbedingungen’ sind alle latent. Von daher sind Messfehler wahrscheinlicher.\nWir sollten uns außerdem für die reliabelsten Erhebungsinstrumente für die Messung der Prädiktoren entscheiden. Wenn wir nicht an der Erhebung der Variablen beteiligt waren, sollten wir uns nachträglich über die Reliabilität der Erhebungsinstrumente informieren.\nIn unserem Fall des erstis-Datensatz gibt es leider keine weiteren Informationen zu den Erhebungsinstrumenten. So können wir leider nicht einschätzen, wie reliabel die Erhebungsinstrumente sind.\nDie Reliabilität erhobener Variablen können wir auf verschiedene Arten schätzen. Diese werden in Abhängigkeit des Forschungsdesign und der Fragestellung ausgewählt.\nDie Reliabilität der Messungen in unserem Beispiel könnten wir beispielsweise mit McDonald’s Omega bzw. dem gewichteten Omega berechnen.\n\n\n14.2.2 Umgang\nWenn die Prädiktoren stark messfehlerbehaftet sind, sollten wir auf Regressionsmodelle mit latenten Variablen zurückgreifen. Beispielsweise können wir Messfehler mittels Strukturgleichungsmodellierung berücksichtigen."
  },
  {
    "objectID": "Voraussetzungspruefung.html#homoskedastizität",
    "href": "Voraussetzungspruefung.html#homoskedastizität",
    "title": "14  Annahmen der Multiplen Linearen Regression",
    "section": "14.3 Homoskedastizität",
    "text": "14.3 Homoskedastizität\n\nDie Varianz der Residuen \\(s^2_{e}\\) an einer bestimmten Stelle des Prädiktors ist für alle Prädiktorwerte gleich. Diese Varianz entspricht dem quadrierten Standardschätzfehler \\(\\sigma_e^2\\) in der Population.\n\n\nHomoskedastizität wird auch Varianzhomogenität genannt.\n\nDie Annahme wäre beispielsweise verletzt, wenn mit steigenden Prädiktorwerten die Residuen größer, d.h. die Vorhersage mittels der Regressionsgerade ungenauer, werden würde.\nEs kann vielfältige Gründe für Varianzheterogenität geben. So können z.B. stark abweichende Werte dafür verantwortlich sein (siehe Extreme Werte und einflussreiche Datenpunkte).\nNur unter Gültigkeit der Annahme ist die Berechnung der Standardfehler korrekt, aber Heteroskedastizität führt nicht zu verzerrten Regressionkoeffizienten.\nAndere als die vorgestellten Möglichkeiten zur Überprüfung und zum Umgang mit Heteroskedastizität inklusive der Umsetzung in R finden wir z.B. auf R-bloggers.\n\n14.3.1 Überprüfung\n\n14.3.1.1 Residualplot\nZur Überprüfung der Homoskedastizität können wir ebenfalls einen Residualplot, wie schon bei der Überprüfung der Annahme der Linearität, verwenden.\n\nplot(lm_lz, which = 1) \n# erster Plot der plot()-Funktion für ein lm-Objekt ist der Residualplot\n\n\n\n\n\n\nDie gestrichelte Linie bei \\(y = 0\\) zeigt den Erwartungswert der Residuen. Diese ist immer null und die Residuen sollten sich ohne erkennbares Muster, um diese Linie verteilen.\nDie Annahme wäre z.B. verletzt, wenn die Residuen einen nach rechts geöffneten Trichter bilden würden. Das würde bedeuten, dass die Varianz mit größer werdenden \\(X\\)-Werten wachsen würde wie in nachfolgendem Beispiel illustriert:\n\n\n\n\n\n\n\n\n\n\n\n14.3.1.2 Scale Location Plot\nMit dem Scale Location Plot schauen wir, ob die Standardabweichung der standardisierten Residuen \\(\\frac{e_i}{s_e}\\) über den Bereich der vorhergesagten Werte hinweg gleich bleibt.\n\nplot(lm_lz, which = 3) \n\n\n\n# dritter Plot der plot()-Funktion für ein lm-Objekt ist der Scale Location Plot\n\nDie standardisierten Residuen sollten auch hier gleichmäßig (zufällig) über den gesamten Bereich streuen. Außerdem sollte die Lowess Line möglichst horizontal zur \\(x\\)-Achse sein.\nAuch der Scale Location Plot weist in unserem Beispiel auf ungefähre Varianzhomogenität hin. Das heißt, dass die standardisierten Residuen des Teils vom Kriterium ‘Lebenszufriedenheit’, der nicht durch die Prädiktoren ‘Zufriedenheit mit Studieninhalten’ und ‘Zufriedenheit mit Studienbedingungen’ vorhergesagt werden kann, gleichmäßig über die vorhergesagten Werte vom Kriterium ‘Lebenszufriedenheit’ streut.\n\n\nExkurs: Quantifizieren des Ausmaßes\n\nBisher haben wir uns angeschaut, ob Heteroskedastizität vorliegt. Diese hat aber erst einen substanziellen Einfluss auf die Regression, wenn das Ausmaß “groß” ist. Wir können die Varianzheterogenität (wenn die graphische Überprüfung darauf hindeutet) quantifizieren, um zu entscheiden, ob wir korrektive Maßnahmen durchführen sollten. Dazu schauen wir uns eine Möglichkeit aus dem Lehrbuch von Cohen, Cohen, West & Aiken (2003, S.146) an.\nDabei betrachten wir die konditionale (Fehler-)Varianz \\(s^2_{(e | slice)}\\) in sog. “Slices” (d.h. Gruppen) für einzelne Prädiktoren. Wir schauen uns das exemplarisch bei Zufriedenheit mit Studieninhalten an. Dazu nutzt man die Ergebnisse der einfachen Regression der Prädiktoren. Diese haben wir im Abschnitt Bivariate Streudiagramme schon einmal berechnet und darauf greifen wir jetzt wieder zurück.\nDas Vorgehen:\n\nSortieren der Residuen nach aufsteigendem Prädiktor X (Zufriedenheit mit Studieninhalten)\nin ähnlich große Slices einteilen\nDie Wahl der Anzahl der Slices ist ein Tradeoff zwischen stabiler Varianzschätzung in jedem Slice (d.h. wenig Gruppen) und der Begutachtung verschiedener Anteile der Daten (d.h. viele Gruppen).\nkonditionale Varianz in den Slices berechnen\nDazu quadrieren wir die einzelnen Residuen, teilen diese jeweils durch die Anzahl der Personen in diesem Slice \\(n_{slice}\\) minus 2, und summieren die Quotienten auf.\n\nZur Beurteilung der konditionalen Varianz gibt es zwei Kriterien:\n\nder Quotient aus dem Slice mit der größten konditionalen Varianz geteilt durch den Slice mit der kleinsten konditionalen Varianz sollte kleiner als 10 sein\n\nmit größer werdendem X sollte die konditionale Varianz in den Slices nicht systematisch variieren (z.B. konstant kleiner oder größer werden)\n\nWenn der Quotient \\(> 10\\) ist oder es systematische Variation gibt, sollten wir korrektive Maßnahmen einleiten\nNachfolgend schauen wir uns die Umsetzung dazu in R an:\n\n# Objekt erzeugen, ...\n# ... in dem die Residuen nach aufsteigender Größe in X sortiert sind:\nquant <- data.frame(erstis$zuf.inh.1, \n                    residuals(lz_inh))[order(erstis$zuf.inh.1, \n                                             residuals(lz_inh)),]\n# data.frame() erstellt einen Dataframe aus den übergebenen Variablen\n# order() sortiert das erste Argument X aufsteigend (Default) ...\n# ... und gleiche Ausprägungen in X werden nach den Ausprägungen ..\n# ... des zweiten Arguments (hier: Residuen) sortiert\n  # das Komma am Ende sagt, dass wir Zeilen sortieren wollen\n  # mit [ ] bekommen wir die Indizes (und nicht die Werte) ausgegeben, ...\n  # ... welche wiederum auf den Dataframe angewendet werden\n\nStandardmäßig wird so die erste Variable X aufsteigend sortiert, weil order(..., decreasing=FALSE) der Default ist. Mit decreasing=TRUE können wir absteigend sortieren.\n\n\n\nAchtung: Bei order(X, -resid) ändert sich die Sortierung innerhalb der gleichen Prädiktorwerte X in Abhängigkeit davon, ob wir bei order() vor die zweite Variable (resid) ein - setzen oder nicht:\nMit resid (so wie wir es im Beispiel machen) wird innerhalb der gleichen Prädiktorwerte aufsteigend sortiert.\nMit -resid wird innerhalb der gleichen Prädiktorwerte absteigend sortiert.\nDadurch kommen unterschiedliche Residuen in die Slices und folglich werden auch unterschiedliche konditionale Varianzen berechnet!\n\n\n\nquant <- na.exclude(quant) # Missings raus\n\n\n\n\n\n  \n\n\n\n\n# Anzahl der Fälle\nnrow(quant) # 165\n\n# Einteilung: 3 Gruppen á 41 Personen, 1 Gruppe mit 42 Personen\nslice_1 <- quant$residuals.lz_inh.[1:41] \nslice_2 <- quant$residuals.lz_inh.[42:82]\nslice_3 <- quant$residuals.lz_inh.[83:123]\nslice_4 <- quant$residuals.lz_inh.[124:165]\n\n# konditionale Varianz in Slices berechnen\nvar_slice_1 <- sum((slice_1^2/(length(slice_1)-2)))\nvar_slice_2 <- sum((slice_2^2/(length(slice_2)-2)))\nvar_slice_3 <- sum((slice_3^2/(length(slice_3)-2)))\nvar_slice_4 <- sum((slice_4^2/(length(slice_4)-2)))\n\n# konditionale Varianzen in Vektor speichern\nsort_var <- c(var_slice_1 , var_slice_2, var_slice_3, var_slice_4)\n\n\nsort_var\n\n[1] 40.79699 25.91895 21.15083 24.16912\n\n\nIn unserem Beispiel scheint es keine systematischen Zu- oder Abhnahme in den konditionalen Varianzen der Slices zu geben …\n\nround((var_slice_1 / var_slice_3), 3)\n\n[1] 1.929\n\n\n… und auch der Quotient zwischem dem Slice mit der größten und der kleinsten konditionalen Varianz ist relativ klein.\n\n\n\n\n\n14.3.2 Umgang\nWenn wir heteroskedastische Residuen haben, sollten wir gegen die potentielle Verzerrung der Standardfehler vorgehen. Dazu könnten wir beispielsweise Methoden zur Korrektur der Standardfehler oder varianzstabilisierende Verfahren, wie die Box-Cox-Transformation, nutzen. Eine andere Alternative wäre es, auf robustere Verfahren wie die gewichtete Regression mittels WLS zurückzugreifen.\n\n\nExkurs: WLS-Regression\n\nDer weighted least squares Schätzer gewichtet jeden Fall (d.h. jede Person) danach, wie präzise die Beobachtung von \\(Y\\) für diesen Fall war. Das bedeutet, dass Beobachtungen mit geringen (Fehler-)Varianzen höher gewichtet werden als Beobachtungen mit großen (Fehler-)Varianzen. Anders ausgedrückt werden Beobachtungen, die näher an der Regressionsgerade sind, stärker gewichtet. Im Vergleich dazu gewichtet der standardmäßig genutzte OLS-Schätzer (Ordinary Least Squares, Methode der kleinsten Quadrate) jede Beobachtung gleich.\nEs gibt mehrere Möglichkeiten, die Gewichte \\(w_i\\) zu schätzen. Im Folgenden schauen wir uns exemplarisch eine Methode aus dem Buch von Cohen, Cohen, West & Aiken (2003, S.146-147) an.\n\n\n\nAchtung: Wenn wir Ausreißer in unseren Daten haben, finden wir in der Fußnote 19 auf S.147 des Lehrbuchs eine geeignetere Methode zur Schätzung der Gewichte \\(w_i\\).\nDazu nehmen wir die Residuen aus der OLS-Regression, quadrieren diese und regredieren sie auf unsere Prädiktoren. Die Inversen dieser vorhergesagten quadrierten Residuen sind das unsere Gewichte für die WLS-Regression.\n\n\nweights_lm <- lm(daten$resid ^ 2 ~ daten$zufr_inhalt + daten$zufr_beding,\n     na.action = \"na.exclude\")\n# wieder na.exclude nutzen\n# sonst hat der Vektor mit den Gewichten wieder eine geringere Länge \n\ndaten$gewichte <- fitted.values(weights_lm)\n\nZur Modellschätzung können wir wieder lm() nutzen. Dafür müssen wir nur zusätzlich den Gewichtsvektor im Argument weights spezifizieren.\n\n# WLS-Regression\nlm_wls <- lm(daten$leb_zufr ~ daten$zufr_inhalt + daten$zufr_beding,\n    weights = 1 / daten$gewichte, na.action = \"na.exclude\")\n\n# Vergleich der OLS- und WLS-Schätzung\nsummary(lm_lz)$coefficients\n\n                   Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)       12.791048  2.3391903 5.468152 1.704375e-07\ndaten$zufr_inhalt  2.499315  0.6869872 3.638081 3.694881e-04\ndaten$zufr_beding  1.312796  0.5743345 2.285769 2.357195e-02\n\nsummary(lm_wls)$coefficients\n\n                   Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)       12.889931  2.3951572 5.381664 2.566204e-07\ndaten$zufr_inhalt  2.545507  0.6966131 3.654119 3.488119e-04\ndaten$zufr_beding  1.222937  0.5605375 2.181723 3.057982e-02\n\n\nWir sehen, dass die Schätzungen von WLS denen von OLS sehr nah ist. Relevant wären hier prinzipiell mehr die Standardfehler (die bei Varianzheterogenität und OLS verzerrt wären). Da wir aber von Varianzhomogenität in unserem Beispiel ausgehen, sind auch diese sehr ähnlich.\n\nAchtung: Es gibt zwei Schwierigkeiten im Umgang mit dem WLS-Schätzer:\n\nwir müssen ein angemessenes Gewicht für jede Beobachtung wählen\n\nwenn die Gewichte nicht angemessen sind, ist WLS kein guter Schätzer\ndaher ist die WLS-Schätzung am besten, wenn die Stichprobengröße \\(N\\) groß ist oder wenn es mehrere Beobachtungen mit identischen \\(X\\)-Werten gibt (d.h. mehrere Personen mit den gleichen Ausprägungen auf dem jeweiligen Prädiktor)\n\nstandardisierte Effektgrößen, wie z.B. der Determinationskoeffizient \\(R^2\\), haben keine eindeutige Interpretation (wie bei OLS)\n\n\nDaher sollten wir eher OLS als WLS nutzen, außer wenn die Stichprobengröße sehr groß ist oder es ein großes Problem mit Heteroskedastizität gibt."
  },
  {
    "objectID": "Voraussetzungspruefung.html#unabhängigkeit-der-residuen",
    "href": "Voraussetzungspruefung.html#unabhängigkeit-der-residuen",
    "title": "14  Annahmen der Multiplen Linearen Regression",
    "section": "14.4 Unabhängigkeit der Residuen",
    "text": "14.4 Unabhängigkeit der Residuen\n\nDie Höhe des Residuums einer Beobachtung \\(\\hat e_i\\) hängt nicht von der Höhe des Residuums einer anderen Beobachtung \\(\\hat e_x\\) ab.\n\nWenn Residuen abhängig sind, kann sich das auch in Heteroskedastizität äußern.\nResiduen sind abhängig, wenn wir z.B. wiederholte Messungen oder Gruppenstrukturen untersuchen. Ersteres meint, dass mehrere Messungen von einer Person, d.h. zu mehreren Zeitpunkten, vorliegen. In diesem Fall sprechen wir auch von seriellen Abhängigkeiten. Zweiteres meint, dass die Daten “geclustert” sind und es somit systematische Zusammenhänge zwischen Personen gibt z.B. Schüler in einer Schulklasse. Beide Fälle müssen bei der Modellspezifikation berücksichtigt werden.\n\nIn unserem Beispiel könnte beispielsweise das Kriterium ‘Lebenszufriedenheit’ in Abhängigkeit vom Wohnort variieren. Damit wären die Daten geclustert.\nAbhängigkeit der Residuen führt zwar nicht zu verzerrten Regressionskoeffizienten, aber es kann zu verzerrten Standardfehlern führen.\n\n\n14.4.1 Überprüfung\nGanz grundsätzlich können wir diese Annahme zuerst einmal theoretisch überprüfen, indem wir uns das Studiendesign der erhobenen Daten anschauen.\nWenn nur zu einem Messzeitpunkt Daten erhoben wurden und jede Person nur einmal zu diesem befragt wurde, können wir serielle Abhängigkeiten ausschließen.\nBeim Clustering ist es etwas schwieriger, dieses nur durch Überlegung auszuschließen. Auch wenn im Studiendesign nicht vorgesehen war, dass Daten von unterschiedlichen Gruppen erhoben wurden, könnte es dennoch Ähnlichkeiten zwischen Personen hinsichtlich bestimmter Variablen geben. Um mögliche Verletzungen der Unabhängigkeitsannahme prüfen zu können, benötigen wir eine Vermutung darüber, welche Gruppierungsvariablen relevant sein könnten.\n\n\n14.4.1.1 Ausmaß serieller Abhängigkeit\nEs gibt mehrere Möglichkeiten, das Ausmaß serieller Abhängigkeit zu beurteilen. Im Folgenden werden zwei grafische Visualisierungen vorgestellt.\n\n\n14.4.1.1.1 Index Plot\nEinen Eindruck darüber, ob es systematische Abhängigkeiten zwischen den Residuen gibt, können wir mithilfe des Index Plot bekommen. Dieser ist ein Scatterplot, der die Residuen \\(\\hat e_i = y_i - \\hat y_i\\) (\\(y\\)-Achse) gegen den Index \\(i\\) (\\(x\\)-Achse), der zumeist durch die (aufsteigende) Zeilennummerierung repräsentiert wird, plottet.\nHierbei können wir u.a. visualisieren, ob es zeitliche Abhängigkeiten gibt, z.B. ob Personen, die später an der Befragung teilgenommen haben, systematische Unterschiede in ihrer Lebenszufriedenheit (Kriterium) zeigen.\nUm das besser beurteilen zu können, können wir auch hier wieder die Lowess Line einzeichnen. Diese sollte wieder möglichst horizontal zur \\(x\\)-Achse bei \\(y = 0\\) liegen. Beide Linien fügen wir zur Vereinfachung der Beurteilung des Plots hinzu.\n\nplot(lm_lz$residuals)\nlines(lowess(lm_lz$residuals), col = \"red\") # Lowess Line einfügen (in rot)\nabline( # Linie einzeichnen\n  h=0, # h - horizontal, 0 - Schnittpunkt mit y-Achse\n  col=\"grey\", # Farbe\n  lty=3) # Linien-Art: gestrichelt\n\n\n\n\nDie Abbildung impliziert, dass die Residuen unabhängig sind. Das entspricht unseren Vorüberlegungen, das wir nur Variablen haben, die zum gleichen Messzeitpunkt erhoben wurden.\n\n\n\nExkurs: Grafische Interpretationshilfe für den Index Plot\n\nFalls wir ein Messwiederholungsdesign haben und die Daten im Long-Format (d.h. Messungen einer Person in mehreren Zeilen) vorliegen, können wir die Messungen verschiedener Person farblich unterschiedlich darstellen.\nIn unserem Fall haben wir zwar keine Messwiederholung, aber wir illustrieren die farbliche Darstellung einmal am gleichen Beispiel.\nDie Farbe der geplotteten Elemente können wir mit dem Argument col verändern. Um die Farbe ab einer bestimmten Anzahl an Messungen (d.h. Messwiederholungen für eine Person) jeweils zu verändern, können wir rep() benutzen. Diese Funktion wiederholt das erste Argument x (bzw. die einzelnen Elemente des ersten Arguments) jeweils so oft wie das zweite Argument each.\n\nplot(lm_lz$residuals, col = rep(1:17, each = 10))\nlines(lowess(lm_lz$residuals), col = \"red\")\nabline(h=0, col=\"darkblue\", lty=3)\n\n\n\n\nDie 10 steht dafür, dass wir 10 Messwiederholungen haben. 1 bis 17 kodiert unterschiedliche Farbe. Alternativ können wir auch die Farbnamen in einem Vektor spezifizieren - dann würden wir anstatt 1:17 z.B. red', 'blue', 'green', ...) einfügen.\nMit dem Argument pch können wir analog die Form der Punkte ändern.\n\nplot(lm_lz$residuals, pch = rep(1:17, each = 10))\nlines(lowess(lm_lz$residuals), col = \"red\")\nabline(h=0, col=\"darkblue\", lty=3)\n\n\n\n\nWenn es starke serielle Abhängigkeiten geben würde, würden sich die Fälle sichtlich voneinander unterscheiden bzw. Cluster bilden. Illustriert ist das im folgenden Plot:\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.4.1.2 Ausmaß des Clusterings\nZu allererst können wir uns den Intraklassenkoeffizienten (IKK) anschauen. Dieser schätzt den Anteil der Varianz einer Variable, die durch Gruppenzugehörigkeit (Clustering) erklärt wird.\n\nAchtung: Der IKK sollte allerdings nicht als einzige Methode zur Überprüfung des Clusterings herangezogen werden, weil wir uns hier nur Mittelwertsunterschiede anschauen. Daher wird noch eine weitere Methode, die Betrachtung von Boxplots der geclusterten Gruppen, illustriert.\n\n\n14.4.1.2.1 Intraklassenkorrelation\nBeispielsweise können wir mit Hilfe des IKK beurteilen, wie stark sich Personen innerhalb einer Gruppe ähneln.\n\n\nJe näher der Wert an \\(0\\) ist, desto geringer ist der relative Anteil an Unterschieden zwischen den Gruppen (d.h. desto weniger Clustering liegt vor).\nJe näher der Wert an \\(1\\) ist, desto geringer ist der relative Anteil an Unterschieden innerhalb einer Gruppe (d.h. desto mehr Clustering liegt vor).\n\nZur Überprüfung können wir z.B. die Funktion ICCest aus dem Paket ICC nutzen.\nFür unser Beispiel schauen wir uns exemplarisch an, inwieweit Unterschiede in ‘Lebenszufriedenheit’ (Kriterium) durch ‘Gruppenzugehörigkeit im Wintersemester’ (erstis$gruppe) erklärt werden können.\nDazu fügen wir diese Variable zu unserem Datensatz hinzu.\n\ndaten$gruppe <- erstis$gruppe\n\nlibrary(ICC)\nICCest(daten$gruppe, daten$leb_zufr)\n\nNAs removed from rows:\n 27 140 \n\n\n$ICC\n[1] 0.005398465\n\n$LowerCI\n[1] -0.01305084\n\n$UpperCI\n[1] 0.2595086\n\n$N\n[1] 4\n\n$k\n[1] 47.05115\n\n$varw\n[1] 31.43056\n\n$vara\n[1] 0.1705978\n\n\nDie Funktion gibt uns folgende Outputs:\n\nwelche Missings (d.h. Zeilen) entfernt wurden\n$ICC: die IKK (Punktschätzung)\n$LowerCI und $UpperCI: die Grenzen des Konfidenzintervalls für den IKK (Intervallschätzung)\n$N: die Anzahl an Gruppen\n$k: die Anzahl der Personen in jeder Gruppe\n$varw: die Varianz innerhalb einer Gruppe\n$vara: die Varianz zwischen den Gruppen\n\n\n\n\n\n\n\nBei unbalancierter Gruppengröße ist k, die Anzahl der Personen in jeder Gruppe, kleiner als die mittlere Gruppengröße. Für mehr Informationen dazu schau dir die R-Dokumentation für ICCest() an.\n\nDie IKK weist darauf hin, dass wenig Varianz durch die Kurszugehörigkeit erklärt wird. Es ist also nicht notwendig, dass wir die Gruppenzugehörigkeit in unserem Modell berücksichtigen.\n\n\n14.4.1.2.2 Boxplots der geclusterten Gruppen\nZuletzt schauen wir uns die Boxplots der Verteilungen der Residuen in den Gruppen an.\nZur Erleichterung der Interpretation zeichnen wir wieder mit Hilfe von abline(h=0, col=\"darkblue\", type=\"l\", lty=3) eine Senkrechte bei \\(y=0\\) (weil der erwartete Mittelwert der Residuen \\(0\\) ist).\n\nplot(daten$resid ~ daten$gruppe)\nabline(h=0, col=\"darkblue\", lty=3)\n\n\n\n\nDie Interpretation hier ist ähnlich zu den Resiualplots: Der Median der einzelnen Gruppen sollte jeweils nah an der gestrichelten Linie bei \\(y=0\\) sein. Zusätzlich sollten die Verteilungen sich stark überlappen.\nDie Verteilungen der Residuen in den Gruppen sieht sehr ähnlich aus. Die Verteilungen überlappen sich deutlich. Es scheint keine systematischen Streuungsunterschiede in den Residuen in Abhängigkeit der Gruppenzugehörigkeit zu geben.\n\n\n\n\n14.4.2 Umgang\n\n14.4.2.1 Serielle Abhängigkeit\nLiegen serielle Abhängigkeiten vor, müssen wir auf geeignete Verfahren zur Analyse von Längsschnittdaten zurückgreifen (z.B. gemischte lineare Modelle).\n\n\n14.4.2.2 Clustering\nLiegen Abhängigkeiten aufgrund von Gruppenunterschieden vor, können wir versuchen, die Gruppenunterschiede, in Form von weiteren Variablen, ins Modell aufzunehmen. Gelingt dies nicht, müssen wir auf geeignete Verfahren zur Analyse von „geclusterten“ Daten zurückgreifen (z.B. gemischte lineare Modelle)."
  },
  {
    "objectID": "Voraussetzungspruefung.html#weitere-wichtige-aspekte",
    "href": "Voraussetzungspruefung.html#weitere-wichtige-aspekte",
    "title": "14  Annahmen der Multiplen Linearen Regression",
    "section": "14.5 Weitere wichtige Aspekte",
    "text": "14.5 Weitere wichtige Aspekte\n\n14.5.1 Normalverteilung der Residuen\nDie Annahme, dass die Residuen in der Population normalverteilt sind, ist eigentlich gar keine Annahme im engeren Sinne. Aufgrund des zentralen Grenzwertsatzes sind die Regressionskoeffizienten in großen Stichproben selbst dann asymptotisch normalverteilt, wenn die Annahme nicht erfüllt ist. Daher ist eine Verletzung der Annahme eher in kleineren Stichproben problematisch. Da in der Praxis aber unklar ist, wann eine Stichprobe als groß genug anzusehen ist, ist es ratsam, die Verteilung der Residuen immer auf Abweichung von einer Normalverteilung hin zu prüfen.\nWenn die Normalverteilungsannahme der Residuen verletzt ist, ist der Standardfehler womöglich verzerrt, was zu falschen inferenzstatistischen Schlüssen führen kann.\nNicht normalverteilte Residuen können auch auf andere Probleme wie Modellmissspezifikation hinweisen.\n\n14.5.1.1 Überprüfung\n\n14.5.1.1.1 Histogramm\nMit Hilfe eines Histogramms können wir uns die Häufigkeitsverteilung einer metrischen Variable anzeigen lassen. Dafür wird diese in verschiedene Klassen (‘bins’) eingeteilt. Das schauen wir uns für die Residuen an.\n\nhist(lm_lz$residuals)\n\n\n\n\nOptisch sollte ungefähr eine Normalverteilung zu erkennen sein.\n\nIn unserem Beispiel sieht es jedoch nach einer linksschiefen (d.h. rechtssteilen) Verteilung der Residuen aus.\n\n\nExkurs: Graphische Interpretationshilfe für das Histogramm\n\nUm die Beurteilung des Histogramms zu erleichtern, können wir zwei Linien einzeichnen - die reale Dichtefunktion der Variablen und die erwartete Dichtefunktion, wenn die Variable normalverteilt wäre. Diese können wir vergleichen, um die Annahme der Normalverteilung zu überprüfen.\nEine wichtige Voraussetzung dafür, dass wir über das Histogramm die Dichtefunktionen legen können, ist dass wir das Argument prob=TRUE setzen.\n\nDann zeichnen wir die reale Dichtefunktion der Residuen ein. Dafür kombinieren wir line() und density(). Erstere zeichnet eine Linie in eine Grafik ein und mit zweiterer spezifizieren wir, dass die Linie die geschätzte Dichte (der Residuen) visualisieren soll.\nFür die normalverteilte Dichtefunktion der Residuen nutzen wir curve() und dnorm(). Mit ersterer wird eine Kurve auf Basis einer mathematischen Funktion eingezeichnet. Mit zweiterer geben wir an, dass es eine Dichtefunktion einer Normalverteilung sein soll, für die wir Mittelwert und Standardabweichung (der Residuen) angeben. Zusätzlich müssen wir hier mit add=TRUE festlegen, dass die Kurve über das Histogramm gelegt werden soll.\nUm die Normalverteilung besser anschauen zu können, erweitern wir die \\(x\\)-Achse. Das machen wir mit xlim=c(-15, >15). Die Wahl der Grenzen von \\(-15\\) bis \\(+ 15\\) - ist eine theoretische Überlegung. Bei einer Normalverteilung liegen ~ \\(99\\)% der Werte im Bereich \\(+/- 3\\) Standardabweichungen vom Mittelwert entfernt. Die Standardabweichung der Residuen beträgt ~ 5.105.\nFür eine bessere Übersichtlichkeit stellen wir die  reale  Dichtefunktion und die  erwartete Dichtefunktion  farblich dar.\n\nhist(lm_lz$residuals, # Histogramm der Residuen ...\n     prob = TRUE, # ... als Dichte ...\n     xlim = c(-20, 20)) # ... mit veränderter X-Achse\n\nlines( # erstellt eine Linie ...\n  density( # ... auf Basis der geschätzten Dichtefunktion ...\n    lm_lz$residuals), # ... der Residuen ...\n  col = \"orange\")  # ... in orange\n\n\ncurve( # erstellt eine Kurve ...\n  dnorm( # ... einer normalverteilten Dichtefunktion ...\n    x, mean = mean(lm_lz$residuals), # ... auf Basis des Mittelwerts ...\n    sd = sd(lm_lz$residuals)), # ... und der Standardabweichung der Residuen ...\n      col = \"green\", #... in grün ...\n  add = TRUE) # ... die zu bestehendem Plot hinzugefügt wird\n\n\n\n\nAnhand dieser beiden Linien wird noch deutlicher, dass die Verteilung der Residuen in unserem Beispiel von einer Normalverteilung abweicht.\n\n\n\n14.5.1.1.2 QQ-Plot\nDer QQ-Plot (Quantil-Quantil-Plot) plottet die aufsteigend geordneten standardisierten Residuen gegen die korrespondierenden Quantile der Normalverteilung. Dafür werden die Residuen durch den Standardschätzfehler geteilt \\(\\frac {e_i} {\\hat {\\sigma}}\\).\n\nplot(lm_lz, which = 2) \n# zweiter Plot der plot()-Funktion für ein lm-Objekt ist der QQ-Plot\nabline(v = 1, col = \"blue\") # zur Illustration für unser Beispiel\n\n\n\n\nIn der obigen Abbildung werden die theoretischen Quantile der Normalverteilung (\\(x\\)-Achse) gegen die standardisierten Residuen (\\(y\\)-Achse) abgetragen. Wenn die Residuen normalverteilt sind, sollten die Punkte ungefähr auf der winkelhalbierenden Geraden (gestrichelte Linie) liegen. Kleinere Abweichungen, vor allem an den Enden der Geraden (unten links und oben rechts) sind in der Praxis aber nicht ungewöhnlich und oftmals nicht weiter problematisch.\nIn unserem Beispiel weichen viele Punkte mit höheren Ausprägungen (rechts von der blauen Linie) von der winkelhalbierenden Geraden ab, was für eine gewisse Verletzung der Normalverteilungsannahme spricht.\nWenn wir sehen wollen, wie der QQ-Plot bei verschiedenen Verteilungsformen aussehen kann, können wir uns diesen Forumseintrag zu QQ-Plots anschauen.\n\n\nExkurs: Shapiro-Wilk-Test\n\nWir haben eingangs schon angerissen, warum graphische Verfahren besser geeignet sind, um Annahmen zu überprüfen. Für die Überprüfung der Normalverteilung der Residuen mittels statistischer Tests gelten die gleichen Vorbehalte: Wenn die Stichprobe groß ist, haben wir viel Power auch kleinste Abweichungen von der Normalverteilung zu finden. Gerade in dieser Situation (d.h. bei grossem \\(N\\)) sind Abweichungen von der Normalverteilung aber gar nicht so problematisch. Bei kleinem \\(N\\) hingegen (wenn Abweichungen potentiell gefährlich sind), fehlt dem Test dann aber oft die Teststärke Abweichungen korrekt zu erkennen.\nMit dem Shapiro-Wilk-Test kann getestet werden, ob Daten normaltverteilt sind (\\(H_0\\)) oder nicht (\\(H_1\\)).\nDer Test wird mit der Funktion shapiro.test() durchgeführt. Um die Residuen der Regression zu testen, extrahieren wir diese wieder aus dem Ergebnisobjekt mit lm_lz$residuals.\n\nshapiro.test(lm_lz$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm_lz$residuals\nW = 0.96198, p-value = 0.0001845\n\n\nIn unserem Beispiel kann die Nullhypothese abgelehnt werden, d.h. dass die Residuen nicht normalverteilt sind.\n\n\n\n\n14.5.1.2 Umgang\nZuerst sollten wir überprüfen, ob die Abweichung von der Normalverteilung auf eine Missspezifikation des Modells zurückzuführen ist.\nWenn das nicht zutrifft und die Stichprobe klein ist, sollten wir korrektive Maßnahmen einleiten. Wir könnten die Daten transformieren, wie bei Umgang mit Nicht-Linearität, allerdings könnte das auch die getesteten Hypothesen ändern, z.B. vergleichen wir nach der log-Transformation keine arithmetischen Mittel mehr sondern geometrische Mittel. Alternativ könnten wir auch robuste Testverfahren anwenden.\n\n\n\n14.5.2 Multikollinearität\n\nWenn Prädiktoren sehr hoch miteinander korrelieren, spricht man von Multikollinearität. Dabei können wir außerdem zwischen non-essentieller und essentieller Mutlikollinearität unterscheiden.\n\nNon-essentielle Multikollinearität entsteht dadurch, dass Prädiktoren nicht zentriert sind.\nEssentielle Multikollinearität entsteht durch tatsächliche Zusammenhänge zwischen Variablen in der Population. Diese Abhängigkeiten zwischen Prädiktoren äußern sich dadurch, dass die Varianz eines Prädiktors großteilig durch die anderen Prädiktoren erklärt werden kann, er sich also aus einer Linearkombination der anderen Prädiktoren ergibt z.B. \\(X_1=1,2,3\\) und \\(X_2=2,4,6\\).\n\nBeispielsweise würde zwischen ‘Größe in cm’ und ‘Größe in m’ perfekte Multikollinearität bestehen, weil beide Vielfache voneinander sind.\n\nMultikollinearität kann aber beispielsweise auch vorliegen, wenn die Anzahl an Prädiktoren \\(k\\) größer ist als die Größe der Stichprobe \\(N\\) oder wenn wir im Modell versehentlich zweimal den gleichen Prädiktor spezifiziert haben. Multikollinearität tritt außerdem häufig bei Interaktionen zwischen Prädiktoren auf. Das kommt daher, dass eine Interaktion ein Produkt aus zwei Prädiktoren ist und folglich viel Gemeinsamkeit mit beiden Prädiktoren hat.\nBei sehr hohen Korrelationen zwischen den Prädiktoren, ergeben sich folgende Probleme:\n\nErschwerte Interpretation der partiellen Korrelationen. Weil sich die Prädiktoren einen großen Anteil an der erklärten Varianz des Kriteriums teilen ist unklar, welchem Prädiktor welcher Anteil zugeschrieben werden soll. Um die Anteile der Varianzaufklärung besser den Prädiktoren zuordnen zu können, könnten wir die Reihenfolge der Aufnahme der Prädiktoren im Modell ändern (hierarchisches Vorgehen). Dabei schauen wir uns die Differenzen der Determinationskoeffizienten \\(R^2_{k+m} - R^2_k\\) der Modelle (die sich durch einen weiteren Prädiktor \\(m\\) unterscheiden) an.\nErhöhte Standardfehler. Die Standardfehler der von Multikollinearität betroffenen Regressionkoeffizienten vergrößern sich (siehe Toleranz und Formel des Standardfehlers der Regressionsgewichte), was wiederum zu breiteren Konfidenzintervallen und einer geringeren Wahrscheinlichkeit führt, die Nullhypothese abzulehnen, wenn tatsächlich ein Effekt vorliegt (d.h. geringere Power).\n\n\n\n\n\n\n\n\n\n\nFormel des Standardfehlers der Regressionsgewichte:\n\\(SE_{b_k} = \\frac{sd_y}{sd_x} \\cdot \\sqrt{\\frac{1}{1-R^2_k}} \\cdot \\sqrt{\\frac{1-R^2_y}{N - K - 1}}\\)\nToleranz: \\(1-R^2_k\\)\nIndeterminationskoeffizient: \\(1-R^2_y\\)\n\n\n14.5.2.1 Überprüfung\nUm festzustellen, ob Prädiktoren hoch miteinander korreliert sind, können wir uns die Toleranz und den Variance Inflation Factor anschauen. Beide hängen direkt miteinander zusammen, aber ihre Interpretationen sind unterschiedlich.\nFalls wir ein ernsthaftes Problem mit Multikollinearität haben, wird uns das in R teilweise mit z.B. aliased coefficients oder 1 coefficient not defined because of singularities angezeigt.\nIm Paket mctest gibt es zwei Funktionen, die helfen, Multikollinearität zu entdecken - omctest() - und zu lokalisieren - imctest(). Hierfür werden jeweils verschiedene Maße zu Rate gezogen u.a. auch die Toleranz und der VIF. Da wir die anderen Maße aber nicht vertiefend behandeln, sei an dieser Stelle nur auf das Paket hingewiesen. Bei Interesse empfielt es sich, den Artikel der Entwickler mctest: An R Package for Detection of Multicollinearity among Regressors zu lesen, in dem die Maße kurz erklärt und weiterführende Quellen angegeben werden.\n\n14.5.2.1.1 Toleranz\n\nDie Toleranz sagt uns, wie viel Varianz in \\(X_j\\) unabhängig von den anderen Prädiktoren ist.\n\nDafür wird eine Regression von Prädiktor \\(X_j\\) auf alle anderen Prädiktoren im Modell gerechnet (d.h. das Kriterium bleibt außen vor) und davon der Kehrwert gebildet. Sie ist somit ein Maß für die Uniqueness von \\(X_j\\). Die Tolreanz wird wie folgt berechnet: \\(1 - R^2_j\\)\nIn R können wir die Toleranz z.B. über den Kehrwert des Variance Inflation Factors, mit der Funktion vif() aus dem Paket car, berechnen. Der Funktion übergeben wir unser lm-Objekt.\n\nlibrary(car)\n1 / vif(lm_lz)\n\ndaten$zufr_inhalt daten$zufr_beding \n        0.8988664         0.8988664 \n\n\n\n\n\nJe kleiner die Toleranz ist, desto größer das Problem mit Multikollinearität. \\(Tol_j = 0\\) impliziert perfekte Multikollinearität. \\(Tol_j~ < 0.10\\) deutet auf ein ernsthaftes Problem mit Multikollinearität hin.\nUngefähr 89.9% der Varianz in Zufriedenheit mit Studienbedingungen ist unabhängig von Zufriedenheit mit Studieninhalten (für den Fall von genau zwei Prädiktoren, wie in unserem Beispiel, gilt auch die umgekehrte Interpretation).\n\n\n\n14.5.2.1.2 Variance Inflation Factor\n\n\\(VIF_j\\) gibt an, um wie viel die Varianz des Regressionskoeffizients \\(X_j\\) (durch die Korrelation mit den anderen Prädiktoren) erhöht wird, verglichen mit dem Fall, dass alle Prädiktoren unkorreliert sind.\n\n\\(\\sqrt {VIF_j}\\) gibt an, um welchen Faktor sich der Standardfehler \\(SE_{b_j}\\) durch Einschluss weiterer korrelierter Prädiktoren erhöht (verglichen mit dem Fall, dass alle Prädiktoren unkorreliert sind).\nDer VIF wird für jeden Prädiktor im Modell berechnet. Er wird folgendermaßen bestimmt: \\(\\frac{1}{1 - R^2_j}\\), wobei \\(R^2_j\\) der Determinationskoeffizient der Regression des \\(j\\)-ten Prädiktors auf alle anderen \\((k - 1)\\) Prädiktoren ist. Das entspricht der quadrierten multiplen Korrelation zwischen \\(X_j\\) und allen anderen Prädiktoren.\n\nAchtung: Im Fall von zwei Prädiktoren bleibt die geteilte Varianz zwischen beiden Variablen gleich; egal welche Variable Kriterium und welche Prädiktor ist. Daher sind beide VIF gleich.\n\nDazu nutzen wir aus dem Paket car die Funktion vif(), welcher wir unser lm-Objekt übergeben.\n\nlibrary(car)\nvif(lm_lz)\n\ndaten$zufr_inhalt daten$zufr_beding \n         1.112512          1.112512 \n\n\nAls Daumenregel gilt, dass ein VIF größer als 10 auf ein Problem mit Multikollinearit hindeutet.\nDer VIF in unserem Beispiel ist sehr klein, d.h. dass unsere beiden Prädiktoren Zufriedenheit mit Studieninhalten und Zufriedenheit mit Studienbedingungen nicht viel gemeinsame Varianz teilen.\n\n\n\n\n14.5.2.2 Umgang\nIn einer Regression, die auch Interaktionen zwischen den Prädiktoren annimmt, können wir die Prädiktoren zentrieren, um non-essentieller Multikollinearität beizukommen. Hierbei sollten wir aber bei Vorhandensein von Dummyvariablen vorsichtig sein. Diese müssen nicht zentriert werden und eine Zentrierung erschwert zusätzlich ihre Interpretation.\nBei hoher essentieller Multikollinearität ist es oftmals ratsam, Prädiktoren auszuschließen, wenn diese weitgehend redundante Informationen liefern.\n\n\n\n14.5.3 Extreme Werte und einflussreiche Datenpunkte\n\nExtreme Werte, auch Ausreißer (Outlier) genannt, meinen untypische Datenpunkte, die nicht zum Rest der Daten passen. Diese können sowohl auf dem Kriterium als auch auf den Prädiktoren vorkommen.\n\n\n\nWenn ein extremer Wert auf dem Kriterium vorkommt, heißt das, dass die Prädiktorwerte einer Person \\(i\\) zwar im Wertebereich der anderen Personen liegen, aber der beobachtete Wert \\(y_i\\) stark vom vorhergesagten \\(\\hat y_i\\) abweicht (Abweichung in \\(Y\\)).\nWenn extreme Werte auf den Prädiktoren vorkommen, heißt das, dass die Prädiktorwerte einer Person \\(i\\) nicht im Wertebereich der anderen Personen liegen (Abweichung in \\(X\\)), aber der beobachtete Wert \\(y_i\\) dennoch nah am vorhergesagten \\(\\hat y_i\\) ist. Extreme Werte auf den Prädiktoren haben zusätzlich eine hohe Leverage/Hebelwirkung und damit potenziell einen starken Einfluss auf die Regressionsgerade.\n\n\n\n\n\n\n\n\n\nDie Leverage ist die absolute Abweichung eines beobachteten Wertes einer Person vom Mittel aller Prädiktoren \\(X\\).\n\nIn den folgenden drei Abbildungen sehen wir verdeutlicht, was mit extremen Abweichungen auf dem Kriterium bzw. den Prädiktoren gemeint ist. Dazu wurden exemplarisch Lebenszufriedenheit (\\(Y\\)) und Zufriedenheit mit Studieninhalten (\\(X_1\\)) gegeneinander geplottet. Hierbei wurde jeweils der \\(Y\\)- bzw. \\(X_1\\)-Wert von  Person 50  geändert. Die gestrichelte Linie stellt dabei jeweils die obere Grenze des Wertebereichs des Kriteriums bzw. des Prädiktors dar.\n\nSpäter greifen wir nochmal auf das gleiche Beispiel zurück, um den Unterschied und die Konsequenzen von extremen Abweichungen auf dem Kriterium bzw. den Prädiktoren noch mehr zu verdeutlichen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWenn die Regressionsgerade stark durch einzelne Beobachtungen beeinflusst wird, bezeichnet man diese als einflussreiche Datenpunkte (influentials). Der Ausschluss dieser Beobachtungen würde stark abweichende Parameterschätzungen hervorbringen. Das gleichzeitige Vorhandensein von Ausreißern und Beobachtungen mit hoher Leverage könnten die Ursache dafür sein.\n\n14.5.3.1 Überprüfung\nStoßen wir über auffällige Werte in unserem Datensatz, sollte wir zunächst kontrollieren, ob diese durch Eingabefehler oder fehlerhafte Kodierungen fehlender Werte (Missings) zustande gekommen sind.\nBeispielsweise werden in manchen Anwendungen Missings nicht mit NA, sondern z.B. wie bei Unipark mit 99 oder -99 kodiert. Wir müssen diese vor der Auswertung auf NA umkodieren, da R diese sonst nicht als Missings erkennt. Für mehr Informationen dazu können wir im Kapitel zu Fehlenden Werten nachschauen.\nStark abweichende Werte lassen sich oft einfach mit Hilfe von Plots oder Deskriptivstatistiken identifizieren.\n\n14.5.3.1.1 Extreme Werte auf dem Kriterium\nHierbei schaut man sich vor allem die Residuen an.\n\n14.5.3.1.1.1 Plot der studentisierten gelöschten Residuen\nZur Exploration von Ausreißern auf dem Kriterium können wir uns die studentisierten gelöschten Residuen, geplottet gegen den Index \\(i\\), anschauen.\n\nStudentisierte gelöschte Residuen werden auch als externally studentized residuals bezeichnet.\n\nStudentisierte gelöschte Residuen werden wie folgt berechnet: \\(\\frac {e_i} {\\hat {\\sigma} \\cdot \\sqrt{1 - h_m}}\\)  \nSie sind theoretisch \\(t\\)-verteilt mit \\(df = N − k − 1\\) (N = Stichprobengröße, k = Anzahl Prädiktoren). Wir erhalten die Anzahl der Freiheitsgrade aus dem lm-Objekt mittels summary(lm_lz)$fstat[\"dendf\"].\n\nStudentisiert heißt, dass die Residuen durch die geschätzte Populationsstandardabweichung der Residuen an der Stelle \\(x_m\\) (das meint den Hebelwert der Person \\(h_m\\)) geteilt werden.\n\n\nGelöscht bedeutet geschätzte Abweichung des vorhergesagten Wertes vom beobachteten Wert (das sind die normalen Residuen) in einem Modell ohne die entsprechende Person. Wir sagen also \\(\\hat y_i\\) für Person \\(i\\) mit Hilfe eines Modells hervor, für welches Person \\(i\\) nicht in die Parameterschätzung mit eingegangen ist.\n\n\nFür mehr Informationen zu studentisierten gelöschten Residuen (externally studentized residuals) siehe S.399ff im Lehrbuch von Cohen, Cohen, West & Aiken (2003).\nWir extrahieren diese mit rstudent() aus dem lm-Objekt.\n\nplot(rstudent(lm_lz))\n\n\n\n\nEs gibt keine einheitlichen Richtlinien darüber, ab wann ein studentisiertes gelöschtes Residuum als extrem groß anzusehen ist. Wir schauen nur, ob einzelne Werte stark vom Cluster der anderen Werte abweichen.\nIn unserem Beispiel gibt es keine Werte, die extrem von der Verteilung der anderen abweichen. Wir schauen uns dennoch exemplarisch einmal die zwei größten Werte an (d.h. die kleiner gleich - 3 sind).\nIn folgender Abbildung schauen wir uns an, wie eine extreme Abweichung eines studentisierten gelöschten Residuums aussehen könnte. In folgendem Plot wurde nur ein studentisiertes gelöschtes Residuum eingefügt; die anderen Werte sind gleich geblieben.\n\n\n\n\n\n\n\n\n\n\nwhich(unname(rstudent(lm_lz) <= -3))  # Indizes aller Werte, die kleiner gleich -3 sind\n\n[1]  37 154\n\n\nMit which() lassen wir uns die Indizes derjenigen studentisierten gelöschten Residuen ausgeben, die \\(\\leq-3\\) sind.\nWir nutzen außerdem unname(), weil rstudent() einen benannten numerischen Vektor (named num) erstellt und bei Nutzung von which() die Indizes sonst doppelt ausgegeben werden würden. Bei named nums werden die Indizes nämlich schon im Vektor mitgespeichert (ohne dass dieser dadurch seine Dimensionalität ändert).\n\n\n14.5.3.1.1.2 Outlier Test\nAlternativ könnten wir auch einen Signifikanztest nur für das größte studentisierte gelöschte Residuum durchführen. Dabei wird dessen p-Wert mit einer Bonferroni-Korrektur angepasst. Dazu wird der p-Wert mit der Stichprobengröße (hier: \\(N=164\\)) multipliziert. Das an der Stichprobengröße relativierte (d.h. durch dieses geteilt) Signifikanzlevel \\(\\alpha\\) markiert den kritischen Wert (d.h. das korrigierte Signifkanzlevel).\nDazu nutzen wir outlierTest() aus dem Paket car. Wir nutzen konventionell \\(\\alpha=0.05\\).\n\nlibrary(car)\noutlierTest(lm_lz)\n\nNo Studentized residuals with Bonferroni p < 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n37 -3.053226          0.0026521      0.43494\n\n\nDer Test liefert Evidenz dafür, dass das größte studentisierte gelöschte Residuum nicht extrem von den anderen abweicht.\n\n\n\n14.5.3.1.2 Extreme Werte auf den Prädiktoren\nHierbei schauen wir uns vor allem die Hebelwerte \\(h_{ii}\\) einzelner Beobachtungen (engl. leverage oder hatvalues) an.\n\nHebelwerte geben die absolute Abweichung eines beobachteten Wertes einer Person vom Mittel aller Prädiktoren \\(X\\) (dem sog. Schwerpunkt) an.\n\nFälle mit großer Hebelwirkung haben potenziell einen größeren Einfluss auf die Parameterschätzungen (\\(0 < h_{ii} < 1\\)).\n\n14.5.3.1.2.1 Plot und Histogramm der Hebelwerte\nIn kleinen bis mittelgroßen Datensätzen ist ein Indexplot der Hebelwerte eine ausreichende Methode, um Ausreißer auf den Prädiktoren zu erkennen.\n\nDazu extrahieren wir die Hebelwerte mit Hilfe der Funktion hatvalues() aus dem lm-Objekt. Damit fehlende Werte (NA) ausgeschlossen werden (ihre Hebelwirkung würde \\(0\\) betragen), wenden wir außerdem [!is.na()] an. is.na() evaluiert, ob die Elemente vorhanden sind (TRUE oder FALSE), ! sorgt dafür, dass keine Missings angewählt werden, und [ ] gibt die Indizes aus.\n\nplot(hatvalues(lm_lz)[!is.na(daten$resid)])\n\n\n\n\nIn größeren Datensätzen sollten wir die Hebelwerte der Übersichtlichkeit halber in einem Histogramm visualisieren.\n\nhist(hatvalues(lm_lz)[!is.na(daten$resid)])\n\n\n\n\nWir sollten uns hierbei auf die Inspektion weniger, extremer Werte beschränken und vor allem nur auf solche, die weit entfernt von den restlichen (nah beieinander liegenden Werten) sind.\nDen größten Hebelwert und seinen Index können wir uns folgendermaßen ausgeben lassen:\n\nsort((hatvalues(lm_lz)), decreasing=TRUE)[1]\n\n       151 \n0.06632099 \n\n\nWenn wir z.B. die größten drei Werte betrachten möchte, schreiben wir [1:3] anstatt [1]. Wenn wir alle Werte absteigend (weil decreasing=TRUE) betrachten möchten, lassen wir die eckigen Klammern mit den Indizes komplett weg.\nDer Hebelwert von Person 151 liegt etwas abseits von der Verteilung der anderen. Noch extremere Werte sollten wir uns immer genauer anschauen. In unserem Beispiel liegen aber keine offensichtlich extremen Hebelwerte vor.\n\n\n\n14.5.3.1.3 Einflussreiche Datenpunkte\n\n14.5.3.1.3.1 Residuen-Leverage Plot\nWenn es große Residuen und Hebelwerte gibt, können wir uns weiterführend den Residuen-Leverage Plot anschauen. Dieser plottet die Hebelwerte (\\(x\\)-Achse) gegen die standardisierten Residuen (\\(y\\)-Achse).\nZur Standardisierung werden die Residuen durch den Standardschätzfehler geteilt \\(\\frac {e_i} {\\hat {\\sigma}}\\)\n\nplot(lm_lz, which=5)\n\n\n\n# fünfter Plot der plot()-Funktion für ein lm-Objekt ist der Residuen-Leverage Plot\n\nDie Lowess Line sollte auch hier wieder flach und nah an der gestrichelten Linie bei \\(y=0\\) sein.\nDie Werte der Person mit dem Index 50 weisen eine leicht erhöhte hohe Cooks Distance auf. Allerdings sind die Grenzen bei 0.5 und 1 nicht einmal sichtbar, weil alle Werte der Cooks Distance relativ klein sind. Auch die Lowess Line ist flach an nah bei y=0. Beides spricht dafür, dass in unserem Datensatz keine einflussreichen Datenpunkte, die die Parameter verzerren könnten, enthalten sind.\n\nIn diesem Forumseintrag wird der Residuen-Leverage-Plot noch ausführlicher erklärt.\n\n\nWas ist die Cooks Distance? Und wie hängt diese mit den standardisierten Residuen, der Leverage und der Lowess Fit Line zusammen?\n\n\nDie Cooks Distance ist ein Maß dafür, wie sich die Regressionsgerade und damit die vorhergesagten Werte ändern würden, wenn wir die Daten einer betrachteten Person \\(i\\) (\\(Cooks D_i\\)) ausschliessen würden, wobei \\(Cooks D_i \\geq 0\\) ist.\n\nJe größer die \\(Cooks D_i\\), desto größer der Einfluss der Beobachtungen von Person \\(i\\).\nIm Residuen-Leverage-Plot werden auch Linien der Cooks Distance mit den Werten \\(0.5\\) und \\(1.0\\) abgetragen, wenn Beobachtungen in die Nähe dieser Grenzen kommen (was im oberen Beispiel nicht der Fall ist).\nWir können uns folgendermaßen auch nur die Werte der Cooks Distance gegen den Index geplottet anschauen:\n\nplot(lm_lz, which=4)\n\n\n\n\n\n\n\n\nSchauen wir uns den Residuen-Leverage-Plot noch einmal an.\nIn den folgenden Abbildungen wurden die Werte von Person 50 auf dem Kriterium und einem Prädiktor (Zufriedenheit mit Studieninhalten) jeweils einzeln und anschließend gemeinsam manipuliert, um den jeweiligen Einfluss im Vergleich zum bestehenden Datensatz (der schon weiter oben dargestellt war) zu veranschaulichen.\nIn der Einführung zu Extremen Werte und einflussreichen Datenpunkte haben wir den Unterschied zwischen Ausreißern auf dem Kriterium und den Prädiktoren schon angesprochen. Die Abbildungen sollen deren Implikationen noch verdeutlichen. Bei der Interpretation helfen die durchgezogene Lowess Fit Line und die rot gestrichelten Grenzwerte der Cooks Distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDas standardisierte Residuum ist größer; die Leverage ist gleich.\nDie Lowess Line ist immer noch flach und nah an der gestrichelten Linie bei \\(y=0\\).\nDie Cooks Distance ist wesentlich größer, sie beträgt fast \\(1\\).\n\n\n\n\n\n\n\n\n\n\nDas standardisierte Residuum ist vom Betrag her ähnlich; die Leverage ist größer.\nDie Lowess Line weicht wesentlich stärker von der gestrichelten Linie bei \\(y=0\\) ab.\nDie Cooks Distance ist etwas größer, aber noch \\(< 0.5\\).\n\n\n\n\n\n\n\n\n\n\nDas standardisierte Residuum ist größer; die Leverage ist größer.\nDie Lowess Line weicht extrem von der gestrichelten Linie bei \\(y=0\\) ab.\nDie Cooks Distance ist \\(> 1\\).\n\n\n\n\n\n\n14.5.3.2 Umgang\n\nAchtung: Wir sollten abweichende Beobachtungen nicht unbedacht aus der Analyse entfernen. Sie können nicht nur durch fehlerhafte Messung, sondern auch korrekte, aber seltene Messungen zutande gekommen sein (z.B. weil wenig Leute eine gewisse Ausprägung auf einer bestimmten Variable besitzen).\n\n\nAußerdem gibt es keine einheitlichen Richtlinien darüber, ab wann wir Ausreißer und einflussreiche Datenpunkte entfernen sollten. Wir sollte diese immer mit Hinblick auf die Gesamtverteilung bewerten. Jeder Ausschluss sollte plausibel begründet werden können.\nAm besten ist es immer, zu überprüfen, ob wir robuste Ergebnisse vorliegen haben. Wir führen die vorgestellten Analysen mit und ohne kritische Werte durch und schauen uns an, was passiert. Ändern sich die Befunde nicht, sind wir wahrscheinlich auf der sicheren Seite."
  },
  {
    "objectID": "Voraussetzungspruefung.html#literatur-weiterführende-hilfe",
    "href": "Voraussetzungspruefung.html#literatur-weiterführende-hilfe",
    "title": "14  Annahmen der Multiplen Linearen Regression",
    "section": "14.6 Literatur & weiterführende Hilfe",
    "text": "14.6 Literatur & weiterführende Hilfe\nDieses Kapitel basiert größtenteils auf dem in der Vorlesung Multivariate Verfahren des Masterstudiengangs Psychologie genutzen Lehrbuch:\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences Hillsdale, NJ: Erlbaum.\nAbschnitt 4.3 Assumptions and Ordinary Least Squares Regresion (S.117-125)\nAbschnitt 4.4 Detecting Violations of Assumptions (S.125-141)\nAbschnitt 4.5 Remedies: Alternative Approaches When Problems Are Detected (S.141-149)\nAbschnitt 10.3 Detecting Outliers: Regression Diagnostics” (S.394-411)\nAbschnitt 10.5 Multicollinearity” (S.419-425)\n(für HU-Studierende über ub.hu-berlin.de zugänglich)\n\n\nFür ein deutschsprachiges Buch:  \n\nGollwitzer, M., Eid, M., & Schmitt, M. (2017). Statistik und Forschungsmethoden. Weinheim: Beltz Verlagsgruppe. (für HU-Studierende online zugänglich)\nKapitel “Regressionsdiagnostik” (S. 704-725)\n(für HU-Studierende über ub.hu-berlin.de zugänglich)\n\nZur weiteren Hilfe bei der Interpretation von Plots, können wir diesen Forumseintrag sowie diese Seite nutzen.\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] car_3.1-2        carData_3.0-5    ICC_2.4.0        rmarkdown_2.21  \n[5] knitr_1.42       kableExtra_1.3.4\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       httr_1.4.5        svglite_2.1.1     cli_3.6.1        \n [5] rlang_1.1.0       xfun_0.39         stringi_1.7.12    jsonlite_1.8.4   \n [9] glue_1.6.2        colorspace_2.1-0  htmltools_0.5.5   scales_1.2.1     \n[13] abind_1.4-5       evaluate_0.20     munsell_0.5.0     fastmap_1.1.1    \n[17] yaml_2.3.7        lifecycle_1.0.3   stringr_1.5.0     compiler_4.3.0   \n[21] rvest_1.0.3       htmlwidgets_1.6.2 rstudioapi_0.14   systemfonts_1.0.4\n[25] digest_0.6.31     viridisLite_0.4.1 R6_2.5.1          magrittr_2.0.3   \n[29] webshot_0.5.4     tools_4.3.0       xml2_1.3.3       \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Outputs-ALM-FAQ.html#github-link",
    "href": "Outputs-ALM-FAQ.html#github-link",
    "title": "15  Outputs - ALM - FAQ",
    "section": "15.1 \\(t\\)-Test",
    "text": "15.1 \\(t\\)-Test\n\nDer \\(t\\)-Test untersucht, ob es signifikante Mittelwertsunterschiede in der Population hinsichtlich einer metrischen Variablen gibt. Der Begriff umfasst eine Gruppe von Hypothesentests, darunter den \\(t\\)-Test für unabhängige Stichproben und den \\(t\\)-Test für abhängige Stichproben (Beobachtungspaare).\n\nIn unserem Beispiel schauen wir, ob sich Personen mit und ohne Nebenjob (job) hinsichtlich der Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) unterscheiden.\nWeil es sich bei Studierenden mit und ohne Nebenjob um zwei voneinander unabhängige Stichproben (d.h. unterschiedliche Personen) handelt, verwenden wir den \\(t\\)-Test für unabhängige Stichproben.\n\n\nÜberprüfung der Annahmen des \\(t\\)-Tests\n\n\n\n\nAchtung: An dieser Stelle sei wieder auf das ausführliche Skript zur Annahmenprüfung verwiesen. Im Folgenden schauen wir uns jeweils nur ein mögliches Verfahren zur Überprüfung der spezifischen Annahmen an.\n\n\nZur Durchführung eines \\(t\\)-Tests für unabhängige Zufallsstichproben müssen folgende Annahmen erfüllt sein:\n\nes muss sich um einfache, voneinander unabhängige Stichproben handeln\nNormalverteilung des untersuchten Merkmals in beiden Populationen\nVarianzhomogenität des untersuchten Merkmals in beiden Populationen\n\n\n Normalverteilung\nSchauen wir uns dazu jeweils für beide Gruppen von job einen QQ-Plot an.\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(data = daten, aes(sample = zf_belastung)) + # Koordinatensystem\n  stat_qq() + stat_qq_line() + # QQ-Plot + Hilfslinie\n  facet_grid(~ job) # einzelne Plots nach den Ausprägungen von job\n\n\n\n# die Kategorie NA fasst die fehlenden Werte auf der Variablen job zusammen\n# diese können wir hier unbeachtet lassen\n\nMehr Informationen zum Erstellen von Grafiken mit dem Paket ggplot2 finden wir im Kapitel zu Grafiken.\n\n\nAchtung: Hilfe zur Interpretation von QQ-Plots finden wir hier: Section 14.5.1.\nHinweis: Die Residuen in der linearen Regression sind beim \\(t\\)-Test dasselbe wie in den Gruppen zentrierte Werte der \\(AV\\) (d.h. Abweichungen vom gruppenspezifischen Mittelwert) in unserem Beispiel.\n\n\nIn beiden Diagrammen weichen die Punkte leicht von der Linie ab, d.h. dass die Daten nicht perfekt normalverteilt sind. Wir können uns aber auf den zentralen Grenzwertsatz stützen, welcher besagt, dass der Mittelwert eines Merkmals bei wachsender Stichprobengröße approximativ normalverteilt ist. Generell gibt es keinen Richtwert, der besagt, wann Stichproben hinreichend groß sind. Mit unseren Stichprobengrößen (\\(n_{ja} = 87\\) und \\(n_{nein} = 86\\)) und den geringfügigen Abweichungen von einer Normalverteilung in den beiden Gruppen können wir erwarten, dass die Annahme hinreichend erfüllt ist.\n\nVarianzhomogenität\n\nVarianzhomogenität bedeutet, dass die Varianzen in den untersuchten Populationen gleich sind. Wenn dies der Fall ist, sollten sich auch die Stichprobenvarianzen ähneln.\n\nIst diese Annahme nicht erfüllt, müssen wir auf robustere Methoden, z.B. den Welch’s \\(t\\)-Test für unabhängige Stichproben, zurückgreifen.\nZur Überprüfung der Varianzhomogenität können wir beispielsweise den Levene Test nutzen. In diesem wird die Nullhypothese überprüft, dass die Populationsvarianzen homogen sind (\\(\\sigma^2_{ja} = \\sigma^2_{nein}\\)). Weil die \\(H_0\\) die Wunschhypothese ist, ist es von größerer Relevanz, diese nicht fälschlicherweise abzulehnen, d.h. einen \\(\\beta\\)-Fehler zu machen (relativ gesehen zum \\(\\alpha\\)-Fehler). Wir können das Risiko für einen \\(\\beta\\)-Fehler nur indirekt kontrollieren, indem wir das \\(\\alpha\\)-Level erhöhen. Daher legen wir unser Signifikanzniveau auf \\(\\alpha=.20\\) (zweiseitig) fest.\nWir führen den Test mit der Funktion leveneTest() aus dem Paket car durch.\n\nlibrary(car)\nleveneTest(zf_belastung ~ job, daten)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   1   0.807 0.3703\n      167               \n\n\nDa der \\(p\\)-Wert (Pr(>F)) größer als unser \\(\\alpha\\)-Level ist, können wir die \\(H_0\\) beibehalten.\nNachdem die Annahmen weitgehend erfüllt erscheinen, können wir den \\(t\\)-Test für unabhängige Stichproben anwenden.\n\n\n\nMit der Funktion t.test() aus dem Basispaket stats können unterschiedliche Arten von \\(t\\)-Tests durchgeführt werden. Welches Verfahren verwendet werden soll, legen wir mit den Parametern paired und var.equal fest.\n\nMit paired spezifizieren wir, ob eine Abhängigkeit der Stichproben besteht. TRUE zu einem \\(t\\)-Test für Beobachtungspaare, FALSE führt zur Durchführung eines \\(t\\)-Tests für unabhängige Stichproben.\nMit var.equal spezifizieren wir, ob Varianzhomogenität vorliegt. Wenn die Annahme nicht erfüllt ist, legen wir mit FALSE fest, dass der Welch’s \\(t\\)-Test durchgeführt werden soll.\n\nWir wollen einen ungerichteten \\(t\\)-Test für unabhängige Stichproben durchführen und legen unser Signifikanzlevel \\(\\alpha=.05\\) fest.\n\nt.test(formula = zf_belastung ~ job, \n       data = daten,\n       paired = FALSE, # unabhängige SP\n       var.equal = TRUE, # Annahme Varianzhomogenität erfüllt\n       alternative =\"two.sided\", # zweiseitige (ungerichtete) Testung\n       na.action = \"na.exclude\") # Ausschluss fehlender Werte\n\n\n\nMehr Informationen zum Umgang mit fehlenden Werten finden wir im Kapitel Fehlende Werte.\n\n\n\n\n\n\nWir erhalten u.a. folgende Informationen:\n\n t : empirische Prüfgröße des \\(t\\)-Tests\n df : Anzahl der Freiheitsgerade der \\(t\\)-Verteilung (entspricht \\(N-2\\))\n\n\n\n\nWir kommen wir auf \\(N=169\\)?\n\nWir haben alle Fälle mit fehlenden Werten auf einer der beiden Variablen ausgeschlossen (casewise deletion durch na.action = \"na.exclude\"). Wir können diese Bedingung folgendermaßen auf unseren Datensatz anwenden, um \\(N\\) für unsere spezifische Analyse zu berechnen:\n\nnrow(daten[!is.na(daten$job) & !is.na(daten$zf_belastung),])\n\n[1] 169\n\n\nFür mehr Informationen siehe das Kapitel zu Fehlenden Werten.\n\n\n p-value : in \\(p\\)-Wert umgerechneter empirischer \\(t\\)-Wert \nDurch den Vergleich mit unserem \\(\\alpha\\)-Niveau können wir anhand dieser Information zu einer Testentscheidung gelangen. Ist der \\(p\\)-Wert kleiner als \\(\\alpha\\), verwerfen wir die \\(H_0\\).\n 95 percent confidence interval : obere und untere Grenze des 95% Konfidenzintervalls der Mittelwertsdifferenz\n sample estimates : Stichprobenmittelwerte der Gruppen\n\nDurch Betrachtung des \\(p\\)-Wertes stellen wir fest, dass die Wahrscheinlichkeit unter Gültigkeit der \\(H_O\\), einen empirischen \\(t\\)-Wert von \\(\\mid -1.278 \\mid\\) oder extremer zu erhalten, bei 20.3% liegt. Da dieser \\(p\\)-Wert deutlich über \\(\\alpha=.05\\) liegt, behalten wir die Nullhypothese bei. Wir gehen davon aus, dass Studierende mit und ohne Nebenjob sich nicht statistisch signifikant in ihrer Zufriedenheit mit der Bewältigung von Studienbelastungen unterscheiden."
  },
  {
    "objectID": "Outputs-ALM-FAQ.html#lineare-regression",
    "href": "Outputs-ALM-FAQ.html#lineare-regression",
    "title": "15  Outputs - ALM - FAQ",
    "section": "15.2 Lineare Regression",
    "text": "15.2 Lineare Regression\n\nDie lineare Regression ermöglicht es, eine abhängige Variable (AV, Kriterium) durch eine oder mehrere unabhängige Variablen (UVs, Prädiktoren) vorherzusagen. Das Kriterium muss metrisch sein, wohingegen die Prädiktoren auch kategorial sein können, wenn sie adäquat kodiert sind (siehe z.B. Indikatorvariablen: Kodierung nominaler Merkmale im Kapitel Datenvorbereitung).\n\n\n\nein Prädiktor: einfache lineare Regression;\n\nmehrere Prädiktoren: multiple lineare Regression\n\nWir führen eine einfache lineare Regression durch, in der wir die Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) auf das Vorhandensein eines Nebenjobs (job) zurückführen.\nWir legen unser Signifikanzlevel auf \\(\\alpha=.05\\) fest.\nBei der linearen Regression ist die sogenannte Residualdiagnostik ein essentieller Teil der Annahmenprüfung. Allerdings müssen wir dafür die Regression bereits durchgeführt haben. Wir können die Annahmenprüfung also erst nach der Regression machen.\nMit der Funktion lm() aus dem Basispaket stats können wir eine lineare Regression durchführen.\n\nlm_belastung <- lm(formula = zf_belastung ~ job,\n                   data = daten, \n                   na.action = \"na.exclude\") # Ausschluss fehlender Werte\n\nMehr Informationen zum Umgang mit fehlenden Werten finden wir im Kapitel Fehlende Werte.\nDas Ergebnisobjekt lm_belastung schauen wir uns erst nach der Annahmenprüfung an.\n\n\nÜberprüfung der Annahmen der (einfachen) linearen Regression\n\n\n\n\nAchtung: Im Rahmen dieses Kapitels besprechen wir die Annahmen und deren Überprüfung nicht im Detail. Mehr Informationen finden wir im Kapitel zur Prüfung der Annahmen der multiplen linearen Regression, die denen der einfachen linearen Regression sehr ähnlich ist.\n\n\n\n\n\nFolgende vier Annahmen sind bei der einfachen linearen Regression mit der inferenzstatistischen Absicherung verbunden:\n\nLinearität\nHomoskedastizität\nNormalverteilung der Residuen\nUnabhängigkeit der Residuen\n\n\n\nLinearität\n\nDie Abhängigkeit zwischen Erwartungswert des Kriteriums und Prädiktor ist linear.\n\nDiese Annahme lässt sich mittels eines Residualplots untersuchen. Dieser plottet die vorhergesagten Kriteriumswerte \\(\\hat y_i\\) gegen die Residuen \\(\\hat e_i\\).\n\nplot(lm_belastung, which=1)\n\n\n\n\nWeil wir nur eine kategoriale \\(UV\\) mit zwei Ausprägungen haben, ordnen sich die Punkte in zwei vertikalen Linien an.\nDa sich die Lowess Fit Line (rote gestrichelte Linie), welche den generellen (nonparametrischen) Trend der Daten beschreibt, dem Erwartungswert der Residuen bei \\(y=0\\) (schwarze gestrichelten Linie) annähert, können wir annehmen, dass Linearität vorliegt.\nMehr Informationen zur Lowess Fit Line und zum Residuenplot finden wir im Kapitel zur Prüfung der Annahmen der multiplen linearen Regression.\nHomoskedastizität\n\nDie Varianz der \\(y\\)-Werte, die an einer bestimmten Stelle des Prädiktors vorliegt, ist für alle Prädiktorwerte gleich (Varianzhomogenität).\n\nDiese haben wir bereits im Abschnitt zum \\(t\\)-Test mittels des Levene-Tests überprüft.\nDen Residualplot, den wir gerade zur Überprüfung der Annahme der Linearität genutzt haben, können wir auch zur Überprüfung der Annahme der Homoskedatizität nutzen. Weil die Residuen sich ohne erkennbares Muster um den Erwartungswert der Residuen bei \\(y=0\\) (schwarze gestrichelten Linie) verteilen, nehmen wir Homoskedastizität an.\nNormalverteilung der Residuen\n\nDie Verteilung der \\(y\\)-Werte an einer bestimmten Stelle der \\(UV\\) ist eine Normalverteilung.\n\n\n\n\nEine Verletzung dieser Annahme ist eher in kleineren Stichproben problematisch. In großen Stichproben sind die Regressionskoeffizienten aufgrund des zentralen Grenzwertsatzes selbst dann asymptotisch normalverteilt, wenn die Annahme nicht erfüllt ist. Es gibt jedoch keinen Richtwert, ab wann eine Stichprobe als hinreichend groß gilt. Wir sollten die Annahme immer überprüfen. Dazu schauen wir uns einen QQ-Plot der Residuen an.\n\nplot(lm_belastung, which=2)\n\n\n\n\nUnser Kriterium Zufriedenheit mit der Bewältigung von Studienbelastungen scheint in Abhängigkeit der Gruppenzugehörigkeit leicht von einer Normalverteilung abzuweichen. Die Größe unserer Stichprobe, \\(N = 169\\), legt nahe, dass (nach dem zentralen Grenzwertsatz) die Regressionskoeffizienten approximativ normalverteilt sind und der Standardfehler der Steigung nicht verzerrt ist.\nUnabhängigkeit der Residuen\n\nDie Höhe des Residuums einer Beobachtung ist unabhängig von der Höhe des Residuums einer anderen Beobachtung.\n\nSerielle Abhängigkeit, d.h. mehrere Messungen von einer Person, können wir ausschließen, da es sich nicht um ein Messwiederholungs-Design handelt.\nZur Überprüfung auf Clustering, d.h. systematische Zusammenhänge zwischen Personen einer Gruppe, müssten wir im Verdacht stehende (erhobene) Gruppenvariablen begutachten. Da dies aber den Rahmen dieses Kapitels sprengen würde, lassen wir das außen vor.\nDa wir keine Hinweise auf Verletzung der Annahmen gefunden haben, schauen wir uns nun die Ergebnisse der einfachen linearen Regression an.\n\n\n\nWir schauen uns die Ergebnis unseres Regressionsmodells mittels summary() an:\n\nsummary(lm_belastung)\n\n\n\n\n\n\nDer Output sagt uns, inwiefern wir Zufriedenheit mit der Bewältigung von Studienbelastungen mit dem Vorliegen eines Nebenjobs vorhersagen können.\nWir bekommen Auskunft über:\n\ndie Signifikanztests der geschätzten Populationskoeffizienten \\(\\hat b_0\\) und \\(\\hat b_1 ... \\hat b_k\\) (unserer \\(k\\) Prädiktoren)\nden Signifikanztest der insgesamt aufgeklärten Varianz des gesamten Regressionsmodells \\(R^2\\)\n\nIn unserem Fall einer einfachen linearen Regression sind die Ergebnisse der Signifikanztestung von Schätzung der Steigung und Schätzung der Varianzaufklärung des Gesamtmodells identisch.\nUnter Coefficients finden wir Informationen zu den geschätzten Regressionskoeffizienten - Intercept und Steigungskoeffizient(en):\n\n\n\n\n\n\n Estimate : Schätzung der (Populations-)Regressionskoeffizienten\n\n(Intercept): vorhergesagter Wert des Kriteriums wenn alle Prädiktoren 0 sind (bei metrischen Prädiktoren) bzw. vorhergesagter Wert des Kriteriums in den jeweiligen Referenzgruppen (bei kategorialen Prädiktoren); Wert von zf_belastung in der Referenzgruppe job == ja\njobnein: Steigungskoeffizient, der die erwartete Veränderung im Kriterium angibt, wenn der Prädiktor um eine Einheit erhöht wird (bei metrischen Prädiktoren) bzw. in (einer) der Vergleichsgruppen (bei kategorialen Prädiktoren); Unterschied von job == nein im Vergleich zur Referenzgruppe job == ja im Hinblick auf zf_belastung\n\n Std.Error : Standardfehler des Regressionskoeffizienten\n t.value : empirischer \\(t\\)-Wert für die Signifikanztestung des (partiellen) Regressionskoeffizienten\n Pr(>|t|) : in \\(p\\)-Wert umgerechneter empirischer \\(t\\)-Wert\n Sign.codes : durch Sternchen gekennzeichnete Signifikanzniveaus der geschätzten Regressionskoeffizienten\n\nDer Intercept \\(b_0 = 2.679\\) gibt uns den Mittelwert der Personen mit Nebenjob (job == \"ja\", Referenzgruppe) auf der Skala von Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) an. Wir gelangen durch Betrachtung von \\(p < 2^{-16}\\) zu der Testentscheidung, die \\(H_0\\), dass der Intercept in der Population 0 ist, abzulehnen.\n\n\n\\(t\\)-Test des Intercepts \\(b_0\\):\n\\(H_0\\): \\(\\beta_0 = 0\\)\n\\(H_1\\): \\(\\beta_0 > 0\\)\n\nDer Steigungskoeffizient \\(b_1 = 0.131\\) sagt uns, dass Personen ohne Nebenjob (job == \"nein\") sich im Mittel 0.131 Punkte höher auf der Skala von Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) beschreiben. Zudem gelangen wir durch Betrachtung von \\(p = 0.203\\) zu der Testentscheidung, die \\(H_0\\), dass der Populationskoeffizient \\(\\beta_1 = 0\\) ist, beizubehalten.\n\n\n\\(t\\)-Test des Steigungs-koeffizienten \\(b_1\\):\n\\(H_0\\): \\(\\beta_1 = 0\\)\n\\(H_1\\): \\(\\beta_1 \\neq 0\\)\n\nIm unteren Abschnitt finden wir Informationen bezüglich des Gesamtmodells:\n\n\n\n\n\n\n Residual Standard Error : Standardschätzfehler \\(s_e\\)\nGütemaß für die Genauigkeit der Regressionsvorhersage\n Multiple R-squared : Determinationskoeffizient \\(R^2\\)\nGütemaß für die Schätzgenauigkeit\n Adjusted R squared : korrigierter Determinationskoeffizient \\(R_{korr}^2\\); gilt durch Korrektur der Freiheitsgrade als erwartungstreuer Schätzer für die Population\n F-statistic : empirischer Wert des \\(F\\)-Tests des Determinationskoeffizienten \\(R^2\\) (d.h. des Gesamtmodells) mit Anzahl der Zähler- (\\(k\\)) und Nenner-Freiheitsgrade (\\(N-k-1\\)) der \\(F\\)-Verteilung mit zugehörigem \\(p\\)-Wert.\n\n\n\n\n\n\n\nHandelt es sich um eine einfache lineare Regression mit nur einem Prädiktor (wie in unserem Fall), führt der \\(F\\)-Test des Determinations-koeffizienten zum selben Ergebnis wie der \\(t\\)-Test des Steigungs-koeffizienten.\n\n\nDer Determinationskoeffizienten \\(R^2 = 0.009685\\) sagt aus, dass in unserer Stichprobe knapp 1% der Variation in der Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) durch das Vorhandensein eines Nebenjobs (job) erklärt werden kann. Wir können die \\(H_0\\), dass der Determinationskoeffizient in der Population \\(0\\) ist (\\(\\rho^2 = 0\\)), durch Betrachtung von \\(p = 0.203\\) beibehalten.\n\n\n\\(F\\)-Test des Determinations-koeffizienten \\(R^2\\):\n\\(H_0: \\rho^2 = 0\\)\n\\(H_1: \\rho^2 > 0\\)"
  },
  {
    "objectID": "Outputs-ALM-FAQ.html#anova",
    "href": "Outputs-ALM-FAQ.html#anova",
    "title": "15  Outputs - ALM - FAQ",
    "section": "15.3 ANOVA",
    "text": "15.3 ANOVA\n\nEine ANOVA (Analysis of Variance) überprüft den Einfluss von einer bzw. mehreren kategorialen unabhängigen Variablen (UVs, Faktoren) mit \\(p\\) Faktorstufen auf eine metrische abhängige Variable (AV). Dazu wird eine Varianzdekomposition durchgeführt, welche die Gesamtvarianz in systematische und Fehlervarianz zerlegt.\n\n\nWir wollen in unserem Beispiel überprüfen inwiefern Unterschiede in der Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) auf das Vorhandensein eines Nebenjobs (job) zurückzuführen ist.\nWir haben ein minimal unbalanciertes Design, weil die Anzahl der Beobachtungen in den Faktorstufen von job sich geringfügig unterscheiden mit \\(n_{ja} = 87\\) und \\(n_{nein} = 86\\).\nUnser Signifikanzlevel legen wir auf \\(\\alpha=.05\\) fest.\n\n\n\n\n\nÜberprüfung der Annahmen der ANOVA\n\nVor der Durchführung der ANOVA mit unseren Beispieldaten müssen wir zwei Annahmen prüfen:\n\nNormalverteilung des untersuchten Merkmals in beiden Populationen\nHomoskedastizität des untersuchten Merkmals in beiden Populationen\n\n\n\nBeides haben wir im Rahmen der Durchführung des \\(t\\)-Test bereits untersucht und Evidenz dafür gefunden.\n\n\n\nZur Durchführung der ANOVA verwenden wir die aov()Funktion.\n\nanova <- aov(zf_belastung ~ job, \n             data = daten, \n             na.action = \"na.exclude\") # Ausschluss fehlender Werte\nsummary(anova)\n\n\nMehr Informationen zum Umgang mit fehlenden Werten im gleichnamigen Kapitel.\n\n\n\n\n\n\n\n\n\n\nDer Output liefert die folgenden Informationen für den Gruppierungsfaktor (job) und für die Residuen (Residuals):\n\n Df : Anzahl der Freiheitsgrade der \\(F\\)-Verteilung\n\n… des Gruppierungsfaktor: \\(df_{job} = p - 1\\)\n… der Residuen: \\(df_{e} = N-p\\)\n\n Sum Sq : Quadratsumme\n\n… des Gruppierungsfaktors (“Treatmentquadratsumme”)\n… der Residuen (“Fehlerquadratsumme”)\n\n Mean Sq  : Mittlere Quadratsumme (MQ)\nrelativieren die Quadratsumme eines Effekts an seinen Freiheitsgraden\n F value : empirische Prüfgröße des \\(F\\)-Tests\nentspricht dem Quotienten \\(\\frac {MQ_{job}} {MQ_e}\\)\n Pr(>F) : in \\(p\\)-Wert umgerechneter empirischer \\(F\\)-Wert\n\n\nUnser \\(p\\)-Wert deutet darauf hin, dass unter Gültigkeit der \\(H_0\\) die Wahrscheinlichkeit, den vorliegenden \\(F\\)-Wert von 1.633 oder einen größeren zu erhalten, 20,3% beträgt. Wir kommen somit zu der Testentscheidung, die \\(H_0\\) beizubehalten. Das bedeutet, dass wir davon ausgehen, dass zwischen den Populationen von Studierenden mit und ohne Nebenjob keine überzufälligen Mittelwertsunterschiede bezüglich der Zufriedenheit mit der Bewältigung von Studienbelastungen existieren."
  },
  {
    "objectID": "Outputs-ALM-FAQ.html#github2",
    "href": "Outputs-ALM-FAQ.html#github2",
    "title": "15  Outputs - ALM - FAQ",
    "section": "15.4 ALM: Zusammenhänge der drei Verfahren",
    "text": "15.4 ALM: Zusammenhänge der drei Verfahren\nWie bereits eingangs erwähnt, gehören \\(t\\)-Test, lineare Regression und ANOVA alle zum Allgemeinen Linearen Modell (ALM).\nFür den hier betrachteten Spezialfall von nur einer UV mit nur zwei Stufen (und adäquater Kodierung; siehe z.B. Indikatorvariablen: Kodierung nominaler Merkmale aus dem Kapitel Datenvorbereitung) kommen die drei Verfahren zum selben Ergebnis bei der Signifikanztestung, obwohl scheinbar andere Hypothesen getestet werden.\n\n\n\n\n \n\n\nEmpirische Prüfgröße\n\n  \n    Verfahren \n    getestete Hypothesen \n    t \n    F \n    p \n  \n \n\n  \n    t-Test für unabhängige Stichproben \n    Mittelwertsunterschied in den Gruppen\n\nH0: $\\mu_1 = \\mu_2$\n\n$H_1$: $\\mu_1 \\neq \\mu_2$ (ungerichtet) \n\n    -1.278 \n     \n    0.203 \n  \n  \n    einfache lineare Regression \n    Steigungskoeffizient $b_1$\n\n$H_0$: $\\beta_1 = 0$\n\n$H_1$: $\\beta_1 \\neq 0$ (ungerichtet) \n\n    1.278 \n     \n    0.203 \n  \n  \n    einfache lineare Regression \n    Determinationskoeffizient $R^2$\n\n$H_0$: $\\rho^2 = 0$\n\n$H_1$: $\\rho^2$ > $0$ (gerichtet) \n\n     \n    1.633 \n    0.203 \n  \n  \n    ANOVA \n    Mittelwertsunterschied in den Gruppen\n\n$H_0$: $\\mu_1 = \\mu_2$\n\n$H_1$: $\\mu_1 \\neq \\mu_2$ (ungerichtet) \n\n     \n    1.633 \n    0.203 \n  \n\n\n\n\n\nWir können \\(t\\)- und \\(F\\)-Werte ineinander überführen durch folgende Formel: \\(F = t^2 \\longrightarrow t^2=1.278^2=1.633=F\\).\n\nAchtung: Dass der \\(t\\)-Wert einmal negativ (\\(t\\)-Test) und einmal positiv (einfache lineare Regression) ist liegt daran, dass die Gruppen jeweils vertauscht wurden.\n\n\nDas Allgemeine Lineare Modell (ALM) umfasst varianzanalytische Verfahren sowie (multiple) Korrelations- und Regressionsrechnung. Dadurch können nicht nur metrische sondern auch kategoriale Merkmale (als \\(UV\\)s) untersucht werden, sofern die kategorialen Merkmale in geeigneter Form kodiert sind.\n\n\n\nMehr Informationen zu Kodierung gibt es im Kapitel Einführung in R.\n\nIn unserem Fall von einer intervallskalierten \\(AV\\) und einer nominalskalierten \\(UV\\) werden in den drei Verfahren jeweils die Mittelwerte der \\(AV\\) für die jeweiligen \\(UV\\) gebildet. Beim \\(t\\)-Test und der einfachen linearen Regression haben wir diese auch ausgegeben bekommen:\n\n\\(t\\)-Test:\nmean in group ja \\(= 2.67\\)\nmean in group nein \\(= 2.80\\)\nEinfache lineare Regression:\n\\(Intercept = 2.67\\)\n\\(b_1 = 0.131\\)\n\\(Intercept + b_1 = 2.81\\)\n\nDadurch sind die getesteten Hypothesen der drei Verfahren in diesem speziellen Fall (\\(AV\\): intervallskaliert, \\(UV\\): nominalskaliert) äquivalent.\n\nUm eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] knitr_1.42       kableExtra_1.3.4 car_3.1-2        carData_3.0-5   \n[5] ggplot2_3.4.2   \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3      jsonlite_1.8.4    dplyr_1.1.2       compiler_4.3.0   \n [5] webshot_0.5.4     tidyselect_1.2.0  stringr_1.5.0     xml2_1.3.3       \n [9] systemfonts_1.0.4 scales_1.2.1      yaml_2.3.7        fastmap_1.1.1    \n[13] R6_2.5.1          labeling_0.4.2    generics_0.1.3    htmlwidgets_1.6.2\n[17] tibble_3.2.1      munsell_0.5.0     svglite_2.1.1     pillar_1.9.0     \n[21] rlang_1.1.0       utf8_1.2.3        stringi_1.7.12    xfun_0.39        \n[25] viridisLite_0.4.1 cli_3.6.1         withr_2.5.0       magrittr_2.0.3   \n[29] digest_0.6.31     rvest_1.0.3       grid_4.3.0        rstudioapi_0.14  \n[33] lifecycle_1.0.3   vctrs_0.6.2       evaluate_0.20     glue_1.6.2       \n[37] farver_2.1.1      abind_1.4-5       fansi_1.0.4       colorspace_2.1-0 \n[41] rmarkdown_2.21    httr_1.4.5        tools_4.3.0       pkgconfig_2.0.3  \n[45] htmltools_0.5.5  \n\n\nFür Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an."
  },
  {
    "objectID": "Zur-Lernplattform-Beitragen.html#was-finde-ich-im-projekt",
    "href": "Zur-Lernplattform-Beitragen.html#was-finde-ich-im-projekt",
    "title": "16  Zur R-Lernplattform Beitragen",
    "section": "16.1 Was finde ich im Projekt?",
    "text": "16.1 Was finde ich im Projekt?\nDie wichtigsten Dokumente sind die Quarto Markdown Dateien. Diese enden mit .qmd und in ihnen sind die tatsächlichen Inhalte der Website gespeichert. Beispielsweise findest Du in “Installation.qmd” die Infos zur Installation von R und RStudio. Im Dokument “Einfuehrung_in_R.qmd” befindet sich\ndann die Einführung in R.\nWichtig: Die .qmd - Dateien landen nicht automatisch auf der Webseite! Damit das Thema, das in der .qmd Datei behandelt wird, auch auf der Webseite landet, müssen wir erst die _quarto.yml Datei öffnen. Dort finden wir beispielsweise folgende Auflistung:\nbook:\n  title: \"R Lernplattform\"\n  author: \"Methods Group Berlin\"\n  chapters:\n    - part: \"R und RStudio\"\n      chapters:\n        - index.qmd\n        - Installation.qmd\n        - Einfuehrung_in_R.qmd\n        - Einfuehrung_in_RStudio.qmd\n        - Pakete.qmd\n        - Daten-importieren.qmd\n        - Fehlermeldungen.qmd\n        - Datenvorbereitung.qmd\n        - Fehlende-Werte.qmd\n        - Wide-and-Long-Format.qmd\n        - Grafiken.qmd\n        - Markdown.qmd\n    - part: \"Spezifische Themen\"\n      chapters:\n        - Voraussetzungspruefung.qmd\n        - Outputs-ALM-FAQ.qmd\n        - Git.qmd\n    - part: \"Projekt-Dokumentation\"\n      chapters:\n        - Interne-Dokumentation.qmd\n        - lineare-regression.qmd\nJede qmd - Datei, die nachher tatsächlich auf der Webseite erscheinen soll, muss hier aufgeführt werden. Achtung: Die Einrückungen sind hier sehr wichtig, sonst funktioniert das Dokument nicht. Mehr Infos gibt es hier.\nIm Ordner “figures” befinden sich alle Bilder, die in der Webseite eingebunden sind. Beispielsweise befinden sich im Unterornder Daten-importieren/Bilder alle Bilder des Themas Daten importieren.\nIm Ordner “data” befinden sich alle Datensätze, die zur Erstellung der Webseite notwendig sind. Diese werden in den Beispiel-Analysen verwendet (z.B. um das Einlesen von Datensätzen zu veanschaulichen).\nDie restlichen Dateien sind notwendig für Quarto. Den größten Teil hiervon können wir ignorieren. Für uns (manchmal) relevant sind hier eigentlich nur “style.scss” und “DESCRIPTION”. In “style.scss” wird das Aussehen der Webseite definiert. Diese Datei ist aktuell einfach von der alten Webseite übernommen, was dazu führt, dass der bisherige Stil beibehalten wird. In “DESCRIPTION” geben wir (unter anderem) an, welche R Pakete wir in den einzelnen .qmd Dateien nutzen. Wenn wir beispielsweise lavaan nutzen wollen, müssen wir hier lavaan zu den Paketen hinzufügen (siehe unten). Wenn wir das vergessen, kommt es zu Fehlern beim erstellen der Webseite!"
  },
  {
    "objectID": "Zur-Lernplattform-Beitragen.html#themen-bearbeiten",
    "href": "Zur-Lernplattform-Beitragen.html#themen-bearbeiten",
    "title": "16  Zur R-Lernplattform Beitragen",
    "section": "16.2 Themen bearbeiten",
    "text": "16.2 Themen bearbeiten\nThemen können entweder direkt auf GitHub bearbeitet werden oder lokal auf dem eigenen Rechner. Wenn Du Themen auf Deinem Rechner bearbeiten möchtest, musst Du Dich mit git und GitHub auskennen. Im Folgenden werden wir uns auf die Bearbeitung direkt in GitHub konzentrieren.\nAngenommen Dir fällt ein Fehler in der Einführung in R auf. Um den Fehler zu beheben, öffne das Dokument “Einfuehrung_in_R.qmd” in GitHub.\n\nAnschließend kannst Du die Datei bearbeiten, indem Du auf den Stift klickst:\n\nDu kannst nun das Dokument verändern. Wenn Du alle Veränderungen vorgenommen hast, dann klicke auf commit (scrolle nach ganz unten).\n\nDie Veränderungen sind nun gespeichert! Wichtig: Die Webseite wird nicht automatisch aktualisiert. Hier ist zur Zeit noch etwas Handarbeit erfordert. Das ist eine bewusste Entscheidung, um sicher zu stellen, dass keine unfertigen Seiten aus Versehen online gehen.\n\n16.2.1 Code hinzufügen\nWenn man Code hinzufügen möchte, funktioniert das identisch wie in RMarkdown also, z.B. so:\n\nmodell <- lm(noten ~ stunden)\nsummary(modell) # Anzeigen der Modellparameter\n\nWir können mit zusätzlichen Code chunk options festlegen, ob der Code z.B. ausgeführt (eval=TRUE oder eval=FALSE) oder überhaupt angezeigt (echo=TRUE oder echo=FALSE) werden soll. Im obigen Fall haben wir keine Variablen noten und stunden. Daher kann der Code nicht ausgeführt werden; sonst kommt es zu einem Fehler. Deshalb setzen wir eval=FALSE. Mehr infos gibt es hier.\n\n\n16.2.2 Daten hinzufügen\nWenn Du einen Datensatz für Deine RMarkdown-Datei brauchst, der nicht in R oder einem Paket enthalten ist, dann speichere diesen im Ordner “data”. Anschließend kannst Du ihn folgendermaßen in Deiner RMarkdown Datei einlesen:\n\nmein_datensatz <- read.csv(\"data/mein_datensatz.csv\")\n\n\n\n16.2.3 Bilder hinzufügen\nWenn Du Bilder für Deine RMarkdown-Datei brauchst, dann speichere diese im Ordner “figures”. Anschließend kannst Du diese beispielsweise folgendermaßen nutzen:\n![](figures/mein_thema/mein_bild.png)\nEventuell musst du auch hierfür auch einen neuen Ordner erstellen (hier: den Ordner mein_thema). Wenn Du diesen online in GitHub erstellen willst, dann musst Du aktuell folgendermaßen vorgehen:\n\nGehe auf “create new file” oben rechts:\n\n\n\nSchreibe den Pfad “mein_thema” in den Dateinamen:\n\n\n\nFüge eine backslash “/” hinzu. Dann wird GitHub einen Ordner erstellen:\n\n\n\nWeil GitHub uns nicht erlaubt, einfach nur den Ordner zu commiten, müssen wir zudem eine neue Datei erstellen. Am besten schreibt man dafür einfach irgendwas in den File, den Github schon aufmacht. Ich nenne diese beispielsweise einfach temp.txt:\n\n\nAnschließend können wir auf Commit klicken und haben nun einen neuen Ordner. Unsere Bilder können wir über “Upload files” einfach von unserem PC hochladen. Oder per Drag & Drop einfügen.\n\n\n\n16.2.4 Links hinzufügen\nWenn Du intern auf ein anderes Thema referenzieren möchtest, kannst Du einen Link einfügen. Dies geht mit:\n[text, den die Leser:innen sehen](Name_der_qmd_Datei.qmd)\nBeispielsweise führt folgender Link zur Einführung in R. Als Rohtext steht hier:\nBeispielsweise führt folgender Link zur [Einführung in R](Einfuehrung_in_R.qmd).\nFür Links zu Subkapitel, können wir einen Hashtag nutzen: [about](about.qmd#section)\n\n\n16.2.5 Paket hinzufügen\nWenn Dein neues Thema auch ein neues R-Paket benötigt, dann musst Du dieses im Dokument DESCRIPTION zu Imports: hinzufügen (so wie beispielsweise dplyr).\n\nWichtig: Wenn wir ein R Paket verwenden und dieses nicht zur DESCRIPTION hinzufügen, dann kann die Webseite nicht erstellt werden! Die Webseite wird von GitHub erstellt und dafür muss GitHub wissen, welche Pakete installiert werden müssen.\n\nWenn wir die Webseite auf unserem eigenen Rechner erstellen wollen, können wir wie im Kapitel ?sec-introduction beschrieben vorgehen.\n\n\n16.2.6 HTML tags\nMarkdown erlaubt es, html-tags zu verwenden, um die Formatierung des Textes weiter anzupassen. Beispielsweise können wir so die Textfarbe und die Schriftgröße anpassen. Da dies jedoch schnell zu relativ unübersichtlichen Dokumenten führt, haben wir uns entschieden, die Nutzung von html-tags einzuschränken. Das soll es auch ermöglichen, das Anpassen und Erweitern bestehender Themen einfacher zu gestalten. Folgende html-tags dürfen genutzt werden:\n<details><summary></summary></details>: Erlaubt das Erstellen aufklappbarer Textelemente.\n\n\nDieser Text wird immer angezeigt\n\nDieser Text wir nur angezeigt, wenn die Textsektion “aufgeklappt” wird.\n\n<aside></aside>: Erlaubt das Erstellen von side-notes.\n\nDies ist ein Beispiel für ein aside\n\n<a id=\"wichtiges Wort\"> Wichtiges Wort </a>: Erlaubt einzelne Wörter wie Überschriften zu referenzieren\n Wichtiges Wort \n\nWenn man hier klickt, kann man jetzt per Link zurück zu dem wichtigen Wort (und z.B. der Definition) springen.\n<kbd>Taste</kbd>: Erlaubt es bei Kurzbefehlen die Tasten hervorzuheben. Beispiel: “So kann man bei einem Mac mit cmd + C markierten Text kopieren.”"
  },
  {
    "objectID": "Zur-Lernplattform-Beitragen.html#neues-thema-erstellen",
    "href": "Zur-Lernplattform-Beitragen.html#neues-thema-erstellen",
    "title": "16  Zur R-Lernplattform Beitragen",
    "section": "16.3 Neues Thema erstellen",
    "text": "16.3 Neues Thema erstellen\nEs gibt mehrere Wege, ein neues Thema zu erstellen. Wir können, wie oben beschrieben, eine neue Datei direkt in GitHub erstellen.\n\nDieser können wir nun beispielsweise den Namen “Mein-neues-Thema.qmd” geben und sie direkt in GitHub bearbeiten. Wenn wir dies tun, können wir aber den R Code nicht ausführen; das geschieht erst, wenn die Webseite erstellt wird.\nEin einfacherer Web ist folgender:\n\nWir erstellen auf unserem Computer in RStudio eine neue Quarto Datei und nennen diese “Mein-neues-Thema.qmd”.\nWir entfernen den gesamten Inhalt der Datei und fangen direkt mit einer Überschrift an (wichtig: es muss eine Überschrift mit nur einem Hashtag sein; siehe beispielsweise Zeile 1 in https://github.com/Methods-Berlin/RLernplattform/blob/main/Pakete.qmd).\nJetzt können wir wie gewohnt unsere Inhalte in die Quarto Datei schreiben und den Code auch lokal laufen lassen.\n\nWenn wir fertig mit dem Erstellen der neuen Datei sind, können wir sie direkt auf GitHub hochladen:\n\nUnser neues Thema wurde nun hinzugefügt!"
  },
  {
    "objectID": "Zur-Lernplattform-Beitragen.html#webseite-erstellen",
    "href": "Zur-Lernplattform-Beitragen.html#webseite-erstellen",
    "title": "16  Zur R-Lernplattform Beitragen",
    "section": "16.4 Webseite erstellen",
    "text": "16.4 Webseite erstellen\n\n16.4.1 Online-Version erstellen\nDie Online-Version können wir updaten, indem wir hier auf run workflow drücken:\n\nDie Webseite wird nun automatisch erstellt.\n\n\n16.4.2 Offline-Version erstellen\nWir können aber eine offline-Version erstellen, die wir dann herunterlagen und auf unserem Computer anschauen können. Hierfür werden GitHub Actions genutzt.\n\nHinweis: Wir haben nur eine begrenzte Rechenkapazität auf GitHub. Bitte erstelle die Webseite nur dann neu, wenn Du alle Änderungen vorgenommen hast, die Du vornehmen wolltest.\n\nDas Vorgehen ist aktuell:\n\nKlicke auf Actions\n\n\n\nKlicke auf create_book\n\n\n\nKlicke auf Run workflow\n\n\n\nWähle Run workflow\n\n\nDas Bookdown Projekt wird nun in eine Webseite übersetzt. Diese ist allerdings nicht online!\nUm die Webseite herunter zu laden, ist das Vorgehen:\n\nKlicke auf Actions\n\n\n\nKlicke auf den neuesten Durchgang der create_book Action\n\n\n\nScrolle ganz nach unten. Dort findest Du den Ordner docs unter Artefacts\n\n\nLade die Datei herunter und entpacke sie auf Deinem Computer.\n\nIm Ordner wirst Du unter anderem mehrere .html Dateien finden. Öffne eine beliebige dieser Dateien mit Deinem Browser, um die lokale Version der Webseite anzuzeigen.\n\n\n\n16.4.3 Fehler beheben\nEs kommt immer wieder vor, dass beim erstellen der Webseite Fehler auftreten. Diese sehen in RStudio beispielsweise so aus:\n\nWir sehen hier auf der rechten Seite den Hinweis:\n\nDieser Fehler sagt uns, dass wir im Dokument _main.Rmd in Zeile 11731-11733 nach einem Fehler ausschau halten müssen. Noch spezifischer sagt uns der Fehler, dass es das Objekt “noten” nicht gibt. Wenn wir das Dokument _main.Rmd öffnen, finden wir dort in den angegebenen Zeilen:  Dieser Teil des Dokuments _main.Rmd kommt aus dem Dokument 15-Interne-Dokumentation.Rmd. Schauen wir dort noch mal genauer nach, dann sehen wir, dass die Variablen noten und stunden gar nicht erstellt wurden und deshalb der Code nicht funktionieren kann. Wir können das Problem lösen, indem wir eval=FALSE hinzufügen.\n\n\n16.4.4 Alle Dependencies installieren\nSolltest Du nicht alle Pakete installiert haben, die auf der Webseite genutzt werden, kannst Du entweder die Datei DESCRIPTION öffnen und alle dort aufgeführten Pakete installieren oder Du öffnest die Datei RLernplattform.Rproj in RStudio und führst folgenden Code aus:\n\nif(!require(\"devtools\"))\n  install.packages(\"devtools\")\nlibrary(\"devtools\")\ndevtools::install_deps()"
  }
]