[["index.html", "R Lernplattform Chapter 1 About 1.1 Usage 1.2 Render book 1.3 Preview book", " R Lernplattform Methodengruppe Berlin 2022-11-25 Chapter 1 About This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports; for example, a math equation \\(a^2 + b^2 = c^2\\). 1.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. 1.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select “All formats” if you’d like to use multiple formats from the same book source files. Or build the book from the R console: bookdown::render_book() To render this example to PDF as a bookdown::pdf_book, you’ll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. 1.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in “Preview book”, or from the R console: bookdown::serve_book() "],["installation-und-aktualisierung-von-r-und-rstudio.html", "Chapter 2 Installation und Aktualisierung von R und RStudio 2.1 Windows 2.2 Mac 2.3 Hinweis zur Replizierbarkeit von Analysen", " Chapter 2 Installation und Aktualisierung von R und RStudio Einleitung R ist eine Statistiksoftware, die mittlerweile weit verbreitet ist - sowohl in der Forschung als auch in der Wirtschaft. R ist kostenlos, für alle gängigen Betriebssysteme verfügbar (Windows, Max OS X und Linux) und darüber hinaus auch noch open-source. Das heißt, um R zu nutzen, muss man keine teure Lizenz kaufen und jede/r kann sich an der (Weiter-)Entwicklung des Programms oder einzelner Pakete beteiligen. Durch die weite Verbreitung von R finden sich außerdem ausführliche Anleitungen im Internet, sollten wir mal nicht mehr weiter wissen. R selbst ist relativ spartanisch und die meisten AnwenderInnen nutzen daher die Entwicklungsumgebung RStudio, welche auch eine umfassendere Benutzeroberfläche besitzt. Mit dieser können wir z.B. Skripte nutzen, in denen wir unsere Analysen speichern können. Außerdem gibt es ein Environment, das uns anzeigt, welche Objekte in R vorhanden sind (d.h. die wir eingelesen oder direkt in R erstellt haben). Hier finden wir zusätzlich einen Reiter Help, welcher uns Informationen und Anwendungshinweise zu Funktionen gibt. Der Aufbau der Entwicklungsumgebung RStudio wird im Kapitel Einführung in R erklärt. Im vorliegenden Kapitel schauen wir uns jeweils für Windows und Mac an, wie wir R und RStudio installieren oder auf eine neuere Version aktualisieren und dabei unsere bisher heruntergeladenen Pakete behalten können. Was ist mit Linux? Bevor wir die R-Lernplattform ins Leben gerufen haben, haben wir eine Umfrage durchgeführt, in der u.a. das Betriebssystem der NutzerInnen erfragt wurde. Hierbei hat nur ein Bruchteil der Personen angegeben, mit Linux bzw. Ubuntu zu arbeiten. Aufgrund der geringen Nachfrage gibt es kein gesondertes Kapitel für Linux-UserInnen. Im Folgenden sind einige Links zur Hilfe aufgeführt. Für die Installation von R und RStudio unter Ubuntu empfiehlt sich die Installationsanleitung auf linuxhunt.com zu nutzen (englisch). Anstatt wget https://download1.rstudio.org/rstudio-0.99.896-amd64.deb ändern wir den Link zur gewünschten RStudio Version für unser Betriebssystem, z.B. wget https://download1.rstudio.org/desktop/bionic/amd64/rstudio-1.3.1073-amd64.deb. Den benötigten Link bekommen wir von der RStudio Seite mittels Rechtsklick auf den Link und Auswahl im erscheinenden Menü. Falls ihr das Problem habt, dass mit der obigen Anleitung nicht die aktuelle R Version 4.0 installiert wird, oder falls ihr zu R 4.0 upgraden wollt, schaut auch diese Anleitung an. Falls ihr nur Probleme mit dem Updaten auf die neuste Version habt, führt aus dem Abschnitt How can I update my R 3.x to the new shiny R4.x? den Code ab sudo apt-key ... im Terminal aus. Achtung: Die R- und RStudio-Versionen auf den Screenshots könnten bereits veraltet sein, wenn ihr die Anleitung nutzt. Die angegebenen Links führen euch aber dennoch zu den aktuellsten Versionen. Achtung: Zur Übernahme der Pakete bei Upgrade auf R 4.0+ Wir können das Paket installr (Windows) bzw. das manuelle Verschieben der Pakete (Mac) hier nicht nutzen, um unsere alten Pakete zu erhalten. Wir müssen alle Pakete neu installieren. Falls wir sehr viele Pakete haben, hilft uns folgende Anleitung von r-bloggers. 2.1 Windows Die folgenden Installationen wurden unter Nutzung von Firefox 70.0.1 durchgeführt. 2.1.1 Installation von R Auf dieser Seite klicken wir auf Download R for Windows und auf der folgenden Seite auf Install R for the first time. Dann kommen wir auf eine Seite, auf der wir die aktuellste R-Version finden. Diese Version läuft unter Windows XP, Windows Vista, Windows 8 und Windows 10. Sollten wir eine dieser Windows Versionen auf unserem Computer haben, drücken wir auf Download R 3.6.1 for Windows. Wie finde ich meine Windows Version heraus? Um die aktuelle Version unseres Windows Systems herauszufinden, drücken wir die Windows-Taste und die Pause Taste gleichzeitig. Achtung: Bei manchen Rechner muss man zur Aktivierung der Pause-Taste zusätzlich noch die fn-Taste drücken. Es öffnet sich ein Fenster mit den grundlegenden Informationen über unseren Computer. Ganz oben sehen wir dort unter Windows Edition, welche Windows Version wir installiert haben. Wichtig ist auch der Eintrag unter Systemtyp. Hier steht entweder 64 bit-Betriebssystem oder 32 bit-Betriebssystem. R gibt es sowohl für das 64- als auch für das 32 bit-Betriebssystem. Wenn wir auf Download R 3.6.1 for Windows drücken, laden wir beide Versionen herunter und R erkennt dann automatisch, welche installiert werden kann. Ältere Windows-Versionen Falls unser Betriebssystem nicht kompatibel ist, können wir hier ältere R-Versionen finden. Wir müssen jeweils auf die Version klicken und kommen dann auf eine Seite, die genau so aufgebaut ist wie die Seite der aktuellsten Version. Um zu erfahren, ob die Version mit unserem Betriebssystem kompatibel ist, müssen wir jeweils auf Does R run under my version of Windows? klicken. Wenn wir die passende Version gefunden haben, klicken wir auf Download R [Versionsnummer] for Windows. Die Versionsnummer ist abhängig davon, welche R Version tatsächlich mit unserem Betriebssystem kompatibel ist. Nach dem Herunterladen der .exe-Datei können wir diese öffnen. Es folgen einige Einstellungen und Zustimmungen, die wir uns kurz anschauen wollen. Wir werden gefragt, ob wir Änderungen an unseren Gerät zulassen wollen und wir bejahen. Dann können wir die Standardsprache für R einstellen. Diese kann man später auch im Programm noch ändern. Achtung: Es kann hilfreich sein, Englisch als Sprache festzulegen. So würden wir Fehlermeldungen auf englisch ausgegeben bekommen, zu denen wir bei Suchen im Internet zumeist mehr finden. Später können wir noch den Zielordner festlegen und auswählen, welche Komponenten installiert werden sollen. Normalerweise sollten alle wichtigen Komponenten automatisch ausgewählt sein. Wenn wir ein 64 bit-Betriebssystem haben, werden sowohl die 64- als auch die 32 bit-Version von R ausgewählt. Zweiteres können wir abwählen, damit wir das Programm nicht zweimal installieren. Nun werden wir gefragt, ob wir die Startoptionen von R ändern wollen. Wir können hier erstmal verneinen und diese später im Programm noch anpassen. Was sind Startoptionen? Bei den Startoptionen können wir z.B. einstellen, wie R später die Hilfe-Seiten einzelner Funktionen anzeigt. Da wir mit der Entwicklungsumgebung RStudio arbeiten werden, können wir diese Einstellungen einfach überspringen. Anschließend können wir den Startmenü-Ordner auswählen, d.h. festlegen, ob und wo Verknüpfungen zum Programm erstellt werden sollen. Da wir R später über RStudio aufrufen werden, können wir auch hier einfach mit der Voreinstellung auf Weiter drücken. Nun können wir noch zusätzliche Aufgaben auswählen z.B. ob auf dem Startbildschirm eine Verknüpfung zum Programm erstellt werden soll. Danach müssen wir die Installation nur noch fertigstellen. 2.1.2 Installation von RStudio Nachdem wir R heruntergeladen haben, können wir nun auch die Entwicklungsumgebung RStudio herunterladen. Dazu gehen wir auf die RStudio-Seite und scrollen auf der Seite nach unten. Hier überprüfen wir, ob die uns empfohlene Version mit unserem Betriebssystem sowie unserer R-Version kompatibel ist. Wenn ja, können wir auf Download RStudio for Windows klicken. Bei mir steht zwar Download RStudio Desktop, aber ich kann nicht darauf klicken. In diesem Fall können wir auf der Seite weiter nach unten scrollen. Dort werden unter All Installers alle aktuellen Versionen von RStudio Desktop aufgeführt. Unter dem Eintrag OS sind alle Betriebssysteme aufgelistet, für die es die aktuellste RStudio Desktop Version gibt. Wenn wir unser Betriebssystem gefunden haben, wählen wir in der Spalte Download die passende RStudio Desktop Version aus und laden diese herunter. Was bedeutet “RStudio 1.2 requires a 64-bit operating system. If you are on a 32 bit system, you can use an older version of RStudio.”? Ob wir die 32- oder die 64-bit Version von RStudio Desktop brauchen, hängt von unserem Computer ab. Für fast alle Computer können wir die 64-bit Version herunterladen. Sind wir uns nicht sicher, welche Version wir brauchen, können wir hier nachschauen. Wenn wir ein 32 bit-Betriebssystem haben, müssen wir eine ältere Version von RStudio Desktop herunterladen. Diese finden wir unter diesem Link. Andernfalls können wir einfach die aktuelle Version mit Download RStudio for Windows herunterladen. Wir speichern die .exe-Datei und klicken dann auf diese. Wir müssen auch hier wieder zustimmen, dass Änderungen am System vorgenommen werden. Dann öffnet sich Fenster mit dem Installationsassistent. Hier werden ähnliche Einstellungen wie bei der Installation von R besprochen. Abschließend klicken wir auch hier auf fertigstellen. 2.1.3 Aktualisierung von R mit Übernahme der Pakete aus der älteren Version (Paket installr) Wenn eine aktuellere (für unser Betriebssystem kompatible) Version von R vorhanden ist, ist es ratsam, diese herunterzuladen. Mit Aktualisierungen werden etwaige Fehler und Sicherheitslücken behoben und ggf. neue Funktionen eines Programms implementiert. Für Windows können wir zur Aktualisierung von R auf das Paket installr zurück greifen, welches den Prozess weitestgehend automatisiert. Achtung: Wenn wir von einer älteren R -Version auf R 4.0.0 wechseln, müssen wir all unsere Pakete neu installieren. Das Verschieben dieser mit dem Paket installr funktioniert hier nicht. Dazu installieren wir das Paket z.B. via install.packages(\"installr\"). Nachdem wir das Paket mit library(\"installr\") geladen haben, führen wir die enthaltene Funktion updateR() aus. Wenn wir die aktuellste R-Version installiert haben, bekommen wir ein FALSE ausgegeben. Wenn es eine aktuellere (kompatible) R-Version gibt, öffnet sich ein neues Fenster. Wenn wir auf OK geklickt haben, werden wir gefragt, ob wir uns die Neuerungen dieser Version anschauen wollen (optional). Diese würden sich in einem neuen Tab im Browser öffnen. Anschließend können wir die neuere Version installieren, indem wir auf Ja klicken. Wenn wir in RStudio sind, werden wir gefragt, ob wir die Installation via updateR() lieber in R ausführen wollen. Wir können die Installation aber auch einfach in RStudio fortführen. Nun kommen die gleichen Einstellungen und Zustimmungen wie bei der Installation von R. Nach Abschluss der Installation werden wir gefragt, ob wir die Pakete unserer alten R-Version in die neue übernehmen wollen … … und die Pakete aus den alten Ordnern löschen wollen. Hier können wir Ja anklicken, da wir nicht vorhaben, die alte R-Version noch zu nutzen (diese könnten wir also auch löschen). Wir können unsere Starteinstellungen für R (Rprofile.site) ebenso in die neuere Version übernehmen. Abschließend werden wir sogar noch gefragt, ob wir unsere (verschobenen) Pakete aktualisieren wollen. 2.1.4 Aktualisierung von RStudio Aktualisierungen für die Entwicklungsumgebung RStudio gibt es wesentlich seltener als für das dahinter liegende Basisprogramm R. Zur Überprüfung, ob eine aktuellere Version vorliegt, können wir in RStudio auf Help –&gt; Check for Updates klicken (in R gibt es diese Option nicht). Wenn es eine aktuellere Version gibt, öffnet sich die RStudio-Seite im Browser. Ich finde die Option Check for Updates nicht. Manchmal gibt es die Option Check für Updates nicht im Help-Menü. Analog dazu können wir auch manuell unsere bestehende mit der aktuellsten RStudio-Version abgleichen. Dazu klicken wir in RStudio auf RStudio –&gt; About RStudio … … so dass sich folgendes Fenster öffnet, in dem wir unsere bestehende RStudio-Version in Erfahrung bringen können. Informationen über die aktuellste RStudio-Version finden wir wieder unten auf der RStudio-Seite. Wir überprüfen, ob die uns empfohlene Version mit unserem Betriebssystem sowie unserer R-Version kompatibel ist. Wenn ja, können wir auf Download RStudio for Windows klicken. Das weitere Vorgehen ist ebenso analog zur Installation von RStudio. 2.2 Mac Die folgenden Installationen und Aktualisierungen wurden unter Nutzung von Safari 13.0.3 durchgeführt. 2.2.1 Installation von R Unter folgendem Link unter Download R for (Mac) OS X finden wir die aktuellste R-Version ganz oben. Wir kommen dann auf die folgende Seite: Achtung: Wir werden auf der Seite darauf hingewiesen, dass wir ab Mac OS X 10.9 (Mavericks) XQuartz nach jedem Upgrade der Betriebssoftware neu installieren sollten. Falls wir das noch nicht gemacht haben, sollten wir das vor der Installation von R noch tun. Installation von XQuartz Wir klicken auf die Verlinkung und gelangen auf die XQuartz-Seite. Zuerst schauen wir, ob die aktuelle XQuartz-Version mit unserem Betriebssystem kompatibel ist. Wenn das der Fall sein sollte, laden wir sie herunter. Anschließend klicken wir auf die .dmg-Datei und ein neues Fenster öffnet sich. Wir klicken auf die .pkg-Datei. Dadurch öffnet sich das Installationsmenü. Wir klicken jeweils auf Fortfahren und stimmen dem Lizenzvertrag zu. Wir müssen den Änderungen an unserem System zustimmen, indem wir unser Benutzerpasswort eingeben. Anschließend ist die Installation abgeschlossen. Nun müssen wir schauen, ob unser Betriebssystem den Mindestanforderungen für die aktuellste R Version entspricht. Wenn ja, können wir auf die .pkg-Datei klicken und den Download starten. Wie finde ich meine Mac OS Version heraus? Unsere Version des Betriebssystems bestimmt, welche Versionen von R und R Studio wir herunterladen können. Wir finden unsere Betriebssystem-Version heraus, indem wir auf den Apfel in der Menüleiste am oberen Bildschirm und dann auf Über diesen Mac klicken. Daraufhin erscheint ein Fenster, in dem wir die Informationen ablesen können. Falls die Mindestanforderungen nicht erfüllt sind, können wir trotzdem auf der gleichen Seite runterscrollen und nachschauen, ob eine ältere Version (für Mac OS X 10.9+ oder 10.6-10.8) vorliegt. Noch ältere R-Versionen für Mac OS X 10.4 und älter finden wir hier. Wenn die .pkg- bzw. .dmg-Datei heruntergeladen wurde, klicken wir auf diese. Dann öffnet sich folgendes Fenster: Nun müssen wir uns durchklicken und den Bedingungen zustimmen. Wenn die Installation erfolgreich war, erscheint folgendes Bild: Die .pkg- bzw. .dmg-Datei(en) können wir nach der Installation löschen. .dmg-Dateien müssen vorher noch ausgeworfen werden. Wir finden diese bei Geräte im Finder. 2.2.2 Installation von RStudio Nun können wir auf die RStudio-Seite gehen, nach unten scrollen und prüfen, ob die uns empfohlene Version mit unserem Betriebssystem sowie unserer R-Version kompatibel ist. Wenn unser Betriebssystem älter ist, können wir hier eine ältere RStudio-Version herunterladen. Ganz oben finden wir hier kompatible RStudio-Versionen für Mac OS X 10.11 (El Capitan) und ältere Betriebssystemversionen. Wenn wir die für unser Betriebssystem kompatible Version heruntergeladen haben, können wir die .dmg-Datei öffnen und installieren. Um diesen Prozess abzuschließen, müssen wir das Programm in den Applications-Ordner verschieben. Die .dmg-Datei können wir nach der Installation löschen. Vorher muss diese noch ausgeworfen werden. Wir finden die .dmg-Datei bei Geräte im Finder. 2.2.3 Aktualisierung von R Wenn eine aktuellere (für unser Betriebssystem kompatible) Version von R vorhanden ist, ist es ratsam, diese herunterzuladen. Mit Aktualisierungen werden etwaige Fehler und Sicherheitslücken behoben und ggf. neue Funktionen eines Programms implementiert. Zur Überprüfung, ob Aktualisierungen vorhanden sind, können wir in R oben in der Menüleiste auf R –&gt; Nach R Updates suchen klicken (in RStudio gibt es diese Option nicht). Entweder wir bekomen in der Konsole nun die Ausgabe, dass unsere Version aktuell ist … … oder in unserem Browser öffnet sich die Seite von CRAN, auf der wir die aktuellste Version von R herunterladen können. Nun müssen wir noch überprüfen, ob die Mindestanforderungen an unser Betriebssystem erfüllt sind. Falls die Mindestanforderungen nicht erfüllt sind, können wir trotzdem auf der gleichen Seite runterscrollen und nachschauen, ob eine aktuellere als unsere derzeitige Version vorliegt. Das weiterführende Vorgehen ist das Gleiche wie bei der initialen Installation von R: Wir laden die .pkg-Datei herunter, öffnen sie und führen die Installation aus. Achtung: Aus unbekannten Gründen kann es vorkommen, dass uns über Nach R Updates suchen nicht mitgeteilt wird, dass unsere derzeitige Version nicht die aktuellste ist. Wir können auch analog unsere derzeitige mit der aktuellsten Version abgleichen. Die Information über unsere derzeitige R-Version wird uns sowohl in R als auch in RStudio nach Öffnen des Programms ganz oben in der Konsole angezeigt. Der Screenshot ist aus RStudio, aber in R bekommen wir dieselbe Information. Dann müssen wir diese nur noch mit der aktuellsten Version auf CRAN abgleichen. 2.2.3.1 Pakete aus der älteren in die neuere R-Version übernehmen Achtung: Die nachfolgend beschriebene Verschiebung von Paketen ist nur notwendig, wenn sich die R-Version in der ersten Nachkommastelle ändert z.B. Version 3.5 zu 3.6. Bei kleineren Updates z.B. Version 3.6.0 zu 3.6.1 verändert sich der Pfad der Pakete nicht. Achtung: Wenn wir von einer älteren R -Version auf R 4.0.0 wechseln, müssen wir all unsere Pakete neu installieren. Das manuelle Verschieben dieser mit dem nachfolgend vorgestellten Weg funktioniert hier nicht. Wenn wir eine neuere R-Version heruntergeladen haben, sind unsere Pakete, die wir unter der vorherigen R-Version installiert haben, nicht mehr nutzbar im aktualisierten Programm. Das liegt daran, dass ein neuer Ordner für die aktuelle Version erstellt wurde, in den die Pakete nicht automatisch verschoben wurden. Um unsere Pakete im aktualisierten Programm nutzen zu können, müssen wir diese in den neuen Ordner verschieben. Dazu öffnen wir den Finder und nutzen den Kurzbefehl shift + cmd + G, um die Dateipfadsuche zu öffnen. In dieser suchen wir den Pfad /Library/Frameworks/R.framework/Versions/. Wir bekommen nun die Ordner der verschiedenen R-Versionen angezeigt. Wir machen nun einen Rechtsklick auf den Ordner der vorherig genutzten R-Version (hier: 3.5) und öffnen diesen in einem neuen Tab. Der Ordner enthält nur den Ordner Resources und dieser wiederum nur den Ordner library. Wen wir diesen öffnen, sehen wir die Ordner aller Pakete. Wir wählen schonmal alle Ordner an. Nun gehen wir zurück auf den anderen Tab, in dem die Ordner der verschiedenen Versionen von R gelistet sind. Wir öffnen den Ordner der aktuellesten Version (hier: 3.6) und klicken auch hier auf den Ordner Resources und dann library. Wir gehen jetzt zurück auf den anderen Tab (der älteren R-Version) und ziehen die Pakete in den library-Ordner der neuen R-Version. Achtung: Wenn Paket-Ordner doppelt vorliegen, dann werden wir gefragt, ob wir diese ersetzen wollen. Wir sollten verneinen, da wir ansonsten Probleme mit der Nutzung der alten Standard-Pakete in der neuen R-Version bekommen könnten. Tipp: Nach der Aktualisierung von R können wir mit großer Wahrscheinlichkeit auch einige Pakete aktualisieren. Für Hilfe dabei können wir uns das gleichnamige Kapitel anschauen. 2.2.4 Aktualisierung von RStudio Aktualisierungen für die Entwicklungsumgebung RStudio gibt es wesentlich seltener als für das dahinter liegende Basisprogramm R. Zur Überprüfung, ob eine aktuellere Version vorliegt, können wir in RStudio auf Help –&gt; Check for Updates klicken (in R gibt es diese Option nicht). Wenn es eine aktuellere Version gibt, öffnet sich die RStudio-Seite im Browser. Ich finde die Option Check for Updates nicht. Manchmal gibt es die Option Check für Updates nicht im Help-Menü. Analog dazu können wir auch manuell unsere bestehende mit der aktuellsten RStudio-Version abgleichen. Dazu klicken wir in RStudio auf RStudio –&gt; About RStudio … … so dass sich folgendes Fenster öffnet, in dem wir unsere bestehende RStudio-Version in Erfahrung bringen können. Informationen über die aktuellste RStudio-Version finden wir wieder auf der RStudio-Seite. Wenn unsere Version nicht mehr aktuell ist, überprüfen wir, ob die uns empfohlene Version mit unserem Betriebssystem sowie unserer R-Version kompatibel ist. Das weitere Vorgehen ist weitestgehend deckungsgleich mit dem der initialen Installation von RStudio. Einziger Unterschied ist, dass wir die ältere durch die neuere RStudio-Version ersetzen. 2.3 Hinweis zur Replizierbarkeit von Analysen Wir sollten bei Analysen in R immer berichten, in welcher Version unser Betriebssystem, R und unsere genutzten Pakete vorliegen. Sonst kann die Replikation unserer Ergebnisse Anderen schwer fallen, z.B. weil die Funktionen sich in ihrer Berechnung oder Funktionsweise zwischen verschiedenen Versionen von Paketen unterscheiden können. Mit der Funktion sessionInfo() bekommen wir all diese Informationen auf einen Schlag. "],["einführung-in-r.html", "Chapter 3 Einführung in R 3.1 1. Funktionen &amp; Pakete 3.2 2. Daten 3.3 3. Weitere Hilfen", " Chapter 3 Einführung in R Einleitung R ist eine kostenlose und quelloffene Statistiksoftware, die in Wissenschaft und Wirtschaft genutzt wird. Wir finden in R eine Fülle an Funktionen, die uns (statistische) Berechnungen und grafische Visualisierungen ermöglichen. R ist die grundlegende Software, die unsere Berechnungen anstellt. RStudio ist eine zusätzliche Entwicklungsumgebung, die uns die Arbeit mit R vereinfacht. Im Verlauf dieses Kapitels lernen wir, was Funktionen und Pakete sind und wie wir diese in R nutzen. Danach schauen wir uns an, wie wir Daten speichern, um mit ihnen arbeiten zu können. Am Ende des Kapitels finden wir wesentliche Kurzbefehle für Windows und Mac und weiterführende Hilfen. Achtung: Voraussetzung zur Bearbeitung des Kapitels ist, dass wir (zumindest) R installiert haben. Weiterführend ist es sinnvoll, sich die Einführung in RStudio anzuschauen. 3.1 1. Funktionen &amp; Pakete Fangen wir damit an, was Funktionen und Pakete sind, warum wir diese nutzen und wie wir diese in R anwenden können. Achtung: Alles, was wir in einer Zeile hinter eine Raute (#) schreiben, wird nicht als Funktion, sondern als Kommentar interpretiert. Unseren Code zu kommentieren ist sehr nützlich, da es uns und Anderen die Nachvollziehbarkeit unseres Vorgehens erleichtert. 3.1.1 Funktionen Funktionen sind (Unter)Programme, die eine gewisse Funktionalität haben, d.h. eine bestimmte Aufgabe ausführen. Warum ist es sinnvoll, Funktionen zu nutzen? Wenn wir beispielsweise den Mittelwert einer Zahlenreihe errechnen wollen, nutzen wir folgenden Code: # Mittelwert der Zahlenreihe selbst berechnen (4+3+6+2+3)/5 ## [1] 3.6 Dabei müssen wir die Zahlen aufsummieren und durch deren Anzahl teilen. Wenn wir weiter mit der Zahlenreihe arbeiten wollen, müssen wir sie außerdem wieder eingeben. Viel einfacher können wir die Aufgabe ausführen indem wir vorgefertigte Funktionen nutzen. # Zahlenreihe (Vektor) erstellen nums &lt;- c(4, 3, 6, 2, 3) nums ## [1] 4 3 6 2 3 # Mittelwert errechnen lassen mean(x = nums) ## [1] 3.6 Wir können Funktionen variierenden Input übergeben. Dieser steht immer in Klammern direkt hinter der Funktion. Beispielsweise könnten wir auch den Mittelwert einer anderen Zahlenreihe als nums berechnen. Den Input übergeben wir an einen Parameter. Das ist eine (formale) Variable einer Funktion (z.B. mean(x)), die in der Funktionsdefinition festgelegt ist. Der tatsächliche Input nennt sich Argument (z.B. mean(x = nums)). Die Parameter einer Funktion werden mit Kommata getrennt. Schematisch sieht eine Funktion somit folgendermaßen aus: function(parameter_1 = argument_1, parameter_2 = argument_2, …) Werfen wir einmal einen Blick in die Funktionsdefinition von mean(): Die Funktionsdefinition finden wir in der Dokumentation, welche wir in RStudio unter Help öffnen können. Manche Parameter besitzen voreingestellte Argumente (z.B. na.rm=FALSE); diese bezeichnet man als Defaults. Funktionen mit (min. einem) Parameter ohne Default (z.B. mean()) werden ohne Spezifikation dieser nicht ausgeführt. Beispielsweise müssen wir dem Parameter x einen Vektor, von dem wir den Mittelwert berechnen wollen, übergeben. Parameter ermöglichen uns aber nicht nur, eine Aufgabe mit verschiedenen Daten durchzuführen, sondern auch weitere Optionen zu wählen (z.B. na.rm: Ausschluss von fehlenden Werten; trim: trimmen der Enden der Verteilung der Zahlenreihe vor Berechnung des Mittelwerts). Wenn unsere Zahlenreihe beispielsweise fehlende Werte (in R: NA) besitzt, müssen wir den Default von mean(..., na.rm=FALSE) ändern, sodass fehlende Werte aus der Berechnung entfernt werden. # fehlenden Wert hinzufügen nums &lt;- c(nums, NA) nums ## [1] 4 3 6 2 3 NA # Mittelwert errechnen lassen mean(nums) # funktioniert nicht, weil unklar was mit NA passieren soll ## [1] NA mean(nums, na.rm=TRUE) # funktioniert, weil NA aus Berechnung entfernt ## [1] 3.6 Wann können wir Parameternamen weglassen (wie bei mean(nums))? Wir müssen x=nums nicht ausschreiben, weil x der erste Parameter in der Funktionsdefinition ist, und unser Argument num automatisch dem Parameter x zugeordnet wird. Das funktioniert mit jedem Argument solange wir die Reihenfolge der Parameter in der Funktionsdefinition beachten. mean(nums, TRUE) # funktioniert nicht (wir würden eine Fehlermeldung erhalten), ... # ... weil trim (und nicht na.rm) an zweiter Stelle steht ... # ... und trim numerischen Input (eine Zahl zwischen 0 und 0.5) ... # ... und na.rm logischen Input (TRUE oder FALSE) verlangt mean(nums, 0, TRUE) ## [1] 3.6 So erleichtern uns Funktionen unsere Arbeit. Zusammengefasst hat die Nutzung von Funktionen folgende Vorteile: Organisation Programme (z.B. statistische Analysen) können sehr komplex werden. Durch die Nutzung von Funktionen teilen wir unser komplexes Programm in mehrere, kleinere (Unter)Programme (z.B. Vektor erstellen, Mittelwert berechnen, …). Wiederverwendbarkeit Wir können Funktionen immer wieder aufrufen. So wird unser Programm kompakter (DRY-Prinzip, Don’t Repeat Yourself) und wir reduzieren Fehler durch Kopieren von Code (z.B. Verzählen bei den Elementen durch die wir teilen wollen). Testen Weil wir weniger (redundanten) Code haben, können wir schneller Fehler finden (z.B. Tippfehler, fehlende Elemente). Erweiterbarkeit Funktionen können erweitert werden, um verschiedene Szenarien zu händeln. Meist hat das schon jemand für uns gemacht. Wir müssen den Parametern einer Funktion nur verschiedene Argumente übergeben (z.B. na.rm=TRUE zum Ausschluss von fehlenden Werten). Abstraktion Wir müssen die Funktion nicht im Detail verstehen. Es reicht zu wissen, wie der Name der Funktion ist (mean()), welchen Input wir übergeben (x; trim und na.rm optional weil mit Default) und welchen Output (arithmetisches Mittel) wir bekommen und wo wir sie finden, d.h. aus welchem Paket (base) sie stammt. All diese Informationen erhalten wir in der Dokumentation der Funktion z.B. in RStudio unter Help. Auf freecodecamp.org finden wir eine kurze Einführung zu R: R Programming Language explained. Es lohnt sich, vertiefend den Part zu Funktionen anzuschauen (~ 10min), um ein besseres Verständnis für die Arbeit mit R zu bekommen. 3.1.2 Pakete Funktionen (und Dateien) werden in sogenannten Paketen gespeichert. Dabei sind in einem Paket (häufig) Funktionen, die für einen begrenzten Aufgabenbereich genutzt werden. Es gibt Standardpakete, die man automatisch mit dem Download von R erhält und deren Funktionen und Dateien man einfach nutzen kann. Diese sind base (basale Funktionen wie z.B. c() und mean(), die wir gerade genutzt haben), datasets (Beispieldatensätze), graphics (Grafiken erstellen), grDevices (Farben und Schriften), methods (Methoden und Klassen erstellen bzw. Informationen erhalten), stats (statistische Methoden) und utils (z.B. Informationen zu Add-On Paketen erhalten und diese herunterladen). Zum Beispiel können wir mit der Funktion data() auf die in datasets enthaltenen Datensatze zugreifen. data {utils} data(women) Die Größe des Datensatzes sehen wir mit dim(). dim {base} dim(women) ## [1] 15 2 Der Datensatz women enthält 15 Fälle (Zeilen) und zwei Variablen (Spalten). Die Namen der Variablen erfahren wir mit names(). names {base} names(women) ## [1] &quot;height&quot; &quot;weight&quot; Mit den Funktionen aus den Standardpaketen können wir schon vieles machen. Weil R open-source ist, kann jeder eigene Pakete schreiben und Anderen zugänglich machen. Wir können auf diese Add-on Pakete, die andere R-NutzerInnen erstellt haben, über CRAN (Comprehensive R Archive Network) zugreifen. Natürlich können wir selbstgeschriebene Funktionen, die wir häufig nutzen, auch in eigenen Paketen speichern, um sie unkompliziert wieder nutzen zu können oder sie der R-Community zur Verfügung zu stellen. Add-on Pakete müssen wir einmalig herunterladen und jedes Mal, wenn wir sie (in einer R-Session) nutzen wollen, laden. Ein bei PsychologInnen beliebtes Paket ist psych. Dieses enthält Funktionen, die häufig in der Persönlichkeitspsychologie und Psychometrie genutzt werden. Wir können ein Add-on Paket mit folgendem Befehl herunterladen: install.packages(&quot;psych&quot;) # auf Anführungszeichen achten! Die im Paket enthaltene Funktion describe() gibt uns beispielsweise eine kompakte Übersicht relevanter deskriptiv-statistischer Kennwerte von Daten aus. Wenn wir enthaltene Funktionen nutzen wollen, müssen wir das Paket zuerst laden. library(psych) # keine Anführungszeichen notwendig describe(women) ## women ## ## 2 Variables 15 Observations ## -------------------------------------------------------------------------------- ## height ## n missing distinct Info Mean Gmd .05 .10 ## 15 0 15 1 65 5.333 58.7 59.4 ## .25 .50 .75 .90 .95 ## 61.5 65.0 68.5 70.6 71.3 ## ## lowest : 58 59 60 61 62, highest: 68 69 70 71 72 ## ## Value 58 59 60 61 62 63 64 65 66 67 68 ## Frequency 1 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 ## ## Value 69 70 71 72 ## Frequency 1 1 1 1 ## Proportion 0.067 0.067 0.067 0.067 ## -------------------------------------------------------------------------------- ## weight ## n missing distinct Info Mean Gmd .05 .10 ## 15 0 15 1 136.7 18.4 116.4 118.2 ## .25 .50 .75 .90 .95 ## 124.5 135.0 148.0 157.0 160.5 ## ## lowest : 115 117 120 123 126, highest: 146 150 154 159 164 ## ## Value 115 117 120 123 126 129 132 135 139 142 146 ## Frequency 1 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 0.067 ## ## Value 150 154 159 164 ## Frequency 1 1 1 1 ## Proportion 0.067 0.067 0.067 0.067 ## -------------------------------------------------------------------------------- Mit describe erhalten wir folgende Informationen zu jeder Variable im Data Frame bzw. Matrix: Namen, Spaltennummer (vars), Anzahl (gültiger) Fälle (n), Mittelwert (mean), Standarabweichung (sd), Median (median), getrimmter Mittelwert (trimmed), Median Absolute Deviation (mad), Minimum (min), Maximum (max), Schiefe (skew), Exzess (kurtosis) und Standardfehler des Mittelwerts (se). Diese Informationen finden wir in der Dokumentation, welche wir unter Help in RStudio ansehen können. Wenn wir R bzw. RStudio schließen und erneut öffnen, müssen wir zusätzliche Pakete vor der Nutzung erneut laden. Für mehr Informationen zu Paketen, u.a. wie wir diese aktualisieren können, schaut euch unser dazugehöriges Kapitel an. 3.2 2. Daten Um zu verstehen, wie R arbeitet, benötigen wir ein Verständnis dafür, wie Daten in R repräsentiert werden. Dazu schauen wir uns drei wichtige Konzepte an: Datentypen, Datenstrukturen und Objekte. Bevor wir uns die einzelnen Konzepte im Detail anschauen, sehen wir nachfolgend eine Veranschaulichung des Zusammenhangs dieser, um bereits eine grobe Vorstellung zu haben, was uns in den folgenden Abschnitten erwartet. Achtung: Die Gliederung nach “Datentyp” und “Datenstruktur” sind getreu des Manuals von R. Man stößt in anderen Quellen teils auf abweichende Benennungen. 3.2.1 Datentypen Der Datentyp gibt die Art der Daten an, d.h. welche konkreten Werte(bereiche) die Daten annehmen können und welche Operationen darauf anwendbar sind. Wir beschäftigen uns in R zumeist mit den folgenden Datentypen: character, logical, integer und double. Die letzten beiden werden (häufig) als numeric zusammengefasst. Nachfolgend finden wir eine Übersicht dieser Datentypen. Art der Daten Werte Operationen Datentyp in R Zeichen(ketten) z.B. “Ball” oder ‘@’ gleich oder ungleich character Wahrheitswerte TRUE, FALSE (einige) Logische Operatoren logical _ Ganze Zahlen z.B. 2 Arithmetische und Logische Operatoren integer numeric Kommazahlen z.B. 3.4 Arithmetische und Logische Operatoren double numeric Es gibt in R noch zwei weitere Datentypen, mit denen wir uns aber nicht weiter beschäftigen werden: complex und raw. Achtung: Kommazahlen werden mit . und nicht mit , dargestellt, weil Kommata genutzt werden, um Argumente einer Funktion voneinander zu trennen. Logische Operatoren in R Ein logischer Operator ist ein Operator, dessen Ergebnis ein Wahrheitswert (logical; TRUE oder FALSE) ist. Operator Vergleich Beispiel kleiner 1 FALSE kleiner gleich 1 TRUE &gt; größer 2 &gt; 1TRUE &gt;= größer gleich 1 &gt;= 3FALSE == (genau) gleich TRUE == FALSEFALSE != ungleich TRUE != FALSETRUE ! nicht (Negation von Bedingungen) !TRUEFALSE oder || Arithmetische Operatoren in R Über (die meisten) arithmetischen Operatoren sind wir wohl schon zu Grundschulzeiten gestoßen. Das sind Operatoren, die wir zum Rechnen mit Zahlen (numeric) benötigen. Operator Rechenoperation Beispiel .+ Addition 1 + 12 .- Subtraktion 4 - 31 .* Multiplikation 2 * 36 / Division 5 / 31.666667 ^ oder ** Exponenzieren 8^264 %% ganzzahliger Rest bei der Division (Modulo) 5 %% 32 %/% ganzzahliger Quotient 5 %/% 31 3.2.1.1 Messniveaus und Datentypen Recap: Messniveaus Das Messniveau (oder auch Skalenniveau) ist eine wichtige Eigenschaft von Merkmalen (Variablen) von Untersuchungseinheiten. Es beschreibt, welche Informationen in unseren Messwerten abgebildet werden und damit auch welche mathematischen Transformationen mit den Messwerten sinnvoll sind (z.B. das Berechnen von Mittelwerten). Somit begrenzt das Messniveau auch die zulässigen Datenauswertungsverfahren unserer Variablen. Die Kodierung von nominalskalierten Merkmalen ist insofern willkürlich, als dass lediglich auf Gleichheit versus Ungleichheit geachtet werden muss (z.B. 1, 4, 9 oder A, Y, M). Die Kodierung von ordinalskalierten Merkmalen geschieht der Größe nach, d.h. dass die Rangfolge der Kodierungen einzelner Gruppen relevant ist (z.B. 1 &lt; 4 &lt; 9 oder A &lt; M &lt; Y). Man kann aber auch eine eigene Sortierung festlegen, die nicht der “natürlichen” Rangfolge (Zahlen: aufsteigend; Buchstaben: alphabetisch) entspricht (z.B. Y &lt; A &lt; M). Ein Realschulabschluss ist beispielsweise besser als ein Hauptschulabschluss. Wir können aber nicht festlegen, wie viel besser er ist. Bei der Kodierung von intervallskalierten Merkmalen sind sowohl die Rangfolge als auch die Abstände zwischen den Ausprägungen relevant (z.B. 1, 4, 7; jeweils mit gleichem Abstand zueinander; oder 1.4, 1.5, 2.3; jeweils mit verschiedenen Abständen zueinander). Ein Beispiel dafür ist die Temperatur in Grad Celsius oder Grad Fahrenheit. Bei der Kodierung von verhältnisskalierten Merkmalen ist zusätzlich noch ein Nullpunkt vorhanden. Dieser erlaubt es, dass Quotienten zwischen Werten gebildet werden können. Ein beliebtes Beispiel ist die Kelvin Skala. Bei dieser ist bei 0°K keine Bewegungsenergie mehr vorhanden und 20°K sind halb so viel wie 40°K. Zu guter Letzt gibt es noch absolutskalierte Merkmale, welche sowohl einen eindeutigen Nullpunkt als auch eine eindeutige Einheit der Skala (z.B. Anzahl der Kinder) vorweisen kann. Die Kodierung entspricht der natürlichen Einheit. Nachfolgend finden wir eine Tabelle der möglichen Unterscheidungen der jeweiligen Messniveaus. (Un-) Gleichheit Rangordnung Abstände Verhältnisse natürliche Einheit Nominal X Ordinal X X Intervall X X X Verhältnis X X X X Absolut X X X X X Bildquelle: https://de.wikipedia.org/wiki/Datei:Skalenniveau.png Die verschiedenen Messniveaus können mit unterschiedlichen Datentypen repräsentiert werden. Hauptsächlich nutzt man dafür character und numeric. Nachfolgend finden wir eine Übersicht der möglichen Kodierungen der Messniveaus. _ Art der Skala: Nominal- Ordinal- Intervall- Verhältnis- Absolut- Datentyp: character X X\\(^2\\) Datentyp: numeric X\\(^1\\) X\\(^2\\) X X X \\(^1\\) Faktorisieren (unordered factor) notwendig wenn keine Indikatorvariable(n) genutzt \\(^2\\) Faktorisieren (ordered factor) notwendig Ein Faktor ist eine Art von Vektor. Mehr dazu im nächsten Abschnitt. Können Merkmale auch mit logical kodiert werden? Wir könnten auch logische Werte nutzen, um Merkmale zu kodieren, allerdings kann es sich dabei nur um dichotome nominalskalierte Merkmale handeln (d.h. diese können nur zwei diskrete Ausprägungen besitzen). Logische Werte und Operatoren kommen hauptsächlich in der Indexierung von Vektoren (diese lernen wir im nächsten Abschnitt kennen) … x &lt;- c(7,4,3,6,1) # Vektor x erstellen x ## [1] 7 4 3 6 1 # Welche Elemente in Vektor x sind größer als 5? x &gt; 5 # Output: logischer Vektor (mit T/F zu jedem Element) ... ## [1] TRUE FALSE FALSE TRUE FALSE x[x &gt; 5] # ... den wir auf x anwenden können, um die Elemente zu erhalten. ## [1] 7 6 … und der konditionalen Programmierung vor. for (i in 1:length(x)) { # Für jedes Element in x, begonnen bei 1 ... if (x[i] &gt; 5) { # ... wenn i-tes Element in x größer als 5 ... x[i] = x[i] * 2 # ... multipliziere Element mit 2 } } x ## [1] 14 4 3 12 1 Damit werden wir uns erst später beschäftigen. Anwendung findet die konditionale Programmierung z.B. wenn wir subsetten, d.h. Elemente, auf die eine Kondition (z.B. größer als 5) zutrifft, einer Datenstruktur (z.B. Vektor, Matrix, Dataframe) entnehmen wollen. 3.2.2 Datenstrukturen Die Datenstruktur bestimmt die Organisation und Speicherung von Daten(typen), und folglich auch, welche Funktionen wir anwenden können. Datenstrukturen können nach Dimensionalität und enthaltenen Datentypen klassifiziert werden. Nachfolgend befindet sich eine Übersicht der in R enthaltenen Datenstrukturen. _ Beinhaltet unterschiedliche Datentypen? nein (homogen) ja (heterogen) Anzahl der Dimensionen 1 Vektor Liste Anzahl der Dimensionen 2 Matrix Data Frame Anzahl der Dimensionen n Array Aus Platzgründen werden wir Arrays im Folgenden nicht behandeln. 3.2.2.1 Vektor Vektoren sind die elementare Datenstruktur, aus der sich alle anderen Datenstrukturen zusammensetzen. Sie besitzen nur eine Dimension. Mit c() können wir Vektoren erstellen. Generell können sie unterschiedlichen Typs sein … vek_1 &lt;- c(&quot;A&quot;, &#39;B&#39;) # egal ob &quot; oder &#39; vek_2 &lt;- c(F, T, T) # Abkürzung von FALSE und TRUE … aber ein Vektor kann nur einen Datentyp beinhalten. vek_3 &lt;- c(1, &quot;3&quot;) # alles wird zu character Zahlen können wir auf unterschiedliche Weisen speichern. vek_4 &lt;- c(1, 2, 3) # ganze Zahlen vek_5 &lt;- c(1.3, 4.5) # Kommazahlen vek_6 &lt;- c(1L, 4L) # ganze Zahlen Mit str() können wir uns den Datentyp, die Länge der Dimension (Anzahl der Elemente) und die ersten 10 Elemente ausgeben lassen. str(vek_1) ## chr [1:2] &quot;A&quot; &quot;B&quot; # chr --&gt; Datentyp character # [1:2] --&gt; enthält zwei Elemente # &quot;A&quot; &quot;B&quot; --&gt; ersten zwei (von max. 10) Elementen str(vek_2) ## logi [1:3] FALSE TRUE TRUE str(vek_3) ## chr [1:2] &quot;1&quot; &quot;3&quot; str(vek_4) ## num [1:3] 1 2 3 str(vek_5) ## num [1:2] 1.3 4.5 str(vek_6) ## int [1:2] 1 4 Warum sind die numerischen Vektoren nur vom Typ numeric oder integer? Bei Betrachtung der numerischen Vektoren fällt auf, dass vek_4 und vek_5 als numeric und vek_6 als integer gespeichert wurden. Aber warum wurden vek_4 und vek_5 als numeric gespeichert, obwohl wir integer (ganze Zahlen) bzw. double (Kommazahlen) erwartet hätten? Das liegt daran, dass R alle Zahlen (d.h. ganze, reelle und komplexe Zahlen) als numeric zusammenfasst (wie bereits in der Einführung zu Datentypen erwähnt), solange wir diese nicht explizit (als integer) definieren. Genau genommen lautete der exakte Datentyp von ganzen und reellen Zahlen, die als numeric zusammengefasst sind, double. Mit typeof() sehen wir diesen. typeof(vek_4) ## [1] &quot;double&quot; typeof(vek_5) ## [1] &quot;double&quot; Warum double (und nicht integer)? Wenn wir arithmetische Operatoren (v.a. Division) anwenden, dann werden unsere ganzen Zahlen zu Kommazahlen. Daher werden ganze und gebrochene Zahlen in numeric “präventiv” als double gespeichert. Und warum ist vek_6 vom Typ integer? Hier haben wir mit dem L hinter den (ganzen) Zahlen (c(1L, 4L)) explizit festgelegt, dass wir diese als integer speichern wollen. Wenn wir integer-Zahlenfolgen erstellen wollen, können wir das auch mit Anfang:Ende machen. vek_7 &lt;- 2:5 vek_8 &lt;- c(6:9, 1:4) str(vek_7) ## int [1:4] 2 3 4 5 str(vek_8) ## int [1:8] 6 7 8 9 1 2 3 4 Generell reicht für uns aber die Unterscheidung zwischen numeric und den anderen, nicht-numerischen Datentypen. Ob integer oder double ist zumeist nicht von Relevanz. Wenn wir Zahlenfolgen (numeric) erstellen wollen, können wir seq() nutzen. # seq(from,to,by) vek_9 &lt;- seq(1,10,2) vek_10 &lt;- seq(1,10,0.5) str(vek_9) ## num [1:5] 1 3 5 7 9 str(vek_10) ## num [1:19] 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 ... Wenn wir wollen, dass sich Elemente wiederholen, können wir die Funktion rep() nutzen. # rep(x, times) vek_11 &lt;- rep(&quot;A&quot;, 10) vek_12 &lt;- c(rep(1, 3), rep(2:3, 3)) str(vek_11) ## chr [1:10] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; str(vek_12) ## num [1:9] 1 1 1 2 3 2 3 2 3 Wie genau funktioniert rep()? Dem Parameter x übergeben wir die Zeichen(folge), die wir wiederholen wollen; times übergeben wir die Anzahl der Wiederholungen der Zeichenfolge bzw. each die Anzahl der Wiederholungen der einzelnen Zeichen. Die Zahl 1 wird 10 mal (times) wiederholt: rep(1, 10) # das gleiche wie: rep(x=1, times=10) ## [1] 1 1 1 1 1 1 1 1 1 1 Die Zahlenfolge 0, 1 bzw. die Zeichenfolge \"A\", \"B wird 10 mal (times) wiederholt: rep(0:1, 10) ## [1] 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 rep(c(&quot;A&quot;, &quot;B&quot;), 10) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; ## [20] &quot;B&quot; Wenn wir erst 10 mal die 0 bzw. \"A\" und anschließend 10 mal die 1 bzw. \"B\" haben wollen, nutzen wir den Parameter each. rep(0:1, each=10) # das gleiche wie c(rep(0, 10), rep(1, 10)) ## [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 rep(c(&quot;A&quot;, &quot;B&quot;), each=10) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; ## [20] &quot;B&quot; Durch Indexierung via [] können wir uns einzelne Elemente ausgeben lassen. vek_1[2] # zweites Element ## [1] &quot;B&quot; vek_5[1] # erstes Element ## [1] 1.3 3.2.2.1.1 Spezialfall Faktor Ein Faktor ist ein spezieller Vektor, der genutzt wird, um diskrete Klassifikationen zu kodieren. Mit der Funktion factor() können wir Vektoren in ungeordnete und geordnete Faktoren umwandeln. Unsortierte Faktoren können nominalskalierte Merkmale kodieren. # Vektor erstellen x &lt;- c(1,3,2,3,2) y &lt;- c(&quot;f&quot;, &quot;B&quot;, &quot;c&quot;, &quot;b&quot;, &quot;c&quot;) nominal_x &lt;- factor(x) nominal_y &lt;- factor(y) str(nominal_x) ## Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 3 2 3 2 str(nominal_y) ## Factor w/ 4 levels &quot;b&quot;,&quot;B&quot;,&quot;c&quot;,&quot;f&quot;: 4 2 3 1 3 Die Zahlen hinter den Ausprägungen zeigen die interne Kodierung. Bei numerischen Vektoren wie nominal_x entsprechen diese auch den möglichen Ausprägungen. Sortierte Faktoren können ordinalskalierte Merkmale kodieren. Um Faktoren zu sortieren, müssen wir dem Parameter ordered das Argument TRUE übergeben. ordinal_x &lt;- factor(x, ordered=TRUE) ordinal_y &lt;- factor(y, ordered=TRUE) str(ordinal_x) ## Ord.factor w/ 3 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;: 1 3 2 3 2 str(ordinal_y) ## Ord.factor w/ 4 levels &quot;b&quot;&lt;&quot;B&quot;&lt;&quot;c&quot;&lt;&quot;f&quot;: 4 2 3 1 3 Wie wir sehen wurde automatisch eine Sortierung festgelegt. Zahlen werden standardmäßig aufsteigend; Zeichen alphabetisch sortiert (wobei Kleinbuchstaben vor Großbuchstaben auftauchen). Bei den unsortierten Faktoren gab es diese Sortierung auch bereits, allerdings wird diese nur zum Darstellen der Ausprägungen genutzt (bei nominal_ sind die Ausprägungen mit , getrennt; bei ordinal_ mit &lt;). Mit dem Parameter levels können wir auch eigene Sortierungen festlegen. Das übergebene Argument muss selbst ein Vektor mit den möglichen Ausprägungen sein. ordinal_x.2 &lt;- factor(x, ordered=TRUE, levels=c(3,2,1)) ordinal_y.2 &lt;- factor(y, ordered=TRUE, levels=c(&quot;B&quot;, &quot;f&quot;, &quot;b&quot;, &quot;c&quot;)) str(ordinal_x.2) ## Ord.factor w/ 3 levels &quot;3&quot;&lt;&quot;2&quot;&lt;&quot;1&quot;: 3 1 2 1 2 str(ordinal_y.2) ## Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;f&quot;&lt;&quot;b&quot;&lt;&quot;c&quot;: 2 1 4 3 4 Jetzt sehen wir auch, dass sich (mit einer anderen als der natürlichen Sortierung) auch die internen Kodierungen geändert haben. 3.2.2.2 Matrix Matrizen sind zweidimensionale Vektoren, die nur einen Datentyp beinhalten können. In mathematischen Kontexten werden Matrizen uns häufiger begegnen. Wir erstellen sie mit matrix(data, nrow, ncol). mat_1 &lt;- matrix(data=c(1,2,3,4), # Daten-Vektor nrow=2, # Anzahl Zeilen ncol=2, # Anzahl Spalten # eine Angabe (Zeilen oder Spalten) reicht auch byrow=TRUE) # reihenweise Eintragen der Daten mat_1 ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 str(mat_1) # [Länge Zeilen, Länge Spalten] ## num [1:2, 1:2] 1 3 2 4 Wenn wir einzelne Elemente indexieren wollen, müssen wir zwei Indizes angeben, weil Matrizen zweidimensional sind. Zuerst die Zeile, dann die Spalte. # [Zeile, Spalte] mat_1[1,2] # Zeile 1, Spalte 2 ## [1] 2 mat_1[2,1] # Zeile 2, Spalte 1 ## [1] 3 Wir können auch nur einen Index angeben, um uns die komplette Zeile bzw. Spalte ausgeben zu lassen. Dabei müssen wir aber daran denken, das Komma zu setzen! mat_1[1,] # komplette erste Zeile ## [1] 1 2 mat_1[,2] # komplette zweite Spalte ## [1] 2 4 Zusätzlich können wir die Spalten und Zeilen von Matrizen benennen. colnames(mat_1) &lt;- c(&quot;A&quot;, &quot;B&quot;) # Spalten benennen rownames(mat_1) &lt;- c(&quot;Vpn_1&quot;, &quot;Vpn_2&quot;) mat_1 ## A B ## Vpn_1 1 2 ## Vpn_2 3 4 3.2.2.3 Liste Listen bestehen aus geordneten Sammlungen von Objekten (Komponenten) unterschiedlichen Datentyps. Diese Objekte können wiederum selbst Vektoren, Matrizen oder Dataframes sein. Listen haben nur eine Dimension. Mit list() können wir eigene Listen erstellen. list_kurs &lt;- list(kurs=&quot;Programmieren&quot;, teilnehmer=3, namen.teilnehmer=c(&quot;Tina&quot;, &quot;Paul&quot;, &quot;Lena&quot;), vorerfahrung=c(T, F, F)) list_kurs ## $kurs ## [1] &quot;Programmieren&quot; ## ## $teilnehmer ## [1] 3 ## ## $namen.teilnehmer ## [1] &quot;Tina&quot; &quot;Paul&quot; &quot;Lena&quot; ## ## $vorerfahrung ## [1] TRUE FALSE FALSE str(list_kurs) # &quot;List of ...&quot; gibt die Länge der (einen) Dimension der Liste an ## List of 4 ## $ kurs : chr &quot;Programmieren&quot; ## $ teilnehmer : num 3 ## $ namen.teilnehmer: chr [1:3] &quot;Tina&quot; &quot;Paul&quot; &quot;Lena&quot; ## $ vorerfahrung : logi [1:3] TRUE FALSE FALSE Wir können Komponenten bzw. ihre Elemente auf verschiedene Arten indexieren. list_kurs[3] # Name und Elemente der dritten Komponente ## $namen.teilnehmer ## [1] &quot;Tina&quot; &quot;Paul&quot; &quot;Lena&quot; list_kurs[[3]] # nur Elemente der dritten Komponente ## [1] &quot;Tina&quot; &quot;Paul&quot; &quot;Lena&quot; list_kurs[[3]][2] # zweites Element der dritten Komponente ## [1] &quot;Paul&quot; Wir können alle Elemente einer Komponente ebenso mit ihren Namen extrahieren. list_kurs$vorerfahrung ## [1] TRUE FALSE FALSE Listen werden uns häufiger als Output statistischer Funktionen begegnen. 3.2.2.4 Data Frame In der Psychologie arbeiten wir zumeist mit Data Frames. Diese haben, wie Matrizen, zwei Dimensionen, aber sie können auch unterschiedliche Datentypen beinhalten. Um Data Frames zu erstellen, spezifizieren wir zuerst Vektoren (unterschiedlichen Typs) und führen diese dann mit data.frame() zusammen. # Vektoren erstellen: eins &lt;- c(1, 3, 2, 1) zwei &lt;- c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;) # in Data Frame zusammenführen df_1 &lt;- data.frame(eins, zwei) df_1 ## eins zwei ## 1 1 A ## 2 3 A ## 3 2 B ## 4 1 B str(df_1) # obs. = Länge Zeilen, variables = Länge Spalten ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ eins: num 1 3 2 1 ## $ zwei: chr &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; Die Benennung der Vektoren wird als Spaltenbenennung übernommen. Per Default werden Daten vom Typ character (z.B. zwei) als Faktoren gespeichert (stringsAsFactors = default.stringsAsFactors(), was wiederum TRUE ist). Mit colnames() bzw. rownames() können wir wieder Spalten- bzw. Zeilennamen ändern bzw. hinzufügen. colnames(df_1) &lt;- c(&quot;AV&quot;, &quot;UV&quot;) rownames(df_1) &lt;- c(&quot;Fall_1&quot;, &quot;Fall_2&quot;, &quot;Fall_3&quot;, &quot;Fall_4&quot;) # mit data.frame(..., row.names) könnten wir auch initial Zeilennamen übergeben df_1 ## AV UV ## Fall_1 1 A ## Fall_2 3 A ## Fall_3 2 B ## Fall_4 1 B Wir können mit Indexieren wieder einzelne Elemente oder Spalten bzw. Zeilen extrahieren. Spalten bzw. Zeilen können wir hier auch mit ihren Namen ansprechen. df_1[3,2] # dritte Zeile, zweite Spalte ## [1] &quot;B&quot; df_1[&quot;Fall_3&quot;, &quot;UV&quot;] # dritte Zeile, zweite Spalte ## [1] &quot;B&quot; df_1[1,] # erste Zeile ## AV UV ## Fall_1 1 A df_1[&quot;Fall_1&quot;,] # erste Zeile ## AV UV ## Fall_1 1 A Spalten können wir uns auch mit dem $-Operator ausgeben lassen mit der Form df_name$spalten_name. df_1$AV ## [1] 1 3 2 1 Einige Funktionen verlangen Data Frames als Input. Wir können z.B. Matrizen mittels as.data.frame() in Data Frames umwandeln. 3.2.3 Objekte “Everything that exists in R is an object” - John Chambers (Entwicklungsteam von R) R arbeitet mit sogenannten Objekten. Alle Entitäten, mit denen wir in R operieren, sind Objekte. So sind alle Datenstrukturen, die wir gerade kennengelernt haben, Objekte sobald wir ihnen einen Namen zugewiesen haben. Um ein Objekt zu erstellen, nutzen wir den Zuweisungspfeil &lt;-. obj &lt;- c(1, 2, 3) Wir können uns Objekte anschauen, indem wir ihren Namen ausführen oder indem wir View() nutzen. Zweiteres öffnet das Objekt im Data Viewer (in RStudio). obj ## [1] 1 2 3 Alle Objekte, die derzeit in R vorhanden sind, bekommen wir mit ls() angezeigt. Wenn wir Objekte löschen wollen, nutzen wir rm(). Mit rm(objekt_1, objekt_2&gt;, …) löschen wir einzelne Objekte; mit rm(list = ls()) löschen wir alle. Zur Benennung von Objekten ist folgendes zu wissen: alle alphanumerischen Zeichen sowie . und _ sind erlaubt In Deutsch schließt das Groß- und Kleinbuchstaben des gesamten Alphabets und der Umlaute sowie die Zahlen 0-9 ein. Um Enkodierungsprobleme (u.a. zwischen verschiedenen Systemen) zu reduzieren, sollten wir aber auf Umlaute verzichten; auch in der Benennung unserer Ordner außerhalb von R. Namen sollten nicht mit . oder _ beginnen. Groß- und Kleinschreibung beachten (case-sensitivity) Das gilt auch für Funktionen; z.B. funktioniert View() nur, wenn der erste Buchstabe groß geschrieben wird. Bestehende Objekte können de facto nicht mehr umbenannt werden. Wir können sie aber in einem neuen Objekt (mit einem neuen Namen) speichern (und ggf. das alte Objekt löschen). 3.3 3. Weitere Hilfen 3.3.1 Kurzbefehle Nachfolgend finden wir einige Kurzbefehle für die beiden Betriebssysteme Windows und Mac, die unseren Workflow verbessern. Achtung: Mit dem letzten Kurzbefehl können wir schneller Dateipfade kopieren, was uns das Einlesen von Daten in R erleichtert. Dieser Kurzbefehl ist, im Gegensatz zu den anderen, nicht zur Nutzung in R geeignet; er funktioniert nur im Explorer (Windows) bzw. Finder (Mac). Windows Mac Code der aktuellen Linie bzw. markierten Code ausführen Strg + enter cmd + enter Code bis zur aktuellen Linie ausführen alt + Strg + B alt + cmd + B Skript speichern Strg + S cmd + S Dateipfad kopieren shift + Rechtsklick auf Dateidann Als Pfad kopieren alt + cmd + C Eine Übersicht weiterer Kurzbefehle für R finden wir in der Leiste ganz oben unter Help &gt; Keyboard Shortcuts Help (nicht zu verwechseln mit dem Bereich Help, der uns Zugang zur Dokumentation verschafft). 3.3.2 Andere Lernplattformen und Übungen Wie bereits im Abschnitt Funktionen erwähnt, können wir auf freecodecamp.org eine weiterführende Vertiefung zu Funktionen im gleichnamigen Abschnitt bekommen. Das dauert nur ca. 10min und ist gut investierte Zeit, wenn man wenig Erfahrung mit dem Programmieren hat. Wenn ihr einen amüsanten Einstieg in R haben wollte, schaut euch YaRrr! The Pirate’s Guide to R (auf englisch) an. Nach einer charismatischen Einführung befinden sich mehrere Abschnitte, teils mit Abschlussübung z.B. zu Skalaren und Vektoren, Vektor-Funktionen, Indexierung, Plotten, uvm.. www.r-exercises.com bietet eine Fülle an Übungen (mit Lösungen) zu verschiedenen Themenbereichen, wie z.B. Vektoren und Data Frames. Mit Ausnahme der mit Protected gekennzeichneten Seiten können wir alle nutzen. Rechts unter Filter by Topic können wir die Themen filtern. Wenn ihr den Umgang mit R direkt in R lernen wollt, dann schaut euch das Paket swirl an. Es gibt mehrere Kurse mit mehreren kleinen Einheiten zu absolvieren. Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ICC_2.4.0 readr_2.1.3 Hmisc_4.7-1 Formula_1.2-4 ## [5] survival_3.2-13 lattice_0.20-45 ggplot2_3.4.0 colorspace_2.0-3 ## [9] psych_2.2.9 car_3.1-1 carData_3.0-5 kableExtra_1.3.4 ## [13] dplyr_1.0.10 htmltools_0.5.3 rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.2 sass_0.4.2 jsonlite_1.8.3 ## [4] viridisLite_0.4.1 splines_4.2.0 bslib_0.4.0 ## [7] assertthat_0.2.1 highr_0.9 latticeExtra_0.6-30 ## [10] yaml_2.3.5 pillar_1.8.1 backports_1.4.1 ## [13] glue_1.6.2 digest_0.6.30 RColorBrewer_1.1-2 ## [16] checkmate_2.0.0 rvest_1.0.2 Matrix_1.5-1 ## [19] pkgconfig_2.0.3 bookdown_0.29 scales_1.2.1 ## [22] webshot_0.5.4 svglite_2.1.0 jpeg_0.1-9 ## [25] tzdb_0.3.0 tibble_3.1.8 htmlTable_2.4.1 ## [28] generics_0.1.2 ellipsis_0.3.2 cachem_1.0.6 ## [31] withr_2.5.0 nnet_7.3-17 cli_3.4.1 ## [34] mnormt_2.1.1 magrittr_2.0.2 deldir_1.0-6 ## [37] evaluate_0.15 fansi_1.0.3 nlme_3.1-155 ## [40] xml2_1.3.3 foreign_0.8-82 tools_4.2.0 ## [43] data.table_1.14.4 hms_1.1.1 lifecycle_1.0.3 ## [46] stringr_1.4.0 interp_1.0-33 munsell_0.5.0 ## [49] cluster_2.1.2 compiler_4.2.0 jquerylib_0.1.4 ## [52] systemfonts_1.0.4 rlang_1.0.6 rstudioapi_0.13 ## [55] htmlwidgets_1.5.4 base64enc_0.1-3 gtable_0.3.0 ## [58] abind_1.4-5 DBI_1.1.2 R6_2.5.1 ## [61] gridExtra_2.3 fastmap_1.1.0 utf8_1.2.2 ## [64] stringi_1.7.8 parallel_4.2.0 Rcpp_1.0.9 ## [67] vctrs_0.5.0 rpart_4.1.16 png_0.1-7 ## [70] tidyselect_1.2.0 xfun_0.34 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["einführung-in-rstudio.html", "Chapter 4 Einführung in RStudio 4.1 Allgemeines zu RStudio 4.2 Weitere Hilfen", " Chapter 4 Einführung in RStudio Im Rahmen dieses Kapitels schauen wir uns an, wie die Benutzeroberfläche der Entwicklungsumgebung RStudio aufgebaut ist und wie sie uns den Umgang mit der (grundlegenden) Software R erleichtert. Wir lernen die vier grundlegenden Bereiche (Panels) von RStudio kennen und werden uns besonders die R-Dokumentation genauer anschauen. In dieser finden wir viele wertvolle Informationen zu Funktionen. Am Ende finden wir noch eine Übersicht über Kurzbefehle, die uns die Arbeit mit R zusätzlich erleichtern. Es ist von Vorteil, wenn wir die Einführung in R, in der Funktionen und Pakete, sowie die Speicherung von Daten erklärt werden, bereits durchgearbeitet haben. Achtung: Voraussetzung zur Bearbeitung des Kapitels ist, dass wir R und Rstudio installiert. Weiterführend bietet es sich an, das Kapitel zum Daten einlesen anzuschauen. 4.1 Allgemeines zu RStudio RStudio ist eine integrierte Entwicklungsumgebung (integrated development environment, IDE) für die Statistiksoftware R. Eine IDE bietet uns verschiedene Werkzeuge an, die uns den Umgang mit einer (grundlegenden) Software erleichtern. In R steht uns standardmäßig nur die Konsole und das Skript zur Verfügung. In RStudio gibt es u.a. zusätzlich: Code Hervorhebung In Abhängigkeit davon, um was für Code es sich handelt (z.B. Funktionen, Datentyp, bestehende Objekte, Kommentare; Fehlermeldungen) wird dieser farblich verschieden im Skript und auch in der Konsole hervorgehoben. Befehlszeilenergänzung (Autovervollständigung) Wenn wir anfangen, Code einzutippen, bekommen wir bereits Vorschläge, welche Funktionen oder bestehenden Objekte wir meinen könnten. Wenn wir mit der Maus über eine vorgeschlagene Funktion fahren, bekommen wir außerdem eine kurze Erklärung, was diese macht und welche Parameter sie besitzt. Wir können auf die Vorschläge klicken, um unseren Code (d.h. den Namen der Funktion oder des Objekts) automatisch vervollständigen zu lassen. Die Befehlszeilenergänzung können wir sowohl in der Konsole als auch im Skript nutzen. Code Diagnostik Wenn wir Code im Skript schreiben, bekommen wir schon vor der Ausführung Hinweise auf Probleme bzw. Unvollständigkeiten. Unser Skript muss dazu aber bereits gespeichert sein. Auf die Code Diagnostik gehen wir im Kapitel zu Fehlermeldungen noch etwas detaillierter ein. Wir können aus verschiedenen Möglichkeiten zur Code Hervorhebung wählen. Diese finden wir in der oberen Menüleiste auf Tools &gt; Global Options… &gt; Appearance &gt; Editor Theme. Beim ersten Öffnen sieht RStudio folgendermaßen aus: Zu allererst öffnen wir ein neues Skript, in das wir unseren Code schreiben werden. Dazu gehen wir in der oberen Leiste ganz links auf und dann auf R Script. Nun gliedert sich die Entwicklungsumgebung RStudio in vier verschiedene Bereiche: In das Skript schreiben wir unseren Code. In der Konsole wird dieser ausgeführt und die Ergebnisse angezeigt. Bestehende Objekte sehen wir im Environment. Eine Übersicht unserer Pakete finden wir unter Packages und Informationen zu R und zu Funktionen finden wir unter Help. Die Größe der Bereiche lässt sich an den Kanten oder an den Icons in der oberen rechten Ecke verändern. Die Aufteilung der Panels können wir über Tools &gt; Global Options… und dann im neuen Fenster links unter Pane Layout ändern. Die verschiedenen Bereiche (Panels) und ihre (für uns wichtigsten) Features schauen wir uns im Folgenden einmal genauer an. 4.1.1 Skript Hier schreiben wir unseren Code und Kommentare rein. Alle Zeichen, die in einer Zeile nach einem # erscheinen, gelten als Kommentar. Von der Kommentarfunktion sollten wir ausgiebig Gebrauch machen. So können wir (und Andere) zu einem späteren Zeitpunkt noch nachvollziehen, was genau wir gemacht haben. Unsere Skripte sollten wir regelmäßig (oder spätestens vor Beendigung des Programms) speichern. Um die Datei anzulegen gehen wir in der oberen Leiste auf File &gt; Save as…. Wir geben der R-Datei einen Namen und wählen einen Speicherort. Skript Speichern: Win: Strg + S Mac: cmd + S Beim allerersten Speichern müssen wir außerdem die Zeichen-Enkodierung festlegen. Wir wählen UTF-8, das ist die am häufigsten genutzte Kodierung für Unicode-Zeichen. Ab dann können wir unser Skript über File &gt; Save speichern. Unseren Code führen wir aus, indem wir ihn markieren und auf Run klicken. Aktuelle Linie bzw. markierten Code ausführen: Win: Strg + enter Mac: cmd + enter 4.1.2 Konsole Hier bekommen wir den Output unseres Codes ausgegeben. Kommentare werden nicht ausgeführt, sondern nur wiedergegeben. Wir bekommen in der Konsole einen Hinweis, wenn wir eine eindeutig unvollständige Funktion, d.h. wenn wir nur am Ende einer Funktion die schließende Klammer vergessen haben, ausführen wollen. Dann wird uns in der Konsole in der nachfolgenden Zeile ein + ausgegeben. In der Konsole bekommen wir manchmal auch Fehler- (error) und Warnmeldungen (warning) sowie andere Informationen zu einer Funktion (message) angezeigt. Wenn wir einen Fehler in einer Funktion gemacht haben, wird die Ausführung unterbrochen und wir bekommen eine Fehlermeldungen. Wenn wir z.B. ein Komma zwischen den Elementen in c() vergessen hätten, bekämen wir folgende Fehlermeldung: Mehr Informationen zur Interpretation von Fehlermeldungen finden wir im gleichnamigen Kapitel. Der Inhalt der Konsole wird nach jedem Schließen des Programms gelöscht und kann nicht ohne weiteres gespeichert werden. Deswegen ist es wichtig, die eigene Arbeit in Skripten zu speichern. 4.1.3 Environment &amp; History Im Environment sehen wir alle Objekte, die derzeit in R geladen sind. Wenn wir (externe) Datensätze einlesen, sehen wir diese auch hier. Wir bekommen außerdem weitere Informationen zu den Objekten und wir sehen ggf. die ersten Elemente. Beispielsweise haben wir gerade das Objekt vektor erstellt, welches uns hier angezeigt wird. Wir sehen, dass es sich um einen numerischen Vektor mit zwei Elementen, 1 und 3, handelt. Was für Informationen wir zu einem Objekt angezeigt bekommen ist abhängig von seiner Datenstruktur. Schauen wir uns das einmal am Beispiel der Objekte, die wir im Abschnitt zu Daten im Kapitel Einführung in R erstellt haben, an. Die Objekte werden in zwei Kategorien aufgeteilt: Unter Data finden wir Data Frames, Listen und Matrizen. Bei allen bekommen wir die Länge der einzelnen Dimensionen angezeigt (Dataframe: obs. = Zeilen, variables = Spalten; Matrix: [Zeilen, Spalten]; Liste: List of …). Data Frames werden mit einem vorangestellten markiert. Bei Matrizen bekommen wir zusätzlich die ersten Elemente angezeigt. Durch Klicken können wir uns diese Objekte im Data Viewer anschauen (alternativ zu View()). Unter Values finden wir Vektoren. Wir bekommen die selben Informationen angezeigt, die uns str(vektor) gibt: Datentyp, Länge, ersten 10 Elemente. Bei Faktoren wird uns de facto nicht der Datentyp sondern die -struktur angezeigt. Der Datentyp ist eigentlich auch irrelevant, weil es nur eine diskrete Anzahl an Ausprägungen gibt, die uns auch angezeigt wird. Die Länge des Faktors erfahren wir auch nicht. Den Datentyp eines Faktors können wir mit typeof(), die Länge mit length() in Erfahrung bringen. Zum Löschen aller Objekte können wir auf klicken (alternativ zu rm(list=ls())). Zusätzlich können wir über das Environment auch externe Datensätze einlesen. Wie wir das machen, schauen wir uns im Kapitel zum Daten einlesen im Abschnitt dazu an. In der History sehen wir den zuletzt ausgeführten Code. Mit To Source bekommen wir den markierten Code in unser Skript; mit To Console in die Konsole. Der Vorteil gegenüber der Konsole ist, dass der Inhalt der History nicht mit Beenden einer R-Session gelöscht wird, sondern wir auf den Code zugreifen können bis dieser explizit gelöscht wird (auch mit einem ). 4.1.4 Files, Plots, Packages, Help &amp; Viewer Im Folgenden werden wir hauptsächlich die Reiter Packages und Help besprechen. Auf die Dokumentation, auf die wir mit letzerem direkt in RStudio (anstatt im Browser) zugreifen können, gehen wir besonders stark ein. Unter Packages sehen wir die Standardpakete und die von uns installierten Add-On Pakete. Wir sehen eine kurze Beschreibung des Pakets und seine Versionsnummer. Im Kästchen ganz links sehen wir außerdem, ob Pakete derzeit geladen sind. Standardpakete sind immer geladen; Add-On Pakete müssen wir bei jeder Session neu laden (wenn wir sie nutzen wollen). Mehr Informationen zum Installieren, Laden und Aktualisieren von Paketen mit Funktionen sowie der Entwicklungsumgebung R finden wir im gleichnamigen Kapitel. Unter Files sehen wir die Ordner(struktur) auf unserem Rechner. Im Kapitel zu Daten einlesen erfahren wir, wie wir diesen Bereich nutzen können. Unter Plots werden (von uns erstellte) Grafiken angezeigt; unter Viewer (von uns erstellte) Tabellen. Über Help bekommen wir Zugang zur R-Dokumentation, welche wir uns nachfolgend etwas genauer anschauen wollen. 4.1.4.1 R-Dokumentation Die R-Dokumentation bietet uns umfassende Hilfe zum Umgang mit R im Allgemeinen und zu Funktionen an. Den Namen der Funktion geben wir in das Suchfeld ein. Alternativ können wir auch die Funktionen help(funktion) oder ?funktion nutzen. Wenn wir auf klicken, öffnet sich die Dokumentationsseite in einem neuen Fenster, was die Nutzung wesentlich übersichtlicher gestaltet. Neben der Informationen, aus welchem Paket eine Funktion stammt, finden wir hier zumeist folgende Abschnitte: Description: Beschreibung, was die Funktion macht Usage: Funktionsdefinition (Parameter der Funktion und ggf. Defaults) Arguments: Beschreibung der Parameter und ihrer möglichen Argumente Details: detaillierte Beschreibung zur Nutzung der Funktion und etwaigen Sonderfällen See Also: verwandte Funktionen (meist aus dem gleichen Paket) Examples: Beispiele zur Nutzung der Funktion Im Folgenden schauen wir uns die R-Dokumentation exemplarisch für die Funktion matrix() an. Dazu öffnen wir die dazugehörige R-Dokumentations-Seite, indem wir matrix in das Suchfeld eingeben, oder eine der Hilfe-Funktionen, help(matrix) oder ?matrix, ausführen. Folgende Seite sollte sich nun öffnen: Oben links sehen wir, dass die Funktion aus dem Basispaket base stammt. 4.1.4.1.1 Description Man findet hier zu einer Funktion bzw. einem Set an verwandten Funktionen kurze Ausführungen zum Zweck einer Funktion. Die Funktion matrix erstellt eine Matrix von einem gegebenem Set an Werten. Es gibt noch zwei verwandte Funktionen - as.matrix und is.matrix - welche ein Objekt in eine Matrix umwandeln bzw. überprüfen, ob das (als Argument) übergebene Objekt eine Matrix ist. 4.1.4.1.2 Usage Hier sehen wir die Funktionsdefinition. Diese zeigt, welche Parameter die Funktion besitzt und ggf. welche dieser Parameter voreingestellte Argumente (Defaults) besitzen. Per Default …: … wird an data nur ein NA (Missing) übergeben … gibt es eine Reihe (nrow=1) … und eine Spalte (ncol=1) … wird die Matrix spaltenweise (d.h. von oben nach unten) mit Werten befüllt (byrow=FALSE) … gibt es keine Spalten- und Zeilenbenennung (dimnames=NULL) Um Defaults besser zu verstehen, führen wir matrix() ohne Spezifikation der Parameter aus und schauen uns den Output an. ## [,1] ## [1,] NA Bis auf byrow=FALSE können wir so alle Voreinstellungen im Output nachvollziehen. Dessen Funktionsweise sieht man erst bei mehreren Elementen in einer Matrix. 4.1.4.1.3 Arguments Argumente sind Einstellungen, die wir für eine Funktion festlegen können. In der R-Dokumentation sieht man links den Namen des Arguments (z.B. byrow) und rechts eine Beschreibung dazu (z.B. logical) verbunden mit verschiedenen Einstellungsoptionen (z.B. FALSE - spaltenweise Befüllen der Matrix). 4.1.4.1.4 Details Im Abschnitt Details stehen weitere Detailinformationen zur Nutzung der Funktion. 1. Abschnitt von matrix(): Wir erfahren hier, dass wenn nrow (Zeilenanzahl) oder ncol (Spaltenanzahl) nicht festgelegt wird, versucht wird, auf dessen Länge zu schließen. Das schauen wir uns mal an einem Beispiel an. # Beispiel 1: matrix(data=c(1,1,2,2,3,3,4,4,5,5), nrow=5) ## [,1] [,2] ## [1,] 1 3 ## [2,] 1 4 ## [3,] 2 4 ## [4,] 2 5 ## [5,] 3 5 Wir haben einen Vektor mit 10 Elementen reingegeben und nrow=5 festlegt. Daraus kann R schließen, dass die Matrix zwei Spalten (ncol=2) haben muss. 2. Abschnitt matrix(): Wenn die Anzahl der Elemente, die wir an data übergeben, kleiner ist, als die Anzahl der gewünschten Elemente in Matrix (nrow x ncol), dann wird data recycled. Das bedeutet, dass data nochmal genutzt wird, um die Matrix zu befüllen. # Beispiel 2: matrix(data=c(1,1,2,2,3), nrow=3, ncol=2) ## Warning in matrix(data = c(1, 1, 2, 2, 3), nrow = 3, ncol = 2): data length [5] ## is not a sub-multiple or multiple of the number of rows [3] ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 3 ## [3,] 2 1 In Beispiel 1 und 2 können wir auch die Auswirkung des Default byrow=FALSE sehen. Bei beiden wurden die Elemente spaltenweise in die Matrix eingetragen, ohne dass wir das so festgelegt haben. Es ist ratsam, vor der Nutzung einer (unbekannten) Funktionen oder bei einer Fehlermeldung zu einer Funktion im Abschnitt Details nachzuschauen. Oftmals findet man hier hilfreiche Erklärungen. 4.1.4.1.5 See also In diesem Abschnitt bekommt man Funktionen angezeigt, die mit der vorliegenden Funktion in enger Verbindung stehen. Diese sind häufig mit einer kurzen Erklärung versehen. Teilweise findet man hier auch Funktionen, die besser für das eigene Vorhaben geeignet sind. 4.1.4.1.6 Examples Hier findet man einige beispielhafte Anwendungen der Funktion. Anhand der Beispiele bekommt man ein besseres Verständnis von der Syntax und Funktionsweise der Funktion. Was genau gemacht wird ist oftmals in kurzen Kommentaren (# ...) erklärt. 4.2 Weitere Hilfen 4.2.1 Kurzbefehle Nachfolgend finden wir einige Kurzbefehle für die beiden Betriebssysteme Windows und Mac, die unseren Workflow verbessern. Mit dem letzten Kurzbefehl können wir schneller Dateipfade kopieren, was uns das Einlesen von Daten in R erleichtert. Dieser Kurzbefehl ist, im Gegensatz zu den anderen, nicht zur Nutzung in R geeignet; er funktioniert nur im Explorer (Windows) bzw. Finder (Mac). Windows Mac Code der aktuellen Linie bzw. markierten Code ausführen Strg + enter cmd + enter Code bis zur aktuellen Linie ausführen alt + Strg + B alt + cmd + B Skript speichern Strg + S cmd + S Dateipfad kopieren shift + Rechtsklick auf Dateidann Als Pfad kopieren alt + cmd + C Eine Übersicht weiterer Kurzbefehle für R finden wir in der Leiste ganz oben unter Help &gt; Keyboard Shortcuts Help (nicht zu verwechseln mit dem Bereich Help, der uns Zugang zur Dokumentation verschafft). Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ICC_2.4.0 readr_2.1.3 Hmisc_4.7-1 Formula_1.2-4 ## [5] survival_3.2-13 lattice_0.20-45 ggplot2_3.4.0 colorspace_2.0-3 ## [9] psych_2.2.9 car_3.1-1 carData_3.0-5 kableExtra_1.3.4 ## [13] dplyr_1.0.10 htmltools_0.5.3 rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.2 sass_0.4.2 jsonlite_1.8.3 ## [4] viridisLite_0.4.1 splines_4.2.0 bslib_0.4.0 ## [7] assertthat_0.2.1 highr_0.9 latticeExtra_0.6-30 ## [10] yaml_2.3.5 pillar_1.8.1 backports_1.4.1 ## [13] glue_1.6.2 digest_0.6.30 RColorBrewer_1.1-2 ## [16] checkmate_2.0.0 rvest_1.0.2 Matrix_1.5-1 ## [19] pkgconfig_2.0.3 bookdown_0.29 scales_1.2.1 ## [22] webshot_0.5.4 svglite_2.1.0 jpeg_0.1-9 ## [25] tzdb_0.3.0 tibble_3.1.8 htmlTable_2.4.1 ## [28] generics_0.1.2 ellipsis_0.3.2 cachem_1.0.6 ## [31] withr_2.5.0 nnet_7.3-17 cli_3.4.1 ## [34] mnormt_2.1.1 magrittr_2.0.2 deldir_1.0-6 ## [37] evaluate_0.15 fansi_1.0.3 nlme_3.1-155 ## [40] xml2_1.3.3 foreign_0.8-82 tools_4.2.0 ## [43] data.table_1.14.4 hms_1.1.1 lifecycle_1.0.3 ## [46] stringr_1.4.0 interp_1.0-33 munsell_0.5.0 ## [49] cluster_2.1.2 compiler_4.2.0 jquerylib_0.1.4 ## [52] systemfonts_1.0.4 rlang_1.0.6 rstudioapi_0.13 ## [55] htmlwidgets_1.5.4 base64enc_0.1-3 gtable_0.3.0 ## [58] abind_1.4-5 DBI_1.1.2 R6_2.5.1 ## [61] gridExtra_2.3 fastmap_1.1.0 utf8_1.2.2 ## [64] stringi_1.7.8 parallel_4.2.0 Rcpp_1.0.9 ## [67] vctrs_0.5.0 rpart_4.1.16 png_0.1-7 ## [70] tidyselect_1.2.0 xfun_0.34 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["pakete-1.html", "Chapter 5 Pakete 5.1 Was ist eine grafische Benutzeroberfläche? 5.2 Was ist CRAN? 5.3 Pakete Installieren 5.4 Pakete Laden 5.5 Pakete Aktualisieren 5.6 Wichtige Hinweise zur Replizierbarkeit 5.7 Weitere Hilfen 5.8 FAQ", " Chapter 5 Pakete Ein Grund, weshalb R von so vielen BenutzerInnen verwendet wird, ist, dass es sich hierbei um eine Open Source Software handelt. R ist für alle kostenlos, frei verfügbar und der Quellcode ist öffentlich. Dies ermöglicht es WissenschaftlerInnen und EntwicklerInnen auf der ganzen Welt, ständig neue Funktionen in R zu implementieren und diese mit anderen NutzerInnen über „Pakete“ zu teilen. Wenn wir diese neu entwickelten Funktionen, die nicht ohnehin grundlegend in R vorhanden sind (z.B. über die stets eingeladenen Pakete base und graphics), für unsere Analysen verwenden wollen, müssen wir die Pakete, welche die entsprechenden Funktionen beinhalten, installieren und laden. Hinweis: Pakete, die grundlegend in R enthalten sind, nennt man auch Standard- oder Basispakete. In diesem Abschnitt schauen wir uns am Beispiel des Pakets psych an, wie wir zusätzliche Pakete nutzen können. psych ist ein Paket, welches uns ermöglicht, in R Analysemethoden umzusetzen, die besonders häufig in psychologisch-empirischer Forschung eingesetzt werden (z.B. können wir mit der Funktion alpha() die interne Konsistenz von Items eines Tests mit verschiedenen Methoden schätzen). Um mit einem zusätzlichen Paket arbeiten zu können, muss dieses eimalig in R installiert werden. In jedem Skript, in welchem wir Funktionen aus dem Paket verwenden, müssen wir das Paket dann laden. Eine Analogie, die wir verwenden können, um uns diesen Ablauf besser vorzustellen, wäre, dass Pakete wie Bücher in einer Bibliothek verstanden werden können. Die Bücher müssen nur einmal gekauft werden (Installation), aber wir müssen sie vor jeder Nutzung aus dem Regal holen und aufschlagen (Laden). Pakete sollten weiterhin bei Bedarf aktualisiert werden, da EntwicklerInnen ständig daran arbeiten und versuchen, sie zu verbessern. Im Folgenden wollen wir also am Beispiel des psych-Pakets nachvollziehen, wie Pakete installiert, geladen und aktualisiert werden können. Dabei schauen wir uns jeweils die zwei grundlegenden Möglichkeiten an: Funktionen ausführen oder die grafische Benutzeroberfläche von RStudio nutzen. Für beide Möglichkeiten benötigen wir eine Internetverbindung zum Herunterladen der Pakete. Am Ende des Kapitels befinden sich noch wichtige Hinweise zur Replizierbarkeit von Skripten und Analysen. 5.1 Was ist eine grafische Benutzeroberfläche? Die grafische Benutzeroberfläche, oder auch Benutzungsschnittstelle, wird häufig auch mit GUI (Graphical User Interface) abgekürzt. Durch diese können wir mit der Maus auf Symbole und andere Steuerelemente klicken anstatt Funktionen in der Konsole auszuführen. Die grafische Benutzeroberfläche in RStudio ist viel besser ausgebaut, weswegen wir diese auch nutzen wollen. 5.2 Was ist CRAN? CRAN steht für The Comprehensive R Archive Network. Von hier laden wir R sowie alle annerkannten Pakete herunterladen können. Wenn wir diese Pakete installieren oder aktualisieren, greift R automatisch auf CRAN zu. Hinweis: Wenn wir versuchen, eine Funktion auszuführen, für welche wir das entsprechende Paket noch nicht installiert und/oder geladen haben, erhalten wir eine Fehlermeldung. Wir bekommen beispielsweise folgende Fehlermeldung, wenn wir die Funktion alpha() aus dem Paket psych nutzen wollen. 5.3 Pakete Installieren Zuerst einmal schauen wir uns an, wie wir diese zusätzlichen Pakete in R installieren können. Die meisten bekannten Pakete werden über CRAN zur Verfügung gestellt und können von dort heruntergeladen werden. Zur Installation von Paketen schauen wir uns beide Herangehensweisen, das Nutzen von Funktionen sowie der grafischen Benutzeroberfläche, an. Vorweg: Bei beiden Wegen wird uns nach erfolgreicher Installation folgender Output in der Konsole angezeigt: 5.3.1 Über die Funktion install.packages() Wir können psych installieren, indem wir install.packages(“psych”, dependencies = TRUE) ausführen. Hierbei ist darauf zu achten, dass wir den Namen des Pakets in Anführungszeichen setzen müssen. Mit dependencies=TRUE geben wir an, dass noch nicht installierte Pakete, welche von dem Zielpaket benötigt oder empfohlen werden, ebenfalls installiert werden sollen. Hinweis: Die Funktion install.packages() ist Bestandteil des Standardpakets utils. 5.3.2 Über das Icon Install oder den Menüpunkt Install Packages… Wir können das Paket auch installieren, indem wir im Packages-Tab auf das Icon Install … … oder in der Menüleiste am oberen Bildschirmrand auf Tools –&gt; Install Packages… klicken. Nun öffnet sich ein neues Fenster, in welchem wir unter dem Reiter Packages (…) den Namen des Pakets eingeben können. Anschließen müssen wir noch auf das Icon Install klicken. Mit dem Häkchen in dem Kästchen bei Install Dependencies werden von dem Zielpaket benötigte oder empfohlene bisher nicht installierte Pakete auch heruntergeladen. 5.4 Pakete Laden Nachdem wir das benötigte Paket installiert haben, müssen wir es noch laden. Nur dann können wir die im Paket enthaltenen Funktionen nutzen. Hierzu schauen wir uns wieder die beiden Herangehensweisen, über eine Funktion oder die grafische Benutzeroberfläche, an. Vorweg: Bei beiden Möglichkeiten erscheint nach erfolgreichem Laden des Pakets folgender Output in der Konsole: 5.4.1 Über die Funktion library() Wir können das Paket psych laden, indem wir library(psych) ausführen. Hierbei ist es nicht wichtig, den Namen des Pakets in Anführungszeichen zu setzen. Wir können diese auch weglassen. Hinweis: Die Funktion library() ist Bestandteil des Standardpakets base. Alternativ kann man auch die Syntax paket::funktion(), z.B. psych::alpha(), nutzen. Was dieser Weg für Vorteile hat, erfahren wir im Unterabschnitt Maskierung. 5.4.2 Über das Häkchen-Setzen in der System Library Alternativ können wir im Packages-Tab ein Häkchen bei dem Paket setzen, welches wir laden wollen. Um das gewünschte Paket schneller zu finden, können wir das Suchfeld nutzen. 5.4.3 Maskierung: Wenn verschiedene Pakete gleich benannte Funktionen enthalten Es kann vorkommen, dass Funktionen aus verschiedenen Paketen die gleiche Bezeichnung haben. Beispielsweise gibt es in psych und in ggplot2 eine Funktion mit dem Namen alpha(). Wenn wir ein Paket laden, und vorher ein anderes Paket geladen wurde, in dem eine gleichnamige Funktion vorkommt, bekommen wir folgende Meldung in der Konsole ausgegeben: Die Funktion des zuletzt eingeladenen Pakets wird mit dem gemeinsamen Funktionsnamen (hier: alpha() aus dem Paket psych) aufgerufen. Die Funktion aus dem anderen Paket wird maskiert, d.h. wir können sie jetzt erstmal nicht mehr nutzen. Nachfolgend schauen wir uns drei Möglichkeiten an, Probleme mit dem Maskieren von Funktionen handzuhaben: 1) deaktivieren und neu laden von Paketen, 2) eindeutige Referenzierung von Funktionen und 3) das Paket conflicted nutzen. Wenn wir die Funktion aus dem anderen Paket (ggplot2) nutzen wollen, können wir das Paket erst deaktivieren und dann neu laden. Wir können Pakete deaktivieren, indem wir detach(“package:ggplot2”, unload = TRUE) nutzen oder indem wir das Häkchen neben dem Paket im Packages-Tab entfernen. Wenn wir das Paket mit der gewünschten Funktion laden, erhalten wir folgende Meldung. Es handelt sich um die gleiche Meldung wie oben, nur das nun das Paket ggplot2 als zweites Paket eingelesen wurde und entsprechend die Funktionen im Paket psych maskiert wurden. Eine Alternative zu dem Laden und Deaktivieren von Paketen ist die exakte Referenzierung der Funktion auf das Paket mittels ::. Das schreiben wir zwischen das Paket und die Funktion z.B. psych::alpha() oder ggplot2::alpha(). So weiß R eindeutig, welche Funktion wir nutzen wollen. Hinweis: Es kommt häufiger zu Problemen bei der Ausführung der Funktionen filter(), select() und summarise() aus dem Paket dplyr, wenn die Pakete stats (Basispaket; filter()), MASS (select()) oder plyr (summarise()) ebenfalls geladen sind. Die eindeutige Auswahl von Funktionen mittels :: kann bestehende Probleme lösen. Mehr Informationen zur Problematik finden wir in diesem Forumseintrag. Außerdem können wir das Paket conflicted nutzen. Wenn wir dieses zu Beginn laden, wird uns jedes Mal, wenn wir eine Funktion nutzen wollen, die nicht eindeutig einem geladenen Paket zugeordnet werden kann, eine detaillierte Fehlermeldung ausgegeben. So können wir Probleme durch die Nutzung falscher Funktionen und/oder Ratlosigkeit bzgl. missverständlicher Fehlermeldungen vermeiden. Wir haben außerdem die Möglichkeit, einmalig bzw. für das gesamte Skript eine von mehreren gleichnamigen Funktionen festzulegen. Mehr Informationen zum Paket und dessen Anwendung finden wir hier. 5.5 Pakete Aktualisieren Pakete werden von Zeit zu Zeit aktualisiert. Wir sollten hin und wieder überprüfen, ob es Updates für unsere installierten Pakete gibt. Das können wir wieder wahlweise mit Funktionen oder der grafischen Benutzeroberfläche machen. 5.5.1 Über die Funktion update.packages() Wir können update.packages() ausführen, und bekommen so für jedes Paket, für das eine aktuellere Version vorliegt, in der Konsole die Frage gestellt, ob wir dieses aktualisieren wollen. Mit Yes oder No bzw. deren Anfangsbuchstaben können wir antworten. Hinweis: Die Funktion update.packages() ist Bestandteil des Standardpakets utils. Es kann vorkommen, dass für einige der Pakete, die wir aktualisieren wollen, noch nicht die Binärcodes der aktuellsten Versionen der Pakete für unser Betriebssystem auf CRAN bereit gestellt wurde. Was sind Binär- und Quellcode? Binärcode ist eine Sprache, die zur Verarbeitung digitaler Informationen, d.h. von Rechnern, genutzt wird (Synonym: Maschinencode). Es heißt binär weil es zwei mögliche Zeichen gibt: 0 und 1. Quellcode bezeichnet für Menschen lesbare Programmiersprachen. Quellcode wird in Binärcode übersetzt und kann dann von Rechnern ausgeführt werden. Wir können die jüngsten Binärcodes für die Pakete herunterladen oder die Quellcodes der aktuellsten Versionen kompilieren. Allerdings müssen wir dafür spezielle Tools in R installiert haben, welche man unter Mac OS und einer älteren als R-Version 4.0.0 zusätzlich herunterladen muss (z.B. hier). Wir können hier auch pauschal nein antworten. Es ist ausreichend dafür ein n in die Konsole zu tippen. Nach einiger Zeit sollten wir erneut versuchen, die aktuellste Version herunterzuladen, da dann häufig auch die Binärcodes zur Verfügung stehen. 5.5.2 Über das Icon Update oder den Menüpunkt Check for Package Updates… Dazu können wir im Packages-Tab auf das Icon Update … … oder in der Menüleiste am oberen Bildschirmrand auf Tools –&gt; Check for Package Updates… klicken. Damit öffnet sich ein neues Fenster, in dem all unsere Pakete angezeigt werden, für die es Aktualisierungen gibt. Wir sehen hier welche Version eines Pakets wir haben (Installed), welche neuere Version verfügbar ist (Available) und welche Änderungen in der neueren Version vorgenommen wurden (NEWS). Bei letzterem müssen wir das jeweilige Seiten-Symbol klicken und werden auf eine Website verwiesen, in der eine Übersicht der Änderungen zu finden ist. Wenn wir alle Pakete aktualisieren wollen, wählen wir Select All. Dann müssen wir nur noch auf Install Updates klicken. Wenn wir Pakete, die wir installieren wollen, derzeit geladen haben, öffnet sich ein Fenster, in dem werden wir gefragt, ob wir R neustarten möchten bevor die gewählten Pakete installiert werden sollen. Wir sollten hier Ja anklicken. Manchmal kann es vorkommen, dass uns dieses Fenster immer wieder angezeigt wird. Wenn das der Fall ist, sollten wir auf Nein klicken. Manchmal werden die Updates nicht einfach installiert, sondern der Vorgang wird unterbrochen und wir bekommen eine Meldung in der Konsole angezeigt. Wir werden auch hier darauf hingewiesen, dass für einige der Pakete, die wir aktualisieren wollen, noch nicht die Binärcodes der aktuellsten Versionen der Pakete für unser Betriebssystem auf CRAN bereit gestellt wurden. Wir sollen uns nun entscheiden, ob wir die betroffenen Pakete mit den Quellcodes der aktuellsten Versionen kompilieren wollen. Dafür müssen wir aber gewisse Tools in R implementiert haben, welche man mit Mac OS zusätzlich herunterladen muss (z.B. hier) Besser ist es, hier pauschal nein anzuwählen. Dafür reicht es auch, ein n in die Konsole zu tippen. Wir haben so aber nicht die aktuellste Version der Pakete und sollten in nächster Zeit erneut versuchen, diese zu aktualisieren. 5.5.3 Entwicklerpakete runterladen Das Paket devtools enthält verschiedene Funktion zum Installieren von Paketen von verschiedenen Quellen (engl.: repositories, z.B. CRAN). Mit diesem können wir u.a. sogenannte Entwicklerpakete herunterladen. Das sind Pakete, die man nicht direkt von CRAN runterladen kann (z.B. weil die Dokumentation des Pakets und seiner Funktionen nicht den CRAN-Standards entspricht). Solche Entwicklerpakete können häufig von GitHub runtergeladen werden. Zuerst müssen wir dafür das Paket devtools installieren. Dann müssen wir dieses laden und anschließend können wir das gewünschte Zielpaket herunterladen. install.packages(&quot;devtools&quot;) library(devtools) # Beispiel: Paket horst install_github(&quot;kthorstmann/horst&quot;) # EntwicklerIn / Paket Mit devtools kann man auch ältere Versionen von Paketen runterladen. Für mehr Informationen dazu siehe Ältere Paket-Versionen installieren. 5.6 Wichtige Hinweise zur Replizierbarkeit 5.6.1 Replizierbarkeit von R-Skripten Wir haben beide Möglichkeiten, die Nutzung von Funktionen und die der grafischen Benutzeroberfläche vorgestellt, aber die Verwendung von Funktionen ist mit Hinblick auf die Replizierbarkeit von Skripten zu bevorzugen. Wenn man die grafische Benutzeroberfläche nutzt, findet man im Skript keine Hinweise darüber, welche Pakete genutzt wurden, d.h. library(paket) erscheint nur in der Konsole und eben nicht im R-Skript. 5.6.2 Replizierbarkeit von Analysen Es ist sinnvoll, bei Analysen zu vermerken, mit welcher Version eines Pakets wir diese durchgeführt haben. Wenn sich die Funktionen eines Pakets durch Updates ändern, kann das auch die Ergebnisse unserer Analysen beeinflussen. Gleiches gilt auch für zu der Zeit genutzte Versionen von unserem Betriebssystem und von R. Mit der Funktion sessionInfo() aus dem Standardpaket utils bekommen wir viele wichtige Informationen auf einen Schlag: R-Version Betriebssystem-Version geladene Pakete mit Angaben zur Version Hinweis: Hiermit werden nur Pakete gelistet, die mit library() oder mit Hilfe des GUIs geladen wurden. Wenn wir die eindeutige Zuweisung von Funktionen zu Paketen aus dem Abschnitt Maskierung nutzen z.B. psych::alpha(), werden unsere genutzten Pakete nicht gelistet. In diesem Fall müssten wir die Informationen zu den Versionen der Pakete selbst in Erfahrung bringen. Für noch detailliertere Informationen zu unseren geladenen Paketen können wir session_info() aus dem Paket devtools nutzen. Hier sehen wir z.B. bei source wo wir die Paket heruntergeladen haben (z.B. CRAN) und bei version mit welcher R-Version wir sie erstellt haben (z.B. R 3.6.0). 5.7 Weitere Hilfen 5.7.1 Probleme mit Paketen und Funktionen Bei Problemen mit dem Installieren bzw. Updaten oder Laden von Paketen lohnt es sich, in unserem FAQ-Eintrag nachzuschauen. Wenn wir bestimmte Funktionen nicht ausführen können, obwohl wir das notwendige Paket geladen haben, kann das auf Maskierung von gleichnamigen Funktionen aus verschiedenen geladenen Paketen zurückzuführen sein. 5.7.2 Ältere Paket-Versionen installieren Manchmal möchten wir ältere Versionen von Paketen nutzen. Das kann z.B. der Fall sein, wenn wir eine Analyse, die mit einer älteren Version eines Pakets durchgeführt wurde, replizieren möchten. Hier finden wir eine Anleitung, wie wir dafür devtools() oder URLs nutzen können. 5.8 FAQ Manchmal kann es vorkommen, dass wir bestimmte Pakete nicht laden oder gar nicht erst installieren können. Dafür kann es vielfältige Ursachen geben. Im Folgenden schauen wir uns an, wie man das Problem (mit großer Wahrscheinlichkeit) lösen kann. Wir führen die folgenden drei Möglichkeiten der Reihe nach durch und überprüfen nach jeder Möglichkeit, ob wir das Paket schon nutzen können. Wir schauen uns das exemplarisch für das Paket car an. Du änderst nur noch den Namen des Pakets bzw. der Pakete. Manchmal werden noch weitere Pakete - sog. dependencies - geladen. Diese solltest du in die folgenden Lösungswege auch mit einbeziehen. Die Pakete via Befehl deinstallieren und neu installieren remove.packages('car') bzw. remove.packages(c('car', 'survey')) install.packages('car', dependencies=TRUE) Hilfe bei der Installation von Paketen finden wir im gleichnamigen Kapitel. Die neuste R-Version von R und/oder RStudio (hierbei die kostenlose Variante) auf dem Computer installieren und dann versuchen, das Paket neu zu installieren bzw. Schritt 1 durchzuführen. Achte dabei auf die Kompatibilität mit deiner Hardware und Software. Unter Umständen kannst du vielleicht nicht die neueste Version installieren, aber eine neuere als deine aktuelle Version. Ältere Versionen von R finden wir auch unter obigem Link. Ältere Versionen von RStudio finden wir hier. Hilfe bei der Installation von R und RStudio finden wir unserem Kapitel dazu. Die Pakete deinstallieren und manuell neu installieren remove.packages('car') bzw. remove.packages(c('car', 'survey')) Das Paket bzw. die Pakete von CRAN als zip-Datei/en runterladen, dann öffnen (entpacken), den R Library Ordner lokalisieren (in dem werden alle R-internen Dateien gespeichert) und den/die Paket-Ordner dorthin verschieben. 5.8.1 Entwicklerpakete Es gibt auch sog. Entwicklerpakete, die man nicht direkt von CRAN runterladen kann. Wie man diese installieren kann, schauen wir uns am Beispiel des Pakets horst an. Mit den im Paket horst enthaltenen Funktionen können wir u.a. den Modus einer Variablen oder Omega’s W berechnen. Hier finden wir eine Übersicht der enthaltenen Funktionen. Wenn man versucht Entwicklerpakete mit install.packages() herunterzuladen, bekommt man eine Fehlermeldung ausgegeben: Entwicklerpakete erhält man zumeist direkt von den Entwicklern (z.B. als Zusatzmaterial in wissenschaftlichen Publikationen oder zum Download von persönlichen Webseiten). Häufig können Entwicklerpakete auch von GitHub runtergeladen werden. Dazu müssen wir zunächst das Paket devtools, welches wir zum Herunterladen benötigen, installieren. Dann müssen wir dieses laden und anschließend können wir das gewünschte Zielpaket installieren. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;kthorstmann/horst&quot;) # EntwicklerIn / Paket Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] devtools_2.4.5 usethis_2.1.6 ICC_2.4.0 readr_2.1.3 ## [5] Hmisc_4.7-1 Formula_1.2-4 survival_3.2-13 lattice_0.20-45 ## [9] ggplot2_3.4.0 colorspace_2.0-3 psych_2.2.9 car_3.1-1 ## [13] carData_3.0-5 kableExtra_1.3.4 dplyr_1.0.10 htmltools_0.5.3 ## [17] rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-155 fs_1.5.2 webshot_0.5.4 ## [4] RColorBrewer_1.1-2 httr_1.4.2 profvis_0.3.7 ## [7] tools_4.2.0 backports_1.4.1 bslib_0.4.0 ## [10] utf8_1.2.2 R6_2.5.1 rpart_4.1.16 ## [13] DBI_1.1.2 nnet_7.3-17 urlchecker_1.0.1 ## [16] withr_2.5.0 prettyunits_1.1.1 processx_3.8.0 ## [19] tidyselect_1.2.0 gridExtra_2.3 mnormt_2.1.1 ## [22] compiler_4.2.0 cli_3.4.1 rvest_1.0.2 ## [25] htmlTable_2.4.1 xml2_1.3.3 bookdown_0.29 ## [28] sass_0.4.2 scales_1.2.1 checkmate_2.0.0 ## [31] callr_3.7.2 systemfonts_1.0.4 stringr_1.4.0 ## [34] digest_0.6.30 foreign_0.8-82 svglite_2.1.0 ## [37] base64enc_0.1-3 jpeg_0.1-9 pkgconfig_2.0.3 ## [40] sessioninfo_1.2.2 fastmap_1.1.0 highr_0.9 ## [43] htmlwidgets_1.5.4 rlang_1.0.6 rstudioapi_0.13 ## [46] shiny_1.7.3 jquerylib_0.1.4 generics_0.1.2 ## [49] jsonlite_1.8.3 magrittr_2.0.2 interp_1.0-33 ## [52] Matrix_1.5-1 Rcpp_1.0.9 munsell_0.5.0 ## [55] fansi_1.0.3 abind_1.4-5 lifecycle_1.0.3 ## [58] stringi_1.7.8 yaml_2.3.5 pkgbuild_1.3.1 ## [61] promises_1.2.0.1 parallel_4.2.0 crayon_1.5.2 ## [64] miniUI_0.1.1.1 deldir_1.0-6 splines_4.2.0 ## [67] hms_1.1.1 ps_1.7.2 pillar_1.8.1 ## [70] pkgload_1.3.0 glue_1.6.2 evaluate_0.15 ## [73] latticeExtra_0.6-30 remotes_2.4.2 data.table_1.14.4 ## [76] png_0.1-7 vctrs_0.5.0 tzdb_0.3.0 ## [79] httpuv_1.6.5 gtable_0.3.0 purrr_0.3.4 ## [82] assertthat_0.2.1 cachem_1.0.6 xfun_0.34 ## [85] mime_0.12 xtable_1.8-4 later_1.3.0 ## [88] viridisLite_0.4.1 tibble_3.1.8 memoise_2.0.1 ## [91] cluster_2.1.2 ellipsis_0.3.2 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["daten-importieren.html", "Chapter 6 Daten importieren 6.1 Vor dem Einlesen in R 6.2 Windows 6.3 Mac 6.4 Weg 1: Environment &gt; Import Dataset 6.5 Weg 2: Files &gt; Import Dataset 6.6 Weg 3: Manuell Importieren mit Funktionen 6.7 FAQ 6.8 Weiterführende Hilfe 6.9 Übung", " Chapter 6 Daten importieren Wir können in R nicht nur selbst Daten erzeugen, sondern selbstverständlich auch externe Dateien unterschiedlichen Typs einlesen. Dabei geschehen in der Regel zwei Dinge: 1) Die Informationen werden ausgelesen und 2) in einem Objekt (oftmals in Form eines Dataframes) gespeichert. Im Rahmen dieses Kapitels schauen wir uns an, wie wir Dateien aus dem Internet herunterladen, in unseren Arbeitsordner verschieben und anschließend in R einlesen. Wir lernen außerdem, wie wir unser Working Directory setzen können. Wir lernen drei verschiedene Wege zum Einlesen von Dateien kennen: Die ersten beiden Wege nutzen die grafische Benutzeroberfläche der Entwicklungsumgebung RStudio, letzterer nutzt direkt Funktionen. Exemplarisch schauen wir uns die drei Wege für die Datei neuro.csv an. Wir können mit den vorgestellten Wegen aber auch andere Dateiformate einlesen. Was ist eine grafische Benutzeroberfläche? Die Grafische Benutzeroberfläche, oder auch Benutzungsschnittstelle, wird häufig auch mit GUI (Graphical User Interface) abgekürzt. Durch diese kann man mit der Maus auf Symbole und andere Steuerelemente klicken anstatt Funktionen in der Konsole auszuführen (z.B. Weg 1 und Weg 2 zum Daten einlesen). Die grafische Benutzeroberfläche in der Entwicklungsumgebung RStudio ist viel besser ausgebaut (als die des Basisprogramms R), weswegen wir diese auch nutzen wollen. Was ist das Working Directory? Mit dem Working Directory (WD; Arbeitsverzeichnis) legen wir u.a. fest, wo unser aktuelles R-Skript gespeichert wird und wo andere Objekte, die wir aus R exportieren (z.B. Grafiken), standarmäßig (während der aktuellen Sitzung) gespeichert werden. Das WD müssen wir (in der Regel) in jeder R-Sitzung erneut festlegen. 6.1 Vor dem Einlesen in R In diesem Abschnitt schauen wir uns an, wie wir Dateien aus dem Internet herunterladen (z.B. aus moodle) und in unseren Arbeitsordner verschieben können. Wir sollten der Übersichtlichkeit halber für jedes neue Projekt einen neuen Ordner anlegen. Die beiden Schritte schauen wir uns jeweils für das Windows- und Mac-Betriebssystem bzw. die Browser Google Chrome und Safari an. Dieser Abschnitt ist optional. Wenn du bereits weißt, wie du Daten herunterlädst und verschiebst, kannst du diesen Abschnitt überspringen. 6.2 Windows Im nachfolgenden Beispiel wird gezeigt, wie wir unter Benutzung des Browsers Google Chrome eine Datei aus einem Moodle-Kurs herunterladen und in unseren Arbeitsordner verschieben. 6.2.1 Datei herunterladen Wir begeben uns in den entsprechenden Moodle-Kurs und wählen die Datei mit einem Rechtsklick an, wählen die Option Link speichern unter… sowie den gewünschten Zielordner zur Ablage aus. Sehr wichtig ist es, sich immer zu merken, in welchem Ordner die heruntergeladene Datei gespeichert wird. Es ist sinnvoll, die Datei bereits jetzt im Arbeitsordner zu speichern (in dem wir unser R-Skript später speichern möchten). In Google Chrome können wir den Zielordner herausfinden, indem wir rechts oben auf die drei Punkte klickt und den Menüpunkt Downloads anwählen. Es öffnet sich ein neuer Tab im Browser, in dem wir die Option In Ordner anzeigen auswählen können. 6.2.2 In Arbeitsordner verschieben Wir wählen die Datei mit einem Rechtsklick im Ordner aus und klicken dann auf die Option Ausschneiden. Im Gegensatz zu Kopieren entfernt das Ausschneiden die Datei auch aus dem ursprünglichen Ordner. Als nächstes begeben wir uns in unseren Arbeitsordner (ggf. müssen wir diesen vorher noch erstellen). Wir machen einen Rechtsklick und wählen die Option Einfügen aus. Jetzt befindet sich die Datei in unserem Arbeitsordner und wir können nun RStudio öffnen, um die Datei einzulesen. 6.3 Mac Im folgenden Beispiel wird gezeigt, wie wir unter Benutzung des Browsers Safari eine Datei aus einem Moodle-Kurs herunterladen und in unseren Arbeitsordner verschieben. 6.3.1 Datei herunterladen Wir begeben uns in den entsprechenden Moodle-Kurs und öffnen die Datei im Browser. Dann machen wir einen Rechtsklick (dabei darf nichts markiert sein) und klicken auf Seite sichern unter…. In dem Fenster, welches sich dann öffnet, müssen wir bei Format noch festlegen, dass wir den Quelltext der Seite herunterladen wollen. Wir könnten auch schon unseren Arbeitsordner als Zielordner festlegen. Achtung: Manchmal werden wir beim Speichern gefragt, ob die Endung .txt angehängt werden soll (d.h. ob die Datei als Textformat gespeichert werden soll). Das sollten wir verneinen, da ansonsten unser (.csv-)Dateiformat geändert wird. Oben rechts im Browser sehen wir einen nach unten zeigenden Pfeil . Wenn wir auf diesen klicken, können wir uns die heruntergeladene Datei im Finder anzeigen lassen. Standardmäßig werden heruntergeladene Dateien im Ordner Downloads gespeichert. 6.3.2 In Arbeitsordner verschieben Wir machen einen Rechtsklick auf die Datei. Nun öffnet sich ein Menü, in welchem wir Kopieren auswählen. Als nächstes begeben wir uns in unseren Arbeitsordner (ggf. müssen wir diesen vorher noch erstellen). Wir machen einen Rechtsklick und wählen die Option Objekt einsetzen aus. Die Datei ist nun im Arbeitsordner gespeichert; wir können sie nun auch aus dem Download-Ordner löschen. Jetzt öffnen wir RStudio, um die Datei einzulesen. 6.4 Weg 1: Environment &gt; Import Dataset Eine Variante, Daten in R ganz ohne Code zu importieren, ist es, das Icon Import Dataset zu nutzen. Dieses finden wir im rechten oberen Panel bei Environment. Nun klicken wir auf From CSV. Daraufhin öffnet sich ein Fenster, in dem wir verschiedene Optionen zum Einlesen haben. Achtung: In neueren RStudio-Versionen gibt es die Optionen From Text (base) und From Text (readr) (anstatt zusammengefasst From CSV). Beides kann genutzt werden, um .csv-Dateien einzulesen. base ist ein Standardpaket, welches in R von Beginn an vorinstalliert ist. Um readr nutzen zu können, müssen wir erst das gleichnamige Paket herunterladen. Die nachfolgend genannten Schritte beziehen sich auf die Benutzung von From Text (readr); das Fenster bei From Text (base) sieht auch anders aus. Nachdem wir eine Option ausgewählt haben, öffnet sich ein Fenster, in welchem wir die gewünschte Datei in unserem Arbeitsordner auswählen können. Dann öffnet sich ein neues Fenster, welches eine Vorschau beinhaltet, die uns zeigt, wie die Datei mit den derzeitig festgelegten Optionen in R aussehen würde. Wenn es Probleme gibt (z.B. mit der Interpretation der Trennungszeichen), sehen wir das sofort an der Darstellung der Daten. Zum Einlesen sind folgende Schritte nötig: Anklicken des Browse-Buttons. Daraufhin öffnet sich ein Fenster, in welchem wir die Datei auswählen können. Überprüfen der Vorschau, ob die Daten korrekt dargestellt werden. Die häufigsten Probleme bei der Repräsentation der Daten kommen durch die Trennungzeichen (zwischen den einzelnen Datenelementen) zustande. Diese können wir bei Delimiter ändern. Anschließend drücken des Import-Buttons. Wenn die Datei neuro.csv erfolgreich eingelesen wurde, erscheint das neu erstellte Objekt neuro (oder welchen anderen Namen wir dem Objekt gegeben haben) im rechten oberen Panel bei Environment. 6.5 Weg 2: Files &gt; Import Dataset Schauen wir uns einen weiteren Weg an, mit der Benutzeroberfläche der Entwicklungsumgebung RStudio Dateien einzulesen. Das Vorgehen hier ist weitestgehend analog zu Weg 1. Wir klicken auf die Datei neuro.csv in unserem Arbeitsordner und dann auf die Option Import Dataset. Es öffnet sich (weitestgehend) das gleiche Fenster wie in Weg 1. Wir haben hier mit Hilfe der Vorschau wieder die Möglichkeit vor dem Einlesen zu Überprüfen, ob die Datei von R richtig repräsentiert wird. Die meisten Probleme hängen mit den Trennungszeichen zwischen den einzelnen Datenelementen zusammen. Diese Option können wir unter Delimiter anpassen. Stimmt die Vorschau mit unseren Erwartungen überein, können wir rechts unten auf Importieren klicken. Im Workspace sollten wir nun den eben eingelesenen Dataframe neuro finden. 6.6 Weg 3: Manuell Importieren mit Funktionen Auch wenn Möglichkeiten existieren, Dateien mithilfe der Benutzeroberfläche von RStudio einzulesen, ist es ratsam, auch einmal selbst Funktionen zu nutzen. Für die meisten Arbeiten in R nutzen wir nämlich Funktionen. Welche Funktion hierfür angebracht ist, hängt von der Struktur der Datei ab. Nachfolgend schauen wir uns an, welche Funktionen wir für .csv, .txt und .dat nutzen können. # nutzbare Funktionen zum Einlesen von .csv, .txt. und .dat daten &lt;- read.table(&quot;Dateipfad/neuro.csv&quot;) daten &lt;- read.delim(&quot;Dateipfad/neuro.csv&quot;) daten &lt;- read.csv(&quot;Dateipfad/neuro.csv&quot;) Einen Dateipfad kopieren Unter Windows können wir auf shift drücken und dann einen Rechtsklick auf die Datei machen. Nun öffnet sich ein Menü, in welchem wir Als Dateipfad kopieren auswählen. Wichtig dabei ist, dass wir noch alle \\ (backslashes) aus dem kopierten Pfad in / (forwardslashes) ändern müssen. Unter Mac können wir die Tastaturkürzel alt + cmd + c nutzen, um unseren Dateipfad zu kopieren. Diese drei Funktionen sind sehr ähnlich aufgebaut. Sind haben aber teilweise unterschiedliche Voreinstellungen (sog. “Defaults”). Zum Beispiel nimmt read.csv() an, dass einzelne Datenelemente mit Kommata (Default: sep=\",\") getrennt werden. Dafür werden bei read.table() standarmäßig Spaltennamen nicht eingelesen (Default: header=FALSE). Achtung: Alle Funktionsdefinitionen (mit Defaults) finden wir in der R-Dokumentation, die wir im unteren rechten Panel bei Help finden. Alternativ können wir sie auch mit der Hilfefunktion ?, z.B. ?read.table, öffnen. In Abhängigkeit der Speicherung der Dateien müssen wir manchmal den Parametern der Funktionen andere Argumente übergeben. Die zwei wichtigsten Parameter sind header und sep. header ob Spaltennamen übernommen werden sollen TRUE oder FALSE möglich Wenn es Spaltennamen gibt, aber header = FALSE festgelegt ist, stehen diese in der ersten Zeile und die Spalten werden alternativ mit V1, V2, V3, … benannt. sep wie (angenommen wird, dass) einzelne Datenelemente getrennt sind u.a. Komma (,), Semikolon (,) und Freizeichen () möglich Dass wir hier etwas ändern müssen erkennen wir daran, dass nicht die gesamte Anzahl an Spalten im R-Objekt vorhanden sind. Wenn wir uns das Objekt anschauen, dann sehen wir, mit welchem Zeichen die Elemente getrennt sind. Achtung: Es kann dabei sein, dass unterschiedliche Personen zum korrekten Einlesen derselben Datei andere Argumenten nutzen. Das kann auf unterschiedliche Betriebssysteme oder Programme zum Öffnen der Dateien zurückzuführen sein. Wenn wir den Dataframe eingelesen haben, erscheint er im Environment. Probleme? Nutze Trial-and-Error! Um in Erfahrung zu bringen, welche Argumente wir nutzen müssen, um die Daten korrekt einzulesen, können wir einen Trial-and-Error Ansatz verwenden: Wir lesen die Datei erstmal ohne Spezifikation von Argumenten ein z.B. mit read.table(\"Dateipfad\"). Dann schauen wir uns die Datei in R ein und beurteilen, ob diese korrekt angezeigt wird. Schauen wir uns dazu beispielhaft einmal folgende .csv-Datei an: V1 uni,satis_uni,residence,satis_location_uni FU,5.5130602101329025,S,4.4057101367098666 FU,7.233871516077954,Z,5.760182068473526 HU,12.890984224974451,Z,7.205686095942897 HU,8.691844540148681,Z,5.2505748559609025 FU,5.136949092682058,N,5.421972864044606 Die Datei wird scheinbar nicht korrekt angezeigt. Dabei fallen zwei Sachen ins Auge: Es gibt keine Spaltennamen bzw. stehen diese in der ersten Zeile. Daher müssen wird das Argument header=TRUE nutzen, damit die Spaltennamen als solche übernommen werden. Es existiert nur ein Spalte. Die Daten aus verschiedenen Spalten werden alle in einer Zeile dargestellt. Wenn man sich das genauer anschaut sieht man, dass die einzelnen Daten jeweils mit einem Komma voneinander getrennt sind. Folglich müssen wird das Argument sep=\",\" benutzen, damit die Spalten korrekt getrennt werden. Jetzt lesen wir die Datei nochmal mit diesen Argumenten ein: read.table(\"Dateipfad\", header=TRUE, sep=\",\") uni satis_uni residence satis_location_uni FU 5.513060 S 4.405710 FU 7.233872 Z 5.760182 HU 12.890984 Z 7.205686 HU 8.691845 Z 5.250575 FU 5.136949 N 5.421973 HU 7.777371 O 6.593942 Nun wird die Datei korrekt dargestellt. 6.7 FAQ Um Daten in R aufbereiten und bearbeiten zu können, muss man diese erst einmal einlesen können. Manchmal gestaltet sich das leider schwieriger als erwartet. Im Folgenden wollen wir uns einige Methoden zum Einlesen gängiger Dateiformate in R anschauen. Das Dateiformat einer Datei erkennt man an seiner Endung. Kurzbefehle zum Kopieren des Dateipfads Windows Wir drücken shift und machen dann einen Rechtsklick auf die Datei. Nun öffnet sich ein Menü, in welchem wir Als Dateipfad kopieren auswählen. Wichtig dabei ist, dass wir noch alle \\ (backslashes) aus dem kopierten Pfad in / (forwardslashes) ändern müssen. Mac Wir klicken einmal auf die Datei (sodass sie markiert ist; dann ist sie blau hinterlegt) und führen dann den Kurzbefehl alt + cmd + C aus. Für eine ausführlichere Anleitung zum Einlesen von Dateien in R gibt es ein eigenständiges Kapitel, in wir lernen, wie wir die Benutzeroberfläche von RStudio und Funktionen nutzen können. 6.7.1 .csv, .txt und .dat Diese Dateiformate sind die am weitesten verbreiteten Tabellendateien. Alle drei können u.a. mit den folgenden Funktionen eingelesen werden: read.table(\"Dateipfad\") read.delim(\"Dateipfad\") read.csv(\"Dateipfad\") Diese drei Funktionen sind sehr ähnlich aufgebaut. Sind haben aber teilweise unterschiedliche Voreinstellungen (sog. “Defaults”). Zum Beispiel nimmt read.csv() an, dass Elemente aus verschiedenen Spalten in einer Zeile jeweils mit einem Komma (sep=\",\") getrennt sind. Dafür werden bei read.table() ohne weitere Spezifikation die Spaltennamen nicht eingelesen (header=FALSE). In Abhängigkeit der Speicherung der Dateien muss man teilweise noch weitere Argumente der Funktionen nutzen (d.h. nicht die Defaults einer Funktion nutzen). header = TRUE (bei read.table) Die richtigen Spaltennamen stehen sonst in der ersten Zeile und Spalten werden mit V1, V2, V3 etc. benannt. sep = \";\" … wenn die Elemente verschiedener Spalten in einer Zeile mit Semikolon getrennt sind Dass wir hier etwas ändern müssen erkennen wir daran, dass nicht die gesamte Anzahl an Spalten im R-Objekt vorhanden sind. Wenn wir uns das Objekt anschauen, dann sehen wir, mit welchem Zeichen die Elemente getrennt sind. fill = TRUE (bei read.table) … wenn fehlende Werte mit NA (R-Kodierung für Missings) befüllt werden sollen. Um in Erfahrung zu bringen, welche Argumente wir nutzen müsst, um die Daten korrekt einzulesen, können wir einen Trial-and-Error Ansatz verwenden: Wir lesen die Datei erstmal ohne Spezifikation von Argumenten ein z.B. mit read.table(\"Dateipfad\"). Dann schauen wir uns die Datei in R ein und beurteilen, ob diese korrekt angezeigt wird. Schauen wir uns dazu beispielhaft einmal folgende csv-Datei an: uni satis_uni residence satis_location_uni FU 5.513060 S 4.405710 FU 7.233872 Z 5.760182 HU 12.890984 Z 7.205686 HU 8.691845 Z 5.250575 FU 5.136949 N 5.421973 HU 7.777371 O 6.593942 Die Datei wird scheinbar nicht korrekt angezeigt. Dabei fallen zwei Sachen ins Auge: Es gibt keine Spaltennamen bzw. stehen diese in der ersten Zeile. Daher müssen wird das Argument header=TRUE nutzen, damit die Spaltennamen als solche übernommen werden. Es existiert nur ein Spalte. Die Daten aus verschiedenen Spalten werden alle in einer Zeile dargestellt. Wenn man sich das genauer anschaut sieht man, dass die einzelnen Daten jeweils mit einem Komma voneinander getrennt sind. Folglich müssen wird das Argument sep=\",\" benutzen, damit die Spalten korrekt getrennt werden. Jetzt lesen wir die Datei nochmal mit diesen Argumenten ein: read.table(\"Dateipfad\", header=TRUE, sep=\",\") uni satis_uni residence satis_location_uni FU 5.513060 S 4.405710 FU 7.233872 Z 5.760182 HU 12.890984 Z 7.205686 HU 8.691845 Z 5.250575 FU 5.136949 N 5.421973 HU 7.777371 O 6.593942 Nun wird die Datei korrekt dargestellt. Achtung: Es kann dabei sein, dass unterschiedliche Personen zum korrekten Einlesen (derselben Datei) andere Argumenten nutzen. Das kann auf unterschiedliche Betriebssysteme oder Programme zum Öffnen der Dateien zurückzuführen sein. Mehr Informationen zu den Funktionen und ihren Argumenten findet wir, wenn wir unter read.table in der R-Dokumentation (Help im unteren rechten Panel) nachschauen, oder indem wir ?read.table in die Konsole eingeben. 6.7.2 .xls und .xlsx Diese Endungen gehören zum Programm Excel. Man muss externe Pakete installieren, um Excel-Dateien in R einlesen zu können. Um .xlsx-Dateien einzulesen, nutzen wir das Paket openxlsx. library(openxlsx) daten &lt;- read.xlsx(&quot;Dateipfad&quot;) Alternativ können wir das Paket readxl nutzen. Dieses ermöglicht uns sogar .xlsx- und .xls-Dateien einzulesen. library(readxl) daten &lt;- read_xlsx(&quot;Dateipfad&quot;) 6.7.3 .sav Die Endung .sav wird z.B. für SPSS-Dateien genutzt wird. Hierfür muss man wieder ein zusätzliches Paket (z.B. foreign) runterladen, um SPSS-Dateien in R einlesen zu können. library(foreign) daten &lt;- read.spss(&quot;Dateipfad&quot;, to.data.frame = TRUE) # ohne to.data.frame wird eine Liste erzeugt 6.7.4 Dateien via URL direkt aus dem Internet laden Man kann Dateien auch direkt aus dem Internet laden mit Hilfe ihrer URL. daten &lt;- load(url(&quot;Webadresse&quot;)) Alternativ kann man die Datei auch herunterladen und dann in Abhängigkeit ihres Formats mit einem der oberen Befehle einlesen. 6.7.5 .R, .Rda und .Rmd R-eigene Dateien kann man am besten öffnen, indem man auf sie klickt. Mac-Benutzer haben dabei manchmal das Problem, das sich die Datei per default in R öffnet. Auf Mac-Rechnern kann man R-Studio folgendermaßen zum Standardprogramm zum Öffnen von R-Dateien machen (siehe Abb. unten) Rechtsklick auf die Datei auf Informationen klicken Unter Öffnen mit R-Studio auswählen und auf Alle ändern… klicken Wenn man nur die eine R-Datei regulär mit R-Studio öffnen möchte: Rechtsklick auf die Datei Öffnen mit Anderem Programm … R-Studio auswählen unten mittig ein Häkchen in das Kästchen Immer öffnen mit setzen (wenn das nicht angezeigt wird, muss man erst unten rechts auf Optionen klicken) Öffnen 6.8 Weiterführende Hilfe Falls wir ein seltener genutztes Dateiformat (z.B. Stata, JSON) in R einlesen möchten, können wir in dem Data Import Tutorial auf Datacamp nachschauen. 6.9 Übung In diesem Abschnitt finden wir verschiedene Dateien, die wir zur Übung in R einlesen können. Wie wir dabei vorgehen (d.h. welchen Weg wir nutzen) bleibt ganz uns überlassen. Wenn wir möchten, können wir die Tipps nutzen, um die Aufgaben zu lösen. Zur Überprüfung finden wir mögliche Lösungswege und die eingelesenen Daten. Wenn wir Hilfe beim Einlesen von Daten brauchen, können wir uns das ausführliche Kapitel dazu anschauen. Wenn wir Probleme beim Installieren oder Laden von Paketen haben, können wir unseren FAQ-Eintrag dazu anschauen. 6.9.1 Übung 1: .csv Lade dir von openpsychometrics.org die zip-Datei NPI runter. Entpacke diese und lese data.csv in R ein. Tipp 1 Der Datensatz besteht aus 11243 Zeilen und 44 Spalten. Lösung Man kan die Datei z.B. mit read.csv(\"Dateipfad\") oder mit read_csv(\"Dateipfad\") (aus dem Paket readr) korrekt einlesen. Hier siehst du, wie die ersten 6 Zeilen der insgesamt 44 Spalten der Datei. score Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 Q21 Q22 Q23 Q24 Q25 Q26 Q27 Q28 Q29 Q30 Q31 Q32 Q33 Q34 Q35 Q36 Q37 Q38 Q39 Q40 elapse gender age 18 2 2 2 2 1 2 1 2 2 2 1 1 2 1 1 1 2 1 1 1 1 1 1 2 2 2 1 2 2 2 1 2 1 1 1 2 2 2 1 2 211 1 50 6 2 2 2 1 2 2 1 2 1 1 2 2 2 1 2 2 1 1 2 1 2 2 1 2 2 2 2 1 2 2 2 1 2 2 1 2 2 2 2 1 149 1 40 27 1 2 2 1 2 1 2 1 2 2 2 1 1 1 1 1 2 2 1 1 2 2 2 2 1 2 1 1 2 1 2 2 1 1 2 1 1 2 1 2 168 1 28 29 1 1 2 2 2 1 2 1 1 2 1 1 1 1 1 1 2 2 1 2 1 1 1 2 1 2 1 2 2 1 1 2 1 1 2 1 2 2 1 1 230 1 37 6 1 2 1 1 1 2 1 2 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 2 2 1 2 1 2 2 2 1 2 2 1 2 2 2 0 1 389 1 50 19 1 2 2 1 2 1 1 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 2 1 2 1 1 2 2 2 2 361 1 27 6.9.2 Übung 2: .csv Lade dir von openpsychometrics.org die zip-Datei 16PF runter. Entpacke diese und lese data.csv in R ein. Tipp 1 Der Datensatz besteht aus 49159 Zeilen und 169 Spalten. Tipp 2 Die einzelnen Zellen (d.h. Elemente der Tabelle) sind durch Leerzeichen (white space) getrennt. Tipp 3 Die Information, wie die Zellen getrennt sind (siehe Tipp 2) übergibt man dem Argument sep. Lösung Man kan die Datei z.B. mit read.csv(\"Dateipfad\", sep=\"\") oder mit read_table2(\"Dateipfad\") (aus dem Paket readr) korrekt einlesen. Hier siehst du die ersten 6 Zeilen der ersten 50 Spalten der Datei. A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 B1 B2 B3 B4 B5 B6 B7 B8 B9 B10 B11 B12 B13 C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 E1 E2 E3 E4 E5 E6 E7 1 4 2 3 3 2 3 4 4 3 4 4 5 4 5 4 5 4 1 2 1 1 1 4 5 4 4 2 4 4 3 3 2 4 3 5 5 4 4 3 2 4 3 1 1 4 3 4 5 1 4 3 4 3 4 4 4 4 2 2 4 4 4 4 5 4 3 2 3 2 4 1 1 1 2 3 3 2 5 4 4 3 3 4 2 4 4 4 5 4 2 3 1 1 2 4 1 4 2 2 3 4 4 4 4 4 4 3 2 2 4 4 5 5 4 4 4 4 2 2 2 2 2 2 4 4 3 3 3 4 2 3 0 3 3 2 2 3 4 3 1 3 3 1 1 3 1 4 2 3 4 5 4 4 4 3 3 2 2 2 4 2 4 5 4 5 4 4 3 3 3 2 4 3 2 3 4 3 3 2 2 3 4 3 2 3 4 2 3 3 3 4 3 3 2 4 1 4 4 1 4 0 4 4 4 3 5 1 2 4 2 4 4 5 5 4 4 5 4 1 5 1 2 2 4 3 3 4 4 4 4 3 2 5 4 3 4 5 4 1 1 1 3 1 1 3 1 4 2 4 3 5 4 4 4 5 5 1 1 4 4 1 4 5 3 4 3 3 2 2 1 2 2 2 3 4 2 4 2 2 4 3 1 4 4 5 5 4 3 2 2 2 3 3 4 3 2 4 3 2 6.9.3 Übung 3: .sav Lade dir die Datei ges7.sav von metheval.uni-jena.de herunter und lese diese in R ein. Tipp 1 Die Endung .sav kennzeichnet SPSS-Dateien. Um diese einzulesen benötigt man zusätzliche Pakete, weil es in base R keine Funktion dafür gibt. Tipp 2 Der Datensatz besteht aus 503 Zeilen und 1650 Spalten. Lösung Man kan die Datei z.B. mit read.spss(\"Dateipfad\", to.data.frame = TRUE) (aus dem Paket foreign) oder mit read_sav(\"Dateipfad\") (aus dem Paket haven) korrekt einlesen. Hier siehst du die ersten 6 Zeilen der ersten 50 Spalten der Datei. CODE T1SEX T1AGE T1KNR T1MZP T1TIME T1DAY T1MON T1SB T1ST01 T1ST02 T1ST03 T1ST04 T1ST05 T1ST06 T1ST07 T1ST08 T1ST09 T1ST10 T1ST11 T1ST12 T1ST13 T1ST14 T1ST15 T1ST16 T1ST17 T1ST18 T1ST19 T1ST20 T1ST21 T1ST22 T1ST23 T1ST24 T1ST25 T1ST26 T1ST27 T1ST28 T1ST29 T1ST30 T1ST31 T1ST32 T1ST33 T1ST34 T1ST35 T1ST36 T1ST37 T1ST38 T1ST39 T1ST40 T1ST41 aa02 2 22 1 1 23 NA NA 1 4 3 1 3 4 4 1 1 4 1 1 1 1 3 2 3 4 4 2 1 2 3 3 3 2 4 4 1 2 4 3 1 5 1 4 1 3 1 1 1 1 aa19 1 35 1 1 11 8 5 2 3 3 4 5 2 3 3 4 2 5 5 5 3 2 4 3 2 3 4 5 4 2 1 1 5 2 4 4 3 2 4 5 5 4 1 4 1 5 3 3 4 ab17 1 58 1 1 21 NA NA 1 1 3 4 2 1 2 4 1 1 3 2 4 1 3 3 1 1 3 5 1 4 1 1 3 3 1 2 1 2 4 4 2 1 1 4 1 4 2 2 4 3 ac03 2 53 1 1 19 15 5 1 5 4 3 1 3 1 3 1 4 2 1 4 1 5 2 1 4 2 1 1 4 3 3 1 1 3 1 1 1 4 5 1 1 1 3 1 3 3 1 4 3 ac09 1 25 1 1 22 3 5 1 5 5 1 1 5 5 1 1 5 1 1 1 1 5 1 1 5 3 1 1 1 5 5 1 1 5 1 1 1 5 3 1 1 1 4 1 5 4 1 4 4 ad03 2 23 1 1 16 10 4 1 3 3 2 1 3 3 4 1 4 1 1 1 1 4 1 1 4 3 1 1 1 3 3 1 2 2 2 1 3 2 1 2 1 2 2 1 2 2 3 3 2 6.9.4 Übung 4: .xlsx Lade dir die Bahnsteigdaten (RNI) von data.deutschebahn.com herunter und lese diese in R ein. Tipp 1 Die Endung .xlsx kennzeichnet Excel-Dateien. Um diese einzulesen benötigt man zusätzliche Pakete, weil es in base R keine Funktion dafür gibt. Tipp 2 Der Datensatz besteht aus 345 Zeilen und 4 Spalten. Lösung Man kan die Datei z.B. mit read_xlsx(\"Dateipfad\") (aus dem Paket readxl) oder mit read.xlsx(\"Dateipfad\") (aus dem Paket openxlsx) korrekt einlesen. Hier siehst du die ersten 6 Zeilen der insgesamt 4 Spalten der Datei. bf_nr Bahnsteig_Nr Bahnsteig_Hoehe_cm Nettobahnsteiglaenge_m 8263 1 38 115 8263 2 38 115 2616 1 38 115 6618 1 38 113 6618 2 38 123 33 1 38 115 Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] readxl_1.3.1 foreign_0.8-82 devtools_2.4.5 usethis_2.1.6 ## [5] ICC_2.4.0 readr_2.1.3 Hmisc_4.7-1 Formula_1.2-4 ## [9] survival_3.2-13 lattice_0.20-45 ggplot2_3.4.0 colorspace_2.0-3 ## [13] psych_2.2.9 car_3.1-1 carData_3.0-5 kableExtra_1.3.4 ## [17] dplyr_1.0.10 htmltools_0.5.3 rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-155 fs_1.5.2 webshot_0.5.4 ## [4] RColorBrewer_1.1-2 httr_1.4.2 profvis_0.3.7 ## [7] tools_4.2.0 backports_1.4.1 bslib_0.4.0 ## [10] utf8_1.2.2 R6_2.5.1 rpart_4.1.16 ## [13] DBI_1.1.2 nnet_7.3-17 urlchecker_1.0.1 ## [16] withr_2.5.0 prettyunits_1.1.1 processx_3.8.0 ## [19] tidyselect_1.2.0 gridExtra_2.3 mnormt_2.1.1 ## [22] compiler_4.2.0 cli_3.4.1 rvest_1.0.2 ## [25] htmlTable_2.4.1 xml2_1.3.3 bookdown_0.29 ## [28] sass_0.4.2 scales_1.2.1 checkmate_2.0.0 ## [31] callr_3.7.2 systemfonts_1.0.4 stringr_1.4.0 ## [34] digest_0.6.30 svglite_2.1.0 base64enc_0.1-3 ## [37] jpeg_0.1-9 pkgconfig_2.0.3 sessioninfo_1.2.2 ## [40] fastmap_1.1.0 highr_0.9 htmlwidgets_1.5.4 ## [43] rlang_1.0.6 rstudioapi_0.13 shiny_1.7.3 ## [46] jquerylib_0.1.4 generics_0.1.2 jsonlite_1.8.3 ## [49] magrittr_2.0.2 interp_1.0-33 Matrix_1.5-1 ## [52] Rcpp_1.0.9 munsell_0.5.0 fansi_1.0.3 ## [55] abind_1.4-5 lifecycle_1.0.3 stringi_1.7.8 ## [58] yaml_2.3.5 pkgbuild_1.3.1 promises_1.2.0.1 ## [61] parallel_4.2.0 crayon_1.5.2 miniUI_0.1.1.1 ## [64] deldir_1.0-6 splines_4.2.0 hms_1.1.1 ## [67] ps_1.7.2 pillar_1.8.1 pkgload_1.3.0 ## [70] glue_1.6.2 evaluate_0.15 latticeExtra_0.6-30 ## [73] remotes_2.4.2 data.table_1.14.4 png_0.1-7 ## [76] vctrs_0.5.0 tzdb_0.3.0 httpuv_1.6.5 ## [79] cellranger_1.1.0 gtable_0.3.0 purrr_0.3.4 ## [82] assertthat_0.2.1 cachem_1.0.6 xfun_0.34 ## [85] mime_0.12 xtable_1.8-4 later_1.3.0 ## [88] viridisLite_0.4.1 tibble_3.1.8 memoise_2.0.1 ## [91] cluster_2.1.2 ellipsis_0.3.2 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["fehlermeldungen.html", "Chapter 7 Fehlermeldungen 7.1 Tools die uns helfen, Fehler zu vermeiden 7.2 Fehlermeldungen verstehen 7.3 Suchen im Internet 7.4 Weiterführende Hilfe 7.5 Automatische Suche mit dem Paket errorist", " Chapter 7 Fehlermeldungen Beim Arbeiten mit R wird es früher oder später einmal dazu kommen, dass wir in der Konsole eine Fehlermeldung ausgegeben bekommen. Generell kann man in R drei verschiedene Meldungen (Conditions) erhalten: error: Durch Fehler wird die Ausführung der Funktion unterbrochen. warning Im Gegensatz zu echten Fehlern (errors) wird die Ausführung des Codes bei warnings nicht unterbrochen. Warnmeldungen dienen dazu, die Aufmerksamkeit des Benutzers auf potentielle Probleme zu lenken. Im Unterschied zu errors kann man mehrere Warnungen von einer Funktion erhalten. message Geben Informationen dazu, was eine Funktion (zusätzlich) gemacht hat. Eine vertiefte Einführung in die verschiedenen Conditions finden wir auf Advanced R. Wir konzentrieren uns in diesem Abschnitt auf ersteres: Fehlermeldungen, die unsere Arbeit in R anhalten lassen und nach einer Lösung verlangen. Fehlermeldungen können unterschiedliche Ursachen haben. Häufig sind es Tipp- oder Syntaxfehler, aber auch Probleme mit Datentypen (z.B. eine Funktion erfordert einen anderen als den genutzen Datentyp) und Paketen (z.B. Laden des Paketes einer Funktion vergessen) kommen häufiger vor. Es gibt keine fixe Taxonomie der Arten von Fehlern in R. Es seien hier nur einige genannt. Fehlermeldungen sind häufig nicht einfach zu verstehen. Das liegt u.a. daran, dass EntwicklerInnen von Paketes jeweils eigene Fehlermeldungen für die enthaltenen Funktionen formulieren. Deswegen folgen diese zumeist keinem einheitlichen Muster und sind teils schwer zu verstehen. Im Folgenden gehen wir uns zuerst darauf ein, welche Hilfe uns RStudio gibt, um Fehler zu vermeiden. Anschließend schauen wir uns an einigen konkreten (Syntax-)Fehlermeldungen an, wie man diese verstehen kann. Danach lernen wir, wie wir Suchmaschinen sinnvoll nutzen können, um Lösungen für unsere Fehlermeldungen zu finden. Beispieldatensatz für dieses Kapitel Für das vorliegende Kapitel werden wir uns einen kleinen Datensatz generieren, mit dem wir arbeiten werden. # Matrix erstellen daten &lt;- matrix(c(0, 1, 3, 2, 2, 3, 2, 0, 3, 1, 3, 0, 1, 3, 1, 1, 2, 1, 0, 3), nrow = 5, ncol = 4) # in Dataframe umwandeln daten &lt;- data.frame(daten) # Spalten benennen colnames(daten) &lt;- c(&quot;Var_1&quot;, &quot;Var_2&quot;, &quot;Var_3&quot;, &quot;Var_4&quot;) ## Var_1 Var_2 Var_3 Var_4 ## 1 0 3 3 1 ## 2 1 2 0 2 ## 3 3 0 1 1 ## 4 2 3 3 0 ## 5 2 1 1 3 7.1 Tools die uns helfen, Fehler zu vermeiden Bevor wir uns dem Verstehen von konkreten (Syntax-)Fehlermeldungen widmen und anschauen, wie wir effizient im Internet suchen können, schauen wir uns zwei Hilfsmittel an, die uns die Entwicklungsumgebung RStudio gibt, um Fehler zu vermeiden. 7.1.1 R-Dokumentation Wenn wir eine uns noch unbekannte Funktion nutzen (aber auch wenn wir aus einer Fehlermeldung nicht schlau werden) können wir die Dokumentation nutzen. Diese können wir in RStudio im unteren rechten Panel unter Help öffnen. In das Suchfeld geben wir den Namen der Funktion ein. Alternativ können wir die Hilfefunktionen ? oder help() nutzen (z.B. ?matrix oder help(matrix)). Mit Klick auf das eingekreiste Icon kann man sich die Hilfe-Seite auch in einem extra Fenster anzeigen lassen. Wir finden die R-Dokumentation auch im Internet, allerdings ermöglicht uns RStudio, diese ohne Internetverbindung und direkt in der Entwicklungsumgebung zu öffnen. Wir bekommen hier (zumeist) folgende Informationen: aus welchem Paket die Funktion ist (oben links in { }), was die Funktion macht (Description), die Funktionsdefinition ggf. mit Defaults (Usage), ihre Parameter mit jeweils gültigen Argumenten (Arguments), weitere Details zur Nutzung (Details), ähnliche Funktionen (See also) und Beispiele zur Nutzung (Examples). Was sind Parameter, Argumente und Defaults? Parameter bezeichnet die (formalen) Variablen einer Funktion (z.B. nrow aus matrix()), denen wir unsere (tatsächlichen) Argumente (z.B. 3), d.h. unseren Input, übergeben. Default bezeichnet ein voreingestelltes Argument (z.B. nrow=1). Ohne explizite Spezifikation des Parameters unsererseits wird (wenn vorhanden) der Default verwendet. Wir könnten natürlich auch ein anderes (als das voreingestellte) Argument festlegen. Funktionen, die ausschließlich Parameter mit Defaults besitzen, (z.B. matrix()) werden auch ohne Spezifikation ausgeführt. Funktionen mit (min. einem) Parameter ohne Default (z.B. mean()) werden ohne Spezifikation dieser nicht ausgeführt (wir müssen dem Parameter x einen Vektor, von dem wir den Mittelwert berechnen wollen, übergeben). Mehr Informationen zu Funktionsdefinition und etwaigen Defaults finden wir im Abschnitt Usage in der R-Dokumentation. Mehr Informationen zum Aufbau von Funktionen finden wir in der Einführung in R Eine detailliertere Einführung zur Nutzung der Dokumentation befindet sich im Kapitel Einführung in RStudio. 7.1.2 Code Diagnostik Achtung: Wir können die Code Diagnostik nur nutzen, wenn wir unser Skript gespeichert haben. Vor Ausführung unseres Codes erhalten wir Hinweise, beispielsweise wenn Argumente einer Funktion fehlen oder unerwartete Zeichen auftauchen. Der Fehler wird unterstrichen und es erscheint außerdem ein Symbol links neben der Zeilennummerierung. Zusätzlich erhalten wir einen ausformulierten Hinweis, wenn wir mit der Maus über das Symbol fahren. Lösung Das dritte Argument, welches nach dem zweiten Komma in c() folgen sollte, ist leer. In unserem Beispiel sind die Argumente von c() Zahlen (zur Indexierung der Spalten vom Dataframe daten). Wenn wir die Spalten 1, 2 und 3 extrahieren wollten, würden wir das überflüssige Komma löschen: daten[,c(1,2,3)] Lösung Unser Code endet mit einer schließenden runden Klammer, welche kein öffnendes Pendant hat. Außerdem hat unsere öffnende eckige Klammer kein abschließendes Pendant. Wahrscheinlich haben wir uns vertippt und anstatt einer schließenden eckigen Klammer eine schließende runde Klammer eingefügt. So würde der richtige Code aussehen: daten[,c(1,2,3)] Auf dieser Seite wird die Möglichkeiten der Code Diagnostics noch detaillierter erklärt und wir erhalten eine Anleitung dazu, wie wir verschiedene Einstellungen tätigen können (in der oberen Menüleiste über Tools &gt; Global Options &gt; Code &gt; Diagnostics). 7.2 Fehlermeldungen verstehen Schauen wir uns einmal den typischen Aufbau von Fehlermeldungen an: Diese Fehlermeldung werden wir im Abschnitt [Suchen im Internet] noch aufklären. Die Information vor dem Doppelpunkt gibt uns an, in welcher Funktion der Fehler steckt; die Information nach dem Doppelpunkt gibt Aufschluss über die Art des Fehlers. Zweiteres ist für die Fehlersuche (zumeist) von großer Bedeutung. Im Folgenden konzentrieren wir uns auf eine Art von Fehler, mit der vor allem AnfängerInnen häufig konfrontiert sind: Syntaxfehler. Syntaxfehler sind Fehler, die durch eine Verletzung des formalen Aufbaus einer Funktion zustande kommen. Manchmal entstehen sie auch nur durch Vertippen. Syntaxfehler können zumeist relativ einfach durch Korrigieren der fehlerhaften Syntax, i.d.R. dem Einfügen oder Entfernen von bestimmten Zeichen, zumeist Kommata oder Klammern, gelöst werden. Schauen wir uns nun einige Beispiele an: Wenn ein oder mehrere Zeichen überflüssig sind bzw. fehlen, dann bekommt man Unerwartete(s) '...' in \"...\" ausgegeben. Hierbei teilt uns die Meldung mit, wo der Fehler liegt: In den Anführungszeichen \"...\" wird nur ein Teil des Codes ausgegeben und der Fehler ist zumeist das zuletzt ausgegebene Zeichen (oder es liegt unmittelbar davor). Lösung z.B. daten[2] Damit extrahiert man auch die zweite Spalte aus daten, aber hier wird der Name der Spalte (Variable) nicht mit ausgegeben. Außerdem werden die Elemente der Spalte zeilwenweise wiedergegeben. Wenn mehrere Syntaxfehler enthalten sind, und man nicht gleich alle erkennt, muss man diese nach und nach beseitigen. Lösung z.B. daten[2] Zuerst wurde nur auf das fehlerhafte $ hingewiesen. Nachdem dieses gelöscht wurde, wird erst auf das falsche ) verwiesen. Manchmal bekommt man auch nur unerwartetes Symbol in: \"...\" ausgegeben. In den Anführungszeichen \"...\" wird hier nur der Teil des Codes ausgegeben, in dem der Fehler ist, nicht aber (wie oben), auch das problematische Zeichen. Lösung z.B. daten[,2] oder daten[2] Hier wurde nicht bemerkt, dass bei der ersten Funktion daten[,2 noch ein ] fehlt und einfach schon die nächste Funktion daten[,3] ausgeführt. So nahm R an, dass beide zusammen gehören würden. Wenn man Komma-Fehler hat, bekommt man beispielsweise folgende Meldung angezeigt. Lösung z.B. daten[c(1,2)] oder daten[c(1,2,3)] Hier weist uns R darauf hin, dass das dritte Argument, welches nach dem zweiten Komma folgen sollte, leer ist. In Abhängigkeit davon, welche Spalten aus Daten extrahiert werden sollen, löscht man das Komma (um Spalte 1 und 2 zu extrahieren) oder man ergänzt einen weiteren Spaltenindex (z.B. 3; um Spalte 1, 2 und 3 zu extrahieren). 7.3 Suchen im Internet Nun gibt es auch Fehlermeldungen, die wir nicht auf Anhieb verstehen. Dafür kann es vielfältige Ursachen geben. Wenn wir schon in der R-Dokumentation nachgeschaut haben und trotzdem noch ratlos sind, können wir im Internet nach Hilfe suchen. Wir können dafür Suchmaschinen direkt spezielle Foren nutzen. Hierbei gibt es nicht den einen Weg, fündig zu werden. Häufig gibt es eine Fehlermeldung, die bei verschiedenen Problemen (mit unterschiedlichen Funktionen) angezeigt wird. Dann muss man filtern, welche Seite für das eigene Anliegen relevant sein könnte. Es kann ebenso möglich sein, dass es zu manchen Problemen einfach noch keine Lösung gibt. Dieser Fall ist eher seltener und tritt vor allem bei neueren oder seltener genutzen Paketen auf. Im Folgenden schauen wir uns einige wichtige Aspekte an, auf die man achten sollte, wenn man im Internet nach Lösungen zu Fehlermeldungen sucht. Dafür schauen wir uns folgendes Beispiel an: Wir wollen eine Korrelationstabelle erstellen, in der wir aus daten die ersten beiden Spalten [1:2] mit den letzten beiden Spalten [3:4] korrelieren. Dazu wurde die Funktion cor.test() genutzt. Wir bekommen folgende Fehlermeldung: a) Passende Suchbegriffe nutzen Sinnvoll ist es im Suchtext drei Aspekte miteinzubeziehen: das Programm, mit dem es Probleme gibt die Funktion, mit der es Probleme gibt auch ersichtlich an der Information vor dem Doppelpunkt Ausschnitte aus der Fehlermeldung von den Informationen nach dem Doppelpunkt Unsere Suche könnte folgendermaßen aussehen: Anstatt eine globale Suchmaschine wie google oder ecosia zu nutzen, kann man auch eine spezifisch für Anliegen in R konzipierte Suchmaschine wie rseek nutzen. Dann kann man im Suchtext den Namen des Programms weglassen, beispielsweise nur cor.test x numerischer Vektor suchen. b) Ergebnisse filtern Nachfolgend sehen wir die ersten fünf Ergebnisse der Suche. Suche mit ecosia am 05.09.2019 um 17:30Uhr Mit Ausnahme des zweiten Links sieht es so aus, als würden die Seiten zu unspezifisch für unser Problem sein. Der zweite Link weist gleich zwei Übereinstimmungen auf: Wir wollten einen correlation test rechnen und haben die selbe Fehlermeldung (‘x’ must be a numeric vector) bekommen. Schauen wir uns also das zweite Ergebnis an. c) Lösungsvorschläge ausprobieren In Foren findet man in aller Regel oben die Problembeschreibung und darunter die geposteten Lösungsvorschläge. In unserem Beispiel, hat der/die Fragende mit der gleichen Funktion (cor.test()) gearbeitet. Schauen wir uns den Lösungsvorschlag an: Es wird vorgeschlagen den Datensatz mit str() zu überprüfen, ob die enthaltenen Variablen numerische Vektoren sind. Wenn dem nicht so ist, muss man sie dementsprechend umwandeln. str(daten) ## &#39;data.frame&#39;: 5 obs. of 4 variables: ## $ Var_1: num 0 1 3 2 2 ## $ Var_2: num 3 2 0 3 1 ## $ Var_3: num 3 0 1 3 1 ## $ Var_4: num 1 2 1 0 3 Bei uns scheinen alle Spalten numerisch (num) zu sein. Diese Lösung scheint also nicht passend für unser Problem zu sein. Zurück zu b) Ergebnisse filtern und c) Lösungsvorschläge ausprobieren Gehen wir also zurück zu unserer Suche und schauen uns andere Seiten an. So sehen die nächsten fünf Ergebnisse der Suche aus. Bis auf den zweiten Link sehen die Vorschläge hier auch wieder sehr unspezifisch aus. Bei diesem gibt es zwei Parallelen zu unserem Problem: cor.test wird auf einen data frame angewendet. Sehen wir uns die Seite mal an. Wir landen in einem Foreneintrag. Die Antwort von dasonk weißt uns darauf hin, dass man an cor.test() nur zwei Vektoren übergeben kann. Wir haben aber 4 Vektoren übergeben. In der nächsten Antwort von anbende wird vorgeschlagen, die Korrelationstabelle mit cor() zu erstellen. Allerdings muss man die Signifikanztestung immer noch mit cor.test() machen. Die Antwort hat uns schon weitergeholfen: Wir haben eine Vorstellung davon, wo der Fehler liegen könnte. Jetzt ist es sinnvoll, eine neue Suche zu starten, um nach einer Funktion zu suchen, die sowohl eine Korrelationstabelle erstellt, als auch eine Signifikanztestung durchführt. Hier kombinieren wir Suchwörter zum Programm und zu dem, was die Funktion, die wir suchen, leisten soll. Dann schauen wir uns wieder die ersten 5 Vorschläge dazu an: Suche mit ecosia am 06.09.2019 um 09:40Uhr Der erste Vorschlag ist nicht ganz das, was wir wollen. Hier geht es scheinbar mehr um elegante Korrelationstabellen. Der zweite Vorschlag hingegen scheint vielversprechender. Schauen wir uns diesen einmal an. Es ist eine Seite, auf der verschiedene Aspekte der Umsetzung von Korrelationstabellen in R erklärt werden. Oben finden wir eine Gliederung. Wir springen gleich zum Punkt ‘Correlation matrix with significance levels (p-value)’. Hier erfahren wir, dass man die Funktion rcorr() aus dem Hmisc-Paket nutzen kann. Außerdem wird man darauf hingewiesen, dass diese Funktion nur mit Matrizen arbeiten kann. Der Befehl, mit dem man Objekte in Matrizen umwandeln kann, wird auch angegeben. Probieren wir diesen Weg einmal aus. # install.packages(&quot;Hmisc&quot;) library(Hmisc) rcorr(as.matrix(daten[1:2]), as.matrix(daten[3:4])) ## Var_1 Var_2 Var_3 Var_4 ## Var_1 1.00 -0.74 -0.29 -0.04 ## Var_2 -0.74 1.00 0.66 -0.44 ## Var_3 -0.29 0.66 1.00 -0.69 ## Var_4 -0.04 -0.44 -0.69 1.00 ## ## n= 5 ## ## ## P ## Var_1 Var_2 Var_3 Var_4 ## Var_1 0.1528 0.6309 0.9510 ## Var_2 0.1528 0.2279 0.4616 ## Var_3 0.6309 0.2279 0.2006 ## Var_4 0.9510 0.4616 0.2006 Wir erhalten zwei Matrizen als Output. Eine enthält die Korrelationen, die andere die p-Werte. Wir haben also gefunden, wonach wir gesucht haben. Und wenn man keine Lösung gefunden hat? Wenn man im ersten Durchlauf nichts gefunden hat, ist es beispielsweise sinnvoll, andere Ausschnitte aus der Fehlermeldung in den Suchtext zu inkludieren. Manchmal ist ein Fehler so global, dass er bei verschiedenen Funktionen auftaucht. Daher lohnt es sich manchmal auch, eine Suche ohne den Funktionsnamen durchzuführen. Spätestens, wenn man bei der Suche mit allen drei Suchbegriffen nichts gefunden hat, ist es sinnvoll, nur mit Programm + Fehlermeldung zu suchen. Wir können auch auf allgemeine Tipps zum Suchen im Internet zurückgreifen, u.a.: mit Zitaten exakte Phrasen suchen z.B. “Fehler in cor.test.default” alternative Begriffe suchen (logische Operatoren) AND ist der Default wenn wir mehrere Wörter eingeben mit OR (oder |) können wir mehrere Optionen angeben z.B. “cor.test | cor.test.default” aus- bzw. einschließen von Wörtern vor allem in Kombination mit Zitaten ist dieses Tool sehr nützlich ausschließen z.B. “Fehler in cor.test.default” -str()” einschließen z.B. “Fehler in cor.test.default +r” Ein weiterer Tipp ist es, in englisch zu suchen. Die Wahrscheinlichkeit ist hier viel größer, dass man Lösungsvorschläge findet. Wenn man, wie in diesem Kapitel zu sehen, die Fehlermeldungen auf deutsch ausgegeben bekommt, kann man das folgendermaßen auf englisch ändern: Sys.setenv(LANGUAGE=&quot;en&quot;) 7.4 Weiterführende Hilfe Ganz generell kann es sehr hilfreich sein, wenn man ein bisschen mit der Funktion “spielt” - etwas wegnimmt oder verändert - und schaut, ob es dann funktioniert oder wie sich die Fehlermeldung verändert. Das hilft meist, das Problem etwas einzugrenzen - und man lernt nebenbei die Funktion besser kennen. Im R-Fehlermeldungsleitfaden der Uni Münster (unter 3. Fehlertextliste), auf R-bloggers sowie PROGRAMMINGR findet man jeweils eine Übersicht über geläufige Fehlermeldungen mit Lösungen und teilweise auch Beispielen. Eine Einführung in weitere Möglichkeitens des Debuggings und der dafür in R eingebauten Tools finden wir auf Advanced R. 7.5 Automatische Suche mit dem Paket errorist Es gibt das Paket errorist, mit dem bei jeder Fehler- und Warnmeldung eine automatische Suche in Google gestartet wird. Schauen wir uns die Funktionsweise des Pakets am im Abschnitt Suchen im Internet genutzten Beispiels an. Zuerst installieren wir das Paket: install.packages(&quot;errorist&quot;, dependencies=TRUE) Dann laden wir das Paket und schauen uns an, was bei Ausführung unseres fehlerhaften Codes passiert. library(errorist) cor.test(daten[1:2], daten[3:4]) Sofort öffnet sich ein neues Fenster im Browser, in dem in Google folgender Text gesucht wird: “Fehler in cor.test.default(daten[1:2], daten[3:4]) : ‘x’ muss ein numerischer Vektor sein r programming”. Das Paket übernimmt sozusagen den ersten Schritt bei der Suche im Internet, passende Suchbegriffe zu nutzen, für uns. Die Ergebnisse müssen wir nach wie vor alleine filtern, um eine geeignete Lösung zu finden. Wenn wir uns den Aufbau des Suchtextes anschauen, sehen wir, dass die gesamte Fehlermeldung und “r programming” gesucht wird. In unserer manuellen Suche haben wir die gleichen Begriffe genutzt; wir haben lediglich nicht die gesamte Fehlermeldung genutzt, sondern daraus den Namen der Funktion und die eigentliche Fehlermeldung. Unser manueller Suchtext enthielt folglich weniger Wörter und generiert damit u.U. mehr Ergebnisse. Vergleich der Ergebnisse der Suchen für unser Beispiel: manuell 448.000 vs. errorist 17.000. Allerdings wurden auch verschiedene Suchmaschinen genutzt (ecosia vs. google). Ob ihr das Paket nutzen möchtet oder lieber analog sucht bleibt euch überlassen. Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] readxl_1.3.1 foreign_0.8-82 devtools_2.4.5 usethis_2.1.6 ## [5] ICC_2.4.0 readr_2.1.3 Hmisc_4.7-1 Formula_1.2-4 ## [9] survival_3.2-13 lattice_0.20-45 ggplot2_3.4.0 colorspace_2.0-3 ## [13] psych_2.2.9 car_3.1-1 carData_3.0-5 kableExtra_1.3.4 ## [17] dplyr_1.0.10 htmltools_0.5.3 rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-155 fs_1.5.2 webshot_0.5.4 ## [4] RColorBrewer_1.1-2 httr_1.4.2 profvis_0.3.7 ## [7] tools_4.2.0 backports_1.4.1 bslib_0.4.0 ## [10] utf8_1.2.2 R6_2.5.1 rpart_4.1.16 ## [13] DBI_1.1.2 nnet_7.3-17 urlchecker_1.0.1 ## [16] withr_2.5.0 prettyunits_1.1.1 processx_3.8.0 ## [19] tidyselect_1.2.0 gridExtra_2.3 mnormt_2.1.1 ## [22] compiler_4.2.0 cli_3.4.1 rvest_1.0.2 ## [25] htmlTable_2.4.1 xml2_1.3.3 bookdown_0.29 ## [28] sass_0.4.2 scales_1.2.1 checkmate_2.0.0 ## [31] callr_3.7.2 systemfonts_1.0.4 stringr_1.4.0 ## [34] digest_0.6.30 svglite_2.1.0 base64enc_0.1-3 ## [37] jpeg_0.1-9 pkgconfig_2.0.3 sessioninfo_1.2.2 ## [40] fastmap_1.1.0 highr_0.9 htmlwidgets_1.5.4 ## [43] rlang_1.0.6 rstudioapi_0.13 shiny_1.7.3 ## [46] jquerylib_0.1.4 generics_0.1.2 jsonlite_1.8.3 ## [49] magrittr_2.0.2 interp_1.0-33 Matrix_1.5-1 ## [52] Rcpp_1.0.9 munsell_0.5.0 fansi_1.0.3 ## [55] abind_1.4-5 lifecycle_1.0.3 stringi_1.7.8 ## [58] yaml_2.3.5 pkgbuild_1.3.1 promises_1.2.0.1 ## [61] parallel_4.2.0 crayon_1.5.2 miniUI_0.1.1.1 ## [64] deldir_1.0-6 splines_4.2.0 hms_1.1.1 ## [67] ps_1.7.2 pillar_1.8.1 pkgload_1.3.0 ## [70] glue_1.6.2 evaluate_0.15 latticeExtra_0.6-30 ## [73] remotes_2.4.2 data.table_1.14.4 png_0.1-7 ## [76] vctrs_0.5.0 tzdb_0.3.0 httpuv_1.6.5 ## [79] cellranger_1.1.0 gtable_0.3.0 purrr_0.3.4 ## [82] assertthat_0.2.1 cachem_1.0.6 xfun_0.34 ## [85] mime_0.12 xtable_1.8-4 later_1.3.0 ## [88] viridisLite_0.4.1 tibble_3.1.8 memoise_2.0.1 ## [91] cluster_2.1.2 ellipsis_0.3.2 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["datenvorbereitung.html", "Chapter 8 Datenvorbereitung 8.1 Grundlegende erste Schritte 8.2 Datensätze zusammenführen 8.3 Daten extrahieren 8.4 Daten sortieren 8.5 Kodierung ändern 8.6 Summary-Variablen 8.7 Weitere wichtige Hinweise 8.8 Übung", " Chapter 8 Datenvorbereitung Vor jeder statistischen Auswertung ist es notwendig, die Daten entsprechend der angestrebten Analyse aufzubereiten. Beispielsweise kann das beinhalten, Daten zusammenzuführen, zu extrahieren oder zu sortieren, aber auch Variablen umzukodieren oder transformierte Variablen zu erstellen. In diesem Kapitel wollen wir uns verschiedene Schritte der Datenvorbereitung anschauen. Wir nutzen dafür größtenteils Funktionen aus zwei verschiedenen Paketen: dem Standardpaket base und dem Zusatzpaket dplyr. Zweiteres laden wir mit library(dplyr). Wir fangen mit grundsätzlichen Überprüfungen unserer der Daten an. Nachfolgend arbeiten wir uns in spezifischere Aufbereitungsbereiche vor, die wir in Abhängigkeit unserer geplanten Auswertung ggf. benötigen. Beispieldatensätze für dieses Kapitel Für das vorliegende Kapitel nutzen wir mehrere Datensätze, um die unterschiedlichen Verarbeitungsschritte zu demonstrieren. Den Datensatz airquality werden wir am meisten nutzen. Dieser enthält Daten aus einer Untersuchung der Luftqualität in New York, die von Mai bis September des Jahres 1973 stattfand. Der Datensatz ist standardmäßig in R enthalten und wir bekommen ihn mit der Funktion data() in unser R-Environment: data(airquality) head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 Der Datensatz enthält 6 Variablen: Ozone: mittlere Ozonkonzentration in ppb (parts per billion) Solar.R: Sonneneinstrahlung in Langley (Einheit) Wind: durchschnittliche Windgeschwindigkeit in Meilen pro Stunde Temp: maximale Temperatur in Grad Fahrenheit Month: Monatsangabe als Zahl (1-12) Day: Tagesangabe als Zahl (1-31) Mehr Informationen zum Datensatz finden wir hier. Den Datensatz PWE_data, welcher Daten einer psychometrischen Erhebung enthält, werden wir auch häufiger nutzen. Um den Datensatz und das Codebuch herunterzuladen, klicken wir auf diesen Link. Dann gehen wir in den Ordner PWE_data und lesen data.csv ein: library(readr) # zum Einlesen der csv-Datei PWE_data &lt;- read_table(&quot;Dateipfad/data.csv&quot;) ## Warning: 361 parsing failures. ## row col expected actual file ## 1 -- 102 columns 103 columns &#39;data/PWE.csv&#39; ## 6 -- 102 columns 103 columns &#39;data/PWE.csv&#39; ## 8 -- 102 columns 103 columns &#39;data/PWE.csv&#39; ## 20 -- 102 columns 103 columns &#39;data/PWE.csv&#39; ## 33 -- 102 columns 103 columns &#39;data/PWE.csv&#39; ## ... ... ........... ........... .............. ## See problems(...) for more details. names(PWE_data) ## [1] &quot;Q1A&quot; &quot;Q1I&quot; &quot;Q1E&quot; &quot;Q2A&quot; &quot;Q2I&quot; ## [6] &quot;Q2E&quot; &quot;Q3A&quot; &quot;Q3I&quot; &quot;Q3E&quot; &quot;Q4A&quot; ## [11] &quot;Q4I&quot; &quot;Q4E&quot; &quot;Q5A&quot; &quot;Q5I&quot; &quot;Q5E&quot; ## [16] &quot;Q6A&quot; &quot;Q6I&quot; &quot;Q6E&quot; &quot;Q7A&quot; &quot;Q7I&quot; ## [21] &quot;Q7E&quot; &quot;Q8A&quot; &quot;Q8I&quot; &quot;Q8E&quot; &quot;Q9A&quot; ## [26] &quot;Q9I&quot; &quot;Q9E&quot; &quot;Q10A&quot; &quot;Q10I&quot; &quot;Q10E&quot; ## [31] &quot;Q11A&quot; &quot;Q11I&quot; &quot;Q11E&quot; &quot;Q12A&quot; &quot;Q12I&quot; ## [36] &quot;Q12E&quot; &quot;Q13A&quot; &quot;Q13I&quot; &quot;Q13E&quot; &quot;Q14A&quot; ## [41] &quot;Q14I&quot; &quot;Q14E&quot; &quot;Q15A&quot; &quot;Q15I&quot; &quot;Q15E&quot; ## [46] &quot;Q16A&quot; &quot;Q16I&quot; &quot;Q16E&quot; &quot;Q17A&quot; &quot;Q17I&quot; ## [51] &quot;Q17E&quot; &quot;Q18A&quot; &quot;Q18I&quot; &quot;Q18E&quot; &quot;Q19A&quot; ## [56] &quot;Q19I&quot; &quot;Q19E&quot; &quot;country&quot; &quot;introelapse&quot; &quot;testelapse&quot; ## [61] &quot;surveyelapse&quot; &quot;TIPI1&quot; &quot;TIPI2&quot; &quot;TIPI3&quot; &quot;TIPI4&quot; ## [66] &quot;TIPI5&quot; &quot;TIPI6&quot; &quot;TIPI7&quot; &quot;TIPI8&quot; &quot;TIPI9&quot; ## [71] &quot;TIPI10&quot; &quot;VCL1&quot; &quot;VCL2&quot; &quot;VCL3&quot; &quot;VCL4&quot; ## [76] &quot;VCL5&quot; &quot;VCL6&quot; &quot;VCL7&quot; &quot;VCL8&quot; &quot;VCL9&quot; ## [81] &quot;VCL10&quot; &quot;VCL11&quot; &quot;VCL12&quot; &quot;VCL13&quot; &quot;VCL14&quot; ## [86] &quot;VCL15&quot; &quot;VCL16&quot; &quot;education&quot; &quot;urban&quot; &quot;gender&quot; ## [91] &quot;engnat&quot; &quot;age&quot; &quot;screenw&quot; &quot;screenh&quot; &quot;hand&quot; ## [96] &quot;religion&quot; &quot;orientation&quot; &quot;race&quot; &quot;voted&quot; &quot;married&quot; ## [101] &quot;familysize&quot; &quot;major&quot; Der Datensatz enthält 102 Variablen. Einige davon schauen wir uns im Laufe des Kapitels noch genauer an. Im Codebook, welches sich ebenfalls im Ordner PWE_data befindet, finden wir eine Erklärung zu den Variablen. Mehr Informationen zum Erhebungsinstrument, der Protestant Work Ethic Scale, finden wir in Mirels &amp; Garrett (1971) (nur über HU-VPN zugänglich). Achtung: Im Abschnitt Plausibilitäts-Check werden die Kodierungen einiger Variablen noch korrigiert. Außerdem müssen die Werte der Variablen Q9A, Q13A und Q15A noch invertiert werden, da diese negativ gepolt sind. Das passiert im Abschnitt Umkodieren. Die Datensätze vornamen_13 und vornamen_14 enthalten die Vornamen der Neugeborenen in München, jeweils für die Jahre 2013 und 2014. Diese werden wir nur für [Datensätze zusammenführen] nutzen. Zuerst laden wir die csv-Dateien für 2013 und 2014 runter und lesen sie dann folgendermaßen in R ein: vornamen_13 &lt;- read.csv(&quot;Dateipfad/vornamen-von-neugeborenen2013.csv&quot;) vornamen_14 &lt;- read.csv(&quot;Dateipfad/vornamen-von-neugeborenen2014.csv&quot;) head(vornamen_13) ## vorname anzahl geschlecht ## 1 Maximilian 166 m ## 2 Felix 124 m ## 3 Anna 109 w ## 4 David 109 m ## 5 Sophia 108 w ## 6 Emilia 103 w head(vornamen_14) ## vorname anzahl geschlecht ## 1 Maximilian 178 m ## 2 Felix 134 m ## 3 Anna 119 w ## 4 Emma 113 w ## 5 Lukas 109 m ## 6 Emilia 109 w Die Datensätze enthalten jeweils die gleichen 3 Variablen: vorname: Vorname (kann doppelt vorkommen, wenn Name für beide Geschlechter gegeben wurde) anzahl: Häufigkeit des Vornamens in diesem Jahr geschlecht: (binäres) Geschlecht der Kinder mit diesem Vornamen Achtung: Es kommt häufiger zu Problemen bei der Ausführung der Funktionen filter(), select() und summarise() aus dem Paket dpylr, wenn die Pakete stats (Basispaket; filter()), MASS (select()) oder plyr (summarise()) ebenfalls geladen sind. Auch bei anderen gleichnamigen Funktionen aus verschiedenen geladenen Paketen kann es durch die sogenannte Maskierung zu Problemen kommen. Weil wir auch Pakete mit gleich benannten Funktionen nutzen, greifen wir teils auf die eindeutige Auswahl von Funktionen mittels :: zurück. Für mehr Informationen zum Maskieren können wir im gleichnamigen Abschnitt im Kapitel Pakete nachschauen. Mehr Informationen zu dem speziellen Problem mit dpylr finden wir in diesem Forumseintrag. 8.1 Grundlegende erste Schritte Zuerst widmen wir unsere Aufmerksamkeit der Überprüfung wichtiger übergreifender Punkte der Datenvorbereitung. Diese sind: ob unser Datensatz als Dataframe vorliegt, ob unsere Daten plausibel und fehlende Werte korrekt kodiert sind, ob nominal- und v.a. ordinalskalierte Variablen faktorisiert sind, und ob wir unser bestehendes Tabellenformat ggf. ändern müssen. Die Schritte sind bereits in einer sinnvollen Abfolge angeordnet (z.B. ist es vorteilhaft, erst unplausible und fehlende Werte ausfindig zu machen und umzukodieren, bevor man Variablen faktorisiert). Optional können wir vorher unser Wissen zum Messen von Merkmalen und der korrekten Darstellung dieser in R auffrischen. Wir schauen uns nachfolgend nur die Datensätze airquality und PWE_data an. 8.1.1 Recap: Kodierung von Daten Generell wenn wir mit Daten arbeiten, ist es ratsam, sich zuallererst Gedanken darüber zu machen, welche Informationen wir diesen entnehmen können (Messniveau) und ob sie so gespeichert sind, dass R sie richtig erkennt (Datentypen und -strukturen). Die nachfolgende Wiederholung ist eine verkürzte Variante des Abschnitts Daten aus dem Kapitel zu Einführung in R. 8.1.1.1 Messniveaus Das Messniveau (oder auch Skalenniveau) ist eine wichtige Eigenschaft von Merkmalen (Variablen) von Untersuchungseinheiten. Es beschreibt, welche Informationen in unseren Messwerten abgebildet werden und damit auch welche mathematischen Transformationen mit den Messwerten sinnvoll sind (z.B. das Berechnen von Mittelwerten). Somit begrenzt das Messniveau auch die zulässigen Datenauswertungsverfahren unserer Variablen. Die Kodierung von nominalskalierten Merkmalen ist insofern willkürlich, als dass lediglich auf Gleichheit versus Ungleichheit geachtet werden muss (z.B. 1, 4, 9 oder A, Y, M). Mögliche Unterscheidungen: Gleichheit/Ungleichheit airquality: - PWE_data: u.a.school, urban, gender Die Kodierung von ordinalskalierten Merkmalen geschieht der Größe nach, d.h. dass die Rangfolge der Kodierungen einzelner Gruppen relevant ist (z.B. 1 &lt; 4 &lt; 9 oder A &lt; M &lt; Y). Man kann aber auch eine eigene Sortierung festlegen, die nicht der “natürlichen” Rangfolge entspricht (z.B. Y &lt; A &lt; M). Ein Realschulabschluss ist beispielsweise besser als ein Hauptschulabschluss. Wir können aber nicht festlegen, wie viel besser er ist. Mögliche Unterscheidungen: Gleichheit/Ungleichheit Rangordnung airquality: - PWE_data: education Bei der Kodierung von intervallskalierten Merkmalen sind sowohl die Rangfolge als auch die Abstände zwischen den Ausprägungen relevant (z.B. 1, 4, 7; jeweils mit gleichem Abstand zueinander; oder 1.4, 1.5, 2.3; jeweils mit verschiedenen Abständen zueinander). Ein Beispiel dafür ist die Temperatur in Grad Celsius oder Grad Fahrenheit. Mögliche Unterscheidungen: Gleichheit/Ungleichheit Rangordnung Abstände airquality: Temp (Temperatur in Grad Fahrenheit) PWE_data: u.a. Antworten (1-5) auf die Items des PWE (Q1A, …, Q19A), Antworten (1-7) auf die Items des TIPI (TIPI1, …, TIPI10) Bei der Kodierung von verhältnisskalierten Merkmalen ist zusätzlich noch ein Nullpunkt vorhanden. Dieser erlaubt es, dass Quotienten zwischen Werten gebildet werden können. Ein beliebtes Beispiel ist die Kelvin Skala. Bei dieser ist bei 0°K keine Bewegungsenergie mehr vorhanden und 20°K sind doppelt so viel wie 40°K. Mögliche Unterscheidungen: Gleichheit/Ungleichheit Rangordnung Abstände Verhältnisse airquality: Ozone, Solar.R, Wind PWE_data: age Zu guter Letzt gibt es noch absolutskalierte** Merkmale, welche sowohl einen eindeutigen Nullpunkt als auch eine eindeutige Einheit der Skala (z.B. Anzahl der Kinder) vorweisen kann. Die Kodierung entsprcht der natürlichen Einheit. airquality: - PWE_data: familysize Nachfolgend finden wir eine Tabelle der möglichen Unterscheidungen der jeweiligen Messiniveaus. (Un-) Gleichheit Rangordnung Abstände Verhältnisse natürliche Einheit Nominal X Ordinal X X Intervall X X X Verhältnis X X X X Absolut X X X X X 8.1.1.2 Datentypen und Datenstrukturen Achtung: Die Unterteilung nach “Datentyp” und “Datenstruktur” sind getreu des Manuals von R. Man stößt in anderen Quellen aber auch auf abweichende Unterteilungen bzw. Benennungen. Der Datentyp gibt an, um was für Daten es sich handelt, d.h. welche Werte(bereiche) diese haben und welche Operationen wir auf sie anwenden können. Wir nutzen zumeist Zeichen, Wahrheitswerte und Zahlen. Diese werden in R als character, logical, integer und double gespeichert, wobei letztere beiden häufig als numeric zusammengefasst werden. Die Datenstruktur bestimmt die Organisation und Speicherung von Daten(typen). In R gibt es z.B. Vektoren, Matrizen, Dataframes, Listen und Faktoren. Beispielsweise können Vektoren und Matrizen jeweils nur einen Datentypen enthalten, während Dataframes mehrere Datentypen enthalten können. Datentyp und -struktur sind ausschlaggebend dafür, welche Funktionen wir anwenden können bzw. welchen Output wir bekommen. Beispiel 1: Man kann nur mit numeric deskriptiv-statistische Kennwerte bilden. Beispiel 2: Wenn mir nominal- bzw. ordinalskalierte Daten nicht adäquat kodieren, kann es zu ungewollten Konsequenzen kommen. Wenn wir z.B. eine Variable \\(X\\) haben, welche eine Gruppenzugehörigkeit mit 1, 2 und 3 kodiert, und diese als Prädiktor in ein Regressionsmodell aufnehmen, dann wird \\(X\\) als kontinuierliche Variable behandelt und wir würden keine separaten Schätzungen für die Mittelwertsdifferenzen der Gruppen bekommen. Nachfolgend finden wir eine Übersicht zu den Möglichkeiten der Kodierung von Merkmalen mit verschiedenen Skalenniveaus. _ Art der Skala: Nominal- Ordinal- Intervall- Verhältnis- Absolut- Datentyp: character X X\\(^2\\) Datentyp: numeric X\\(^1\\) X\\(^2\\) X X X \\(^1\\) Faktorisieren (unordered factor) notwendig wenn keine Indikatorvariable(n) genutzt \\(^2\\) Faktorisieren (ordered factor) notwendig Mit der Funktion str() können wir uns eine kompakte Übersicht der enthaltenen Variablen (d.h. ihr Datentyp bzw. die Datenstruktur Faktor sowie jeweils die ersten 10 Werte) der Datenstruktur (hier: Dataframe) ausgeben lassen. So können wir schauen, ob die Daten auch entsprechend ihres Messniveaus kodiert sind. str(airquality) ## &#39;data.frame&#39;: 153 obs. of 6 variables: ## $ Ozone : int 41 36 12 18 NA 28 23 19 8 NA ... ## $ Solar.R: int 190 118 149 313 NA NA 299 99 19 194 ... ## $ Wind : num 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ... ## $ Temp : int 67 72 74 62 56 66 65 59 61 69 ... ## $ Month : int 5 5 5 5 5 5 5 5 5 5 ... ## $ Day : int 1 2 3 4 5 6 7 8 9 10 ... Für den Beispieldatensatz airquality liegen die verhältnisskalierten Variablen Ozone,Solar.R, Wind, Month und Day korrekterweise als integer bzw. numeric vor. Die intervallskalierte Variable Temp liegt auch wie erwartet als integer vor (weil die Temperatur in Fahrenheit und nicht in Kelvin gemessen wurde ist die Variable nicht verhältnisskaliert). Für den Datensatz PWE_data schauen wir uns exemplarisch nur die letzten 15 Variablen an: ls.str(PWE_data[,88:102]) ## age : num [1:1350] 24 66 17 23 19 40 50 21 16 27 ... ## education : num [1:1350] 3 2 2 2 2 4 4 2 1 2 ... ## engnat : num [1:1350] 1 1 2 1 1 1 1 1 1 1 ... ## familysize : num [1:1350] 2 5 4 3 2 3 3 1 3 2 ... ## gender : num [1:1350] 1 2 2 2 2 2 1 3 2 1 ... ## hand : num [1:1350] 1 2 1 2 1 1 1 3 3 1 ... ## major : chr [1:1350] &quot;Computer&quot; NA NA &quot;dietetics&quot; NA &quot;Philosophy&quot; &quot;engineering&quot; ... ## married : num [1:1350] 1 3 1 1 1 3 2 1 1 2 ... ## orientation : num [1:1350] 1 1 1 1 1 2 1 5 2 1 ... ## race : num [1:1350] 11 16 16 16 16 16 16 17 16 16 ... ## religion : num [1:1350] 6 6 2 2 1 7 6 4 1 2 ... ## screenh : num [1:1350] 1080 640 1024 1080 615 ... ## screenw : num [1:1350] 1920 360 1280 1920 1093 ... ## urban : num [1:1350] 2 3 2 2 1 3 1 2 3 2 ... ## voted : num [1:1350] 2 2 2 2 2 2 1 2 2 1 ... Wir nutzen hier für PWE_data die Funktion ls.str(), weil der Output für ein Tibble Dataframe so weniger ausführlich ist. Allerdings werden die Variablen mit dieser Funktion alphabetisch sortiert (im Gegensatz zu str(), welche die Variablen der Spaltennummerierung nach darstellt). Wie wir sehen, liegen fast alle Variablen als numeric vor, obwohl viele nominalskaliert sind. So würden sie von R nicht entsprechend ihres Messniveaus behandelt werden. Wir müssen diese also entweder in character umwandeln oder faktorisieren (später mehr dazu). 8.1.2 Dataframe Für viele Anwendungen in R (z.B. für das Erstellen von Grafiken mit gglot2) ist es notwendig, dass der Datensatz als Dataframe vorliegt. Folgendermaßen können wir prüfen, ob ein Datensatz ein Dataframe ist: is.data.frame(airquality) ## [1] TRUE is.data.frame(PWE_data) ## [1] TRUE Was genau ist ein Dataframe? Ein Dataframe ist eine Datenstruktur, die es uns erlaubt, Daten tabellarisch zu speichern. Der Dataframe ist eine Liste aus Vektoren mit gleicher Länge. Listen können (im Gegensatz zu Matrizen) Variablen mit unterschiedlichen Datentypen speichern. Mehr Informationen gibt es im vorhergehenden Abschnitt zu Datentypen und Datenstrukturen. Falls unser Datensatz kein Dataframe ist, können wir ihn folgendermaßen umwandeln: airquality &lt;- as.data.frame(airquality) Die Funktion as.data.frame() enthält das Argument stringsAsFactors, mit dem wir bestimmen können, ob Zeichenketten (character) zu Faktoren umgewandelt werden sollen (TRUE). Falls wir ordinalskalierte Daten haben, müssen wir den Faktor dann aber noch zusätzlich ordnen. Generell ist es sinnvoll, erst nach der Überprüfung auf unplausible und fehlende Werte zu faktorisieren, damit eventuell vorhandene Fehlkodierungen nicht als Faktorstufen behandelt werden. 8.1.3 Plausibilitäts-Check Im nächsten Schritt lohnt es sich zu überprüfen, ob in den vorangegangen Schritten der Erhebung und Kodierung unserer Daten, Fehler passiert sind. Das ist wichtig damit wir solche Fehler nicht in unsere Analyse übertragen, wo sie sehr viel unwahrscheinlicher auffallen werden. Dafür überprüfen wir, ob die Messniveaus und Ausprägungen unserer interessierenden Variablen auch unseren Erwartungen entsprechen. Entweder man hat die Daten selbst erhoben und somit Wissen darüber, welche Werte möglich sind, oder man schaut sich die Dokumentation zu den Daten an. Für den Datensatz airquality finden wir die Dokumentation hier. Konkretisieren wir einmal einen hypothetischen Fall an der Variable Day. Wir wissen, dass diese mit Zahlen von 1-31 kodiert sein kann. Es wäre also unplausibel, wenn andere Werte (z.B. 0 oder 32) vorliegen würden. Das Auftauchen von unplausiblen Werten ist z.B. wahrscheinlicher, wenn Daten manuell digitalisiert (d.h. eingetippt) wurden (z.B. Paper-and-Pencil Tests). Gerade in diesen Fällen sollte man sicher gehen, und die Daten auf unplausible Werte hin überprüfen. Die Funktion unique() gibt uns einen Überblick über alle enthaltenen Ausprägungen eines Vektors. Wenn wir diese mit sapply() kombinieren, können wir das für jede Variable im Datensatz anwenden. Wenn wir den Output wiederum erneut an sapply() übergeben, und sort() anwenden, werden die Ausprägungen aufsteigend sortiert (weil der Default decreasing = FALSE ist), was die Überprüfung erleichtert. In sort() können wir außerdem na.last=TRUE nutzen, um uns das Vorhandensein von NAs am Ende der Ausprägungen anzeigen zu lassen. sapply(sapply(airquality, unique), sort, na.last=TRUE) ## $Ozone ## [1] 1 4 6 7 8 9 10 11 12 13 14 16 18 19 20 21 22 23 24 ## [20] 27 28 29 30 31 32 34 35 36 37 39 40 41 44 45 46 47 48 49 ## [39] 50 52 59 61 63 64 65 66 71 73 76 77 78 79 80 82 84 85 89 ## [58] 91 96 97 108 110 115 118 122 135 168 NA ## ## $Solar.R ## [1] 7 8 13 14 19 20 24 25 27 31 36 37 44 47 48 49 51 59 ## [19] 64 65 66 71 77 78 81 82 83 91 92 95 98 99 101 112 115 118 ## [37] 120 127 131 135 137 138 139 145 148 149 150 153 157 167 175 183 186 187 ## [55] 188 189 190 191 192 193 194 197 201 203 207 212 213 215 220 222 223 224 ## [73] 225 229 230 236 237 238 242 244 248 250 252 253 254 255 256 258 259 260 ## [91] 264 266 267 269 272 273 274 275 276 279 284 285 286 287 290 291 294 295 ## [109] 299 307 313 314 320 322 323 332 334 NA ## ## $Wind ## [1] 1.7 2.3 2.8 3.4 4.0 4.1 4.6 5.1 5.7 6.3 6.9 7.4 8.0 8.6 9.2 ## [16] 9.7 10.3 10.9 11.5 12.0 12.6 13.2 13.8 14.3 14.9 15.5 16.1 16.6 18.4 20.1 ## [31] 20.7 ## ## $Temp ## [1] 56 57 58 59 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 ## [26] 82 83 84 85 86 87 88 89 90 91 92 93 94 96 97 ## ## $Month ## [1] 5 6 7 8 9 ## ## $Day ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 Einen kompakten Überblick über die Verteilung der Variablen können wir mit der Funktion summary() bekommen. Hierbei interessieren uns vor allem die Extremwerte (Min. und Max.), d.h. der Range der Variablen, und die Missings (NA). summary(airquality) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## Insgesamt sehen die Daten plausibel aus. Für eine spezifischere Einschätzung der Wetter-Variablen (Ozone, Solar.R, Wind und Temp) könnte man sich zusätzlich Vergleichsdaten von anderen Erhebungen in einem ähnlichen Zeitraum und Gebiet anschauen. Für den Datensatz PWE_data schauen wir uns hier wieder nur einige Variablen an. Informationen zu den Variablen finden wir in der Codebook, welches sich im Ordner PWE_data befindet. sapply(sapply(PWE_data[,88:101], unique), sort) ## $education ## [1] 0 1 2 3 4 ## ## $urban ## [1] 0 1 2 3 ## ## $gender ## [1] 0 1 2 3 ## ## $engnat ## [1] 0 1 2 ## ## $age ## [1] 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ## [26] 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 ## [51] 63 64 65 66 67 69 70 71 72 74 75 76 77 81 90 ## ## $screenw ## [1] 0 320 347 360 375 393 396 412 414 424 455 486 505 570 586 ## [16] 600 601 640 720 768 780 800 810 834 864 948 960 962 1024 1080 ## [31] 1093 1111 1138 1152 1200 1242 1280 1300 1348 1360 1364 1366 1368 1376 1400 ## [46] 1422 1423 1440 1477 1493 1500 1504 1536 1541 1600 1676 1680 1707 1821 1824 ## [61] 1920 2048 2400 2560 2561 3072 3840 ## ## $screenh ## [1] 0 320 347 360 414 480 486 505 568 569 570 576 592 597 601 ## [16] 614 615 618 640 648 658 667 672 692 698 702 720 731 732 736 ## [31] 740 741 744 747 748 752 753 758 759 760 762 768 774 780 786 ## [46] 800 803 808 809 812 819 820 831 846 848 853 854 863 864 866 ## [61] 869 879 888 892 896 900 902 912 933 943 945 960 962 1000 1003 ## [76] 1022 1024 1050 1080 1112 1152 1194 1200 1280 1350 1366 1440 1536 1728 1920 ## [91] 2160 ## ## $hand ## [1] 0 1 2 3 ## ## $religion ## [1] 0 1 2 3 4 5 6 7 8 9 10 11 12 ## ## $orientation ## [1] 0 1 2 3 4 5 ## ## $race ## [1] 10 11 12 13 14 15 16 17 ## ## $voted ## [1] 0 1 2 ## ## $married ## [1] 0 1 2 3 ## ## $familysize ## [1] 0 1 2 3 4 5 6 7 8 9 10 12 13 14 33 Es fällt auf, dass alle dargestellten Variablen (bis auf age), die Ausprägung 0 enthalten, obwohl diese im Codebook für diese Variablen nicht definiert ist. Auch für die anderen Variablen im Datensatz, mit Ausnahme von VCL1, …, VCL16, gilt, dass 0 nicht als mögliche Ausprägung gegeben wird, obwohl sie vorhanden ist. Wir können daher annehmen, dass 0 wahrscheinlich eine alternative Kodierung für NA ist. Besser wäre es natürlich, wenn wir die Wissenschaftler*innen, welche die Daten erhoben haben, kontaktieren und nachfragen würden. Wie wir sehen, geht die Überprüfung von plausiblen und fehlenden Werten häufig ineinander über. Werte, die außerhalb des Ranges der betrachteten Variable liegen, können eine Kodierung für fehlende Werte darstellen. Wenn wir unplausible Werte in unseren Daten finden, können wir zu [5. Kodierung ändern] springen, und diese umkodieren. Wenn wir fehlkodierte Missings finden, können wir auch case_when() oder alternative Möglichkeiten aus dem Abschnitt Sind die Missings einheitlich kodiert? des Kapitels Fehlende Werte nutzen (z.B. die im Folgenden illustrierte Umkodierung des gesamten Datensatzes). Weil wir in PWE_data sehr viele Variablen haben und es zu umständlich wäre, alle einzeln umzukodieren, kodieren wir erst im gesamten Datensatz 0 zu NA um, und ändern danach wieder die Kodierung für die Variablen VCL1 bisVCL16. # Umkodierung für gesamten Datensatz PWE_data[PWE_data == 0] &lt;- NA # &quot;Rückkodierung&quot; für Variablen, die regulär 0 enthalten # library(dplyr) PWE_data$VCL1 &lt;- case_when(is.na(PWE_data$VCL1) ~ 0, # Umkodierung von NA zu 0 PWE_data$VCL1 == 1 ~ 1) # bleibt gleich PWE_data$VCL2 &lt;- case_when(is.na(PWE_data$VCL2) ~ 0, PWE_data$VCL2 == 1 ~ 1) PWE_data$VCL3 &lt;- case_when(is.na(PWE_data$VCL3) ~ 0, PWE_data$VCL3 == 1 ~ 1) PWE_data$VCL4 &lt;- case_when(is.na(PWE_data$VCL4) ~ 0, PWE_data$VCL4 == 1 ~ 1) PWE_data$VCL5 &lt;- case_when(is.na(PWE_data$VCL5) ~ 0, PWE_data$VCL5 == 1 ~ 1) PWE_data$VCL6 &lt;- case_when(is.na(PWE_data$VCL6) ~ 0, PWE_data$VCL6 == 1 ~ 1) PWE_data$VCL7 &lt;- case_when(is.na(PWE_data$VCL7) ~ 0, PWE_data$VCL7 == 1 ~ 1) PWE_data$VCL8 &lt;- case_when(is.na(PWE_data$VCL8) ~ 0, PWE_data$VCL8 == 1 ~ 1) PWE_data$VCL9 &lt;- case_when(is.na(PWE_data$VCL9) ~ 0, PWE_data$VCL9 == 1 ~ 1) PWE_data$VCL10 &lt;- case_when(is.na(PWE_data$VCL10) ~ 0, PWE_data$VCL10 == 1 ~ 1) PWE_data$VCL11 &lt;- case_when(is.na(PWE_data$VCL11) ~ 0, PWE_data$VCL11 == 1 ~ 1) PWE_data$VCL12 &lt;- case_when(is.na(PWE_data$VCL12) ~ 0, PWE_data$VCL12 == 1 ~ 1) PWE_data$VCL13 &lt;- case_when(is.na(PWE_data$VCL13) ~ 0, PWE_data$VCL13 == 1 ~ 1) PWE_data$VCL14 &lt;- case_when(is.na(PWE_data$VCL14) ~ 0, PWE_data$VCL14 == 1 ~ 1) PWE_data$VCL15 &lt;- case_when(is.na(PWE_data$VCL15) ~ 0, PWE_data$VCL15 == 1 ~ 1) PWE_data$VCL16 &lt;- case_when(is.na(PWE_data$VCL16) ~ 0, PWE_data$VCL16 == 1 ~ 1) Die Funktion case_when() wird im Abschnitt [5. Kodierung ändern] ausführlich erklärt. Nun schauen wir uns noch die Verteilungen der Variablen an. summary(PWE_data[,88:101]) ## education urban gender engnat ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:1.000 ## Median :3.000 Median :2.000 Median :1.000 Median :1.000 ## Mean :2.653 Mean :2.135 Mean :1.528 Mean :1.283 ## 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:2.000 3rd Qu.:2.000 ## Max. :4.000 Max. :3.000 Max. :3.000 Max. :2.000 ## NA&#39;s :17 NA&#39;s :19 NA&#39;s :7 NA&#39;s :2 ## age screenw screenh hand ## Min. :13.00 Min. : 320 Min. : 320.0 Min. :1.000 ## 1st Qu.:20.00 1st Qu.: 414 1st Qu.: 736.0 1st Qu.:1.000 ## Median :26.00 Median :1366 Median : 800.0 Median :1.000 ## Mean :29.74 Mean :1169 Mean : 850.3 Mean :1.165 ## 3rd Qu.:36.00 3rd Qu.:1536 3rd Qu.: 948.8 3rd Qu.:1.000 ## Max. :90.00 Max. :3840 Max. :2160.0 Max. :3.000 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :5 ## religion orientation race voted ## Min. : 1.000 Min. :1.000 Min. :10.00 Min. :1.00 ## 1st Qu.: 2.000 1st Qu.:1.000 1st Qu.:16.00 1st Qu.:1.00 ## Median : 4.000 Median :1.000 Median :16.00 Median :2.00 ## Mean : 4.452 Mean :1.606 Mean :15.43 Mean :1.51 ## 3rd Qu.: 6.000 3rd Qu.:2.000 3rd Qu.:16.00 3rd Qu.:2.00 ## Max. :12.000 Max. :5.000 Max. :17.00 Max. :2.00 ## NA&#39;s :15 NA&#39;s :25 NA&#39;s :11 ## married familysize ## Min. :1.000 Min. : 1.000 ## 1st Qu.:1.000 1st Qu.: 2.000 ## Median :1.000 Median : 2.000 ## Mean :1.358 Mean : 2.616 ## 3rd Qu.:2.000 3rd Qu.: 3.000 ## Max. :3.000 Max. :33.000 ## NA&#39;s :6 NA&#39;s :28 Hier sehen wir auch, dass für nominalskalierte Merkmale, wie z.B. urban, orientation und married, deskriptiv-statistische Kennwerte wie der Mittelwert gebildet werden (d.h. diese werden als mindestens intervallskaliert behandelt), weil sie als numeric vorliegen. Später werden wir diese noch faktorisieren. 8.1.4 Fehlende Werte Generell werden fehlende Werte (Missings) in R mit NA dargestellt. In anderen Programmen mag das anders sein (z.B. werden Missings in Unipark mit 99 oder -99 kodiert). Wie im vorhergehenden Abschnitt demonstriert, überschneidet sich die Überprüfung von plausiblen und fehlenden Werten häufig. Neben der im letzten Abschnitt vorgestellten Varianten, Missings mit summary() zu finden, gibt es noch weitere Optionen. Beispielsweise können wir mit der Kombination von colSums() und is.na() spaltenweise Missings zählen. colSums(is.na(airquality)) ## Ozone Solar.R Wind Temp Month Day ## 37 7 0 0 0 0 colSums(is.na(PWE_data)) ## Q1A Q1I Q1E Q2A Q2I Q2E ## 1 1 1 1 1 1 ## Q3A Q3I Q3E Q4A Q4I Q4E ## 1 1 1 1 1 1 ## Q5A Q5I Q5E Q6A Q6I Q6E ## 1 1 1 1 1 1 ## Q7A Q7I Q7E Q8A Q8I Q8E ## 1 1 1 1 1 1 ## Q9A Q9I Q9E Q10A Q10I Q10E ## 1 1 1 1 1 1 ## Q11A Q11I Q11E Q12A Q12I Q12E ## 1 1 1 1 1 1 ## Q13A Q13I Q13E Q14A Q14I Q14E ## 1 1 1 1 1 1 ## Q15A Q15I Q15E Q16A Q16I Q16E ## 1 1 1 1 1 1 ## Q17A Q17I Q17E Q18A Q18I Q18E ## 1 1 1 1 1 1 ## Q19A Q19I Q19E country introelapse testelapse ## 1 1 1 0 0 0 ## surveyelapse TIPI1 TIPI2 TIPI3 TIPI4 TIPI5 ## 0 8 10 13 9 9 ## TIPI6 TIPI7 TIPI8 TIPI9 TIPI10 VCL1 ## 8 9 8 8 13 0 ## VCL2 VCL3 VCL4 VCL5 VCL6 VCL7 ## 0 0 0 0 0 0 ## VCL8 VCL9 VCL10 VCL11 VCL12 VCL13 ## 0 0 0 0 0 0 ## VCL14 VCL15 VCL16 education urban gender ## 0 0 0 17 19 7 ## engnat age screenw screenh hand religion ## 2 0 2 2 5 15 ## orientation race voted married familysize major ## 25 0 11 6 28 448 Achtung: Wenn wir Variablen mit Missings für unsere Analysen nutzen wollen, sollten wir überprüfen, ob die Missings zufällig sind und in Abhängigkeit davon unseren Umgang anpassen, um systematischen Verzerrungen der Analysen entgegenzuwirken. Einen ausführlichen Überblick zu Missings finden wir im Kapitel Fehlende Werte. 8.1.5 Faktorisieren Wir schauen uns das Faktorisieren exemplarisch an zwei Variablen aus dem Datensatz PWE_data an: nominalskaliert: gender “What is your gender?”: 1 = Male, 2 = Female, 3 = Other ordinalskaliert: education “How much education have you completed?”: 1 = Less than high school, 2 = High school, 3 = University degree, 4 = Graduate degree. Zuerst erstellen wir einen (neuen) unsortierten (d.h. nominalskalierten) Faktor. Dafür benötigen wir nur die Funktion factor(), der wir den zu faktorisierenden Vektor übergeben. # faktorisieren (unsortiert) PWE_data$gender_uf &lt;- factor(PWE_data$gender) Nun erstellen wir einen (neuen) sortierten (d.h. ordinalskalierten) Faktor. Dafür ergänzen wir das Argument ordered=TRUE. # faktorisieren (sortiert; natürliche Sortierung) PWE_data$education_of &lt;- factor(PWE_data$education, ordered=TRUE) Mit dem Argument ordered=TRUE wird eine Variable nach ihrer “natürlichen” Rangfolge sortiert. Bei Zahlen (integer und numeric) bedeutet das, dass größere Zahlen eine höhere Hierarchieebene haben z.B. 1 &lt; 2. Bei einzelnen Buchstaben und Zeichenketten (character) bedeutet das, dass später im Alphabet auftauchende (Anfangs-)Buchstaben eine höhere Hierarchiebene haben z.B. “Hans” &lt; “Rene”. Manchmal wollen wir diese Sortierung aber nicht übernehmen, sondern eine eigene Hierarchie erstellen, die nicht der natürlichen Rangfolge entspricht. Das können wir machen, indem wir zusätzlich das Argument levels spezifizieren, dem wir einen Vektor mit unserer gewünschten Sortierung übergeben. # faktorisieren (sortiert; eigene &quot;non-sense&quot; Sortierung) PWE_data$education_of_s &lt;- factor(PWE_data$education, ordered=TRUE, levels=c(1,4,2,3)) Abschließend vergleichen wir die ursprüngliche numeric-Variable (education) mit den unsortierten (education_uf) und sortierten (education_of und education_of_s) Faktor-Variablen. ls.str(PWE_data[,c(88, 90, 103:105)]) ## education : num [1:1350] 3 2 2 2 2 4 4 2 1 2 ... ## education_of : Ord.factor w/ 4 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;: 3 2 2 2 2 4 4 2 1 2 ... ## education_of_s : Ord.factor w/ 4 levels &quot;1&quot;&lt;&quot;4&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;: 4 3 3 3 3 2 2 3 1 3 ... ## gender : num [1:1350] 1 2 2 2 2 2 1 3 2 1 ... ## gender_uf : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 2 2 2 2 2 1 3 2 1 ... Wir sehen, dass alle Variablen des gleichen Merkmals zwar die gleichen Werte (gender: 1, 2, 3 und education: 1, 2, 3, 4) haben, aber in unterschiedlichen Datentypen bzw. -strukturen (numeric, factor, Ordered factor) und teils unterschiedlichen Sortierungen vorliegen. Außerdem sehen wir, dass die selbst sortierten Faktoren intern eine neue Kodierung bekommen haben (siehe education_of_s). Wir sehen diese nur mit str() bzw. ls.str(). Diese interne Kodierung richtet sich danach, wie die Faktorstufen sortiert sind. Die erste Ausprägung (nach der eigenen Sortierung) beginnt mit 1. 8.1.6 Wide- und Long-Format In Abhängigkeit unserer Daten und der Analyse, die wir durchführen wollen, ist es ggf. erforderlich, dass unsere Daten in ein anderes Tabellenformat überführt werden müssen. Es gibt das Wide- und das Long-Format. Die Unterscheidung von Wide- und Long-Format ist von Bedeutung, wenn unsere Daten eine genestete Struktur aufweisen, das heisst jeweils mehrere Messungen von derselben Untersuchungseinheit vorliegen (z.B. bei Längsschnitterhebungen, mehrere Ratern oder Schülern in Klassen). Im Wide-Format liegen Messungen einer Untersuchungseinheit in einer Zeile vor. Jeder Messzeitpunkt bzw. jede Messung ist eine eigene Variable. Beispiel 1 Wide-Format: Messzeitpunkte Untersuchungseinheit t1 t2 t3 1 4 3 1 2 5 2 3 Untersuchungseinheit 1 Untersuchungseinheit 2 Messwiederholung Rater Beispiel 2 Wide-Format: Rater Untersuchungseinheit self friend parent 1 2 1 3 2 3 4 2 Im Long-Format liegen Messungen einer Untersuchungseinheit in mehreren Zeilen vor. Alle Messzeitpunkte bzw. Messungen von unterschiedlichen Ratern liegen in einer Variable vor und die Messzeitpunkte bzw. Rater werden in einer separaten Variable kodiert. Beispiel 1 Long-Format: Messzeitpunkte Untersuchungseinheit Zeitpunkt Messung 1 1 4 1 2 3 1 3 1 2 1 5 2 2 2 2 3 3 Beispiel 2 Long-Format: Rater Untersuchungseinheit Rater Messung 1 self 2 1 friend 1 1 parent 3 2 self 3 2 friend 4 2 parent 2 Im Beispieldatensatz PWE_data gibt es keine wiederholte Messungen. Psychometrische und demographische Daten wurden einmalig erhoben. Hierfür gibt es keine Notwendigkeit der Formatierung vom Long- ins Wide-Format oder vice-versa. Im Beispieldatensatz airquality gibt es wiederholte Messungen der Untersuchungseinheiten (Ozone, Solar.R, Wind und Temp) zu unterschiedlichen Zeiten, die in Month und Day kodiert werden. Jede dieser Untersuchungseinheiten liegt in mehreren Zeilen vor. Es handelt sich folglich um einen Datensatz im Long-Format. Im Wide-Format hätten wir z.B. die Variablen Ozone_5_1 (Monat 5, Tag 1), Ozone_5_2 (Monat 5, Tag 2), …, Solar.R_5_1 (Monat 5, Tag 1), etc. Je nachdem, wie wir die Daten auswerten wollen, ist es notwendig bzw. nicht notwendig, die Daten umzuformatieren. Im Kapitel zum Wide- &amp; Long-Format erfahren wir, wie wir beide Formate ineinander überführen können. Hierzu werden jeweils zwei Möglichkeiten vorgestellt: reshape() aus dem Standardpaket stats und spread() bzw. gather() aus dem Paket tidyr. 8.2 Datensätze zusammenführen Synonyme: Mergen, Fusionieren, Integrieren Nicht immer haben wir das Glück, dass die für uns relevanten Daten in einem gemeinsamen Dataframe vorliegen. Daher schauen wir uns nachfolgend an, wie man Dataframes zusammenführen kann. Es gibt dabei zwei Szenarien, die man unterscheiden kann: die selben Variablen von verschiedenen Fällen z.B. von einer erneuten Aufnahme von Personen in eine Studie hier werden die Zeilen “ergänzt” verschiedene Variablen von den selben Fällen z.B. wenn einzelne Abschnitte einer Studie (Tests, Fragebögen) in unterschiedlichen Datensätzen gespeichert wurden hier werden die Spalten “ergänzt” Wir schauen uns wieder Funktionen aus zwei verschiedenen Paketen an: merge() aus dem Basispaket base und bind_rows() bzw. die _join()-Funktionen aus dem Zusatzpaket dplyr. Wir nutzen dafür die Datensätze vornamen_13 und vornamen_14, in denen die Vornamen der Neugeborenen in München, jeweils für die Jahre 2013 und 2014 enthalten sind. Diese Datensätze eignen sich für beide Szenarien, weil sowohl dieselben Variablen (vorname, anzahl und geschlecht) als auch dieselben Fälle (d.h. Vornamen) in beiden Datensätzen vorkommen (z.B. Maximilian). Die Untersuchungseinheiten sind hier also nicht einzelne Personen, sondern Vornamen. Was bei der Untersuchung einzelner Personen die ID-Variable ist, ist hier die Variable vorname. Eingangs wurde gezeigt, wie wir diese Datensätze herunterladen. Achtung: Es kann sein, dass wir einen Dataframe nach dem Zusammenführen noch in ein anderes Format überführen müssen, um unsere Auswertung durchführen zu können (siehe Wide- und Long-Format). 8.2.1 Selbe Variablen, unterschiedliche Fälle Die beiden nachfolgend vorgestellten Funktionen unterscheiden sich bezüglich einiger Funktionalitäten. Ein wichtiger Unterschied ist, dass merge() beim Zusammenführen der Dataframes gleiche Fälle (d.h. Fälle mit gleichen Ausprägungen in den Variablen) nur einmalig übernimmt (d.h. Dopplungen löscht) während bind_rows() alle Fälle übernimmt, auch wenn sich diese doppeln. In Abhängigkeit der geplanten Nutzung der Daten sollten wir individuell entscheiden, welche Funktion wir nutzen wollen. merge() Mit merge() können wir zwei Dataframes, deren Namen wir der Funktion übergeben, vertikal zusammenführen. Wir müssen dabei unbedingt all = TRUE spezifizieren, weil der Default (all = FALSE) nur Zeilen behält, die in beiden Dataframes mit genau der gleichen Ausprägung auf den Variablen vorhanden sind (und von diesen jeweils nur eine Version). Mit all.x = TRUE bzw. all.y = TRUE würden wir, neben den Fällen mit den gleichen Ausprägungen in beiden Datensätzen, auch alle nur im ersten bzw. zweiten Datensatz enthaltenen Fälle behalten. vornamen_merge_row &lt;- merge(vornamen_13, vornamen_14, all = TRUE) Die Reihenfolge der Zeilen im gemeinsamen Dataframe richtet sich nach der natürlichen Sortierung der ersten Variable (im zuerst übergebenen Datensatz). Für unser Beispiel sind die Vornamen nach dem Alphabet (beginnend mit “A”) sortiert. Mit der Funktion dim() können wir überprüfen, wie viele Zeilen und Spalten unser Dataframe beinhaltet. dim(vornamen_merge_row) ## [1] 7496 3 Hier sehen wir den eingangs erwähnten Unterschied von merge() und bind_rows(). vornamen_13 beinhaltet 4012 und vornamen_14 4032 Zeilen. Insgesamt würden wir also 8044 erwarten. Die Funktion übernimmt aber bei komplett gleichen Fällen (d.h. gleichen Ausprägungen auf allen Variablen) in den beiden Dataframes nur eine Version (so dass es keine Dopplung gibt). Beispielsweise kommt folgender Fall in beiden Dataframes vor: ## vorname anzahl geschlecht ## 1310 Aadhya 1 w vornamen_13 ## vorname anzahl geschlecht ## 2704 Aadhya 1 w vornamen_14 So verschwindet die gleiche Anzahl an Fällen, die wir mit dem Default-Verhalten der Funktion (all = FALSE) behalten hätten. Achtung: Wir sollten mit merge() demnach auch nur Dataframes zusammenführen, die genau dasselbe Set an Variablen haben (d.h. ein Dataframe sollte nicht noch eine zusätzliche Variable besitzen) oder wir sollten eine ID-Variable haben, deren IDs nur einmal vorkommen (sowohl innerhalb eines Dataframes als auch zwischen den Dataframes). Sonst kann es zum ungewollten Nicht-Übernehmen von Fällen kommen. Schauen wir uns das Problem an einem Beispiel an: Nehmen wir an, dass ein Dataframe x eine zusätzliche Variable a hat. Für die Fälle des anderen Dataframe y würden auf a im zusammengeführten Objekt nur fehlende Werte (NA) stehen. So würde die Funktion solche Fälle (im Vergleich der beiden Dataframes), die bis auf die Variable a die gleichen Ausprägungen haben, nicht übernehmen. x und y haben jeweils drei Fälle. x hat drei Variablen; y hat zwei. x ## c b a ## 1 5 5 5 ## 2 6 6 6 ## 3 7 7 7 y ## c b ## 1 5 7 ## 2 7 5 ## 3 6 6 merge_xy &lt;- merge(x, y, all = TRUE) merge_xy ## c b a ## 1 5 5 5 ## 2 5 7 NA ## 3 6 6 6 ## 4 7 5 NA ## 5 7 7 7 Man könnte denken, dass 6 Fälle in merge_xy zu finden sind. Weil aber jeweils ein Fall (x: Zeile 2; y Zeile 3) bis auf a die gleichen Ausprägungen in den beiden Dataframes hat, wird dieser nicht mit übernommen. bind_rows() Der Funktion bind_rows() übergeben wir einfach die Dataframes, die wir vertikal aneinander reihen wollen. Die Reihenfolge der übergebenen Dataframes entscheidet dabei auch über die Reihenfolge der Zeilen des zusammengeführten Dataframes (z.B. erst alle Zeilen von vornamen_13, dann von vornamen_14). Optional können wir der Funktion das Argument .id übergeben, mit dem wir eine ID-Variable erstellen, welche die Datensätze kodiert (beginnend mit 1). # library(dplyr) vornamen_bind &lt;- bind_rows(vornamen_13, vornamen_14, .id = &quot;id&quot;) id vorname anzahl geschlecht 1 1 Maximilian 166 m 4013 2 Maximilian 178 m Hier sehen wir die ID-Variable. Von den Zeilen 1 bis 4012 ist diese 1; von den Zeilen 4012 bis 8044 ist sie 2. Es ist auch möglich, der Funktion mehr als zwei Dataframes zu übergeben, welche in einem gemeinsamen Dataframe gespeichert werden. Wir können beliebig viele Dataframes mit bind_rows() zusammenführen, solange diese mindestens eine gemeinsame Variable haben. Zur Überprüfung schauen wir uns mit dim() wieder die Anzahl der Zeilen und Spalten an. dim(vornamen_bind) ## [1] 8044 4 Die Anzahl der Zeilen stimmt mit der Summe der Zeilen von vornamen_13 und vornamen_14 überein, d.h. es wurden alle Fälle übernommen. 8.2.2 Unterschiedliche Variablen, selbe Fälle Im Gegensatz zu merge() und bind_rows() unterscheiden sich merge(..., by) und die _join()-Funktionen nicht in ihrer Funktionalität, sondern nur in der Reihenfolge der Fälle im zusammengeführten Dataframe. Allerdings kann man die _join()-Funktionen auch alternativ zu bind_rows() nutzen. Dann unterscheidet sich das Ergebnis im Vergleich zu merge() auch lediglich in der Reihenfolge der Fälle. Mehr Infos dazu finden wir im Abschnitt [_join()]. merge(..., by) Die Funktion merge() können wir auch nutzen, um Spalten aneinander zu heften. Mit dem Argument by geben wir an, welche ID-Variable die (selben) Fälle kodiert. Wenn es unterschiedliche Benennungen der gleichen ID-Variablen in den beiden Datensätzen gibt, müssen wir by.x und by.y nutzen. Die Benennung von by.x wird dann übernommen. x und y spielen auf die Reihenfolge an, in welcher wir die Datensätze an die Funktion merge() übergeben. x ist der zuerst übergebene Datensatz; y der als zweites übergebene. Wir wollen auch hier wieder die Daten aus beiden Dataframes übernehmen, und geben das mit all = TRUE an. Wir könnten aber hier ebenso all.x = TRUE bzw. all.y = TRUE oder all = FALSE nutzen. vornamen_merge_col &lt;- merge(vornamen_13, vornamen_14, by = &quot;vorname&quot;, all = TRUE) dim(vornamen_merge_col) ## [1] 6371 5 Nun schauen wir uns einmal die (ersten 6 Fälle der) neu erstellten Variablen an. vorname anzahl.x geschlecht.x anzahl.y geschlecht.y NA NA 9 w NA NA 7 m Aadhavan NA NA 2 m Aadhya 1 w 1 w Aahana NA NA 1 w Aahel 1 m NA NA Wie kommt man auf die Anzahl der Fälle \\(N = 6371\\)? In den beiden Dataframes vornamen_13 und vornamen_14 wurden genau die gleichen drei Variablen (vorname, anzahl und geschlecht) erhoben. Wenn wir die beiden zusammenführen, können drei unterschiedliche Szenarien mit Hinblick auf vorname und geschlecht auftreten. Nachfolgend schauen wir uns jeweils ein Beispiel sowie die Anzahl der Fälle dieser Szenarien an. vorname und geschlecht sind in beiden Dataframes gleich ## vorname anzahl.x geschlecht.x anzahl.y geschlecht.y ## 6 Aahel 1 m NA &lt;NA&gt; nrow(vornamen_merge_col[which( vornamen_merge_col$geschlecht.x == vornamen_merge_col$geschlecht.y),]) ## [1] 1672 geschlecht bei vorname unterscheidet sich zwischen den Dataframes ## vorname anzahl.x geschlecht.x anzahl.y geschlecht.y ## 38 Abdullah 7 m 4 m nrow(vornamen_merge_col[which( vornamen_merge_col$geschlecht.x != vornamen_merge_col$geschlecht.y),]) ## [1] 112 vorname und geschlecht eines Falles sind nur in einem Dataframe enthalten (für die Daten des anderen sind NAs angegeben) vornamen_merge_col[2,] ## vorname anzahl.x geschlecht.x anzahl.y geschlecht.y ## 2 + NA &lt;NA&gt; 7 m colSums(is.na(vornamen_merge_col)) # spaltenweise Missings gezählt ## vorname anzahl.x geschlecht.x anzahl.y geschlecht.y ## 0 2305 2305 2282 2282 Wir können uns hier nur die Missings in den einzelnen Spalten anschauen, um die Häufigkeiten für dieses Szenario zu bekommen, weil es vorher in den einzelnen Dataframes keine Missings gab. Wenn wir nun alle Werte aufsummieren, kommen wir auf die Anzahl der Zeilen im gemeinsamen Dataframe: 1672 + 112 + 2305 + 2282 ## [1] 6371 _join() Die _join()-Funktionen aus dplyr sind danach differenziert, welche Daten wir aus den Datensätzen übernehmen möchten. Diese Unterscheidung ist analog zu dem Argument all in merge(). Für Daten aus beiden Datensätzen nutzt man full_join(). Analog zu all = TRUE in merge(). Für Daten aus dem ersten bzw. zweiten Datensatz und den überlappenden Fällen nutzt man left_join() bzw. right_join(). Analog zu all.x = TRUE bzw. all.y = TRUE in merge(). Für Daten, die in beiden Datensätzen überlappen nutzt man inner_join(). Analog zu all = FALSE in merge(). # library(dplyr) vornamen_join &lt;- full_join(vornamen_13, vornamen_14, by=&quot;vorname&quot;) Nun überprüfen wir wieder die Dimensionen des neu erstellten Dataframes. dim(vornamen_join) ## [1] 6371 5 Wir sehen, dass der mit full_join(..., by = \"vorname\") zusammengeführte Datensatz genau die gleichen Dimensionen hat wie der mit merge(..., by = \"vorname\", all = TRUE) zusammengeführte. Die beiden Funktionen unterscheiden sich nur in der Sortierung der Fälle (welcher Dataframe zuerst eingegeben wurde vs. natürliche Sortierung der Fälle). full_join() als Alternative zu bind_rows() Mit full_join(x, y) bekommen wir (bis auf die Sortierung der Fälle) das gleiche Ergebnis wie bei merge(x, y, all = TRUE) vornamen_join_row &lt;- full_join(vornamen_13, vornamen_14) Zur Demonstration der Übereinstimmung schauen wir uns die Dimensionen und den Aufbau des Dataframes (am Beispiel der ersten 6 Zeilen) an. dim(vornamen_join_row) ## [1] 7496 3 vorname anzahl geschlecht Maximilian 166 m Felix 124 m Anna 109 w David 109 m Sophia 108 w Emilia 103 w dim(vornamen_merge_row) ## [1] 7496 3 vorname anzahl geschlecht 7 m 9 w Aadhavan 2 m Aadhya 1 w Aahana 1 w Aahel 1 m 8.3 Daten extrahieren Synonyme: Splitten, Subsetten, Filtern, Selektieren, Extrahieren Manchmal möchten wir nur bestimmte Variablen bzw. bestimmte Fälle aus einem Datensatz betrachten. Generell bietet es sich an, dafür reguläre Ausdrücke (regular expressions z.B. die Metacharactere .*, |, ^ und $) und logische Operatoren (logical operators z.B. &gt;, &lt; und ==) zu nutzen. Wie wir Variablen (Spalten) und Fälle (Zeilen) selektieren und in einem neuen Dataframe speichern können, schauen wir uns nun an. 8.3.1 Variablen Wenn wir nur einige Variablen aus einem bzw. aus mehreren Datensätzen benötigen, können wir diese mit verschiedenen Möglichkeiten entnehmen. Im Folgenden schauen wir uns dafür Möglichkeiten aus dem Standardpacket base und dem Zusatzpaket dplyr an. Unten befindet sich eine Übersicht, der wir entnehmen können, welche Methode wir wählen sollten in Abhängigkeit davon, ob die Variablen, die wir extrahieren wollen, ähnlich oder unterschiedlich sind. Die Variablennamen sind … … sich ähnlich … unterschiedlich grep() $, select() z.B. enthalten den Buchstaben ‘o’: Ozone, Solar.R, Month z.B. Month und Day grep() Wenn Variablen eines Datensatzes eine Gemeinsamkeit (z.B. einen gemeinsamen Wortstamm) aufweisen, können wir diese mit der Funktion grep() extrahieren. grep(pattern, names(Datensatz)) Die Funktion durchsucht die Namen der Variablen eines Dataframes - names(Datensatz) - nach bestimmten Zahlen- oder Zeichenketten (pattern). Diese müssen wir in \" \" angeben (weil Variablennamen als Character gespeichert werden). Wir wollen beispielshalber alle Variablen extrahieren, die irgendwo ein o im Namen haben. # Selektion der Namen var_mit_o &lt;- grep(pattern=&quot;o&quot;, names(airquality)) # Anwenden der Selektion auf den Dataframe df_var_mit_o &lt;- airquality[var_mit_o] Mit dieser Methode haben wir gleich den Vorteil, dass die Namen der Variablen im neuen Datensatz gleich denen im ursprünglichen Datensatz ist. Wir können unsere Suche mit grep() auch noch spezifischer machen, indem wir die regulären Operatoren nutzen. Mit ^o suchen wir Variablen, die mit einem “o” beginnen; mit o$ jene die mit “o” enden. Mit ^o|o$ suchen wir Variablen, die entweder mit einem “o” beginnen oder enden. Ein Beispiel dazu finden wir im Abschnitt Fälle mit grep() extrahieren. Mit ^o.*o$ suchen wir Variablen, die mit einem “o” beginnen und enden. $ Einzelne Variablen, die keine Gemeinsamkeit (z.B. einen gemeinsamen Wortstamm) aufweisen, kann man mit dem $-Operator extrahieren. Diesen wendet man an, indem man die Form Data Frame$Variable nutzt. Die Variablen können folglich aus unterschiedlichen Datensätzen stammen, da wir jede Variable jeweils neu ansprechen müssen. Wir entnehmen die Variablen Wind und Ozone und speichern diese in einem neuen Dataframe. df_wind_ozone &lt;- data.frame(airquality$Wind, airquality$Ozone) Mit dieser Methode haben wir den Nachteil, dass die Variablen im neu erstellten Dataframe nicht mit ihrem ursprünglichen Namen, sondern in der Form Datensatz.Variable benannt sind. Wir können den Variablen z.B. colnames(Datensatz) &lt;- c(“Var1”, “Var2”, …) wieder ihren ursprünglichen (oder einen neuen) Namen geben. select() Die Funktion select() kann unterschiedliche Variablen aus dem selben Dataframe extrahieren. Sie ist dabei kompakter zu handhaben als die Extraktion mit $. Man übergibt der Funktion zuerst den Dataframe und anschließend die Namen der Variablen, welche man extrahieren möchte. Man kann diese sogar gleich umbenennen. Wir erstellen einen neuen Dataframe mit den Variablen Month und Day. Die Variable Month werden wir zu Mon umbenennen. # library(dplyr) df_month_day &lt;- select(airquality, Mon = Month, # neuer Name = alter Name Day) Wenn wir bis auf einige wenige Variablen alle übernehmen wollen, können wir das realisieren, indem wir jeweils ein - vor die ungewollten Variablennamen setzen. Wenn der ersten Variable, die wir select() übergeben, ein - vorangestellt wurde, übernimmt die Funktion alle Variablen mit Ausnahme jener, die mit - angegeben werden. Schauen wir uns das für den Fall an, dass wir Month und Day aus dem Dataframe entfernen wollen. # library(dplyr) df_without_month_day &lt;- select(airquality, -Month, -Day) 8.3.1.1 Datums-Variablen splitten Für den Fall, dass wir eine Datums-Variable in unserem Datensatz haben, welche in einem für uns unangemessenen Format vorliegt, können wir diese mit dem Paket lubridate umformatieren. Auf dieser Seite wird der Umgang mit den im Paket enthaltenen Funktionen ymd() und mdy() erklärt. 8.3.2 Fälle Wir schauen uns nachfolgend einige Möglichkeiten der Extraktion von Fällen mit spezifischen Ausprägungen (die man z.B. für eine Subgruppenanalyse benötigt) an. Auch hier schauen wir uns wieder sowohl Funktionen aus dem Standardpaket base als auch aus dem Zusatzpaket dplyr an. Unten befindet sich eine Übersicht, der wir entnehmen können, welche Methode wir wählen sollten in Abhängigkeit davon, ob die Fälle, die wir extrahieren wollen, ähnlich oder unterschiedlich sind. Die Ausprägungen der Fälle haben … … die selben Zeichen … einen gemeinsamen Wertebereich grep() logische Operatoren, filter() z.B. enthalten die Zahl 6: 67, 86, … z.B. genau 57 oder &gt;= 15 grep() Wenn wir Ausprägungen suchen, die sich nicht durch logische Operatoren, sondern durch Ähnlichkeiten (z.B. ein gleiches Zeichen) filtern lassen, dann können wir dafür grep() nutzen. grep(pattern, x, value) Die Funktion durchsucht Elemente eines Vektor (x) nach bestimmten Zahlen- oder Zeichenketten (pattern). Mit grep() werden in Abhängigkeit des Arguments value entweder Indizes (FALSE; voreingestellt), oder konkrete Werte (TRUE) ausgegeben. Die Werte schauen wir uns zur Überprüfung an; die Indizes benötigen wir zur Extraktion jener Fälle aus dem Datensatz. Unsere gesuchten Zahlen- oder Zeichenketten, die wir an das Argument pattern übergeben, sowie die ausgegeben Werte, werden immer als Character behandelt und von daher in \" \" ausgegeben. Wenn eine Ausprägung irgendwo eine bestimmten Zahlen- oder Zeichenketten enthalten soll, geben wir diese einfach ein. Wir durchsuchen die Variable Temp nach den Tagen, an denen eine 6 im Messwert war. grep(&quot;6&quot;, airquality$Temp) # Indizes ## [1] 1 4 5 6 7 9 10 12 13 14 16 17 19 20 23 24 28 31 34 ## [20] 49 51 53 54 55 85 88 90 96 103 104 110 118 122 135 140 141 142 144 ## [39] 147 148 152 153 grep(&quot;6&quot;, airquality$Temp, value=TRUE) # Werte ## [1] &quot;67&quot; &quot;62&quot; &quot;56&quot; &quot;66&quot; &quot;65&quot; &quot;61&quot; &quot;69&quot; &quot;69&quot; &quot;66&quot; &quot;68&quot; &quot;64&quot; &quot;66&quot; &quot;68&quot; &quot;62&quot; &quot;61&quot; ## [16] &quot;61&quot; &quot;67&quot; &quot;76&quot; &quot;67&quot; &quot;65&quot; &quot;76&quot; &quot;76&quot; &quot;76&quot; &quot;76&quot; &quot;86&quot; &quot;86&quot; &quot;86&quot; &quot;86&quot; &quot;86&quot; &quot;86&quot; ## [31] &quot;76&quot; &quot;86&quot; &quot;96&quot; &quot;76&quot; &quot;67&quot; &quot;76&quot; &quot;68&quot; &quot;64&quot; &quot;69&quot; &quot;63&quot; &quot;76&quot; &quot;68&quot; Wenn eine Ausprägung eine bestimmte Zahlen- oder Zeichenketten zu Beginn enthalten soll, setzen wir ein ^ vor diese. Wenn wir beispielsweise alle Tage suchen, an denen die Temperatur (Temp) im Bereich 60-69°F, dann können wir das folgendermaßen tun: Diese Suche könnten wir auch mit den logischen Operatoren durchführen. grep(&quot;^6&quot;, airquality$Temp) # Indizes ## [1] 1 4 6 7 9 10 12 13 14 16 17 19 20 23 24 28 34 49 140 ## [20] 142 144 147 148 153 grep(&quot;^6&quot;, airquality$Temp, value=TRUE) # Werte ## [1] &quot;67&quot; &quot;62&quot; &quot;66&quot; &quot;65&quot; &quot;61&quot; &quot;69&quot; &quot;69&quot; &quot;66&quot; &quot;68&quot; &quot;64&quot; &quot;66&quot; &quot;68&quot; &quot;62&quot; &quot;61&quot; &quot;61&quot; ## [16] &quot;67&quot; &quot;67&quot; &quot;65&quot; &quot;67&quot; &quot;68&quot; &quot;64&quot; &quot;69&quot; &quot;63&quot; &quot;68&quot; Wenn eine Ausprägung eine bestimmte Zahlen- oder Zeichenketten am Ende enthalten soll, setzen wir ein $ ans Ende. Wenn wir beispielsweise alle Tage suchen, an denen die Temperaturangabe (Temp) mit einer 6 endet, dann können wir das folgendermaßen tun: grep(&quot;6$&quot;, airquality$Temp) # Indizes ## [1] 5 6 13 17 31 51 53 54 55 85 88 90 96 103 104 110 118 122 135 ## [20] 141 152 grep(&quot;6$&quot;, airquality$Temp, value=TRUE) # Werte ## [1] &quot;56&quot; &quot;66&quot; &quot;66&quot; &quot;66&quot; &quot;76&quot; &quot;76&quot; &quot;76&quot; &quot;76&quot; &quot;76&quot; &quot;86&quot; &quot;86&quot; &quot;86&quot; &quot;86&quot; &quot;86&quot; &quot;86&quot; ## [16] &quot;76&quot; &quot;86&quot; &quot;96&quot; &quot;76&quot; &quot;76&quot; &quot;76&quot; Wie beim Extrahieren von Variablen können wir auch hier mit grep() verschiedene Bestandteile einer Ausprägung anhand des logischen Operators | suchen. Als Beispiel suchen wir Temperaturangaben, die zu Beginn eine 5 enthalten oder mit einer 5 enden. grep(&quot;^5|5$&quot;, airquality$Temp) # Indizes ## [1] 5 7 8 15 18 21 25 26 27 36 49 56 63 81 86 97 115 132 151 grep(&quot;^5|5$&quot;, airquality$Temp, value=TRUE) # Werte ## [1] &quot;56&quot; &quot;65&quot; &quot;59&quot; &quot;58&quot; &quot;57&quot; &quot;59&quot; &quot;57&quot; &quot;58&quot; &quot;57&quot; &quot;85&quot; &quot;65&quot; &quot;75&quot; &quot;85&quot; &quot;85&quot; &quot;85&quot; ## [16] &quot;85&quot; &quot;75&quot; &quot;75&quot; &quot;75&quot; Achtung: Die Zahlen bzw. Zeichenketten dürfen nicht durch Freizeichen getrennt werden, z.B. würden mit \"6| ^7\" nur Temperaturangaben gefiltert werden, die eine 6 enthalten. Die Suche mit “^x|x$” ergibt gemeinsam die globale Suche nach “x”. Wenn wir hingegen mehrere Bedingungen verknüpfen wollen, z.B. “^x” und “x$”, dann nutzen wir .*, z.B. “^x.x$” (für ein Beispiel siehe [mutate(): Zusammenfassung aller Fälle]). Ähnlich zum Abschnitt zu Variablen mit grep() extrahieren wenden wir die Selektion mit der Form Datensatz[grep(),] an, und speichern diese in einem neuen Objekt. Wir können hierfür nur den Indizes-Vektor (value=FALSE; Default) nutzen. df_temp5 &lt;- airquality[grep(&quot;^5|5$&quot;, airquality$Temp),] Logische Operatoren Wenn wir logische Operatoren auf einzelne Variablen anwenden, können wir Fälle mit bestimmten Ausprägungen filtern. Hier finden wir eine Einführung zu logischen Operatoren mit Übungsfragen. Um nur diese Fälle im gesamten Datensatz zu extrahieren, nutzen wir folgende Syntax: Datensatz[Variable Operator Ausprägung,] Wenn wir Fälle (d.h. Zeilen) exrahieren wollen, müssen wir nach den Indizes immer ein Komma angeben z.B. extrahiert df[2,] die zweite Zeile aus df. Das kommt daher, dass in einem zweidimensionalen Objekt immer erst die Zeilen und dann die Variablen angegeben werden z.B. sehen wir das auch bei der Reihenfolge der Dimensionen unserer Objekte im Environment (bei Data). Beispielsweise können wir mit dem Gleichheits-Operator == nach exakt einer Ausprägung in einer Variablen suchen. Wir filtern die Variable Temp (Temperatur in Grad Fahrenheit) nach Fällen mit der Ausprägung 57. df_temp57 &lt;- airquality[airquality$Temp == 57,] Wir können mittels |(oder) auch mehrere Ausprägungen gleichzeitig auswählen. Nun wollen wir zusätzlich zu nach Fällen mit der Ausprägung 57 auch jene mit der Ausprägung 66 extrahieren. df_temp57_66 &lt;- airquality[airquality$Temp == 57 | airquality$Temp == 66,] Weitere logische Operatoren sind z.B. != (nicht), &lt; (kleiner) und &gt;= (größer gleich). filter() Mit filter() können wir verschiedene Variablen nach bestimmten Kriterien filtern. Dabei greifen wir wieder auf die logischen Operatoren zurück. Man übergibt der Funktion zuerst den Dataframe und anschließend die Namen der Variablen mit den Bedingungen, die auf diese jeweils zutreffen sollen. Wir wollen nur jene Fälle, die in der zweiten Hälfte des Junis erhoben wurden. df_month6_day15ff &lt;- dplyr::filter(airquality, Month == 6, Day &gt;= 15) Zu Beginn haben wir erläutert, warum wir manchmal :: nutzen sollten. 8.4 Daten sortieren Für manche Vorhaben, wie z.B. grafische Darstellungen oder dem Quantifizieren von Heteroskedastizität, benötigt man sortierte Daten. Wir schauen uns nachfolgend zwei Möglichkeiten an, einen Dataframe nach den Ausprägungen seiner Variablen zu sortieren. order() Nachfolgend sehen wir, wie man mit order() aufsteigend sortiert. Weil mit order() nur Zeilenindizes ausgegeben werden, müssen wir diese noch auf den Dataframe anwenden. Das machen wir mit der Form Datensatz[order(Variable),]. df_ascend_temp &lt;- airquality[order(airquality$Temp),] Wenn wir Temp-Werte mit der gleichen Ausprägung zusätzlich noch nach der (aufsteigenden) Variable Wind sortieren wollen, können wir diese einfach ergänzen. df_ascend_temp_wind &lt;- airquality[order(airquality$Temp, airquality$Wind),] Beispielsweise sehen wir hier, dass die Fälle 27, 25 und 18 (auf Seite 1 in den Zeilen 2, 3 und 4) anders sortiert sind als oben. Wenn wir nach den absteigenden Werte der Variablen sortieren wollen, hängen wir jeweils ein - vor diese. Alternativ können wir auch das Argument decreasing=TRUE setzen (dann werden aber, im Gegensatz zu unserem Beispiel, alle Variablen absteigend sortiert). df_descend_temp_wind_1 &lt;- airquality[order(-airquality$Temp, airquality$Wind),] arrange() Die Funktion arrange() aus dem Paket dplyr hat ein sehr ähnliches Prinzip wie order(). Sie ist dabei in der Handhabung übersichtlicher, weil man der Funktion den Namen des Dataframes einmalig übergibt und nachfolgend nur noch die Variablen angeben muss, nach denen sortiert werden soll. Außerdem muss man den Output nicht zusätzlich auf den Dataframe anwenden, weil arrange() das ohnehin macht. Standardmäßig wird hier ebenso wie bei order() aufsteigend sortiert solange man das nicht mit dem Voranstellen eines - ändert. Schauen wir uns das für das letzte Beispiel im vorhergehenden Abschnitt an. Wir sortieren den Dataframe absteigend nach Temp und gleiche Werte aufsteigend nach Wind. df_ascend_temp_wind_2 &lt;- arrange(airquality, -Temp, Wind) 8.5 Kodierung ändern Wenn die Daten nicht in einer angemessenen Kodierung vorliegen, muss man diese nachträglich anpassen bzw. neu erstellen. Die Kodierung einer Variablen ist von ihrem Messniveau abhängig. Für dieses Kapitel beschränken wir uns auf die Nutzung des Datensatzes PWE_data, welchen wir zu Beginn heruntergeladen haben. 8.5.1 Umkodieren Wenn wir zum Beispiel Messwerte in Meter zu Messwerten in Zentimeter ändern oder negativ gepolte Items umpolen wollen, spricht man von Umkodieren. Rekodieren und Umkodieren wird häufig synonym genutzt. Wir können die Kodierung von Merkmalen in R auf verschiedene Arten ändern. Nachfolgend schauen wir uns zwei Funktionen an, die wir nutzen können. recode() In der Funktion recode() (auch Recode() möglich) aus dem Paket car müssen wir grundsätzlich zwei Argumente spezifizieren: var und recodes. Ersterem übergeben wir die umzukodierende Variable, zweiterem die alte und die neue Kodierung der Variablen. Wir müssen hierbei einige syntaktische Besonderheiten von recodes beachten: recodes = “alt_1 = neu_k; alt_2 = neu_k-1; …; alt_k = neu_1” (Ausprägungen der Kodierung von 1 bis \\(k\\)) die Input=Output-Parameter müssen gemeinsam als Zeichenkette (d.h. in \" \") vorliegen die verschiedenen Input=Output-Parameter müssen mit Semikolon (;) getrennt werden Wir invertieren im Folgenden die Werte der Variablen Q9A, Q13A und Q15A (numeric). Die Information, dass die Variablen negativ kodiert sind, finden wir in Mirels &amp; Garrett (1971) (nur über HU-VPN zugänglich). Informationen zur Messskala finden wir auch im Codebuch. So sehen die Daten (der ersten 10 Personen) bisher aus: Jetzt invertieren wir die Skalen: library(car) PWE_data$Q9A &lt;- recode(var=PWE_data$Q9A, recodes=&quot;1=5; 2=4; 3=3; 4=2; 5=1&quot;) PWE_data$Q13A &lt;- recode(var=PWE_data$Q13A, recodes=&quot;1=5; 2=4; 3=3; 4=2; 5=1&quot;) PWE_data$Q15A &lt;- recode(var=PWE_data$Q15A, recodes=&quot;1=5; 2=4; 3=3; 4=2; 5=1&quot;) Abschließend überprüfen wir (visuell), ob die Umkodierung geklappt hat: Wenn die Kodierung aus Zeichenketten (character) besteht, müssen wir diese jeweils noch mit ' ' umschließen. Schauen wir uns an, wie man die Ausprägungen von eduaction (1,2,3,4) zu den Beschreibungen (Less than High School,High School,University Degree,Graduate Degree`) ändert. Mit dem Parameter as.factor legen wir fest, ob die (neue) rekodierte Variable als Faktor gespeichert werden soll. Achtung: Leider können wir so aber nur ungeordnete (nominalskaliert) und keine geordneten (ordinalskaliert) Faktoren erstellen. Dafür müssten wir auf die Funktion factor(..., ordered = TRUE, levels) zurückgreifen (siehe Abschnitt Faktorisieren). # aus Gründen der Darstellung speichern wir die Kodierungen zuerst in einem String: recode &lt;- c(&quot;1=&#39;Less than High School&#39;;2=&#39;High School&#39;;3=&#39;University Degree&#39;;4=&#39;Graduate Degree&#39;&quot;) PWE_data$education_new &lt;- recode(var=PWE_data$education, as.factor=TRUE, recodes=recode) Abschließend vergleichen wir die Daten (der ersten 10 Personen) von education und education_new: case_when() Mit der Funktion case_when() aus dem Paket dplyr können wir für verschiedene Fälle (d.h. Bedingungen) der ursprünglichen Variable angeben, wie diese umkodiert werden soll. Auf die linke Seite schreiben wir eine logische Bedingung (z.B. größer als mit &gt;); auf die rechte Seite die neue Kodierung. Verbunden werden beide mit einer Tilde (~). Im Gegensatz zu recode() können wir durch die Verwendung von logischen Operatoren (z.B. &gt;) ganzen Zahlenintervallen dieselbe Kodierung zuweisen. Als Beispiel bilden wir Kategorien für das intervallskalierte Merkmal Q1E (mit Frage Q1 verbrachte Zeit in Millisekunden). Den Range des Merkmals erfahren wir mit range(PWE_data$Q1E, na.rm=TRUE) (195 - 181.246). Wir teilen Q1E in vier gleich breite Kategorien ein: [195, 45457.75), [45457.75, 90720.5), [90720.5, 135983.2), [135983.2, 181247). [ heißt inklusive ) heißt exklusive Bei der oberen Grenze wird aufgerundet. # library(dplyr) PWE_data$Q1E_kat &lt;- case_when( PWE_data$Q1E &lt; 45457.75 ~ 1, # kleiner damit exklusiv PWE_data$Q1E &lt; 90720.5 ~ 2, PWE_data$Q1E &lt; 135983.2 ~ 3, PWE_data$Q1E &lt; 181247 ~ 4) str(PWE_data$Q1E_kat) # Überprüfung Datentyp ## num [1:1350] 1 1 1 1 1 1 1 1 1 1 ... unique(PWE_data$Q1E_kat) # um alle Kategorien zu sehen ## [1] 1 2 4 3 NA Nun haben wir eine neue Variable Q1E_kat erstellt, welche zusammengefasste Informationen aus Q1E enthält. Die neu erstellte Variable liegt als numeric vor, d.h., dass wir diese noch faktorisieren und ordnen müssen, damit sie als ordinalskaliert gehandhabt wird. PWE_data$Q1E_kat &lt;- factor(PWE_data$Q1E_kat, ordered=TRUE) str(PWE_data$Q1E_kat) # Überprüfung Datentyp ## Ord.factor w/ 4 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... 8.5.2 Indikatorvariablen: Kodierung nominaler Merkmale Nominale Merkmale kann man in Form von Dummy-, Effekt- und Kontrastkodierungen repräsentieren. Eine solche Repräsentation ist vor allem im Rahmen des Allgemeinem Linearen Modells (ALM) von Interesse. Untenstehende Tabelle gibt einen groben Überblick über die Interpretation der Parameter des ALM in Abhängigkeit der Kodierung. Interzept \\(b_0\\) Steigung \\(b_j\\) mögliche Anwendung Dummy Mittelwert in der Referenzgruppe Differenz des Mittelwerts der j-ten Gruppe zur Referenzgruppe Vergleich von Experimental- und Kontrollgruppe Effekt Mittelwert der Gruppenmittelwerte (Gesamtmittelwert) Differenz des Mittelwertes der j-ten Gruppe zum Gesamtmittelwert Vergleich von Gruppen in varianzanalytischen Designs Kontrast Mittelwert über die Mittelwerte der Kontrastgruppen lässt sich als Funktion der Kontrastkoeffizienten darstellen, die den jeweiligen Kontrast kodieren Gezielte Einzelvergleiche von (Kombinationen) von Gruppen Die Interpretation der Mittelwerte und Differenzen hängt zusätzlich davon ab, ob ein balanciertes oder unbalanciertes Design vorliegt (d.h. ob die Gruppengrößen gleich oder ungleich sind). Für mehr Informationen zu Indikatorvariablen können wir z.B. folgende Quelle nutzen: Bortz, J., &amp; Schuster, C. (2010). Allgemeines lineares Modell. In J. Bortz, &amp; C. Schuster (Eds.), Statistik für Sozialwissenschaftler (S.363-384). Heidelberg: Springer (für HU-Studierende über ub.hu-berlin.de zugänglich) Im Folgenden schauen wir uns an, wie man konkret bei der Erstellung der verschiedenen Arten der Indikatorvariablen vorgehen kann. Dafür nutzen wir die im Abschnitt Faktorisieren erstellte Variable gender_uf aus PWE_data_fac. Zusätzlich rechnen wir jeweils eine einfache lineare Regression mit den verschiedenen Kodierungen, um die Unterschiede zwischen den Koderierungsarten zu veranschaulichen. Wir regredieren dafür die Zeit in Sekunden, die die Probanden auf der Instruktionsseite verbracht haben (introelapse), auf ihr Geschlecht (gender_uf). Achtung: Die nachfolgend vorgestellten Funktionen lassen sich auch auf ordinalskalierte Merkmale (d.h. sortierte Faktoren) anwenden. Bei diesen unterscheidet sich aber die Interpretation der geschätzten Koeffizienten der Regression. Wir bekommen Schätzungen für lineare (L), quadratische (Q) und kubische (C) Trends. Daher behandeln wir im folgenden Abschnitt nur die Kodierung von nominalskalierten Merkmalen. Dummy-Kodierung Viele Funktionen in R (z.B. lm(), lme() und lmer()) kodieren nominalskalierte Variablen intern automatisch nach der Dummy-Kodierung um, wenn diese vorher als Faktoren deklariert wurden. Dabei wird die erste Kategorie als Referenzkategorie genutzt. Wenn wir eine andere Referenzkategorie haben wollen, können wir dafür die im Folgenden vorgestellten Funktionen (C(..., contr.treatment(n, base)) oder relevel()) nutzen. Man benötigt für eine Dummykodierung mit \\(k\\)-Kategorien \\(k-1\\) Indikatorvariablen. Die jeweils interessierende Gruppe wird in der jeweiligen Indikatorvariablen mit 1 kodiert; die anderen mit 0. Als Referenzkategorie (von der die Abweichung berechnet wird) gilt jene, welche in allen Indikatorvariablen mit 0 kodiert wird. Die Indikatorvariablen erstellt uns die Funktion contr.treatment() automatisch, wenn wir die Anzahl der Faktorstufen (n) und die Referenz (base) angeben. Um die faktorisierte Variable gender_uf, welche 3 Ausprägungen hat, zu kodieren, benötigen wir 2 Dummy-Variablen. Wir wählen die erste Kategorie (1 = Male) als Referenzkategorie. contr.treatment(n=3, base=1) ## 2 3 ## 1 0 0 ## 2 1 0 ## 3 0 1 C(Faktor, contr.treatment(n, base)) setzt die Konstraste für den kategorialen Prädiktor innerhalb von lm(): lm_ct&lt;- lm(introelapse ~ C(gender_uf, contr.treatment(3, 1)), PWE_data) summary(lm_ct) ## ## Call: ## lm(formula = introelapse ~ C(gender_uf, contr.treatment(3, 1)), ## data = PWE_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2248 -2235 -781 -764 961254 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2249 1038 2.166 0.0305 * ## C(gender_uf, contr.treatment(3, 1))2 -1466 1496 -0.980 0.3274 ## C(gender_uf, contr.treatment(3, 1))3 -1847 4339 -0.426 0.6703 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26970 on 1340 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.0007689, Adjusted R-squared: -0.0007224 ## F-statistic: 0.5156 on 2 and 1340 DF, p-value: 0.5973 Nun vergleichen wir das Regressionsmodell mit den Dummy-kodierten Indikatorariablen (C(gender_uf, contr.treatment(n=4, base=1)) mit dem Regressionsmodell mit dem unsortierten Faktor (gender_uf), bei dem ebenfalls die erste Kategorie als Referenzkategorie genutzt wird. lm_uf &lt;- lm(introelapse ~ gender_uf, PWE_data) summary(lm_uf) ## ## Call: ## lm(formula = introelapse ~ gender_uf, data = PWE_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2248 -2235 -781 -764 961254 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2249 1038 2.166 0.0305 * ## gender_uf2 -1466 1496 -0.980 0.3274 ## gender_uf3 -1847 4339 -0.426 0.6703 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26970 on 1340 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.0007689, Adjusted R-squared: -0.0007224 ## F-statistic: 0.5156 on 2 and 1340 DF, p-value: 0.5973 Da die Referenzkategorie identisch ist, sehen wir, dass wir bei beiden die gleichen Ergebnisse erhalten. Bei der Dummykodierung entspricht unser Interzept \\(b_0\\) dem ungewichteten Mittelwert in der Referenzkategorie (1 = Male). Die partiellen Steigungsgewichte \\(b_j\\) (C(...)2 und C(...)3 bzw. gender_uf2 und gender_uf3) entsprechen den ungewichteten Mittelwertsunterschieden zwischen der jeweiligen Gruppe (2 = Female bzw. 3 = Other) und der Referenzgruppe (1 = Male). Alternativ zu C(..., contr.treatment()) können wir mit der Funktion relevel() die Referenzkategorie eines unsortierten Faktors ändern. Standardmäßig ist immer die erste Gruppe nach der natürlichen Reihenfolge (bei Zahlen aufsteigend und bei Buchstaben alphabetisch) die Referenzkategorie. Dem Argument ref übergeben wir die derzeitige Position der gewünschten Referenzkategorie. # Beispiel: Female als Referenzkategorie PWE_data$gender_uf_ref &lt;- relevel(PWE_data$gender_uf, ref = 2) ls.str(PWE_data[,c(103, 106)]) # zum Überprüfen ## education_new : Factor w/ 4 levels &quot;Graduate Degree&quot;,..: 4 2 2 2 2 1 1 2 3 2 ... ## gender_uf : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 2 2 2 2 2 1 3 2 1 ... Effektkodierung Für diese Kodierung benötigen wir ebenfalls \\(k-1\\) Indikatorvariablen. Die jeweils zutreffende Gruppe wird in der jeweiligen Indikatorvariablen mit 1 kodiert; die nicht zutreffende mit 0 (ebenso wie bei der Dummy-Kodierung). Die “Referenzkategorie” (in unserem Beispiel No) wird in allen Indikatorvariablen mit -1 kodiert. Es gibt eigentlich keine echte Referenzkategorie (wie bei der Dummy-Kodierung). Vielmehr entspricht der Interzept dem Mittelwert über alle Gruppen hinweg. Analog zu contr.treatment(n, ...) bei der Dummy-Kodierung können wir contr.sum(n) nutzen, um eine effektkodierte Matrix eines Faktors zu erstellen. contr.sum(n=3) ## [,1] [,2] ## 1 1 0 ## 2 0 1 ## 3 -1 -1 Wir müssen lediglich in n spezifizieren, wie viele Faktorstufen es gibt. Auch hier übergeben wir den Output der Funktion an C(), wenn wir z.B. eine lineare Regression mit lm() berechnen wollen. lm_cs &lt;- lm(introelapse ~ C(gender_uf, contr.sum(3)), PWE_data) summary(lm_cs) ## ## Call: ## lm(formula = introelapse ~ C(gender_uf, contr.sum(3)), data = PWE_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2248 -2235 -781 -764 961254 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1144.4 1490.2 0.768 0.443 ## C(gender_uf, contr.sum(3))1 1104.5 1606.2 0.688 0.492 ## C(gender_uf, contr.sum(3))2 -361.5 1614.8 -0.224 0.823 ## ## Residual standard error: 26970 on 1340 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.0007689, Adjusted R-squared: -0.0007224 ## F-statistic: 0.5156 on 2 and 1340 DF, p-value: 0.5973 Bei der Effektkodierung entspricht unser Interzept \\(b_0\\) dem Mittelwert der ungewichteten Gruppenmittelwerte. Die partiellen Steigungsgewichte \\(b_j\\) (C(...)1 und C(...)2) entsprechen der Differenz des Mittelwerts der jeweiligen Gruppe (1 = Male bzw. 2 = Female) zum Mittelwert der ungewichteten Gruppenmittelwerte. Leider können wir mit contr.sum() nur die erste Faktorstufe als “Referenzkategorie” nutzen. Mit ref() können wir jedoch wieder die Sortierung der Faktorstufen ändern und den umsortierten Faktor dann wieder an C(..., contr.sum(n)) übergeben. # Beispiel: Female als Referenzkategorie PWE_data$gender_uf_ref &lt;- relevel(PWE_data$gender_uf, ref = 2) ls.str(PWE_data[,c(103, 106)]) # zum Überprüfen Konstrastkodierung Bei der Kontrastkodierung können wir uns eigens gewählte Kontraste zwischen verschiedenen Gruppen anschauen. Die Gewichte \\(c_i\\) eines Kontrastes müssen der Bedingung genügen, dass die Summe der Gewichte über die Anzahl der zu kodierenden Kategorien \\(i\\) null ist, d.h. \\(\\sum\\limits_{i} c_i = 0\\). Die jeweilige Kodierung mit 0 in einer Indikatorvariablen sorgt dafür, dass eine Gruppe bzw. ein Fall nicht mit in einen Kontrast eingeht. Bei multiplen Kontrasten (d.h. mindestens zwei kontrastkodierten Variablen) können wir orthogonale (d.h. unkorrelierte) und nicht orthogonalen (d.h. korrelierte) Kontraste unterscheiden. Zwei Kontraste \\(j\\) und \\(j&#39;\\) sind orthogonal wenn zusätzlich zur oberen Bedingung gilt: \\(\\sum\\limits_{i} \\, c_{ij} \\cdot c_{ij&#39;} = 0\\) Im Folgenden werden wir nur einen Kontrast erstellen. Für mehr Informationen zur Orthogonalität von multiplen Kontrasten können wir z.B. bei Bortz &amp; Schuster (2010) nachschauen. Wir kontrastieren Männer und Frauen hinsichtlicher der verbrachten Zeit auf der Instruktionsseite (introelapse). Dafür sortieren wir den Datensatz PWE_data_fac zuerst mit order() nach gender_uf. Die Variable hat eine Zahlen-Kodierung: Männer sind gender_uf = 1, Frauen gender_uf = 2 und Andere gender_uf = 3. PWE_data &lt;- PWE_data[order(PWE_data$gender_uf),] # natürliche (aufsteigende) Sortierung: 1, 2, 3, NA Anschließend erstellen wir mit rep() die kontrastkodierte Variable. Alternativ könnten wir auch recode() oder case_when() nutzen (siehe Umkodieren). # Anzahl der Fälle in den einzelnen Ausprägungen in Erfahrung bringen: table(PWE_data$gender_uf, useNA = &#39;ifany&#39;) ## ## 1 2 3 &lt;NA&gt; ## 675 627 41 7 # Kontrast erstellen PWE_data$gender_kontrast &lt;- c(rep(1/675, 675), # Male (1) rep(-1/627, 627), # Female (2) rep(0, 48)) # Other (3) &amp; NA; nicht von Interesse rep(Gewichtung, Gruppengröße): Die jeweilige Gewichtung einer Gruppe (bzw. Kombination von Gruppen) richtet sich nach ihrer Größe. Wie genau funktioniert rep()? Für die manuelle Erstellung von Indikatorvariablen kann man die Funktion rep() nutzen, welche die ihr übergebenen Zahlen bzw. Zahlenfolgen (oder auch Zeichen bzw. Zeichenketten) beliebig häufg wiederholt. Schauen wir uns die Funktionsweise der Funktion an einigen Beispielen an. Die Zahl 1 wird 10 mal (times) wiederholt: rep(1, 10) # das gleiche wie: rep(x=1, times=10) ## [1] 1 1 1 1 1 1 1 1 1 1 Die Zahlenfolge 0, 1 wird 10 mal (times) wiederholt: rep(0:1, 10) ## [1] 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 Wenn wir erst 10 mal die 0 und anschließend 10 mal die 1 haben wollen, nutzen wir das Argument each: rep(0:1, each=10) # das gleiche wie c(rep(0, 10), rep(1, 10)) ## [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 Die Gewichte einer kontrastierten Gruppe (hier: jeweils Männer bzw. Frauen) werden so gewählt, dass sie aufsummiert 1 bzw. -1 ergeben. Wenn wir die gleiche Anzahl an Fällen in den zu kontrastierenden Gruppen haben, können wir die einzelnen Gewichte auch zu 1 bzw. -1 vereinfachen. Achtung: Wenn wir ungleich große Gruppen haben, wie in unserem Beispiel, dann liegen die einzelnen Gewichtungen als Brüche vor z.B. \\(c_{Männer} = \\frac{1}{675}\\) und \\(c_{Frauen} = -\\frac{1}{627}\\). Wenn wir die Elemente unserer kontrastkodierten Variablen gender_kontrast aufsummieren, erhalten wir -5.5944832^{-17}. Damit genügen wir de facto der Bedingung \\(\\sum\\limits_{i} c_i = 0\\) nicht, aber die Zahl ist so klein (d.h. so nah an 0), dass wir sie vernachlässigen können. Jetzt nehmen wir die kontrastkodierte Variable als Prädiktor in ein neues Regressionsmodell auf. lm_kontr &lt;- lm(introelapse ~ gender_kontrast, PWE_data) summary(lm_kontr) ## ## Call: ## lm(formula = introelapse ~ gender_kontrast, data = PWE_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2464 -2451 -998 -985 961038 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1758.6 775.3 2.268 0.0235 * ## gender_kontrast 476517.1 513617.1 0.928 0.3537 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28490 on 1348 degrees of freedom ## Multiple R-squared: 0.0006381, Adjusted R-squared: -0.0001032 ## F-statistic: 0.8608 on 1 and 1348 DF, p-value: 0.3537 In unserem Regressionsmodell mit dem Kontrast Männer vs. Frauen entspricht der Interzept \\(b_0\\) dem Mittelwert der Gruppenmittelwerte der betrachteten Gruppen (d.h. Männer und Frauen) und das partiellen Steigungsgewichte \\(b_1\\) dem Unterschied in den Mittelwerten der betrachteten Gruppen. 8.6 Summary-Variablen Wenn wir nicht mit den Rohdaten arbeiten wollen, sondern Informationen von mehreren, aggregierten Variablen (z.B. Summenwerte, Mittelwerte) oder anderweitig transformierten Variablen (z.B. standardisierte Werte) auswerten wollen, müssen wir Summary-Variablen erstellen. Wir nutzen zu diesem Zweck den Datensatz PWE_data, welcher aus einer psychometrischen Erhebung stammt. Zu Beginn des Kapitels haben wir den Datensatz heruntergeladen. Achtung: Die Werte der Variablen Q9A, Q13A und Q15A, die wir im Folgenden nutzen werden, wurden im Abschnitt Umkodieren rekodiert, weil sie negativ gepolt sind. rowSums(), rowMeans() und select(): Summen- und Mittelwerte Wir schauen uns im Folgenden an, wie man Summen- und Mittelwerte von mehreren Variablen erstellt. Dieses Vorgehen ist beispielsweise für die Erstellung von Skalenwerten in der Testkonstruktion von großer Relevanz. Wir schauen uns den Summen- sowie den Mittelwert über alle Items der Protestant Work Ethic Scale an. Die Fragen wurden auf einer intervallskalierten Skala - von 1 (stimme nicht zu) bis 5 (stimme zu) - beantwortet und in den Variablen, die mit einem Q beginnen und einem A enden gespeichert. Um einen Summenwert, d.h. eine Summe einer Person über mehrere Variablen, zu bilden, können wir auf die Funktion rowSums() zurück greifen. Wenn wir fehlende Werte in den Variablen haben, setzen wir das Argument na.rm=TRUE. Es ist zusätzlich sinnvoll, mit einer Kombination aus select(), matches() und den regulären Ausdrücken, die relevanten Variablen auszuwählen. Dafür laden wir das Paket dplyr. matches() ist eine select helper Funktion, welcher man reguläre Ausdrücke übergeben kann. Es gibt noch weitere Hilfsfunktionen, die man auch als Alternative zu regulären Ausdrücken nutzen kann z.B. starts_with(x) anstatt ^x. Wenn wir aber mehrere Bedingungen haben, sollten wir matches() und den regulären Ausdruck .* (zur konjunktiven Verknüpfung) nutzen. # library(dplyr) PWE_data$sum &lt;- rowSums(select(PWE_data, matches(&quot;^Q.*A$&quot;)), na.rm=TRUE) Nun schauen wir uns die neu erstellte Variable (für die die ersten 10 Personen) einmal an: Wenn wir hingegen nicht den Summen- sondern den Mittelwert einer Person über Variablen bilden wollen, nutzen wir rowMeans(). Der Rest bleibt analog zum Vorgehen oben. PWE_data$mean &lt;- rowMeans(select(PWE_data, matches(&quot;^Q.*A$&quot;)), na.rm=TRUE) Abschließend schauen wir uns wieder die neu erstellte Variable (für die die ersten 10 Personen) an: ifelse(): Auswahl bestimmter Fälle Jetzt schauen wir uns an, wie wir eine Variable nur für eine bestimmte Gruppe erstellen können. Dafür nutzen wir die Funktion ifelse(test, yes, no). Mit dieser testen wir, ob eine oder mehrere Bedingungen (test) zutreffen und geben an, was mit den Fällen passieren soll, auf die die Bedingung(en) zutreffen (yes) und jene, auf die diese nicht zutreffen (no). Für unser Beispiel sollen alle Fälle, auf die die Bedingung(en) nicht zutreffen, ein NA auf der neu erstellten Variablen erhalten. Wenn wir mehrere Bedingungen nutzen wollen, können wir auf die logischen Operatoren zurückgreifen. Mit &amp; geben wir an, dass beide Bedingungen zutreffen sollen; mit | dass eine Bedingung zutreffen soll. Mit runden Klammern können wir Bedingungen noch differenzierter angeben z.B. (A | B) &amp; C heißt, dass entweder A oder B eine (noch festzulegende) Ausprägung erfüllen müssen und zusätzlich noch C. Mehr Informationen zu logischen Operatoren finden wir im gleichnamigen Abschnitt. Wir nutzen das gleiche Beispiel wie im Abschnitt vorher, nur dass wir jetzt nur den Summenwert für weibliche (gender == 2) Buddhistinnen (religion == 3) bilden wollen. PWE_data$sum_gr &lt;- ifelse(PWE_data$gender == 2 &amp; PWE_data$religion == 3, # test rowSums(select(PWE_data, matches(&quot;^Q.*A$&quot;)), # yes na.rm=TRUE), NA) # no Schauen wir uns die neu erstellte Variable sowie die Gruppierungsvariablen (der Personen 687 bis 691 an, unter denen sich 2 von insgesamt 9 weibliche Buddhistinnen befinden) einmal an: mutate(): Summary-Variablen als Funktion von anderen Variablen erstellen Wenn wir neue Variablen erstellen wollen, die Funktionen von bestehenden Variablen sind, können wir die Funktion mutate() aus dem Paket dplyr nutzen. Besonders nützlich hierbei ist, dass wir mit dem Parameter .keep festlegen können, welche Variablen wir im (neu erstellten) Datensatz behalten wollen. Nachdem wir schon den Mittelwert der Skale berechnet haben (mean), wollen wir noch eine neue Variable erstellen, die den z-standardisierten Mittelwert der Personen widergibt. # für z-Standardisierung notwendige Kennwerte berechnen: mean_all &lt;- mean(PWE_data$mean, na.rm=TRUE) # Mittelwert über alle Personenmittelwerte sd_all &lt;- sd(PWE_data$mean, na.rm=TRUE) # Standardabweichung der Mittelwerte # library(dplyr) PWE_data_mean &lt;- mutate(PWE_data, sw_mean = (mean - mean_all) / sd_all, .keep = &quot;used&quot;) # alle benutzten und neu erstellen Variablen Abschließend können wir uns den neu erstellten Datensatz PWE_data_mean (für die ersten 10 Personen) einmal anschauen: 8.7 Weitere wichtige Hinweise 8.7.1 Cheat Sheet dplyr Wer Gefallen an den tidyverse-Funktionen select(), filter(), mutate(), und summarise() gefunden hat, kann ein Cheat Sheet zur Data Transformation mit dplyr in deutsch oder englisch herunterladen. 8.7.2 Stichprobengröße Es ist generell sehr wichtig, auch bei der Datenvorbereitung, ein Auge auf die Stichprobengröße zu haben. Teilweise werden bei der Datenvorbereitung einige Untersuchungseinheiten aus der Analyse exkludiert und damit sinkt die Stichprobengröße \\(N\\). Wenn wir Auswertungen machen, in denen wir Ergebnisobjekte bekommen (z.B. bei der Regression mit lm()), können wir die Information zu \\(N\\) daraus ablesen. Dazu klicken wir auf das Ergebnisobjekt im Environment (z.B. lm_kontr). Unter model sehen wir die Dimensionalität des genutzten Teil des Datensatzes und können anhand der Anzahl der Zeilen \\(N\\) ablesen (z.B. bei lm_kontr: [116 x 3] d.h. 116 Fälle). In lm() wird listwise deletion angewendet, d.h. dass jede Zeile, die mindestens ein Missing enthält, aus der Analyse ausgeschlossen wird. Mehr zu Fehlenden Werten im gleichnamigen Kapitel. 8.7.3 Replizierbarkeit Wir sollten unsere R-Skripte generell großzügig kommentieren (mit #), damit wir (und ggf. auch Dritte) schnell nachvollziehen können, was wir da eigentlich gemacht haben. Es lohnt sich auch, wenn man einen Datensatz (teil-)aufbereitet hat, diesen zu speichern, d.h. als neue Datei außerhalb von R zu exportieren (z.B. wenn man einen Datensatz vom Wide- ins Long-Format gebracht hat). Außerdem ist es sinnvoll, alle Schritte der Datenvorbereitung sowie Datenauswertung im gleichen Programm durchzuführen, um möglichen Kompatibilitätsproblemen zwischen verschiedenen Programmen vorzubeugen. 8.8 Übung Im Folgenden wollen wir einige Aufgaben bearbeiten, die in den Bereich der Datenvorbereitung fallen. Dazu gehören u.a. das Extrahieren und Sortieren von Daten, die Änderung der Kodierung von Daten sowie das Erstellen von Summary-Variablen. Zuallererst sollten wir uns jedoch immer mit dem genutzten Datensatz vertraut machen. Hier finden wir das Einführungsskript zu Datenvorbereitung. Dazu nutzen wir einen Datensatz, der im Rahmen eines Projektes zur Untersuchung des Zusammenhangs des Bedürfnisses nach Privatsphäre und verschiedenen Persönlichkeitseigenschaften erhoben wurde. Mehr Informationen zum Projekt und zur Publikation finden wir hier. Den Datensatz sowie das dazugehörige Codebuch finden wir im Open Science Framework. Mehr Informationen zu OSF, der Replikationskrise und der Open Science Bewegung finden wir hier. Den Datensatz können wir, nachdem wir ihn heruntergeladen haben, folgendermaßen in R einlesen: data &lt;- read.csv(&quot;Dateipfad/data.csv&quot;) # hier den eigenen Dateipfad einfügen So sollte der Datensatz aussehen: 8.8.1 Übung 1: Erste Schritte Zuallererst wollen wir uns mit dem Datensatz vertraut machen. Dazu benötigen wir das Codebuch, welches uns Informationen über die erhobenen Variablen sowie deren Messung gibt. Am besten überfliegen wir das Codebuch und den Datensatz einmal, um uns damit vertraut zu machen, bevor wir die nachfolgenden Aufgaben bearbeiten. Achtung: Es sind nicht alle Variablen, die im Codebuch auftauchen, auch im Datensatz. 1.) Es gibt zwei Variablen im Datensatz, die nicht im Codebuch zu finden sind. Welche sind das? Lösung Die Variable sex taucht nicht im Datensatz auf. Nur die Variable male, welche die Ausprägungen 0 und 1 besitzt. Schätzungsweise soll mit beiden dieselbe Information koderit werden: das biologische Geschlecht der befragten Personen. Die Variable time taucht nicht im Codebuch auf. Möglicherweise ist das die individuelle Bearbeitungszeit für den Fragebogen in Sekunden. Vor der Nutzung der der Variablen time müssten wir deren Bedeutung klären. 2.) Wie viele Variablen und Beobachtungen enthält der Datensatz? Tipp Standardmäßig sind Variablen die Spalten und Beobachtungen die Zeilen eines Datensatzes. Lösung Wir finden die Information im R Studio Feld Environment … … oder indem wir folgende Funktionen nutzen: ncol(data) # Variablen = Anzahl der Spalten ## [1] 70 nrow(data) # Beobachtungen (Personen; N) = Anzahl der Zeilen ## [1] 296 3.) Liegen alle Variablen in einem ihrem Messniveau angemessenen Datentyp vor? Tipp Im Codebuch finden wir Informationen zu den Variablen. Über die Funktionen str() bekommen wir Informationen zum Datentyp. Lösung str(data) ## &#39;data.frame&#39;: 296 obs. of 70 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ male : int 0 1 0 0 0 0 1 1 0 0 ... ## $ age : int 19 19 20 19 22 20 20 18 19 19 ... ## $ inc : int 2 3 1 2 3 2 1 1 1 3 ... ## $ time : int 2456 1414 828 1043 1806 1133 1625 2319 7129 1343 ... ## $ pri_nee_gen_1: int 6 7 5 6 4 5 6 5 4 4 ... ## $ pri_nee_gen_2: int 6 2 5 3 5 4 6 6 3 4 ... ## $ pri_nee_gen_3: int 6 6 4 6 7 6 5 7 4 6 ... ## $ pri_nee_gen_4: int 7 7 5 6 7 6 7 7 6 6 ... ## $ pri_nee_soc_1: int 2 4 5 2 5 4 5 4 1 6 ... ## $ pri_nee_soc_2: int 4 5 4 2 5 4 6 4 2 6 ... ## $ pri_nee_soc_3: int 3 6 4 3 5 3 4 6 2 6 ... ## $ pri_nee_soc_4: int 1 6 4 2 4 2 5 4 2 4 ... ## $ pri_nee_soc_5: int 1 4 4 5 4 2 6 6 3 6 ... ## $ pri_nee_soc_6: int 1 3 5 5 2 4 4 4 2 2 ... ## $ pri_nee_soc_7: int 1 1 3 3 1 2 2 6 1 2 ... ## $ pri_nee_soc_8: int 1 7 4 2 2 2 2 6 3 2 ... ## $ pri_nee_soc_9: int 1 3 4 2 6 5 6 7 1 6 ... ## $ pri_nee_int_1: int 1 1 4 4 2 4 2 5 2 5 ... ## $ pri_nee_int_2: int 1 6 5 5 1 2 5 5 1 3 ... ## $ pri_nee_int_3: int 7 7 5 5 6 2 4 3 3 6 ... ## $ pri_nee_int_4: int 3 6 2 7 5 5 4 4 4 6 ... ## $ pri_nee_int_5: int 5 6 4 7 2 5 2 3 6 6 ... ## $ pri_nee_int_6: int 3 6 3 5 2 3 3 2 3 3 ... ## $ pri_nee_int_7: int 7 7 5 6 2 5 6 6 6 4 ... ## $ pri_nee_int_8: int 1 7 4 5 5 3 5 5 6 3 ... ## $ pri_nee_int_9: int 4 7 3 3 5 5 5 5 4 3 ... ## $ soc_1 : int 5 5 3 2 2 3 2 5 3 2 ... ## $ soc_2 : int 4 4 6 6 6 6 5 2 6 6 ... ## $ soc_3 : int 4 4 2 3 2 2 5 6 4 6 ... ## $ soc_4 : int 4 6 4 7 6 6 3 5 3 4 ... ## $ soc_5 : int 7 5 2 6 4 2 2 4 2 3 ... ## $ soc_6 : int 4 5 6 5 2 6 2 4 3 4 ... ## $ soc_7 : int 3 2 2 2 2 2 2 5 3 2 ... ## $ soc_8 : int 3 4 6 6 5 4 2 2 7 5 ... ## $ itg_1 : int 1 1 2 3 2 3 1 4 3 1 ... ## $ itg_2 : int 2 1 4 4 4 6 2 4 1 1 ... ## $ itg_3 : int 5 7 4 5 4 5 4 4 7 1 ... ## $ itg_4 : int 2 7 6 2 6 6 7 4 3 7 ... ## $ itg_5 : int 1 1 2 1 2 2 1 4 1 1 ... ## $ itg_6 : int 4 1 2 3 2 2 4 4 2 1 ... ## $ itg_7 : int 1 6 2 2 3 2 1 4 6 1 ... ## $ itg_8 : int 5 2 4 5 4 4 6 4 3 1 ... ## $ itg_9 : int 5 1 1 2 6 5 2 4 7 2 ... ## $ itg_10 : int 5 2 2 7 6 5 7 4 7 6 ... ## $ itg_11 : int 4 4 4 5 5 5 6 4 4 1 ... ## $ anx_1 : int 1 1 3 5 5 2 6 5 3 6 ... ## $ anx_2 : int 5 4 3 1 5 6 3 3 5 5 ... ## $ anx_3 : int 1 1 5 3 3 2 6 3 2 6 ... ## $ anx_4 : int 4 3 3 3 3 3 5 1 7 2 ... ## $ anx_5 : int 6 6 2 5 5 3 6 3 1 2 ... ## $ anx_6 : int 6 7 3 3 5 6 2 5 6 3 ... ## $ anx_7 : int 3 6 5 6 4 2 2 1 2 5 ... ## $ anx_8 : int 6 4 4 3 5 7 4 6 7 2 ... ## $ ria_1 : int 1 7 5 5 6 3 4 3 7 6 ... ## $ ria_2 : int 5 7 5 4 4 6 6 5 6 6 ... ## $ ria_3 : int 1 6 5 6 6 3 3 5 5 3 ... ## $ ria_4 : int 6 3 3 4 5 5 3 4 6 6 ... ## $ ria_5 : int 4 2 5 6 4 2 6 6 5 5 ... ## $ ria_6 : int 5 6 3 3 5 6 2 3 4 5 ... ## $ ria_7 : int 5 7 5 4 5 5 1 7 5 6 ... ## $ ria_8 : int 6 7 5 5 5 6 6 5 4 6 ... ## $ tra_1 : int 5 7 3 3 2 5 3 4 4 5 ... ## $ tra_2 : int 6 2 6 6 6 4 6 4 7 5 ... ## $ tra_3 : int 5 7 4 3 5 5 5 4 5 5 ... ## $ tra_4 : int 7 6 5 7 4 5 6 4 5 2 ... ## $ tra_5 : int 5 7 4 3 5 5 2 4 4 6 ... ## $ tra_6 : int 7 1 4 6 6 5 6 4 6 2 ... ## $ tra_7 : int 4 7 4 3 5 4 2 4 2 6 ... ## $ tra_8 : int 3 7 4 3 4 5 5 4 5 6 ... Alle Variablen liegen als integer vor. Für die Fragebogenitems (pri_nee_, soc_, itg_, anx_, ria_ und tra_), die intervallskaliert sein sollen, ist das korrekt. Die soziodemographischen Variablen male und inc hingegen sind nominal- bzw. ordinalskaliert. Das bedeutet, dass sie noch faktorisiert werden müssen, um in R als solche erkannt zu werden. # nominalskaliert (unsortierter Faktor): data$male &lt;- factor(data$male) str(data$male) ## Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 1 1 1 2 2 1 1 ... Achtung: Nicht verwirren lassen: Der Faktor male hat die Kodierungen 0 und 1 (wie schon die integer-Variable vorher), aber die interne Kodierung des Faktors ist 1 und 2. # ordinalskaliert (sortierter Faktor): data$inc &lt;- factor(data$inc, ordered=TRUE) str(data$inc) ## Ord.factor w/ 5 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 2 3 1 2 3 2 1 1 1 3 ... levels(data$inc) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; Achtung: Aus dem Codebuch ist leider nicht ersichtlich, welche Kategorie (z.B. &lt; $500) für welche Kodierung steht (z.B. 1). Wir gehen hier davon aus, dass beide aufsteigend gepaart wurden, z.B. &lt; $500 = 1, aber die interne Kodierung eines Faktors beginnt bei 1, d.h. in unserem Fall gibt es 1 und 2. 4.) Wie heißt die Variable, die kodiert, inwieweit die befragte Person gerne viele Menschen um sich herum hat? Welche Antwortoption auf der Messskala (hiermit ist nicht die Kodierung der Daten gemeint) haben die meisten befragten Personen angekreuzt? Tipp 1 Den Namen der Variablen sowie deren Messskala finden wir im Codebuch. Die Häufigkeiten der jeweiligen Ausprägungen der Variablen bringen wir in R in Erfahrung. Tipp 2 Mit der Funktion table() können wir uns die Häufigkeiten der Ausprägungen einer Variablen ausgeben lassen. Die Funktion ist auch bereits sehr hilfreich, um einen schnellen Überblick über die möglichen Ausprägungen zu bekommen. Lösung Die Variable heißt soc_2 und hat eine Messskala, welche von -3 bis 3 (inklusive 0) geht (Codebuch S.20). table(data$soc_2) ## ## 1 2 3 4 5 6 7 ## 2 14 36 69 72 61 27 Die Kodierung 5 kommt am häufigsten vor. Die meisten befragten Personen haben damit eine 1 auf der Messskala angegeben. 5.) Gibt es Werte von Variablen im Datensatz, die unplausibel erscheinen? Wenn ja, entferne die entsprechenden Personen aus dem Datensatz. Tipp 1 Hiervoll ist es sinnvoll, sich eine Übersicht der Ausprägungen aller Variablen anzuschauen und diese ggf. mit den Angaben im Codebuch zu vergleichen. Tipp 2 Es gibt einen Wert einer Variablen der heraussticht. Lösung sapply(sapply(data, unique), sort, na.last=TRUE) # sortierte Ausprägungen der Variablen ## $id ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## [109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 ## [127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 ## [163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 ## [181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ## [199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 ## [217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 ## [235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 ## [253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 ## [271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 ## [289] 289 290 291 292 293 294 295 296 ## ## $male ## [1] 0 1 &lt;NA&gt; ## Levels: 0 1 ## ## $age ## [1] 9 18 19 20 21 22 23 28 29 56 NA ## ## $inc ## [1] 1 2 3 4 5 &lt;NA&gt; ## Levels: 1 &lt; 2 &lt; 3 &lt; 4 &lt; 5 ## ## $time ## [1] 3 5 7 8 9 10 12 13 21 31 ## [11] 86 129 165 194 240 259 313 351 369 371 ## [21] 384 439 443 455 456 458 484 539 562 585 ## [31] 587 593 601 602 606 624 638 639 663 677 ## [41] 694 706 725 741 760 778 814 815 828 829 ## [51] 840 847 851 852 875 877 890 892 899 910 ## [61] 934 986 991 992 999 1035 1039 1040 1043 1052 ## [71] 1056 1061 1071 1073 1075 1077 1088 1090 1094 1097 ## [81] 1104 1118 1119 1133 1141 1143 1144 1148 1152 1155 ## [91] 1157 1158 1166 1167 1176 1177 1187 1197 1200 1209 ## [101] 1211 1219 1220 1222 1227 1236 1243 1248 1250 1255 ## [111] 1258 1259 1265 1290 1292 1303 1313 1314 1316 1318 ## [121] 1319 1332 1343 1345 1349 1363 1367 1372 1376 1387 ## [131] 1388 1397 1402 1405 1411 1414 1419 1425 1438 1448 ## [141] 1453 1460 1465 1481 1492 1496 1497 1499 1514 1524 ## [151] 1525 1560 1581 1595 1596 1602 1606 1625 1628 1633 ## [161] 1651 1655 1656 1660 1669 1671 1672 1683 1695 1702 ## [171] 1706 1713 1714 1716 1732 1734 1735 1745 1753 1756 ## [181] 1772 1778 1780 1803 1806 1815 1820 1831 1845 1847 ## [191] 1869 1879 1889 1905 1930 1932 1945 1949 1985 1994 ## [201] 2022 2077 2125 2135 2139 2142 2144 2183 2185 2197 ## [211] 2248 2255 2319 2342 2348 2360 2373 2393 2420 2447 ## [221] 2456 2493 2495 2515 2518 2523 2537 2597 2606 2649 ## [231] 2654 2673 2759 2769 2927 3204 3224 3273 3386 3466 ## [241] 3739 3803 3808 3858 3913 4084 4436 4702 4748 5031 ## [251] 5108 5229 5469 5927 5944 6064 6282 6598 6685 6849 ## [261] 7129 7246 8016 8817 9274 12533 13928 17371 17973 22834 ## [271] 50047 54973 96391 101191 110358 192301 197593 342508 590589 688498 ## ## $pri_nee_gen_1 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_gen_2 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_gen_3 ## [1] 2 3 4 5 6 7 NA ## ## $pri_nee_gen_4 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_1 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_2 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_3 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_4 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_5 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_6 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_7 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_8 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_soc_9 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_int_1 ## [1] 1 2 3 4 5 6 NA ## ## $pri_nee_int_2 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_int_3 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_int_4 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_int_5 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_int_6 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_int_7 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_int_8 ## [1] 1 2 3 4 5 6 7 NA ## ## $pri_nee_int_9 ## [1] 1 2 3 4 5 6 7 NA ## ## $soc_1 ## [1] 1 2 3 4 5 6 7 NA ## ## $soc_2 ## [1] 1 2 3 4 5 6 7 NA ## ## $soc_3 ## [1] 1 2 3 4 5 6 7 NA ## ## $soc_4 ## [1] 1 2 3 4 5 6 7 NA ## ## $soc_5 ## [1] 1 2 3 4 5 6 7 NA ## ## $soc_6 ## [1] 1 2 3 4 5 6 7 NA ## ## $soc_7 ## [1] 1 2 3 4 5 6 7 NA ## ## $soc_8 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_1 ## [1] 1 2 3 4 5 6 NA ## ## $itg_2 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_3 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_4 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_5 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_6 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_7 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_8 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_9 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_10 ## [1] 1 2 3 4 5 6 7 NA ## ## $itg_11 ## [1] 1 2 3 4 5 6 7 NA ## ## $anx_1 ## [1] 1 2 3 4 5 6 7 NA ## ## $anx_2 ## [1] 1 2 3 4 5 6 7 NA ## ## $anx_3 ## [1] 1 2 3 4 5 6 7 NA ## ## $anx_4 ## [1] 1 2 3 4 5 6 7 NA ## ## $anx_5 ## [1] 1 2 3 4 5 6 7 NA ## ## $anx_6 ## [1] 1 2 3 4 5 6 7 NA ## ## $anx_7 ## [1] 1 2 3 4 5 6 7 NA ## ## $anx_8 ## [1] 1 2 3 4 5 6 7 NA ## ## $ria_1 ## [1] 1 2 3 4 5 6 7 NA ## ## $ria_2 ## [1] 1 2 3 4 5 6 7 NA ## ## $ria_3 ## [1] 1 2 3 4 5 6 7 NA ## ## $ria_4 ## [1] 1 2 3 4 5 6 7 NA ## ## $ria_5 ## [1] 1 2 3 4 5 6 7 NA ## ## $ria_6 ## [1] 1 2 3 4 5 6 7 NA ## ## $ria_7 ## [1] 1 2 3 4 5 6 7 NA ## ## $ria_8 ## [1] 1 2 3 4 5 6 7 NA ## ## $tra_1 ## [1] 1 2 3 4 5 6 7 NA ## ## $tra_2 ## [1] 1 2 3 4 5 6 7 NA ## ## $tra_3 ## [1] 1 2 3 4 5 6 7 NA ## ## $tra_4 ## [1] 1 2 3 4 5 6 7 NA ## ## $tra_5 ## [1] 1 2 3 4 5 6 7 NA ## ## $tra_6 ## [1] 1 2 3 4 5 6 7 NA ## ## $tra_7 ## [1] 1 2 3 4 5 6 7 NA ## ## $tra_8 ## [1] 1 2 3 4 5 6 7 NA which(data$age == 9) ## [1] 232 Die Person mit der Zeile 232 hat (vermutlich versehentlich) als Alter 9 Jahre angegeben. Wir entfernen diese Person aus dem Datensatz. data &lt;- data[-which(data$age == 9),] 6.) Enthält der Datensatz fehlende Werte (“Missings”; NA) und wenn ja, wie viele insgesamt und auf welchen Variablen? Tipp Das Thema “Fehlende Werte” wird im Kapitel zur Datenvorbereitung nur kurz angerissen, weil es ein eigenständiges Kapitel dazu gibt. Lösung anyNA(data) # prüft ob mind. ein Missing im Datensatz ## [1] TRUE Ja, data enthält fehlende Werte. table(is.na(data)) # absolute Anzahl fehlender Werte ## ## FALSE TRUE ## 19337 1313 table(is.na(data))[2]/(table(is.na(data))[1] + table(is.na(data))[2]) # relative Anzahl fehlender Werte ## TRUE ## 0.06358354 colSums(is.na(data)) # Übersicht über Anzahl Missings für alle Variablen ## id male age inc time ## 0 24 25 28 0 ## pri_nee_gen_1 pri_nee_gen_2 pri_nee_gen_3 pri_nee_gen_4 pri_nee_soc_1 ## 19 19 19 19 19 ## pri_nee_soc_2 pri_nee_soc_3 pri_nee_soc_4 pri_nee_soc_5 pri_nee_soc_6 ## 19 19 19 19 19 ## pri_nee_soc_7 pri_nee_soc_8 pri_nee_soc_9 pri_nee_int_1 pri_nee_int_2 ## 19 19 19 21 21 ## pri_nee_int_3 pri_nee_int_4 pri_nee_int_5 pri_nee_int_6 pri_nee_int_7 ## 21 21 21 21 21 ## pri_nee_int_8 pri_nee_int_9 soc_1 soc_2 soc_3 ## 21 21 15 15 15 ## soc_4 soc_5 soc_6 soc_7 soc_8 ## 15 15 16 15 15 ## itg_1 itg_2 itg_3 itg_4 itg_5 ## 21 21 21 21 21 ## itg_6 itg_7 itg_8 itg_9 itg_10 ## 21 21 21 21 21 ## itg_11 anx_1 anx_2 anx_3 anx_4 ## 21 18 18 18 18 ## anx_5 anx_6 anx_7 anx_8 ria_1 ## 18 18 18 18 17 ## ria_2 ria_3 ria_4 ria_5 ria_6 ## 17 17 17 17 17 ## ria_7 ria_8 tra_1 tra_2 tra_3 ## 17 17 21 21 21 ## tra_4 tra_5 tra_6 tra_7 tra_8 ## 21 21 21 21 21 Jede Variable mit Ausnahme von id und time enthält fehlende Werte. 7.) Was ist die höchste Anzahl an fehlenden Werten von einer Person und welche Person hat bzw. welche Personen haben die meisten fehlenden Werte? Tipp In der letzten Aufgabe haben wir uns variablenweise Missings angeschaut mit colSums(). Nun wollen wir uns zeilenweise Missings anschauen. Lösung max(rowSums(is.na(data))) # maximale Anzahl Missings (absoluter Wert) ## [1] 68 max(rowSums(is.na(data))) / ncol(data) # Max-Anzahl Missings einer Person / Anzahl aller Variablen (relativer Wert) ## [1] 0.9714286 Fehlende Werte auf 68 Variablen ist die Höchstzahl an fehlenden Werten pro Person. Das entspricht einem Prozentsatz von ca. 97% fehlenden Werten. # Personen mit diesen Zeilen haben die meisten Missings: names(which(rowSums(is.na(data)) == max(rowSums(is.na(data))))) ## [1] &quot;63&quot; &quot;66&quot; &quot;78&quot; &quot;133&quot; &quot;134&quot; &quot;136&quot; &quot;139&quot; &quot;180&quot; &quot;199&quot; &quot;206&quot; &quot;262&quot; &quot;267&quot; ## [13] &quot;268&quot; &quot;281&quot; &quot;283&quot; Wir müssen hier names() nutzen, damit wir die Zeilennamen ausgegeben bekommen da wir sonst einen benannten Vektor ausgegeben bekommen. Man sollte überlegen, wie mit den Daten dieser 15 Personen bei etwaigen Analysen umzugehen wäre. Wenn die Daten MCAR sind, könnten wir diese Personen aus den Analysen entfernen. Mehr Infos zu Fehlenden Werten im gleichnamigen Kapitel. 8.8.2 Übung 2: Extrahieren von Daten Nun wollen wir einzelne Daten aus dem Datensatz extrahieren. Dabei interessieren uns entweder Variablen mit bestimmten Ausprägungen (von Personen) oder Personen mit bestimmten Ausprägungen (auf Variablen). Achtung: Achte darauf, immer auch die id-Variable mit zu extrahieren, um Beobachtungseinheiten identifizieren zu können, auch wenn das nicht jedes mal erneut in der Aufgabenbeschreibung steht. 1.) Extrahiere alle demographischen Variablen aus data. Tipp 1 Im Codebuch (S. 26ff) steht, welche Variablen zu den demographischen Angaben zählen. Achtung: Im Codebuch steht die Variable sex, welche im Datensatz nicht enthalten ist. Alternativ gibt es die Variable male. Tipp 2 Es gibt drei demographische Variablen in data (aber mehr im Codebuch). Lösung library(dplyr) select(data, id, male, age, inc) 2.) Extrahiere alle Variablen, die grundlegende Bedürfnisse (General Needs) erfassen. Tipp 1 Die Variablen haben denselben Wortstamm: gen. Tipp 2 Es gibt vier Items zu grundlegenden Bedürfnissen. Lösung data[,c(1, # id grep(&quot;gen&quot;, names(data)))] # general needs 3.) Extrahiere alle Variablen, die gesellschaftliche (Societal Needs) und interpersonelle (Interpersonal Needs) Bedürfnisse erfassen. Tipp 1 Die Variablen haben zwar denselben Wortstamm – nee – aber den haben die Variablen zu grundlegenden Bedürfnissen (General Needs) auch. Tipp 2 Es gibt insgesamt 18 Items zu gesellschaftlichen und interpersonellen Bedürfnissen (jeweils 9). Lösung # Vorauswahl von Bedürfnis-Variablen mittels des gemeinsamen Wortstammes: all_needs &lt;- data[,c(1, # id grep(&quot;nee&quot;, names(data)))] # alle Bedürfnis-Variablen # Auswahl der gewollten Variablen aus den Bedürfnis Variablen (neben &quot;id&quot;) .. # .. solche, die &quot;c&quot; oder &quot;t&quot; im Namen haben (trifft nur auf General needs nicht zu): all_needs[grep(&quot;id|c|t&quot;, names(all_needs))] 4.) Extrahiere alle Personen, die weniger als $500 oder mehr als $5.000 monatlich zur Verfügung haben. Tipp Um beide Bedingungen (&lt; $500 und &gt; $5.000) abzufragen, können wir den logischen Operator | (“oder”) nutzen. Lösung Zuerst vergleichen wir die Angaben zu den Ausprägungen von inc im Codebuch (S. 28) und der Kodierung in R. Laut Codebuch ist &lt; $500 die erste Ausprägung; &gt; $5.000 die letzte. Nun schauen wir, mit welchen Kodierungen diese korrespondieren. table(data$inc) ## ## 1 2 3 4 5 ## 148 74 24 15 6 DIe beiden werden mit 1 und 5 kodiert. Anschließend wenden wir dieses Wissen auf unsere Selektion an. filter(data, inc == 1 | inc == 5) 5.) Extrahiere die “Extremkreuzer” (Personen, die nur die niedrigste oder höchste Ausprägung einer Frage ankreuzen) aus dem Geselligkeits-Fragebogen (Sociability). Tipp 1 Wir können wieder den logischen Operator | (“oder”) nutzen, um jeweils Personen mit der niedrigste oder höchsten Ausprägung eines Items auszuwählen. Tipp 2 Es gibt insgesamt zwei Extremkreuzer. Lösung Im Codebuch (S. 20) sehen wir, dass für alle Items des Geselligkeits-Fragebogen dieselbe Skale, welche von -3 bis 3 geht, genutzt wurde. Nun schauen wir uns am Beispiel eines Items an, wie diese in R kodiert werden. # niedrigste und höchste Ausprägung in Erfahrung bringen: table(data$soc_1) ## ## 1 2 3 4 5 6 7 ## 23 71 54 55 52 21 4 # Fall-Selektion anwenden: filter(data, soc_1 == 1 | soc_1 == 7, soc_2 == 1 | soc_2 == 7, soc_3 == 1 | soc_3 == 7, soc_4 == 1 | soc_4 == 7, soc_5 == 1 | soc_5 == 7, soc_6 == 1 | soc_6 == 7, soc_7 == 1 | soc_7 == 7, soc_8 == 1 | soc_8 == 7) Auf den Seiten 6 bis 7 sehen wir die Items des Geselligkeits-Fragebogens (oben rechts ist der Pfeil). 6.) Extrahiere die Items des Fragebogens zu Integrität (Integrity) für alle Personen, die 18 Jahre alt sind. Tipp 1 Die Reihenfolge der Auswahl, d.h. ob zuerst Variablen oder zuerst Fälle selektiert werden, ist eigentlich irrelevant, aber wenn wir erst die Fälle selektieren und dann die Variablen müssen wir die Variable age nicht wieder aus dem finalen Datensatz entfernen. Tipp 2 Der finale Datensatz besteht aus 12 Variablen (davon sind 11 die Integritäts-Items) von 54 Personen. Lösung # Fälle selektieren: data_18 &lt;- filter(data, age == 18) # Variablen selektietren: all_itg_items &lt;- data_18[,c(1, # id grep(&quot;itg&quot;, names(data_18)))] # Integrität Items 7.) Extrahiere alle Personen, die im Fragebogen zu interpersonellen Bedürfnissen (Needs, Interpersonal) über dem Gesamtmittelwert (Mittelwert über alle Personen im Datensatz) liegen. Entferne Personen mit mindestens einem fehlenden Wert auf diesen Items aus der Analyse. Hier muss zusätzlich eine Summary-Variable (Übung 5) erstellt werden. Achtung: Unser Vorgehen mit fehlenden Wert hier (casewise deletion) ist stark vereinfacht und sollte in echten Analysen nicht ohne Belege für MCAR durchgeführt werden. Tipp 1 Wir müssen 1) alle Items des Fragebogens extrahieren, 2) alle Personen mit mind. einem Missing entfernen, 3) die Items jeweils für jede Person aufsummieren (d.h. individuelle Scores bilden), 4) diese individuellen Scores über alle Personen aufsummieren, um den Gesamtmittelwert zu berechnen, 5) die individuellen Scores mit dem Gesamtmittelwert vergleichen, um nur überdurchschnittliche Personen in unserer finalen Auswahl zu haben. Tipp 2 Im finalen Datensatz befinden sich 140 (von initial 296) Personen. Lösung # 1) Items extrahieren: all_int_items &lt;- data[,c(1, # id grep(&quot;int&quot;, names(data)))] # Int. Bedürfnisse Items # 2) Personen mit mind. einem Missing entfernen: all_int_items &lt;- na.omit(all_int_items) # 21 Personen entfernt # 3) individuelle Scores bilden: library(dplyr) all_int_items &lt;- mutate(all_int_items, score = rowSums(all_int_items)) # 4) Mittelwert individueller Scores berechnen: mean_score &lt;- mean(all_int_items$score) mean_score ## [1] 182.6387 # 5) überdurchschnittliche Personen extrahieren: final_data &lt;- filter(all_int_items, score &gt; mean_score) # 135 Personen entfernt 8.8.3 Übung 3: Sortieren von Daten Im Folgenden wollen wir unseren Datensatz nach den aufsteigenden bzw. absteigenden Ausprägungen auf einer der enthaltenen Variablen sortieren. 1.) Sortiere data (primär) nach den aufsteigenden Ausprägungen in age und (sekundär) nach dem Geschlecht (Frauen zuerst). Achtung: Da die Kodierung der Variablen male im Codebuch nicht geklärt wird, gehen wir hier standarmäßig davon aus, dass 0 für nein und 1 für ja steht. Lösung Da Frauen zuerst in der sekundären Sortierung vorkommen sollen, können wir beide Variablen aufsteigend sortieren. data[order(data$age, data$male),] # Default: aufsteigende Sortierung 2.) Sortiere data (primär) nach absteigendem Einkommen und (sekundär, tertiär, ect.) nach den aufsteigenden Ausprägungen auf den Traditionalismus-Items (Traditionalism) in ihrer natürlichen Reihenfolge (beginnend bei _1, endend bei _8). Tipp 1 Mit der Funktion order() können wir nur jeweils auf- oder absteigend für alle angegebenen Variablen sortieren. Daher sollten wir für diese Aufgabe auf andere Funktionen, z.B. arrange() aus dem Paket dplyr zurückgreifen. Tipp 2 Um besser beurteilen zu können, ob die Sortierung erfolgreich war, ist es sinnvoll, zuerst einen neuen Datensatz nur aus den relevanten Variablen (inklusive id) zu erstellen. Lösung library(dplyr) # neuer Datensatz nur mit relevanten Variablen: data_tra &lt;- data[,c(1, # id 4, # inc grep(&quot;tra&quot;, names(data)))] # Sortierung: arrange(data_tra, -inc, tra_1, tra_2, tra_3, tra_4, tra_5, tra_6, tra_7, tra_8) # mit einem &quot;-&quot; sortieren wir absteigend ## Warning in Ops.ordered(inc): &#39;-&#39; is not meaningful for ordered factors 8.8.4 Übung 4: Änderung der Kodierung von Daten Nun wollen wir die Kodierungen einiger Variablen ändern bzw. Variablen mit neuen Kodierungen erstellen. 1.) Erstelle eine neue Variable income, die das monatliche Einkommen der befragten Personen kodiert, und dafür die Kategorien aus dem Codebuch (S.28) nutzt. Tipp Um besser beurteilen zu können, ob die Kodierung erfolgreich war, ist es sinnvoll, zuerst einen neuen Datensatz bestehend aus inc und id zu erstellen. Lösung # neuen Datensatz erstellen: library(dplyr) data_inc &lt;- select(data, id, inc) # neue Variable income erstellen: rec &lt;- c(&quot;1=&#39;&lt;$500&#39;; 2=&#39;$500-$1000&#39;; 3=&#39;$1000-$2000&#39;; 4=&#39;$2000-$4000&#39;; 5=&#39;&gt;$5000&#39;&quot;) library(car) data_inc$income &lt;- recode(data_inc$inc, recodes=rec) Die Variable rec, welche die Überführung der bestehenden in die neue Kodierung enthält, wurde nur aus darstellerischen Gründen erstellt. Der Inhalt kann auch direkt dem Parameter recodes übergeben werden. 2.) Erstelle eine neue Variable sex, die analog zur gleichnamigen Variablen im Codebuch das biologische Geschlecht kodiert. Gehe für diese Aufgabe davon aus, dass alle Personen, die einen fehlender Wert auf male haben, einer Kategorie other (nicht-binäres Geschlecht) zugeordnet werden. Tipp Um besser beurteilen zu können, ob die Kodierung erfolgreich war, ist es sinnvoll, zuerst einen neuen Datensatz bestehend aus male und id zu erstellen. Lösung library(dplyr) # neuen Datensatz erstellen: data_sex &lt;- select(data, id, male) # neue Variable sex erstellen: data_sex$sex &lt;- case_when(data_sex$male == 0 ~ &quot;female&quot;, data_sex$male == 1 ~ &quot;male&quot;, is.na(data_sex$male) ~ &quot;other&quot;) 3.) Rekodiere die negativ kodierten Items des Fragebogens zu Risikovermeidung (Risk Avoidance). Tipp 1 Die negativ kodierten Items sind im Codebuch jeweils mit einem * markiert. Tipp 2 Es ist sinnvoll zur Überprüfung der Aufgabe, einen neuen Datensatz bestehend aus den negativ kodierten Items und ihrem rekodierten Pendant (und natürlich id) zu erstellen. Lösung # neuen Datensatz erstellen: library(dplyr) data_ria &lt;- select(data, id, ria_1, ria_3, ria_5) # negativ kodierte Items # neue rekodierte Variablen erstellen: library(car) data_ria$ria_1_rec &lt;- recode(data_ria$ria_1, recodes=&quot;1=7; 2=6; 3=5; 4=4; 5=3; 6=2; 7=1&quot;) data_ria$ria_3_rec &lt;- recode(data_ria$ria_3, recodes=&quot;1=7; 2=6; 3=5; 4=4; 5=3; 6=2; 7=1&quot;) data_ria$ria_5_rec &lt;- recode(data_ria$ria_5, recodes=&quot;1=7; 2=6; 3=5; 4=4; 5=3; 6=2; 7=1&quot;) 8.8.5 Übung 5: Erstellen von Summary-Variablen Nachfolgend wollen wir neue Variablen erstellen, die Informationen aus mehreren Variablen des Datensatzes zusammenfassen. 1.) Erstelle für die verschiedenen Bereiche von Bedürfnissen – Allgemein (General), Gesellschaftlich (Societal) und Interpersonell (Interpersonal) – jeweils separate Summenwerte und einen Gesamtsummenwert (über alle drei Bereiche). Tipp Die Items aller drei Bedürfnis-Bereiche haben denselben Wortstamm: nee. Lösung # neuer Datensatz mit relevanten Variablen: data_need &lt;- data[,c(1, # id grep(&quot;nee&quot;, names(data)))] # Bedürfnis-Items # Summenwerte der drei Bedürfnis-Bereiche und Gesamtsummenwert: library(dplyr) data_need$score_gen &lt;- rowSums(select(data_need, matches(&quot;nee_gen&quot;))) data_need$score_soc &lt;- rowSums(select(data_need, matches(&quot;nee_soc&quot;))) data_need$score_int &lt;- rowSums(select(data_need, matches(&quot;nee_int&quot;))) data_need$score_all &lt;- rowSums(select(data_need, matches(&quot;nee&quot;))) 2.) Erstelle die personenspezifischen Summenwerte der Items, die interpersonelle Bedürfnisse erfassen (Needs, Interpersonal), für alle Personen, die ein monatliches Einkommen von mehr als $1.000 haben. Bei allen Personen, die weniger zur Verfügung haben, soll ein \"/\" in der Summenwert-Variablen stehen. Tipp Zur Bearbeitung der Aufgabe können wir mutate() mit ifelse() kombinieren, um den beiden Bedingungen (monatliches Einkommen von mehr bzw. weniger als $1.000) unterschiedliche Werte zuzuweisen. Lösung # neuer Datensatz mit relevanten Variablen: data_int &lt;- data[,c(1, # id 4, # inc grep(&quot;int&quot;, names(data)))] # Int. Bedürfnisse Items Nun vergleichen wir die Angaben zu den Ausprägungen von inc im Codebuch (S. 28) und der Kodierung in R. Es gibt insgesamt 5 Ausprägungen. Auf unsere Bedingung (mehr als $1.000) treffen drei Ausprägungen zu. Daher ist es codesparender, wenn wir die zwei nicht zutreffenden Ausprägungen (&lt; $500 und $500 - $1000) negieren (!=). Laut Codebuch ist &lt; $500 die erste Ausprägung; $500 - $1000 die zweite. Nun schauen wir, mit welchen Kodierungen diese korrespondieren. table(data$inc) ## ## 1 2 3 4 5 ## 148 74 24 15 6 Sie haben die Kodierungen 1 und 2. Diese Information können wir nun anwenden. # neue Variable mit Summenwert bzw. &quot;/&quot; erstellen library(dplyr) data_int$score &lt;- ifelse(data_int$inc != 1 &amp; data_int$inc != 2, rowSums(select(data_need, matches(&quot;int&quot;))), &quot;/&quot;) # ifelse(Bedingung, trifft zu, trifft nicht zu) Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] readxl_1.3.1 foreign_0.8-82 devtools_2.4.5 usethis_2.1.6 ## [5] ICC_2.4.0 readr_2.1.3 Hmisc_4.7-1 Formula_1.2-4 ## [9] survival_3.2-13 lattice_0.20-45 ggplot2_3.4.0 colorspace_2.0-3 ## [13] psych_2.2.9 car_3.1-1 carData_3.0-5 kableExtra_1.3.4 ## [17] dplyr_1.0.10 htmltools_0.5.3 rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-155 fs_1.5.2 webshot_0.5.4 ## [4] RColorBrewer_1.1-2 httr_1.4.2 profvis_0.3.7 ## [7] tools_4.2.0 backports_1.4.1 bslib_0.4.0 ## [10] utf8_1.2.2 R6_2.5.1 rpart_4.1.16 ## [13] DBI_1.1.2 nnet_7.3-17 urlchecker_1.0.1 ## [16] withr_2.5.0 prettyunits_1.1.1 processx_3.8.0 ## [19] tidyselect_1.2.0 gridExtra_2.3 mnormt_2.1.1 ## [22] compiler_4.2.0 cli_3.4.1 rvest_1.0.2 ## [25] htmlTable_2.4.1 xml2_1.3.3 bookdown_0.29 ## [28] sass_0.4.2 scales_1.2.1 checkmate_2.0.0 ## [31] callr_3.7.2 systemfonts_1.0.4 stringr_1.4.0 ## [34] digest_0.6.30 svglite_2.1.0 base64enc_0.1-3 ## [37] jpeg_0.1-9 pkgconfig_2.0.3 sessioninfo_1.2.2 ## [40] fastmap_1.1.0 highr_0.9 htmlwidgets_1.5.4 ## [43] rlang_1.0.6 rstudioapi_0.13 shiny_1.7.3 ## [46] jquerylib_0.1.4 generics_0.1.2 jsonlite_1.8.3 ## [49] magrittr_2.0.2 interp_1.0-33 Matrix_1.5-1 ## [52] Rcpp_1.0.9 munsell_0.5.0 fansi_1.0.3 ## [55] abind_1.4-5 lifecycle_1.0.3 stringi_1.7.8 ## [58] yaml_2.3.5 pkgbuild_1.3.1 promises_1.2.0.1 ## [61] parallel_4.2.0 crayon_1.5.2 miniUI_0.1.1.1 ## [64] deldir_1.0-6 splines_4.2.0 hms_1.1.1 ## [67] ps_1.7.2 pillar_1.8.1 pkgload_1.3.0 ## [70] glue_1.6.2 evaluate_0.15 latticeExtra_0.6-30 ## [73] remotes_2.4.2 data.table_1.14.4 png_0.1-7 ## [76] vctrs_0.5.0 tzdb_0.3.0 httpuv_1.6.5 ## [79] cellranger_1.1.0 gtable_0.3.0 purrr_0.3.4 ## [82] assertthat_0.2.1 cachem_1.0.6 xfun_0.34 ## [85] mime_0.12 xtable_1.8-4 later_1.3.0 ## [88] viridisLite_0.4.1 tibble_3.1.8 memoise_2.0.1 ## [91] cluster_2.1.2 ellipsis_0.3.2 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["fehlende-werte-1.html", "Chapter 9 Fehlende Werte 9.1 Sind die Missings einheitlich kodiert? 9.2 2. Enthält ein Datensatz Missings? 9.3 3. Wie kann man die Missings zählen (und verorten)? 9.4 Sind die Missings zufällig? 9.5 Wie kann man mit Missings umgehen? 9.6 Neuen Datensatz erstellen, der keine Missings enthält 9.7 Literaturempfehlungen 9.8 FAQ 9.9 Übung", " Chapter 9 Fehlende Werte Einleitung Es kann in der Praxis häufiger vorkommen, dass Datensätze unvollständig sind z.B. wenn Teilnehmende einer Befragung nicht alle Fragen beantworten. Diese fehlenden Einträge nennt man Missings. In R werden sie (bei numeric) zumeist mit NA (not available) gekennzeichnet. Welche Kodierungen für fehlende Werte gibt es neben NA noch? Beim Datentyp numeric (Zahlen) sind fehlende Werte durch NA oder durch NaN (Not a Number) gekennzeichnet. num_vector &lt;- c(1, 4, NA, NaN) num_vector ## [1] 1 4 NA NaN is.na(num_vector) ## [1] FALSE FALSE TRUE TRUE Die Funktion is.na() gibt für jedes Element ein TRUE wenn es fehlt bzw. ein FALSE wenn es vorhanden ist. Wir werden im Laufe des Kapitels verschiedene Kombinationsmöglichkeiten für diese Funktion kennen lernen. NaN entstehen z.B. bei unlösbaren Rechnungen wie Division durch \\(0\\). num_vector[1] &lt;- num_vector[4]/0 num_vector ## [1] NaN 4 NA NaN is.na(num_vector) ## [1] TRUE FALSE TRUE TRUE Beim Datentyp character (Buchstaben, Zeichen) hingegen könnten fehlende Eingaben auch durch \"\" (leere Felder) gekennzeichnet sein z.B. wenn bei Freitextfeldern in einer Umfrage nichts eingegeben wurde. Das wird in R aber nicht automatisch als fehlender Wert erkannt. Achtung: Wir sehen nachfolgend auch verdeutlicht, dass NA eine selbstständige Kodierung ist, und nicht als character kodiert wird. char_vec &lt;- c(NA, &quot;NA&quot;, &quot;&quot;) char_vec ## [1] NA &quot;NA&quot; &quot;&quot; is.na(char_vec) ## [1] TRUE FALSE FALSE Später lernen wir, wie wir Werte auf NA umkodieren können. Wir beschränken uns in vorliegendem Kapitel auf NA in numerischen Datentypen (d.h. quantitative Daten). Fehlende Werte sind grundsätzlich mit drei Schwierigkeiten verbunden: Der Datenausfall führt zu einer reduzierten Stichprobengröße \\(N\\) welche wiederum zu einer verringerten Effizienz bei der Parameterschätzung und geringerer Power führt. Die Auswertung wird erschwert, weil viele statistische Verfahren vollständige Datensätze voraussetzen. Wenn systematische Unterschiede zwischen den beobachteten und den fehlenden Daten bestehen könnten die Parameterschätzungen verzerrt sein. Daher ist es sehr wichtig, sich vor der Auswertung einen Überblick über die fehlenden Werte zu verschaffen. In diesem Kapitel schauen wir uns an, ob und wenn ja, wo und wie viele Missings sich im Datensatz befinden. Außerdem gibt es einen groben Überblick darüber, ob Missings zufällig sind und wie man mit ihnen umgehen kann. Der Umgang mit Missings ist ein komplexes Thema. In diesem Kapitel werden wir uns auf zwei gängige Methoden zum Umgang mit Missings beschränken. In Abhängigkeit der eigenen Fragestellung empfiehlt es sich, passende Methoden zu recherchieren. Beispieldatensatz für dieses Kapitel Das ist der Code für den Datensatz, an dem wir in diesem Kapitel arbeiten werden. Wenn du die Funktionen, die in diesem Kapitel vorgestellt werden, ausprobieren möchtest, führe den Code aus und erstelle den Datensatz. # Data Frame erstellen daten &lt;- matrix(c(-99, 0, 1, 3, 2, 1, 2, 3, 2, 0, NA, 1, 3, 99, 0, 1, 3, 3, 1, 2, 2, 0, 2, 99, 3), nrow = 5, ncol = 5) # in Dataframe umwandeln daten &lt;- data.frame(daten) # Spalten und Zeilen benennen colnames(daten) &lt;- c(&quot;Var_1&quot;, &quot;Var_2&quot;, &quot;Var_3&quot;, &quot;Var_4&quot;, &quot;Var_5&quot;) rownames(daten) &lt;- c(&quot;Vpn_1&quot;, &quot;Vpn_2&quot;, &quot;Vpn_3&quot;, &quot;Vpn_4&quot;, &quot;Vpn_5&quot;) ## Var_1 Var_2 Var_3 Var_4 Var_5 ## Vpn_1 -99 1 NA 1 2 ## Vpn_2 0 2 1 3 0 ## Vpn_3 1 3 3 3 2 ## Vpn_4 3 2 99 1 99 ## Vpn_5 2 0 0 2 3 Anmerkung: Die Variablen sind intervallskaliert mit einer Skala von 0-3. Achtung: Wenn du Variablen, die Missings enthalten, für eine Analyse nutzt, denke immer daran, dass sich damit auch die Stichprobengröße \\(N\\) für diese spezifische Auswertung ändert. 9.1 Sind die Missings einheitlich kodiert? In manchen Anwendungen werden Missings nicht mit NA sondern anderweitig kodiert (z.B. bei Unipark mit 99 oder -99). Daher müssen wir dafür sorgen, dass R Missings auch als solche erkennt. Das gewährleistet man, indem man alle Missings auf NA setzt. Wenn man weiß, wie Missings im Datensatz kodiert sind, kann man gleich zu Wie kann ich die Missings auf NA setzen? springen. Wenn man weiß, dass alle Missings einheitlich mit NA kodiert sind, kann man den ganzen Abschnitt auslassen. 9.1.1 Wie kann ich prüfen, ob die Missings einheitlich kodiert sind? Um herauszufinden, ob auch alle Missings einer Variablen korrekterweise mit NA kodiert sind, vergleichen wir die angegebenen Werte der Variablen in unseren Daten mit den möglichen Ausprägungen der Variablen (die wir zumeist im Codebuch finden; in unserem Fall steht diese Information in der Einleitung). Dazu kombinieren wir die Funktion unique(), die uns alle Werte eines Vektors (einmalig) ausgibt, mit der Funktion sort(), die uns die Werte noch sortiert (standarmäßig aufsteigend). Mit na.last = TRUE gibt uns sort() sogar die Information zum Vorhandensein von Missings am Ende aus. sort(unique(daten$Var_3), na.last=TRUE) ## [1] 0 1 3 99 NA Da wir wissen, dass Var_3 nur die Ausprägungen \\(0,1,2,3\\) annehmen kann, können wir schließen, dass \\(99\\) auch ein Missing sein muss. Wenn man also alle möglichen Ausprägungen der Variablen kennt, kann man auf diese Weise einfach herausfinden, ob noch anderweitig kodierte Missings im Datensatz vorliegen. Wenn der Datensatz sehr groß ist, ist der oben gezeigte Ansatz allerdings sehr mühsam. Dann können wir die Funktion sapply() integrieren, um unique() und sort() auf jede Variable im Datensatz anzuwenden. Diese hat die Form sapply(Daten, Funktion). Da wir zwei Funktionen auf den Datensatz anwenden wollen - unique() und sort() - müssen wir sapply() zweimal anwenden: sapply(sapply(daten, unique), sort, na.last=TRUE) ## $Var_1 ## [1] -99 0 1 2 3 ## ## $Var_2 ## [1] 0 1 2 3 ## ## $Var_3 ## [1] 0 1 3 99 NA ## ## $Var_4 ## [1] 1 2 3 ## ## $Var_5 ## [1] 0 2 3 99 Die hier durchgeführte Überprüfung ist analog zum Plausibilitätscheck im Kapitel zur Datenvorbereitung. In Var_1 gibt es die Ausprägung -99 und in Var_5 die Ausprägung 99, welche keine möglichen Ausprägungen sind. Es ist davon auszugehen, dass das ebenso Kodierungen für fehlende Werte sind. 9.1.2 Wie kann ich die Missings auf NA setzen? Nun wollen wir diese Missings umkodieren. Vorher wollen wir uns noch einmal anschauen, was passiert, wenn man das nicht macht. Wenn man Missings im Datensatz nicht einheitlich auf NA kodiert, nimmt R an, dass es sich um gültige Werte handelt. Das führt dann zu falschen Ergebnissen. Das schauen wir uns exemplarisch einmal am Mittelwert der Spalte Var_3 an. ## Var_1 Var_2 Var_3 Var_4 Var_5 ## Vpn_1 -99 1 NA 1 2 ## Vpn_2 0 2 1 3 0 ## Vpn_3 1 3 3 3 2 ## Vpn_4 3 2 99 1 99 ## Vpn_5 2 0 0 2 3 # Mittelwert vor Umkodierung mean(daten$Var_3, na.rm=TRUE) ## [1] 25.75 Nun kodieren wir die Missings in Var_3 einheitlich um … # Umkodierung für einzelne Variablen daten$Var_3[daten$Var_3 == 99] &lt;- NA == heißt “ist genau” Der Befehl ersetzt in daten Elemente der Spalte Var_3, welche die Ausprägung 99 besitzen, mit NA. ## Var_1 Var_2 Var_3 Var_4 Var_5 ## Vpn_1 -99 1 NA 1 2 ## Vpn_2 0 2 1 3 0 ## Vpn_3 1 3 3 3 2 ## Vpn_4 3 2 NA 1 99 ## Vpn_5 2 0 0 2 3 … und schauen uns den Mittelwert von Var_3 wieder an. # Mittelwert nach Umkodierung mean(daten$Var_3, na.rm=T) ## [1] 1.333333 Hätten wir die Missings nicht einheitlich auf NA kodiert, hätten wir errechnet, dass der Mittelwert von Var_3 25.75 anstatt ~1.33 betragen würde. Wir sehen also, dass es sehr wichtig ist, in Erfahrung zu bringen, ob im Datensatz alle Missings einheitlich auf NA gesetzt sind, und wenn nicht, diese einheitlich zu kodieren, da man sonst falsche Ergebnisse erhält. Jetzt enthält die Spalte Var_3 schon keine Elemente mit der Ausprägung 99 mehr, aber in Var_1 gibt es noch ein -99 und in Var_5 noch ein 99. Um nicht einzeln Spalten und Ausprägungen ansprechen zu müssen, kann man alles in einem Befehl kombinieren. # Umkodierung für den gesamten Datensatz daten[daten == 99 | daten == -99] &lt;- NA | heißt “oder” Hiermit werden im gesamten daten jene Elemente, welche die Ausprägung 99 oder -99 besitzen, durch NA ersetzt. 9.2 2. Enthält ein Datensatz Missings? Wenn wir wissen wie unsere fehlenden Werte kodiert sind, wollen wir in einem nächsten Schritt natürlich wissen, ob ein Datensatz überhaupt Missings enthält. Es gibt zahlreiche Ansätze, um das herauszufinden. Einige davon schauen wir uns einmal genauer an. Bei kleineren Datensätzen ist eine visuelle Inspektion möglich. Dafür nutzt man entweder View() (Großbuchstabe am Anfang beachten!) oder man klickt auf den Datensatz im Environment (oberes rechtes Panel). ## Var_1 Var_2 Var_3 Var_4 Var_5 ## Vpn_1 NA 1 NA 1 2 ## Vpn_2 0 2 1 3 0 ## Vpn_3 1 3 3 3 2 ## Vpn_4 3 2 NA 1 NA ## Vpn_5 2 0 0 2 3 Um zu überprüfen, ob ein Datensatz mindestens einen fehlenden Wert enthält, kann man anyNA() nutzen. Man bekommt ein TRUE (d.h. ja, mindestens ein Missing enthalten) oder FALSE (d.h. nein, keine Missings enthalten) ausgegeben. anyNA(daten) ## [1] TRUE Um einen groben Eindruck davon zu bekommen, welche Elemente fehlen, kann man is.na() nutzen. Der Output besteht aus FALSE oder TRUE für jedes Element des Datensatzes. TRUE bedeutet dabei, dass an dieser Stelle ein Missing ist. is.na(daten) ## Var_1 Var_2 Var_3 Var_4 Var_5 ## Vpn_1 TRUE FALSE TRUE FALSE FALSE ## Vpn_2 FALSE FALSE FALSE FALSE FALSE ## Vpn_3 FALSE FALSE FALSE FALSE FALSE ## Vpn_4 FALSE FALSE TRUE FALSE TRUE ## Vpn_5 FALSE FALSE FALSE FALSE FALSE Achtung: Bei is.na() und anyNA() wird auch NaN (Not a number; entsteht bei unlösbaren Rechnungen) mitgezählt. Da Zweitere aber wesentlich seltener vorkommen, konzentrieren wir uns nur auf NA. Den logischen Vektor, den is.na() erzeugt, kann man mit which() kombinieren, um sich die Positionen der Missings ausgeben zu lassen. Mithilfe des Arguments arr.ind = TRUE lässt man sich die Reihe und die Spalte dieser ausgeben. Ohne arr.ind = TRUE würde man nur die Indizes ausgegeben bekommen. Für Matrizen sind diese weniger leicht zu nutzen, weil die Nummerierung fortlaufend spaltenweise vorliegt. In unserem Fall einer 5 x 5 Matrix heißt das z.B., dass das Element in der 1. Zeile der 3. Spalte (also der eine fehlende Wert) den Index 11 trägt. Bei Vektoren kann man arr.ind = TRUE weglassen, da diese entweder nur aus einer Spalte oder einer Zeile bestehen. which(is.na(daten), arr.ind = TRUE) ## row col ## Vpn_1 1 1 ## Vpn_1 1 3 ## Vpn_4 4 3 ## Vpn_4 4 5 9.3 3. Wie kann man die Missings zählen (und verorten)? Die genaue Anzahl der Missings zu kennen ist wichtig, um ein Gefühl dafür zu kriegen, wie vollständig ein Datensatz ist. Dazu kombinieren wir die is.na()-Funktion mit anderen Funktionen, die FALSE (d.h. vorhandenen Werte) und TRUE (d.h. fehlenden Werte) zählen. 9.3.1 Alle Missings eines Datensatzes Zuerst schauen wir uns die Gesamtanzahl der Missings aller Elemente im Datensatz an. table(is.na(daten)) ## ## FALSE TRUE ## 21 4 9.3.2 Missings in einzelnen Spalten oder Zeilen Spaltenweises Zählen der Missings gibt Informationen über mögliche Probleme mit bestimmten Variablen. Zeilenweises Zählen der Missings gibt beispielsweise Informationen über Teilnehmende, die die Fragen nicht vollständig beantwortet haben. Es ist daher wichtig, sich einen Überblick darüber zu machen, ob sich bei bestimmten Variablen oder bei bestimmten Personen besonders viele Missings häufen. Wenn das der Fall sein sollte, muss man überlegen, wie man damit umgeht (dazu mehr im späteren Verlauf). Wenn wir eine bestimmte Spalte oder Zeile betrachten möchten, können wir die is.na()-Funktion mit der table()-Funktion kombinieren. Zweiteres sorgt dafür, dass wir eine Häufigkeitstabelle von TRUE und FALSE ausgegeben bekommen. Wir können auf verschiedenem Wege auf eine Spalte bzw. Zeile eines Datensatzes referenzieren. table(is.na(daten$Var_1)) # Datensatz$Spaltenname ## ## FALSE TRUE ## 4 1 table(is.na(daten[&quot;Var_1&quot;])) # Datensatz[&quot;Spaltenname&quot;] ## ## FALSE TRUE ## 4 1 table(is.na(daten[&quot;Vpn_1&quot;,])) # Datensatz[&quot;Zeilenname&quot;] ## ## FALSE TRUE ## 3 2 table(is.na(daten[,1])) # Datensatz[,Spaltenindex] ## ## FALSE TRUE ## 4 1 table(is.na(daten[1,])) # Datensatz[,Zeilenindex] ## ## FALSE TRUE ## 3 2 Achtung: Die ersten drei vorgestellten Möglichkeiten, $ und Datensatz[“Spalten- bzw. Zeilenname”], funktionieren nur bei Dataframes, und nicht bei Matrizen. Die Möglichkeit der Indexierung können wir auch bei Matrizen nutzen. Mehr Informationen zu Datenstrukturen finden wir im Kapitel Einführung in R. 9.3.3 Missings in allen Spalten oder Zeilen Wenn man sich einen Überblick über die Missings in allen Variablen bzw. bei allen Personen verschaffen möchte, kann man dafür colSums() bzw. rowSums() mit dem is.na()-Befehl kombinieren. Damit werden spalten- bzw. zeilenweise Summen von TRUE (d.h. den Missings) gebildet. Um die Größenordnung der Missings besser beurteilen zu können, sollte man sich der maximal möglichen Anzahl der Elemente in einer Spalte bzw. Zeile bewusst sein. Diese können wir mit nrow() bzw. ncol() in Erfahrung bringen. # Übersicht der Missings in allen Variablen (Spalten) colSums(is.na(daten)) ## Var_1 Var_2 Var_3 Var_4 Var_5 ## 1 0 2 0 1 # ... im Vergleich zur maximalen Anzahl an Beantwortungen nrow(daten) ## [1] 5 # Übersicht der Missings in allen Personen (Zeilen) rowSums(is.na(daten)) ## Vpn_1 Vpn_2 Vpn_3 Vpn_4 Vpn_5 ## 2 0 0 2 0 # ... im Vergleich zur maximalen Anzahl der beantwortbaren Fragen ncol(daten) ## [1] 5 9.3.3.1 Visualisierung der Missings Mit der Funktion aggr() aus dem Paket VIM kann man sich zwei Plots ausgeben lassen, die den relativen Anteil von Missings in den einzelnen Variablen und die Anzahl an Missings in bestimmten Kombinationen von Variablen (d.h. in den Zeilen) ausgeben. Wenn man summary(aggr()) nutzt, bekommt man sowohl die grafische Visualisierung als auch eine Übersicht der Häufigkeiten. # install.packages(&quot;VIM&quot;) library(VIM) summary(aggr(daten)) ## ## Missings per variable: ## Variable Count ## Var_1 1 ## Var_2 0 ## Var_3 2 ## Var_4 0 ## Var_5 1 ## ## Missings in combinations of variables: ## Combinations Count Percent ## 0:0:0:0:0 3 60 ## 0:0:1:0:1 1 20 ## 1:0:1:0:0 1 20 Im linken Plot sehen wir, dass nur Missings in Var_1, Var_3 und Var_5 vorhanden sind. Außerdem sehen wir auf der \\(y\\)-Achse den relativen Anteil an Fällen in den Variablen. In der Übersicht unter der Tabelle sehen wir die absoluten Häufigkeiten für alle Variablen (Missings per variable). Im rechten Plot sehen wir die vorhanden Kombinationen von Missings in den Variablen. Blau zeigt an, dass kein Missing vorhanden ist; rot zeigt an, dass ein Missing vorhanden ist. Beispielsweise zeigt die unterste Reihe (die komplett blau ist) eine Kombination, in der keine Missings in Variablen vorhanden sind. Rechts daneben sieht man einen Balken, der den Anteil dieser Kombination im Verhältnis zu den anderen Kombinationen darstellt. Der Balken in der untersten Reihe ist der größte, d.h. dass diese Kombination am häufigsten vorkommt und somit die meisten Fälle (Zeilen) im Datensatz keine Missings enthalten. Leider bekommen wir hier keine Häufigkeiten dafür angezeigt. Dazu können wir aber in die unten stehende Übersicht schauen. (Missings in combinations of variables), in der wir absolute und relative Häufigkeiten ausgegeben bekommen. 9.4 Sind die Missings zufällig? Am Anfang des Kapitels wurde bereits erwähnt, dass systematische Missings eine Auswertung verzerren können. Was es aber genau bedeutet, wenn Missings zufällig oder nicht zufällig sind und wie man das überprüfen kann, beleuchten wir in diesem Abschnitt. 9.4.1 Arten von Missings Es gibt grundlegend drei Mechanismen, die zur Entstehung von fehlenden Werten führen können: Missing Completely at Random (MCAR), Missing at Random (MAR) und Missing not at Random (MNAR). Im folgenden schauen wir uns die Definition dieser Arten an. Missing Completely at Random (MCAR) Missings in einer Variable sind völlig zufällig, wenn sie unabhängig von allen anderen Variablen und dem Missing selbst (d.h. der eigentlichen Ausprägung in dieser Variable, die nicht angegeben wurde) sind. Das heißt, dass fehlende Werte zufällig über alle Beobachtungen verteilt sind. Es gibt somit keine systematischen Missing-Muster. Man kann zwar nicht testen, ob ein Missing auf einer Variable aufgrund der eigentlichen Ausprägung in dieser Variablen fehlt (da wir keine Informationen über diese haben), aber man kann testen, ob ein Missing in einer Variable mit den anderen Variablen zusammen hängt. Streng genommen ist also nur ein Teil der Annahme testbar. Missing at Random (MAR) Missings in einer Variable sind zufällig, wenn sie durch andere Variablen erklärt werden können. Es gibt somit systematische Missing-Muster. Das heißt, dass Missings häufiger in einem oder mehreren Teilstichproben des Datensatzes vorkommen können. Nach der Kontrolle für die anderen Variablen hängt die Wahrscheinlichkeit für diese Missings aber nicht mehr von ihren eigentlichen (fehlenden) Ausprägungen ab. Fiktives Beispiel: Männer füllen mit geringerer Wahrscheinlichkeit einen Depressionsfragebogen aus. Das hat aber, nach Kontrolle für Geschlecht, nichts mit ihren Angaben in dem Depressionsfragebogen zu tun. Die MAR-Annahme ist nicht direkt testbar, weil man nicht ausschließen kann, dass die Missings nach Kontrolle für die anderen Variablen nicht mehr von ihren eigentlichen Ausprägungen abhängen (da wir keine Informationen über diese haben). Man kann dafür indirekt kontrollieren, in dem man sich beispielsweise Variablen anschaut, die mit der Variable, in der die Missings sind, hoch korrelieren. Missing not at Random (MNAR) Missings sind nicht zufällig verteilt und können nicht durch andere Variablen erklärt werden. Dass bedeutet, dass die Ausprägung in der Variable, die fehlt, der Grund dafür ist, das sie fehlt. Fiktives Beispiel: Männer füllen einen Depressionsfragebogen aufgrund der mit dem Fragebogen zu erfassenden Höhe der Depressivität nicht aus (z.B. bei besonders hoher Depressivität werden Fragen nicht beantwortet). Was bedeutet “(alle) anderen Variablen”? Die Auffassung darüber, von welchen “anderen Variablen” die Missings in einer Variable unabhängig sein sollen, unterscheidet sich zwischen verschiedenen AutorInnen und ist nicht immer eindeutig. Während einige grob von beobachtbaren Variablen (Vgl. Schafer &amp; Graham, 2002), verfügbaren Variablen (Vgl. Cohen, Cohen, West &amp; Aiken, 2003) oder Variablen im Datensatz, die analysiert werden (Vgl. Little, 1988) sprechen, grenzen Andere diese mehr ein z.B. Variablen, die im Modell spezifiziert sind (Vgl. Allison, 2002). Letztere Definition erleichtert die Überprüfung der Zufälligkeit der Missings (d.h. ob diese MCAR, MAR, oder MNAR sind). Übersicht der Arten von Missings Fehlende Werte sind unabhängig von … MCAR MAR MNAR … allen andere Variablen X … ihren eigentlichen (fehlenden) Ausprägungen X X MCAR ist eine strengere Annahme als MAR. Wenn die Daten MCAR sind, dann sind sie auch MAR. Bei MAR und MNAR kann es zu Parameterverzerrungen kommen, wenn man Methoden nutzt, welche die strengere Annahme MCAR voraussetzen. Bsp.1: Das Löschen von Fällen wenn die Daten nicht MCAR sind. Bsp.2: Die Nutzung der Maximum Likelihood Schätzung oder der multiplen Imputation wenn die Daten nicht (mindestens) MAR sind. Schematisch könnte unser Vorgehen bei der Exploration der Zufälligkeit von Missings folgendermaßen aussehen: Beispiel für Test auf MCAR Exemplarisch wollen wir uns einen möglichen Test anschauen, der überprüft, ob die Annahme von MCAR verletzt ist. \\(\\chi^2\\)-Test für multivariate Daten von Little (1988): Dieser überprüft, ob es signifikante Unterschiede zwischen den Mittelwerten der Muster von fehlenden Werten gibt. Die Nullhypothese (\\(H_0\\)) besagt, dass die Mittelwerte der Variablen (Spalten) nicht in Abhängigkeit der Missingmuster variieren (MCAR). Die Alternativhypothese (\\(H_1\\)) besagt, dass die Mittelwerte sich zwischen den verschiedenen Mustern von Missings unterscheiden (MAR oder MNAR). Auch hier muss man sich vorher überlegen, wo man das Signifikanzniveau \\(\\alpha\\) setzt. Für unser Beispiel legen wir es entsprechend der gängigen Konventionen auf \\(\\alpha= 0.05\\) fest. Zur Durchführung des Tests in R greifen wir auf die Funktion mcar_test() aus dem Paket naniar zu. Das laden wir uns über Github herunter (wofür wir wiederum das Paket remotes benötigen). Achtung: Die Funktion mcar_test() kann wir nur mit (quantitativen) Daten des Typs numeric (integer und double), logical und factor umgehen. # install.packages(&quot;remotes&quot;) # remotes::install_github(&quot;njtierney/naniar&quot;) library(naniar) mcar_test(daten) ## # A tibble: 1 × 4 ## statistic df p.value missing.patterns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 5.88 6 0.437 3 Als Output bekommen wir den \\(\\chi^2\\)-Wert (statistic), die Anzahl der Freiheitsgrade (df), den p-Wert (p-value), sowie die Anzahl der Missing-Muster (missing.patterns). Der \\(p\\)-Wert für den MCAR-Test für unseren Datensatz ist größer als die Irrtumswahrscheinlichkeit \\(\\alpha\\). Dies bedeutet, dass wir die \\(H_0\\) beibehalten können und (bis auf weiteres) davon ausgehen, dass die MCAR-Annahme erfüllt ist. Für mehr Informationen zu diesem Test siehe Little (1988). Im Paket naniar gibt es noch viele weitere Funktionen zur Zusammenfassung, Visualisierung und Manipulation von fehlenden Werten. Auf der Github-Seite finden wir eine Übersicht einiger dieser Funktionen. 9.5 Wie kann man mit Missings umgehen? Es gibt verschiedene Möglichkeiten, um mit unvollständigen Datensätzen umzugehen. Diese sind mehr oder weniger geeignet in Abhängigkeit davon, welche Annahme (MCAR, MAR, MNAR) die Missings erfüllen. Es gibt aber keine einheitlichen Richtlinien darüber, wie man mit Missings umgehen sollte. Das liegt u.a. auch daran, dass schon die Überprüfung der Zufälligkeit von Missings schwierig ist. Wichtig ist grundsätzlich, dass man sich mit den Missings eines Datensatzes auseinander setzt und einen Weg findet, mit ihnen umzugehen, ohne dass dies die Ergebnisse verzerren könnte. Man muss also für die eigene Fragestellung und Auswertungsmethode einen geeigneten Weg finden. Weitere Methoden zum Umgang mit Missings finden wir auch in den Quellen, die wir unter Literaturempfehlungen finden. Im Folgenden wollen wir uns darauf beschränken, uns einige gängige Methoden anzuschauen, die man nutzen kann, wenn die fehlenden Werte MCAR sind. Wenn diese Annahme nicht erfüllt ist, können bei Nutzung der folgenden Methoden verzerrte Parameterschätzungen resultieren. Zwei grundlegende Möglichkeiten sind entweder ganze Fälle mit Missings zu exkludieren (listwise/casewise deletion) oder vorhandene Elemente von Fällen für einen Teil der Analysen zu nutzen (pairwise deletion). In vielen Funktionen in R können wir zwischen beiden Möglichkeiten entscheiden. Nachteil von beiden (d.h. kompletter bzw. partieller Ausschluss von Zeilen) ist generell, dass die Stichprobengröße \\(N\\) sinkt und damit einhergehend größere Standardfehler und eine geringere Power resultieren. Ein weiteres Problem bei pairwise deletion ist außerdem, dass sich die Stichprobengröße \\(N\\) sowie die Zusammensetzung der Stichproben für unterschiedliche Analysen unterscheiden wird. Eine weitere Möglichkeit mit fehlenden Werten umzugehen ist diese zu imputieren (d.h. diese “vorherzusagen”). Da Imputation aber ein vielschichtiges Thema mit vielen verschiedenen Methodiken ist, gehen wir im Weiteren nicht darauf ein. 9.6 Neuen Datensatz erstellen, der keine Missings enthält Mit na.omit() löscht man listwise/casewise, d.h. Fälle, die mindestens ein Missing aufweisen werden komplett gelöscht. Es ist ratsam, den damit neu erstellten Datensatz als ein neues Objekt zu speichern (anstatt den originalen Satensatz zu überschreiben): daten_cw &lt;- na.omit(daten) ## Var_1 Var_2 Var_3 Var_4 Var_5 ## Vpn_2 0 2 1 3 0 ## Vpn_3 1 3 3 3 2 ## Vpn_5 2 0 0 2 3 Wir haben jetzt leider den Nachteil, dass unser Datensatz von fünf auf drei Personen geschrumpft ist (weil zwei Personen mindestens auf einer Variablen ein Missing hatten). Abhängig von der eigenen Auswertung möchte man das vielleicht eher nicht so machen, sondern die vorhandenen Werte in den hier gelöschten Zeilen noch anderweitig nutzen. 9.6.1 Festlegen, wie Funktionen mit Missings umgehen sollen Anstatt die Daten in einem ersten Schritt hinsichtlich der fehlenden Werte zu bereinigen, erlauben viele Funktionen den Umgang mit fehlenden Werten direkt mittels zusätzlicher Argumente zu spezifizieren. Um zu erfahren, welche Argumente eine Funktion nutzen kann, können wir im unteren rechten Panel bei Help nachschauen. Dazu schauen wir uns exemplarisch drei verschiedene Funktionen und einige ihrer Möglichkeiten im Umgang mit fehlenden Werten an. 9.6.1.1 lm(…, na.action) Mit lm() können wir eine (einfache oder multiple) lineare Regression durchführen. Mit dem Parameter na.action können wir über den Umgang mit den Missings bestimmen. Ein mögliches Argument dafür ist na.omit. Dabei wird bei Vorhandenseins eines Missings in einer Zeile die komplette Zeile aus der Berechnung genommen (listwise/casewise deletion). Das ist der Default dieser Funktion. In manchen Situationen ist es wichtig, Informationen darüber zu haben, wo Missings in einer Zeile sind (z.B. bei der Prüfung der Annahmen der Unabhängigkeit der Fehlerterme in der Multiplen Linearen Regression). Wenn ich beispielsweise einen Boxplot der Residuen einer linearen Regression in einer bestimmten Gruppe erstellen möchte, benötige ich einen Vektor der Residuen, in dem noch die Information darüber enthalten ist, in welcher Zeile Werte fehlen. Wenn der Residuenvektor und der Gruppenvektor unterschiedliche Zeilenanzahlen - in Abhängigkeit der Missings - haben, kann ich den Boxplot sonst nicht erstellen. Dafür nutzen wir das Argument na.exclude. Hierbei werden die Indizes der Missings nicht einfach gelöscht (und dadurch die Zeilenanzahl reduziert) sondern gespeichert. Ansonsten ist die Berechnung äquivalent zu na.omit (d.h. auch listwise/casewise deletion). Mittels residuals(lm_Ergebnisobjekt) können wir dann den Residuenvektor extrahieren. Hier gehts zur Überprüfung der Unabhängigkeit der Fehlerterme mittels Boxplots aus dem Kapitel zu Annahmen der Regression. 9.6.1.2 mean(…, na.rm) Die Funktion mean() enthält den Parameter na.rm, welcher festlegt, ob einzelne fehlende Elemente vor der Ausführung der Funktion entfernt werden sollen. Mit TRUE entfernen wir die Missings; mit FALSE behalten wir sie. Bei vielen Funktionen ist letzteres voreingestellt, was häufig aber eine Durchführung der Funktion verhindert. mean(daten$Var_1) # kann nicht berechnet werden weil Default na.rm = FALSE ## [1] NA mean(daten$Var_1, na.rm = TRUE) ## [1] 1.5 9.6.1.3 colMeans(…, na.rm) Die Funktion colMeans(), mit der wir Spaltenmittelwerte von mehrdimensionalen Datenstrukturen (z.B. Matrizen oder Dataframes) berechnen können, besitzt ebenfalls den Parameter na.rm. TRUE lässt uns hier (direkt) pairwise deletion anwenden. Schauen wir uns das einmal genauer an. Exemplarisch begrenzen wir uns auf die ersten drei Spalten von daten mittels [, 1:3]. mean(daten[, 1], na.rm = TRUE) mean(daten[, 2]) # na.rm=TRUE nicht notwendig mean(daten[, 3], na.rm = TRUE) ## [1] 1.5 ## [1] 1.6 ## [1] 1.333333 colMeans(daten[, 1:3], na.rm = TRUE) ## Var_1 Var_2 Var_3 ## 1.500000 1.600000 1.333333 Wie wir sehen, bekommen wir die gleichen Ergebnisse bei mean() und colMeans(). Beide nutzen (quasi) pairwise deletion. Allerdings sprechen wir nur im Fall von colMeans() davon, weil es bei mean() keine andere Möglichkeit gibt, als jeweils die fehlende Werte eines Vektors (eindimensionale Datenstruktur) zu entfernen oder eben nicht. colMeans(daten_cw[, 1:3]) # daten_cw sind die mit na.omit() bereinigten Daten (listwise deletion) ## Var_1 Var_2 Var_3 ## 1.000000 1.666667 1.333333 Vergleichen wir diese Ergebnisse nun mit denen von oben, sehen wir, dass nur der Mittelwert von Var_3 gleich. Bei den Mittelwerten der beiden anderen Spalten unterscheiden sich die Ergebnisse. ## Var_1 Var_2 Var_3 ## Vpn_1 NA 1 NA ## Vpn_2 0 2 1 ## Vpn_3 1 3 3 ## Vpn_4 3 2 NA ## Vpn_5 2 0 0 Die unterschiedlichen Spaltenmittelwerte kommen daher zustande, dass na.omit() für alle Berechnungen Vpn_1 und Vpn_4 ausschließt (listwise/casewise deletion), wohingegen na.rm die Missings nur in den Spalten ausschließt, die gerade zur Berechnung benötigt werden (pairwise deletion), z.B. Vpn_4 bei der Berechnung des Mittelwerts von Var_3, aber nicht bei denen von Var_1 und Var_2. 9.6.1.4 cor(…, use) Mit cor() können wir Korrelationstabellen berechnen. Dabei können wir mit dem Parameter use festlegen, wie mit Missings umgegangen werden sollen. Wir beschränken uns hier auf zwei Möglichkeiten von use. Mit complete.obs nutzen wir listwise/casewise deletion; mit pairwise.complete.obs nutzen wir pairwise deletion. Um den Unterschied zwischen beiden Möglichkeiten besser zu verstehen, schauen wir uns die jeweiligen Korrelationstabellen (der ersten drei Variablen) an. Da die Korrelationsmatrizen symmetrisch sind (d.h. ober- und unterhalb der Diagonalen gleich sind) wird jeweils die obere Diagonale für die Tabellen ausgeblendet. cor_co &lt;- cor(daten[, 1:3], use = &quot;complete.obs&quot;) cor_co &lt;- round(cor_co, 3) Table 9.1: Korrelation mit complete.obs Var_1 Var_2 Var_3 Var_1 1 Var_2 -0.655 1 Var_3 -0.327 0.929 1 cor_pco &lt;- cor(daten[, 1:3], use = &quot;pairwise.complete.obs&quot;) cor_pco &lt;- round(cor_pco, 3) Table 9.2: Korrelation mit pairwise.complete.obs Var_1 Var_2 Var_3 Var_1 1 Var_2 -0.308 1 Var_3 -0.327 0.929 1 Wenn man die beiden Korrelationstabellen vergleicht, sieht man, dass sich die Korrelation zwischen Var_1 und Var_2 unterscheidet. Das liegt daran, dass Vpn_4 in allen Berechnungen mit complete.obs ausgeschlossen wurde, weil Var_3 dort ein Missing enthält, während pairwise.complete.obs diese Zeile bei der Korrelation von Var_1 und Var_2 miteinbezogen hat. ## Var_1 Var_2 Var_3 ## Vpn_1 NA 1 NA ## Vpn_2 0 2 1 ## Vpn_3 1 3 3 ## Vpn_4 3 2 NA ## Vpn_5 2 0 0 Achtung: Im Gegensatz zu complete.obs basieren die verschiedenen Korrelationen bei pairwise.complete.obs auf Werten aus unterschiedlichen Zeilen (d.h. von unterschiedlichen Personen). 9.7 Literaturempfehlungen Für ein tiefergehenden Einblick empfehlen wir Euch die folgenden Arbeiten: Allison, P. D. (2002). Missing Data. In P. D. Allison (Ed.), The Sage Handbook of Quantitative Methods in Psychology (pp.72-89). Thousand Oaks, CA: Sage Publications Ltd. Abgerufen über http://www.statisticalhorizons.com/wp-content/uploads/2012/01/Milsap-Allison.pdf Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2003). Missing Data. In J. Cohen, P. Cohen, S. G. West, &amp; L. S. Aiken (Eds.), Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences (S. 431-451). Hillsdale, NJ: Erlbaum. (für HU-Studierende über ub.hu-berlin.de zugänglich) Lehrbuch der Master-Vorlesung “Multivariate Verfahren” Little, R. J. A. (1988). A test of missing completely at random for multivariate data with missing values. Journal of the American Statistical Association, 83(404), 1198–1202. (für HU-Studierende über ub.hu-berlin.de zugänglich) Schafer, J. L., &amp; Graham, J. W. (2002). Missing Data: Our View of the State of the Art. Psychological Methods, 7(2), 147-177. Abgerufen über http://www.nyu.edu/classes/shrout/G89-2247/Schafer&amp;Graham2002.pdf 9.8 FAQ Wir haben fehlende Werte (sog. Missings) in unserem Datensatz und wissen nicht, wie wir damit umgehen sollen? In diesem Abschnitt bekommen wir eine kurze Antwort darauf. Wenn dieser Abschnitt nicht ausreicht, oder wir mehr zu fehlenden Daten wissen möchtest, können wir uns das detaillierte Einführungskapitel dazu anschauen. Achtung: Wenn wir Variablen, die Missings enthalten, für eine Analyse nutzen wollen, sollten wir immer daran denken, dass sich damit auch die Stichprobengröße \\(N\\) für diese spezifische Auswertung ändert. 9.8.1 Erkennt R deine Missings? Generell werden Missings in verschiedenen Anwendungen (z.B. Unipark, SPSS) häufig anders kodiert als in R. In R werden fehlende Werte mit NA gekennzeichnet. Wenn das in deinem Datensatz nicht (einheitlich) so ist, musst du die Missings erst auf NA kodieren, damit R diese auch als Missings erkennt. Wenn du nicht weißt, ob die Missings in deinem Datensatz auch anders kodiert sein könnten, kannst du das mit einer Häufigkeitstabelle der einzelnen Ausprägung der Variablen (d.h. Spalten) überprüfen. Dazu musst du nur wissen, welche möglichen Ausprägungen es geben kann (z.B. wenn du eine Intervallskala von 1-5 hast dann sollte es nur diese Werte geben), um Abweichungen davon festzustellen. table(daten$Var, useNA=‘ifany’) Wenn die Missings z.B. mit 99 kodiert sind, können wir sie folgendermaßen auf NA setzen: daten[daten == 99] &lt;- NA 9.8.2 Wie können Funktionen mit Missings umgehen? Bei vielen Funktionen muss man festlegen, wie diese mit Missings umgehen sollen. Exemplarisch schauen wir uns das einmal an zwei Funktionen an. Wenn du wissen möchtest, wie du in anderen Funktionen mit Missings umgehen kannst, schau dir entweder die R-Dokumentation dazu an (unteres rechtes Panel bei Help oder alternativ ?mean) oder suche im Internet. In unserem Kapitel zu Fehlermeldungen findest du sowohl einen Abschnitt zum Aufbau der R-Dokumentation sowie einen Abschnitt zum Suchen im Internet. mean( ) Bei der Berechnung des Mittelwerts eines Vektors kann man Missings rausschmeißen, indem man das Argument na.rm nutzt: mean(daten, na.rm=TRUE) lm( ) Bei der Regressionsrechnung ist voreingestellt (“defaulted”), dass Personen mit mindestens einem Missing auf irgendeiner Variable aus der Rechnung ausgeschlossen werden (“listwise deletion”; manchmal auch “casewise deletion” genannt). Andere Optionen kann man mit dem Argument na.action festlegen. Um zu sehen, welche anderen Optionen es gibt, schaue in der Hilfe nach z.B. mit ?lm. 9.9 Übung Im Folgenden wollen wir einen Datensatz hinsichtlich der fehlenden Werte (Missings) beurteilen. Dazu schauen wir, ob die fehlenden Werte korrekt kodiert sind, wie viele und auf welchen Variablen bzw. in welchen Fällen diese vorhanden sind, ob sie zufällig sind und wie wir mit ihnen umgehen können. Hier finden wir das Einführungsskript zu Fehlenden Werten. Achtung: Die Aufgabenstellungen hier überschneiden sich teilweise mit denen aus der Übung zur Datenvorbereitung. Wir arbeiten hier aber mit anderen Datensätzen. Datensatz A: Normed Causality Statements In dieser querschnittlichen Studie untersuchten Hussey &amp; De Houwer inwieweit Personen normativ unmissverständlichen kausalen Aussagen zustimmen (z.B. X ruft Y hervor: Witze rufen Gelächter hervor). Mehr Informationen zur Studie befinden sich auf der OSF-Seite. Den Datensatz finden wir hier; ein Codebuch dazu hier. Achtung: Das Codebuch enthält nicht zu allen Variablen Informationen, da es für den aufbereiteten Datensatz erstellt wurde und wir uns aber die Rohdaten anschauen. Einen Großteil der Variablen, die nicht im Codebuch zu finden sind, entfernen wir noch. Nach dem Herunterladen, können wir den Datensatz folgendermaßen in R einlesen: data_a &lt;- read.csv(&quot;Dateipfad/group_a.csv&quot;) # hier den eigenen Dateipfad einfügen Wir entfernen noch einige für uns irrelevante Informationen zur Erhebung: data_a &lt;- data_a[,-c(2:7, 9, 145:153)] Achtung: Wir gehen im Folgenden davon aus, dass die Variablen statements..c1., statements..c2.. statements..c3. und statements..c4. aus dem Datensatz den Variablen catch_1, catch_2, catch_3 und catch_4 entsprechen. Datensatz B: Affective Forecasting and Social Anxiety In der Studie untersuchen Glenn &amp; Teachman, inwiefern sich Menschen mit geringer und starker Sozialangst bezüglich ihrer Bewertung von zukünftigen emotionalen Situationen unterscheiden. Den Datensatz finden wir hier; ein detailliertes Codebuch mit weiteren Informationen zur Studie hier. Achtung: Das Codebuch enthält leider keine Informationen zu den demographischen Variablen. Einige werden wir uns dennoch anschauen, da sie eindeutig interpretierbar erscheinen. Den Datensatz können wir, nachdem wir ihn heruntergeladen haben, folgendermaßen in R einlesen: ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav: Very long string record(s) found (record type ## 7, subtype 14), each will be imported in consecutive separate variables ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias1 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias2 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias3 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias4 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias5 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias6 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias7 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias8 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias9 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias10 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias11 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias12 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias13 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias14 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias15 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias16 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias17 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias18 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias19 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias20 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 4 added in variable: ## pre_outcome_old ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs1 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs2 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs3 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs4 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs5 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs6 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs7 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs8 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs9 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs10 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs11 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs12 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs13 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs14 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs15 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs16 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs17 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs18 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs19 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs20 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs21 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs22 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq1 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq2 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq3 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq4 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq5 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq17 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq16 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq15 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq8 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq9 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq10 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq14 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq13 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq6 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq7 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq11 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq12 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq18 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq19 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq20 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq21 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq28 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq27 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq26 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq24 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq25 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq22 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq23 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq32 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq31 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq29 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq30 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq33 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq34 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq39 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq38 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq37 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq35 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq36 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes1 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes2 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes3 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes4 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes5 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes6 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes7 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes8 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes9 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes10 # install.packages(&quot;foreign&quot;) library(foreign) data_b &lt;- read.spss(&quot;Dateipfad/AffectiveForecasting_0707017.sav&quot;, to.data.frame = TRUE) # noch den eigenen Dateipfad einfügen Da der Datensatz aus 479 Variablen besteht, wollen wir unsere Auswahl etwas eingrenzen. Wir schauen uns nur folgende Variablen an: # nur Daten aus dem Pretest data_b &lt;- data_b[, c(1:4, 6:7, # soziodemographische Variablen 12:13, 24:27, 30:49, 75:94)] # Pretest Variables # wir schauen uns nur die umkodierten Pretest Variablen an # d.h. jene ohne &quot;_orig&quot; 9.9.1 Übung 1: (Korrekte) Kodierung Bevor wir uns die fehlenden Werte genauer anschauen können, ist es sinnvoll, einen Plausibilitätscheck durchzuführen. Damit überprüfen wir, ob fehlende Werte auch korrekt kodiert sind, d.h. mit NA. 1.) Gibt es fehlende Werte im Datensatz, die nicht mit NA kodiert sind? Tipp Hier vergleichen wir die möglichen Ausprägungen der Variablen, die wir im Codebuch finden, mit den tatsächlichen Ausprägungen der Variablen, die wir uns in R anschauen können. Lösung A # sortierte Ausprägungen der Variablen inklusive NAs anzeigen: sapply(sapply(data_a, unique), sort, na.last=TRUE) ## $id ## [1] 1 11 21 31 41 51 61 71 81 91 101 111 121 131 141 151 161 171 181 ## [20] 191 201 211 221 231 241 251 261 271 281 291 301 311 321 331 341 351 361 371 ## [39] 381 391 401 411 421 431 441 451 461 471 481 491 501 511 521 531 541 551 561 ## [58] 571 581 591 601 611 621 631 641 651 661 671 681 691 701 ## ## $consent ## [1] &quot;&quot; &quot;Y&quot; ## ## $age ## [1] 20 21 22 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 41 42 44 46 49 51 ## [26] 53 54 56 61 NA ## ## $gender ## [1] &quot;&quot; &quot;f&quot; &quot;m&quot; ## ## $gender...comment ## [1] &quot;&quot; ## [2] &quot;Comment? Thanks for the opportunity? Unsure why there is a comment box before the task is completed, but that&#39;s okay.&quot; ## [3] &quot;have a good day&quot; ## [4] &quot;N/A&quot; ## ## $statements..1. ## [1] 1 2 3 5 NA ## ## $statements..2. ## [1] 1 2 3 4 NA ## ## $statements..3. ## [1] 1 2 3 4 5 NA ## ## $statements..4. ## [1] 1 2 3 4 NA ## ## $statements..5. ## [1] 1 2 3 5 NA ## ## $statements..6. ## [1] 1 2 3 4 NA ## ## $statements..7. ## [1] 1 2 3 4 5 NA ## ## $statements..8. ## [1] 3 4 5 NA ## ## $statements..9. ## [1] 1 2 3 4 NA ## ## $statements..10. ## [1] 3 4 5 NA ## ## $statements..11. ## [1] 1 2 3 4 5 NA ## ## $statements..12. ## [1] 1 2 3 4 NA ## ## $statements..13. ## [1] 1 2 3 4 5 NA ## ## $statements..14. ## [1] 1 2 3 4 NA ## ## $statements..15. ## [1] 1 2 3 4 NA ## ## $statements..16. ## [1] 2 4 5 NA ## ## $statements..17. ## [1] 1 2 3 4 5 NA ## ## $statements..18. ## [1] 1 2 3 4 NA ## ## $statements..19. ## [1] 1 2 3 4 5 NA ## ## $statements..20. ## [1] 1 2 3 4 5 NA ## ## $statements..21. ## [1] 1 2 3 4 5 NA ## ## $statements..22. ## [1] 1 2 4 NA ## ## $statements..23. ## [1] 1 2 3 4 5 NA ## ## $statements..24. ## [1] 1 2 3 NA ## ## $statements..25. ## [1] 1 2 3 4 NA ## ## $statements..26. ## [1] 1 2 NA ## ## $statements..27. ## [1] 1 2 3 4 5 NA ## ## $statements..28. ## [1] 1 3 4 5 NA ## ## $statements..29. ## [1] 1 2 3 4 NA ## ## $statements..30. ## [1] 1 2 3 4 NA ## ## $statements..31. ## [1] 1 2 5 NA ## ## $statements..32. ## [1] 1 2 3 4 5 NA ## ## $statements..33. ## [1] 1 2 3 4 NA ## ## $statements..34. ## [1] 1 2 3 4 NA ## ## $statements..35. ## [1] 1 2 3 4 5 NA ## ## $statements..36. ## [1] 1 2 3 NA ## ## $statements..37. ## [1] 1 2 3 4 5 NA ## ## $statements..38. ## [1] 1 2 3 4 5 NA ## ## $statements..39. ## [1] 1 2 3 4 5 NA ## ## $statements..40. ## [1] 1 2 3 4 5 NA ## ## $statements..41. ## [1] 1 2 3 4 NA ## ## $statements..42. ## [1] 1 2 3 4 5 NA ## ## $statements..43. ## [1] 1 2 3 4 5 NA ## ## $statements..44. ## [1] 1 2 3 4 5 NA ## ## $statements..45. ## [1] 1 2 3 4 5 NA ## ## $statements..46. ## [1] 1 2 3 4 5 NA ## ## $statements..47. ## [1] 1 2 3 4 5 NA ## ## $statements..48. ## [1] 1 2 3 4 5 NA ## ## $statements..49. ## [1] 1 2 3 4 5 NA ## ## $statements..50. ## [1] 1 2 3 4 5 NA ## ## $statements..51. ## [1] 1 2 3 4 5 NA ## ## $statements..52. ## [1] 1 2 3 4 5 NA ## ## $statements..53. ## [1] 3 4 5 NA ## ## $statements..54. ## [1] 1 2 3 4 5 NA ## ## $statements..55. ## [1] 1 2 3 4 5 NA ## ## $statements..56. ## [1] 1 2 3 4 5 NA ## ## $statements..57. ## [1] 1 2 3 4 5 NA ## ## $statements..58. ## [1] 1 2 3 4 5 NA ## ## $statements..59. ## [1] 1 2 3 4 NA ## ## $statements..60. ## [1] 1 2 3 4 5 NA ## ## $statements..61. ## [1] 1 2 3 4 5 NA ## ## $statements..62. ## [1] 1 2 3 4 5 NA ## ## $statements..63. ## [1] 1 2 3 NA ## ## $statements..64. ## [1] 2 3 4 5 NA ## ## $statements..65. ## [1] 1 2 3 4 5 NA ## ## $statements..66. ## [1] 1 2 3 4 5 NA ## ## $statements..67. ## [1] 1 3 4 5 NA ## ## $statements..68. ## [1] 1 2 3 4 5 NA ## ## $statements..69. ## [1] 1 2 3 4 5 NA ## ## $statements..70. ## [1] 1 2 3 4 5 NA ## ## $statements..71. ## [1] 1 2 3 4 5 NA ## ## $statements..72. ## [1] 1 2 3 4 NA ## ## $statements..73. ## [1] 1 2 3 4 NA ## ## $statements..74. ## [1] 1 2 3 4 5 NA ## ## $statements..75. ## [1] 1 2 3 4 5 NA ## ## $statements..76. ## [1] 1 2 3 4 5 NA ## ## $statements..77. ## [1] 1 2 3 4 NA ## ## $statements..78. ## [1] 1 2 3 4 5 NA ## ## $statements..79. ## [1] 1 2 3 4 5 NA ## ## $statements..80. ## [1] 1 2 3 4 5 NA ## ## $statements..81. ## [1] 1 2 3 4 5 NA ## ## $statements..82. ## [1] 1 2 3 4 5 NA ## ## $statements..83. ## [1] 1 2 3 4 5 NA ## ## $statements..84. ## [1] 1 2 3 4 NA ## ## $statements..85. ## [1] 1 2 3 4 NA ## ## $statements..86. ## [1] 1 2 3 4 5 NA ## ## $statements..87. ## [1] 2 3 4 5 NA ## ## $statements..88. ## [1] 1 2 3 4 5 NA ## ## $statements..89. ## [1] 1 2 4 5 NA ## ## $statements..90. ## [1] 1 2 3 4 5 NA ## ## $statements..91. ## [1] 1 2 3 5 NA ## ## $statements..92. ## [1] 1 2 3 4 5 NA ## ## $statements..93. ## [1] 1 2 3 4 5 NA ## ## $statements..94. ## [1] 1 2 3 4 5 NA ## ## $statements..95. ## [1] 3 4 5 NA ## ## $statements..96. ## [1] 1 2 3 4 NA ## ## $statements..97. ## [1] 1 2 3 4 NA ## ## $statements..98. ## [1] 1 2 3 4 5 NA ## ## $statements..99. ## [1] 1 2 3 4 NA ## ## $statements..100. ## [1] 2 3 4 5 NA ## ## $statements..101. ## [1] 1 2 3 4 5 NA ## ## $statements..102. ## [1] 1 2 3 4 NA ## ## $statements..103. ## [1] 4 5 NA ## ## $statements..104. ## [1] 1 2 3 4 5 NA ## ## $statements..105. ## [1] 3 4 5 NA ## ## $statements..106. ## [1] 1 2 3 5 NA ## ## $statements..107. ## [1] 1 2 3 4 5 NA ## ## $statements..108. ## [1] 1 2 3 4 NA ## ## $statements..109. ## [1] 1 2 3 4 NA ## ## $statements..110. ## [1] 3 4 5 NA ## ## $statements..111. ## [1] 1 2 3 4 NA ## ## $statements..112. ## [1] 2 3 4 5 NA ## ## $statements..113. ## [1] 1 2 3 4 5 NA ## ## $statements..114. ## [1] 1 2 3 4 5 NA ## ## $statements..115. ## [1] 1 2 3 4 NA ## ## $statements..116. ## [1] 1 2 3 4 5 NA ## ## $statements..117. ## [1] 2 3 4 5 NA ## ## $statements..118. ## [1] 1 2 3 4 NA ## ## $statements..119. ## [1] 2 3 4 5 NA ## ## $statements..120. ## [1] 1 2 3 5 NA ## ## $statements..121. ## [1] 1 2 3 4 5 NA ## ## $statements..122. ## [1] 1 2 3 4 NA ## ## $statements..123. ## [1] 1 3 4 5 NA ## ## $statements..124. ## [1] 1 2 3 4 5 NA ## ## $statements..125. ## [1] 1 2 3 4 5 NA ## ## $statements..126. ## [1] 1 2 3 4 NA ## ## $statements..127. ## [1] 1 2 3 5 NA ## ## $statements..128. ## [1] 1 2 3 4 NA ## ## $statements..c1. ## [1] 4 NA ## ## $statements..c2. ## [1] 4 NA ## ## $statements..c3. ## [1] 1 2 4 NA ## ## $statements..c4. ## [1] 1 2 4 NA In den Variablen consent, gender, gender...comment kommt eine leere Ausprägung (\"\") vor. Außerdem hat hat eine Person in der Variablen gender...comment den Text N/A angegeben. Dieser String wird allerdings nicht als korrekte NA-Kodierung erkannt. Wir können in beiden Fällen davon ausgehen, dass dies Missings sind, die nicht richtig kodiert wurden. Lösung B # sortierte Ausprägungen der Variablen inklusive NAs anzeigen: sapply(sapply(data_b, unique), sort, na.last=TRUE) ## $P_ID ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 61 63 64 65 66 67 68 69 70 71 72 73 74 ## [73] 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 ## [91] 93 94 95 96 98 99 100 101 102 103 104 105 107 108 109 110 111 112 ## [109] 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 ## [127] 131 132 133 134 135 136 137 138 139 141 142 143 144 145 146 147 148 149 ## [145] 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 ## [163] 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 ## [181] 186 187 188 189 190 191 192 ## ## $age ## [1] 17 18 19 20 21 22 24 25 38 ## ## $gender ## [1] Male Female ## Levels: Male Female Other/Prefer not to answer ## ## $race_white_yn ## [1] 0 1 NA ## ## $ethn ## [1] Hispanic/Latino Not Hispanic/Latino Prefer not to answer ## Levels: Hispanic/Latino Not Hispanic/Latino Prefer not to answer ## ## $race ## [1] Asian Black or African American ## [3] White More than one race ## [5] Other Prefer not to answer ## 8 Levels: American Indian/Alaskan Native Asian ... Prefer not to answer ## ## $group ## [1] Low SA High SA ## Levels: Low SA High SA ## ## $cond ## [1] Negative speech evaluation Positive speech evaluation ## Levels: Negative speech evaluation Positive speech evaluation ## ## $preselect_curr_e1 ## [1] -100 -67 -65 -62 -61 -54 -51 -50 -47 -42 -36 -34 -32 -31 -25 ## [16] -23 -22 -21 -20 -19 -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 ## [31] -7 -1 0 4 7 9 10 11 12 13 16 18 19 20 21 ## [46] 22 23 25 26 28 29 30 31 32 33 34 35 36 38 39 ## [61] 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [76] 57 59 60 61 62 64 67 68 69 70 71 72 77 79 83 ## [91] 84 89 90 92 98 100 NA ## ## $preselect_curr_e2 ## [1] -97 -67 -64 -57 -49 -44 -43 -32 -30 -27 -24 -23 -22 -21 -19 -17 -16 -15 -13 ## [20] -12 -9 -8 -7 -2 -1 0 3 4 5 6 9 10 11 12 13 14 15 16 ## [39] 17 19 20 21 22 23 25 26 27 28 29 30 31 32 33 34 35 37 38 ## [58] 41 42 43 45 46 47 48 49 52 53 54 55 57 61 62 63 64 66 67 ## [77] 68 69 72 74 82 83 86 100 NA ## ## $preselect_curr_e3 ## [1] -100 -78 -69 -61 -58 -57 -50 -47 -45 -42 -39 -35 -31 -30 -29 ## [16] -28 -27 -26 -25 -24 -23 -21 -20 -19 -18 -17 -16 -15 -14 -12 ## [31] -11 -10 -9 -8 -7 -6 -4 -1 0 5 7 8 9 10 12 ## [46] 13 14 15 16 18 19 20 21 23 24 25 26 27 28 30 ## [61] 31 33 34 35 36 37 39 40 41 42 44 45 46 49 50 ## [76] 51 52 53 54 55 59 60 62 63 65 72 80 84 85 87 ## [91] 91 93 95 97 100 NA ## ## $preselect_curr_e4 ## [1] -100 -67 -52 -45 -44 -43 -41 -37 -36 -35 -31 -30 -25 -22 -21 ## [16] -17 -16 -12 -11 -10 -8 -7 -6 -2 -1 0 1 2 3 4 ## [31] 5 7 8 9 10 11 12 14 15 16 17 18 20 21 22 ## [46] 23 24 25 26 27 28 29 30 31 34 36 37 38 41 42 ## [61] 43 44 45 46 47 48 49 50 51 52 54 55 57 59 61 ## [76] 64 67 71 72 73 74 75 78 80 81 85 86 87 88 90 ## [91] 95 100 NA ## ## $preselect_ave_e1 ## [1] -100 -75 -69 -67 -51 -49 -48 -44 -42 -41 -40 -37 -36 -35 -34 ## [16] -33 -32 -31 -30 -29 -28 -27 -26 -24 -23 -22 -21 -20 -19 -18 ## [31] -17 -16 -15 -14 -13 -12 -11 -10 -9 -8 -5 -4 -3 -1 0 ## [46] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 ## [61] 17 18 19 21 22 23 25 26 27 28 29 30 33 35 36 ## [76] 38 45 49 52 53 55 57 83 NA ## ## $preselect_ave_e2 ## [1] -100 -56 -54 -52 -51 -48 -47 -44 -43 -41 -40 -39 -38 -37 -34 ## [16] -32 -31 -29 -28 -27 -25 -22 -20 -19 -18 -17 -16 -15 -14 -13 ## [31] -12 -11 -10 -9 -7 -6 -5 -3 -1 0 3 5 6 7 8 ## [46] 9 11 12 13 16 18 19 20 21 22 24 25 27 30 31 ## [61] 32 33 38 39 43 47 49 50 51 59 62 85 100 NA ## ## $preselect_ave_e3 ## [1] -100 -69 -65 -63 -54 -53 -47 -45 -40 -35 -33 -32 -28 -27 -26 ## [16] -25 -22 -21 -20 -19 -18 -17 -16 -14 -13 -11 -10 -9 -8 -7 ## [31] -6 -5 -4 -3 -1 0 2 4 5 6 7 8 9 10 11 ## [46] 12 15 16 17 19 20 21 22 23 24 25 27 30 31 32 ## [61] 37 38 39 42 47 49 50 52 53 72 81 86 92 100 NA ## ## $preselect_ave_e4 ## [1] -100 -91 -80 -72 -69 -63 -56 -53 -49 -47 -41 -40 -37 -36 -35 ## [16] -34 -32 -31 -30 -29 -26 -25 -22 -21 -20 -19 -18 -17 -16 -15 ## [31] -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -1 1 2 3 ## [46] 4 5 7 8 9 10 11 12 13 14 16 19 21 22 23 ## [61] 25 27 28 29 30 31 34 35 36 38 40 43 45 52 55 ## [76] 57 65 86 87 100 NA ## ## $preselect_e1 ## [1] -100 -97 -91 -90 -86 -85 -84 -81 -80 -78 -77 -74 -73 -71 -70 ## [16] -68 -66 -63 -62 -61 -58 -57 -56 -55 -54 -53 -52 -51 -50 -49 ## [31] -48 -47 -45 -44 -43 -41 -39 -38 -37 -36 -35 -34 -33 -32 -31 ## [46] -29 -28 -27 -25 -24 -23 -22 -20 -19 -18 -17 -16 -9 -1 4 ## [61] 10 11 13 15 21 22 24 26 27 28 31 37 41 42 45 ## [76] 46 49 50 52 54 55 56 58 59 60 61 63 64 67 68 ## [91] 71 74 75 76 78 79 81 82 83 85 87 88 91 93 97 ## [106] 100 NA ## ## $preselect_e2 ## [1] -100 -86 -81 -80 -74 -73 -72 -71 -70 -65 -64 -62 -57 -56 -54 ## [16] -51 -50 -48 -47 -46 -45 -44 -39 -37 -35 -34 -33 -32 -31 -30 ## [31] -29 -28 -27 -26 -25 -24 -22 -21 -20 -19 -17 -16 -15 -9 -8 ## [46] -7 -6 -3 -1 4 10 12 13 14 18 19 20 22 23 24 ## [61] 26 27 28 33 34 37 38 41 42 43 45 46 48 50 52 ## [76] 53 54 55 56 57 58 59 61 62 63 65 66 67 71 72 ## [91] 73 74 75 78 79 82 85 86 88 89 91 92 98 99 100 ## [106] NA ## ## $preselect_e3 ## [1] -99 -87 -86 -81 -76 -75 -73 -71 -60 -59 -54 -53 -50 -48 -47 -43 -42 -40 -39 ## [20] -38 -37 -35 -32 -29 -28 -27 -26 -25 -23 -22 -18 -17 -16 -15 -14 -11 -10 -8 ## [39] -7 -6 -5 -3 -1 0 1 2 3 5 6 7 9 12 13 16 18 19 20 ## [58] 21 23 24 25 28 29 34 36 37 38 42 43 44 46 47 48 49 50 52 ## [77] 53 55 57 59 61 63 64 70 72 87 88 91 100 NA ## ## $preselect_e4 ## [1] -100 -95 -87 -85 -83 -82 -80 -79 -74 -72 -71 -69 -68 -66 -63 ## [16] -62 -60 -59 -57 -51 -49 -48 -42 -40 -39 -35 -32 -31 -29 -26 ## [31] -25 -24 -22 -21 -20 -18 -15 -13 -12 -10 -9 -8 -6 -1 3 ## [46] 4 8 10 13 16 17 22 24 25 26 28 29 32 33 34 ## [61] 35 37 42 43 44 46 48 49 51 53 54 57 60 61 63 ## [76] 64 66 67 68 69 70 71 72 73 74 77 78 79 84 85 ## [91] 86 87 92 98 100 NA ## ## $preselect_neg_e1 ## [1] -100 -99 -97 -91 -90 -86 -85 -84 -81 -80 -79 -78 -77 -76 -75 ## [16] -74 -73 -71 -70 -69 -68 -66 -65 -64 -63 -62 -61 -60 -59 -58 ## [31] -57 -56 -55 -54 -53 -52 -51 -50 -49 -48 -47 -45 -44 -43 -41 ## [46] -39 -38 -36 -35 -34 -33 -32 -31 -29 -28 -27 -25 -24 -23 -22 ## [61] -21 -20 -19 -18 -17 -16 -15 -14 -12 -10 -9 -8 -1 4 13 ## [76] 26 40 NA ## ## $preselect_neg_e2 ## [1] -100 -98 -95 -94 -87 -86 -81 -80 -76 -74 -73 -72 -71 -70 -66 ## [16] -65 -64 -62 -59 -57 -56 -54 -53 -51 -50 -49 -48 -47 -46 -45 ## [31] -44 -43 -42 -40 -39 -38 -37 -36 -35 -34 -33 -32 -31 -30 -29 ## [46] -28 -27 -26 -25 -24 -23 -22 -21 -20 -19 -17 -16 -15 -14 -13 ## [61] -12 -10 -9 -8 -7 -6 -4 -3 -1 4 19 NA ## ## $preselect_neg_e3 ## [1] -100 -99 -87 -86 -83 -81 -76 -75 -74 -73 -71 -70 -67 -60 -59 ## [16] -58 -57 -55 -54 -53 -52 -50 -48 -47 -46 -44 -43 -42 -40 -39 ## [31] -38 -37 -36 -35 -34 -33 -32 -31 -30 -29 -28 -27 -26 -25 -24 ## [46] -23 -22 -20 -18 -17 -15 -14 -12 -11 -10 -9 -8 -7 -6 -5 ## [61] -1 0 8 9 11 13 19 20 21 23 25 32 34 36 40 ## [76] 41 45 50 54 100 NA ## ## $preselect_neg_e4 ## [1] -100 -98 -95 -88 -87 -85 -84 -83 -82 -80 -79 -78 -74 -72 -71 ## [16] -69 -68 -67 -66 -65 -64 -63 -62 -61 -60 -59 -58 -57 -55 -54 ## [31] -52 -51 -49 -48 -46 -45 -43 -42 -41 -40 -39 -38 -37 -35 -34 ## [46] -33 -32 -31 -30 -29 -26 -25 -24 -22 -21 -20 -18 -17 -16 -15 ## [61] -13 -12 -11 -10 -9 -8 -6 -5 -4 -3 -1 4 10 17 25 ## [76] 29 56 57 NA ## ## $preselect_pos_e1 ## [1] -37 -17 0 6 7 10 11 13 15 18 20 21 22 23 24 27 28 29 30 ## [20] 31 32 35 36 37 39 40 41 42 43 45 46 49 50 51 52 54 55 56 ## [39] 58 59 60 61 62 63 64 65 66 67 68 70 71 73 74 75 76 77 78 ## [58] 79 80 81 82 83 84 85 86 87 88 90 91 92 93 94 97 100 NA ## ## $preselect_pos_e2 ## [1] 4 5 10 11 12 13 14 15 16 17 18 20 21 22 23 24 25 26 27 ## [20] 28 29 30 31 32 33 34 35 36 37 38 40 41 42 43 45 46 48 50 ## [39] 51 52 53 54 55 56 57 58 59 61 62 63 64 65 66 67 68 69 71 ## [58] 72 73 74 75 76 78 79 80 81 82 85 86 88 89 91 92 93 97 98 ## [77] 99 100 NA ## ## $preselect_pos_e3 ## [1] -75 -73 -60 -50 -40 -35 -33 -29 -27 -16 -15 -12 -11 -8 -7 -3 -1 0 1 ## [20] 2 3 5 6 7 9 11 12 13 15 16 17 18 19 21 22 23 24 25 ## [39] 26 27 28 29 32 33 34 37 38 42 43 44 46 47 48 49 50 52 53 ## [58] 55 57 59 61 62 63 64 66 70 72 75 77 81 82 87 88 91 100 NA ## ## $preselect_pos_e4 ## [1] -14 -1 3 5 8 11 13 14 16 17 21 22 23 24 25 26 27 28 29 ## [20] 32 33 34 35 36 37 38 39 40 42 43 44 45 46 48 49 51 52 53 ## [39] 54 57 59 60 61 63 64 65 66 67 68 69 70 71 72 73 74 75 76 ## [58] 77 78 79 80 81 82 83 84 85 86 87 88 89 90 92 95 98 100 NA ## ## $preselect_peak_e1 ## [1] -100 -96 -88 -79 -77 -75 -73 -72 -66 -64 -62 -58 -56 -53 -52 ## [16] -50 -48 -47 -45 -43 -42 -40 -39 -38 -37 -36 -34 -33 -31 -29 ## [31] -28 -25 -24 -23 -22 -20 -17 -16 -15 -14 -12 -11 -10 -9 -6 ## [46] -4 -1 4 5 9 10 11 12 13 14 15 16 18 19 20 ## [61] 21 22 23 25 26 27 28 29 30 31 32 33 34 36 38 ## [76] 40 41 43 46 47 50 51 54 56 58 61 65 67 73 75 ## [91] 88 91 100 NA ## ## $preselect_peak_e2 ## [1] -78 -69 -55 -53 -31 -30 -25 -21 -20 -19 -18 -17 -15 -14 -7 -5 -3 -2 -1 ## [20] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 17 18 20 ## [39] 21 22 25 29 30 31 33 34 36 40 41 43 44 47 50 52 54 55 60 ## [58] 61 64 66 68 69 75 79 85 94 100 NA ## ## $preselect_peak_e3 ## [1] -100 -99 -96 -94 -92 -91 -90 -89 -88 -87 -86 -85 -84 -83 -79 ## [16] -76 -74 -73 -72 -71 -70 -69 -67 -66 -65 -64 -63 -60 -59 -58 ## [31] -57 -56 -55 -54 -53 -52 -50 -49 -48 -47 -46 -45 -44 -42 -41 ## [46] -39 -38 -37 -36 -35 -34 -33 -32 -31 -30 -29 -28 -27 -26 -25 ## [61] -24 -21 -20 -19 -18 -17 -16 -14 -13 -12 -9 -8 -7 -5 -1 ## [76] 0 3 8 10 15 16 20 26 31 32 33 36 37 39 42 ## [91] 46 51 53 70 100 NA ## ## $preselect_peak_e4 ## [1] -100 -92 -70 -65 -60 -57 -54 -50 -49 -48 -41 -40 -39 -38 -36 ## [16] -35 -33 -32 -31 -30 -29 -27 -25 -23 -22 -21 -20 -19 -18 -17 ## [31] -16 -15 -14 -13 -12 -10 -8 -7 -1 1 2 3 4 7 8 ## [46] 9 10 11 12 13 15 16 18 19 20 21 23 24 25 26 ## [61] 27 28 29 30 32 33 34 35 36 40 43 44 47 48 49 ## [76] 50 59 60 64 65 67 69 70 74 76 100 NA ## ## $sias1 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias2 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias3 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me &lt;NA&gt; ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias4 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias5 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias6 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me &lt;NA&gt; ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias7 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias8 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias9 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias10 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me &lt;NA&gt; ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias11 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me &lt;NA&gt; ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias12 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me &lt;NA&gt; ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias13 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias14 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias15 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias16 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias17 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias18 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias19 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me ## 6 Levels: 0 ... 5= Extremely characteristic of me ## ## $sias20 ## [1] 0 0= Not at all characteristic of me ## [3] 1= Slightly characteristic of me 2= Moderately characteristic of me ## [5] 4= Very characteristic of me &lt;NA&gt; ## 6 Levels: 0 ... 5= Extremely characteristic of me Leider sehen wir nicht alle Ausprägungen von race. Diese können wir uns auch separat mit levels() anschauen. levels(data_b$race) ## [1] &quot;American Indian/Alaskan Native&quot; ## [2] &quot;Asian&quot; ## [3] &quot;Black or African American&quot; ## [4] &quot;Native Hawaiian or Other Pacific Islander&quot; ## [5] &quot;White&quot; ## [6] &quot;More than one race&quot; ## [7] &quot;Other&quot; ## [8] &quot;Prefer not to answer&quot; In race scheint es keine falsch kodierten Missings zu geben. Achtung: Scheinbar wird das Messniveau der SIAS-Items (sias...) als nominalskaliert, und nicht wie sonst üblich als intervallskaliert, angenommen. Zumindest liegen die betreffenden Variables als ungeordnete Faktoren vor. Es fällt außerdem auf, dass die Items des SIAS, welche eine 5-stufige Skala haben sollten, die von 0-4 geht, eine merkwürdige Kodierung der Daten aufweisen: es gibt 6 Ausprägungen der Kodierung die Ausprägung 3=..., welche es laut Codebuch geben sollte, scheint nicht vorhanden zu sein es gibt eine Ausprägung 5=..., welche laut Codebuch nicht vorliegen sollte es gibt zwei Ausprägungen, die die 0 beinhalten Das sollten wir noch weiter explorieren. Dazu schauen wir uns die Häufigkeiten der Ausprägungen genauer an: sapply(data_b[, grep(&quot;sias&quot;, colnames(data_b))], table, useNA=&quot;always&quot;) ## sias1 sias2 sias3 sias4 sias5 sias6 sias7 ## 0 41 94 46 98 20 91 82 ## 0= Not at all characteristic of me 47 46 53 35 16 46 46 ## 1= Slightly characteristic of me 35 26 37 29 36 29 21 ## 2= Moderately characteristic of me 42 12 31 21 71 17 27 ## 4= Very characteristic of me 22 9 19 4 44 3 11 ## 5= Extremely characteristic of me 0 0 0 0 0 0 0 ## &lt;NA&gt; 0 0 1 0 0 1 0 ## sias8 sias9 sias10 sias11 sias12 sias13 ## 0 93 29 107 34 53 85 ## 0= Not at all characteristic of me 50 31 31 45 45 46 ## 1= Slightly characteristic of me 23 35 26 33 28 33 ## 2= Moderately characteristic of me 15 49 13 51 30 21 ## 4= Very characteristic of me 6 43 9 23 30 2 ## 5= Extremely characteristic of me 0 0 0 0 0 0 ## &lt;NA&gt; 0 0 1 1 1 0 ## sias14 sias15 sias16 sias17 sias18 sias19 ## 0 71 57 50 68 64 85 ## 0= Not at all characteristic of me 53 47 50 39 39 25 ## 1= Slightly characteristic of me 22 22 26 29 25 33 ## 2= Moderately characteristic of me 19 39 32 38 33 30 ## 4= Very characteristic of me 22 22 29 13 26 14 ## 5= Extremely characteristic of me 0 0 0 0 0 0 ## &lt;NA&gt; 0 0 0 0 0 0 ## sias20 ## 0 47 ## 0= Not at all characteristic of me 44 ## 1= Slightly characteristic of me 31 ## 2= Moderately characteristic of me 37 ## 4= Very characteristic of me 27 ## 5= Extremely characteristic of me 0 ## &lt;NA&gt; 1 Es fällt auf, dass Die Ausprägung 5=... in keiner der Variablen vorkommt (Häufigkeit \\(0\\)). Um zu überprüfen, wie die Kodierung im Datensatz zur Skala im Codebuch in Bezug zu setzen ist, schauen wir uns ein Item an, welches im Originaldatensatz eine rekodierte Version enthält: sias5 und sias5_RS. Wir speichern beide Variablen in einem neuen Datensatz. ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav: Very long string record(s) found (record type ## 7, subtype 14), each will be imported in consecutive separate variables ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias1 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias2 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias3 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias4 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias5 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias6 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias7 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias8 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias9 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias10 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias11 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias12 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias13 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias14 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias15 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias16 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias17 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias18 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias19 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 0 added in variable: ## sias20 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 4 added in variable: ## pre_outcome_old ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs1 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs2 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs3 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs4 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs5 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs6 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs7 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs8 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs9 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs10 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs11 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs12 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs13 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs14 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs15 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs16 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs17 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs18 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs19 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs20 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs21 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## rrs22 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq1 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq2 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq3 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq4 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq5 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq17 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq16 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq15 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq8 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq9 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq10 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq14 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq13 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq6 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq7 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq11 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq12 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq18 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq19 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq20 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq21 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq28 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq27 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq26 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq24 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq25 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq22 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq23 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq32 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq31 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq29 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq30 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq33 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq34 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq39 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq38 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq37 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq35 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## ffmq36 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes1 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes2 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes3 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes4 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes5 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes6 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes7 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes8 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes9 ## Warning in read.spss(&quot;figures/Fehlende-Werte/Daten/ ## AffectiveForecasting_0707017.sav&quot;, : Undeclared level(s) 999 added in variable: ## fpes10 data_b &lt;- read.spss(&quot;Dateipfad/AffectiveForecasting_0707017.sav&quot;, to.data.frame = TRUE) # noch den eigenen Dateipfad einfügen data_b_test &lt;- data_b_test[, c(79, 95)] # sias5, sias5_RS Achtung: Wir müssen hier umdenken, da sias5_RS die rekodierte Version von sias5 widergibt. Wir wollen allerdings die allgemeingültige Zuordnung von Daten und Angaben im Codebuch verstehen. Die Zuordnung der Kodierung in den Daten (links) und der im Codebuch enthaltenen Skala (rechts) scheint wie folgt: 0 \\(\\rightarrow\\) 0 0=... \\(\\rightarrow\\) 1 1=... \\(\\rightarrow\\) 2 2=... \\(\\rightarrow\\) 3 4=... \\(\\rightarrow\\) 4 Die Ausprägung 5=... wird, wie oben bereits festgestellt, gar nicht genutzt. Sie wurde dennoch in den Faktorstufen der sias...-Items vermerkt. Wenn wir tiefgehender mit den Daten arbeiten würden (z.B. Datenanalyse), würde es sich anbieten, die Kodierung der Items anzupassen, sodass die Ausprägung auf der Skala auch aus der Kodierung ersichtlich wird. Man könnte dann auch darüber nachdenken, die Variablen als intervallskaliert zu behandeln (wie es der Standard ist). Ansonsten scheinen alle fehlenden Werte korrekt kodiert zu sein. 2.) Kodiere ggf. inkorrekt kodierte Missings zu NA um. Tipp Wir kodieren mittels &lt;- NA jene Ausprägungen um, die falsch kodierte fehlende Werte zeigen. Lösung A In der vorhergehenden Aufgabe haben wir jene Ausprägungen von Variablen identifiziert, die auch fehlende Werte kodieren sollen: \"\" und N/A. Diese kodieren wir nun um. data_a[data_a == &quot;&quot; | data_a == &quot;N/A&quot; ] &lt;- NA # Überprüfung: sapply(sapply(data_a[c(2, 4 ,5)], unique), sort, na.last=TRUE) # data_a[c(2, 4 ,5)] ist die Auswahl der Variablen, die die falschen Kodierungen enthielten ## $consent ## [1] &quot;Y&quot; NA ## ## $gender ## [1] &quot;f&quot; &quot;m&quot; NA ## ## $gender...comment ## [1] &quot;Comment? Thanks for the opportunity? Unsure why there is a comment box before the task is completed, but that&#39;s okay.&quot; ## [2] &quot;have a good day&quot; ## [3] NA Jetzt sind alle fehlenden Werte mit NA gekennzeichnet. 9.9.2 Übung 2: Verortung Nun wollen wir uns ein paar deskriptive Statistiken der fehlenden Werte anschauen. Achtung: Datensatz A: Die character-Variable gender...comment stellt einen Kommentar zu der Variablen gender dar. Sie kodiert demnach qualitative Daten, die wir uns im Folgenden nicht weiter anschauen wollen. Daher entfernen wir die Variable nun aus unserem (Analyse-)Datensatz: data_a &lt;- data_a[,-5] 1.) Wie viele Missings gibt es insgesamt im Datensatz (in absoluten und relativen Zahlen)? Tipp Mit der table()-Funktion können wir uns Häufigkeitstabellen ausgeben lassen. Jetzt müssen unsere Daten nur noch dichotom in fehlend und nicht fehlend eingeteilt werden. Lösung A absolute Anzahl: TRUE table(is.na(data_a)) ## ## FALSE TRUE ## 7912 1744 relative Anzahl: TRUE / (FALSE + TRUE) table(is.na(data_a))[2] / ( table(is.na(data_a))[1] + table(is.na(data_a))[2] ) ## TRUE ## 0.1806131 Es gibt 1744 fehlende Werte. Das sind ca. 18.06% aller Werte im Datensatz. Lösung B absolute Anzahl: TRUE table(is.na(data_b)) ## ## FALSE TRUE ## 9065 659 relative Anzahl: TRUE / (FALSE + TRUE) table(is.na(data_b))[2] / ( table(is.na(data_b))[1] + table(is.na(data_b))[2] ) ## TRUE ## 0.06777046 Es gibt 659 fehlende Werte. Das sind ca. 6.78% aller Werte im Datensatz. 2.) Welche Variable enthält die meisten Missings? Wie viele Missings sind das (in absoluten und relativen Zahlen) Tipp: Verortung Missings Mit colSums() werden die Summen der Spalten eines Datensatzes angegeben. Nun müssen wir unsere Daten nur wieder dichotom in fehlend und nicht fehlend einteilen. Tipp: Namen der Variablen Mit colnames() können wir uns Namen von Variablen ausgeben lassen. Wir müssen nur noch die relevanten auswählen. Lösung A # maximale Anzahl an Missings pro Variable: max(colSums(is.na(data_a))) ## [1] 13 # relative Anzahl an Missings pro Variable: max(colSums(is.na(data_a))) / nrow(data_a) ## [1] 0.1830986 # Spaltennamen der Variablen mit den meisten Missings: colnames(data_a[colSums(is.na(data_a)) == max(colSums(is.na(data_a)))]) ## [1] &quot;statements..1.&quot; &quot;statements..2.&quot; &quot;statements..3.&quot; ## [4] &quot;statements..4.&quot; &quot;statements..5.&quot; &quot;statements..6.&quot; ## [7] &quot;statements..7.&quot; &quot;statements..8.&quot; &quot;statements..9.&quot; ## [10] &quot;statements..10.&quot; &quot;statements..11.&quot; &quot;statements..12.&quot; ## [13] &quot;statements..13.&quot; &quot;statements..14.&quot; &quot;statements..15.&quot; ## [16] &quot;statements..16.&quot; &quot;statements..17.&quot; &quot;statements..18.&quot; ## [19] &quot;statements..19.&quot; &quot;statements..20.&quot; &quot;statements..21.&quot; ## [22] &quot;statements..22.&quot; &quot;statements..23.&quot; &quot;statements..24.&quot; ## [25] &quot;statements..25.&quot; &quot;statements..26.&quot; &quot;statements..27.&quot; ## [28] &quot;statements..28.&quot; &quot;statements..29.&quot; &quot;statements..30.&quot; ## [31] &quot;statements..31.&quot; &quot;statements..32.&quot; &quot;statements..33.&quot; ## [34] &quot;statements..34.&quot; &quot;statements..35.&quot; &quot;statements..36.&quot; ## [37] &quot;statements..37.&quot; &quot;statements..38.&quot; &quot;statements..39.&quot; ## [40] &quot;statements..40.&quot; &quot;statements..41.&quot; &quot;statements..42.&quot; ## [43] &quot;statements..43.&quot; &quot;statements..44.&quot; &quot;statements..45.&quot; ## [46] &quot;statements..46.&quot; &quot;statements..47.&quot; &quot;statements..48.&quot; ## [49] &quot;statements..49.&quot; &quot;statements..50.&quot; &quot;statements..51.&quot; ## [52] &quot;statements..52.&quot; &quot;statements..53.&quot; &quot;statements..54.&quot; ## [55] &quot;statements..55.&quot; &quot;statements..56.&quot; &quot;statements..57.&quot; ## [58] &quot;statements..58.&quot; &quot;statements..59.&quot; &quot;statements..60.&quot; ## [61] &quot;statements..61.&quot; &quot;statements..62.&quot; &quot;statements..63.&quot; ## [64] &quot;statements..64.&quot; &quot;statements..65.&quot; &quot;statements..66.&quot; ## [67] &quot;statements..67.&quot; &quot;statements..68.&quot; &quot;statements..69.&quot; ## [70] &quot;statements..70.&quot; &quot;statements..71.&quot; &quot;statements..72.&quot; ## [73] &quot;statements..73.&quot; &quot;statements..74.&quot; &quot;statements..75.&quot; ## [76] &quot;statements..76.&quot; &quot;statements..77.&quot; &quot;statements..78.&quot; ## [79] &quot;statements..79.&quot; &quot;statements..80.&quot; &quot;statements..81.&quot; ## [82] &quot;statements..82.&quot; &quot;statements..83.&quot; &quot;statements..84.&quot; ## [85] &quot;statements..85.&quot; &quot;statements..86.&quot; &quot;statements..87.&quot; ## [88] &quot;statements..88.&quot; &quot;statements..89.&quot; &quot;statements..90.&quot; ## [91] &quot;statements..91.&quot; &quot;statements..92.&quot; &quot;statements..93.&quot; ## [94] &quot;statements..94.&quot; &quot;statements..95.&quot; &quot;statements..96.&quot; ## [97] &quot;statements..97.&quot; &quot;statements..98.&quot; &quot;statements..99.&quot; ## [100] &quot;statements..100.&quot; &quot;statements..101.&quot; &quot;statements..102.&quot; ## [103] &quot;statements..103.&quot; &quot;statements..104.&quot; &quot;statements..105.&quot; ## [106] &quot;statements..106.&quot; &quot;statements..107.&quot; &quot;statements..108.&quot; ## [109] &quot;statements..109.&quot; &quot;statements..110.&quot; &quot;statements..111.&quot; ## [112] &quot;statements..112.&quot; &quot;statements..113.&quot; &quot;statements..114.&quot; ## [115] &quot;statements..115.&quot; &quot;statements..116.&quot; &quot;statements..117.&quot; ## [118] &quot;statements..118.&quot; &quot;statements..119.&quot; &quot;statements..120.&quot; ## [121] &quot;statements..121.&quot; &quot;statements..122.&quot; &quot;statements..123.&quot; ## [124] &quot;statements..124.&quot; &quot;statements..125.&quot; &quot;statements..126.&quot; ## [127] &quot;statements..127.&quot; &quot;statements..128.&quot; &quot;statements..c1.&quot; ## [130] &quot;statements..c2.&quot; &quot;statements..c3.&quot; &quot;statements..c4.&quot; Die meisten Missings sind in den statement-Variablen mit je einer absoluten Anzahl von 13 fehlenden Werten. Das entspricht ca. 18.31%. Lösung B # maximale Anzahl an Missings pro Variable: max(colSums(is.na(data_b))) ## [1] 72 # relative Anzahl an Missings pro Variable: max(colSums(is.na(data_b))) / nrow(data_b) ## [1] 0.3850267 # Spaltennamen der Variablen mit den meisten Missings: colnames(data_b[colSums(is.na(data_b)) == max(colSums(is.na(data_b)))]) ## [1] &quot;preselect_peak_e2&quot; Die meisten Missings hat die Variable preselect_peak_e2 mit 72 fehlenden Werten. Das entspricht ca. 38.5%. 3.) Welche Person hat die meisten Missings? Wie viele Missings sind das (in absoluten und relativen Zahlen)? Tipp: Verortung Missings Mit rowSums() werden die Summen der Zeilen eines Datensatzes angegeben. Zusätzlich müssen wir unsere Daten nur wieder dichotom in fehlend und nicht fehlend einteilen. Tipp: Zeilenindizes der Fälle Mit which() können wir uns die Zeilenindizes von Fällen ausgeben lassen. Wir müssen nur noch die relevanten auswählen. Lösung A # maximale Anzahl an Missings pro Person: max(rowSums(is.na(data_a))) ## [1] 135 # relative Anzahl an Missings pro Person: max(rowSums(is.na(data_a))) / ncol(data_a) ## [1] 0.9926471 # welche Personen haben die meisten Missings: which(rowSums(is.na(data_a)) == max(rowSums(is.na(data_a)))) ## [1] 1 3 4 27 32 41 49 51 Die meisten Missings haben die Personen 1, 3, 4, 27, 32, 41, 49, 51 mit jeweils 135 fehlenden Werten. Das entspricht ca. 99.26%. Lösung B # maximale Anzahl an Missings pro Person: max(rowSums(is.na(data_b))) ## [1] 20 # relative Anzahl an Missings pro Person: max(rowSums(is.na(data_b))) / ncol(data_b) ## [1] 0.3846154 # welche Personen haben die meisten Missings: which(rowSums(is.na(data_b)) == max(rowSums(is.na(data_b)))) ## [1] 183 185 186 187 Die meisten Missings haben die Personen 183, 185, 186, 187 mit jeweils 20 fehlenden Werten. Das sind ca. 38.5% fehlende Werte. 4.) Welche bzw. wie viele Patterns mit Missings (Missings in bestimmten Kombinationen von Variablen) gibt es? Visualisiere sie. (Für Datensatz B reicht es, wenn das am häufigsten auftretende Pattern mit Missings beschrieben wird.) Tipp Im Paket VIM gibt es die Funktion aggr(), die beim Visualisieren von Missings helfen kann. Lösung A Der Output von summary(aggr()) gibt uns verschiedene Informationen über die Missings und Missing-Patterns. Neben der Zusammensetzung der Patterns bekommen wir Auskunft über die absoluten und relativen Häufigkeiten der variablenweisen Missings und der Missing-Patterns. In den Grafiken steht blau für vorhandene Werte; rot für Missings. Wir sehen in der rechten Grafik, dass ein Missing-Pattern keine Missings enthält. #install.packages (&quot;VIM&quot;) library(VIM) summary(aggr(data_a)) ## ## Missings per variable: ## Variable Count ## id 0 ## consent 8 ## age 10 ## gender 10 ## statements..1. 13 ## statements..2. 13 ## statements..3. 13 ## statements..4. 13 ## statements..5. 13 ## statements..6. 13 ## statements..7. 13 ## statements..8. 13 ## statements..9. 13 ## statements..10. 13 ## statements..11. 13 ## statements..12. 13 ## statements..13. 13 ## statements..14. 13 ## statements..15. 13 ## statements..16. 13 ## statements..17. 13 ## statements..18. 13 ## statements..19. 13 ## statements..20. 13 ## statements..21. 13 ## statements..22. 13 ## statements..23. 13 ## statements..24. 13 ## statements..25. 13 ## statements..26. 13 ## statements..27. 13 ## statements..28. 13 ## statements..29. 13 ## statements..30. 13 ## statements..31. 13 ## statements..32. 13 ## statements..33. 13 ## statements..34. 13 ## statements..35. 13 ## statements..36. 13 ## statements..37. 13 ## statements..38. 13 ## statements..39. 13 ## statements..40. 13 ## statements..41. 13 ## statements..42. 13 ## statements..43. 13 ## statements..44. 13 ## statements..45. 13 ## statements..46. 13 ## statements..47. 13 ## statements..48. 13 ## statements..49. 13 ## statements..50. 13 ## statements..51. 13 ## statements..52. 13 ## statements..53. 13 ## statements..54. 13 ## statements..55. 13 ## statements..56. 13 ## statements..57. 13 ## statements..58. 13 ## statements..59. 13 ## statements..60. 13 ## statements..61. 13 ## statements..62. 13 ## statements..63. 13 ## statements..64. 13 ## statements..65. 13 ## statements..66. 13 ## statements..67. 13 ## statements..68. 13 ## statements..69. 13 ## statements..70. 13 ## statements..71. 13 ## statements..72. 13 ## statements..73. 13 ## statements..74. 13 ## statements..75. 13 ## statements..76. 13 ## statements..77. 13 ## statements..78. 13 ## statements..79. 13 ## statements..80. 13 ## statements..81. 13 ## statements..82. 13 ## statements..83. 13 ## statements..84. 13 ## statements..85. 13 ## statements..86. 13 ## statements..87. 13 ## statements..88. 13 ## statements..89. 13 ## statements..90. 13 ## statements..91. 13 ## statements..92. 13 ## statements..93. 13 ## statements..94. 13 ## statements..95. 13 ## statements..96. 13 ## statements..97. 13 ## statements..98. 13 ## statements..99. 13 ## statements..100. 13 ## statements..101. 13 ## statements..102. 13 ## statements..103. 13 ## statements..104. 13 ## statements..105. 13 ## statements..106. 13 ## statements..107. 13 ## statements..108. 13 ## statements..109. 13 ## statements..110. 13 ## statements..111. 13 ## statements..112. 13 ## statements..113. 13 ## statements..114. 13 ## statements..115. 13 ## statements..116. 13 ## statements..117. 13 ## statements..118. 13 ## statements..119. 13 ## statements..120. 13 ## statements..121. 13 ## statements..122. 13 ## statements..123. 13 ## statements..124. 13 ## statements..125. 13 ## statements..126. 13 ## statements..127. 13 ## statements..128. 13 ## statements..c1. 13 ## statements..c2. 13 ## statements..c3. 13 ## statements..c4. 13 ## ## Missings in combinations of variables: ## Combinations ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1 ## 0:0:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1 ## 0:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1 ## Count Percent ## 58 81.690141 ## 3 4.225352 ## 2 2.816901 ## 8 11.267606 Es gibt drei Patterns mit Missings. Generell fehlen in allen drei Missing-Patterns alle statements-variablen (Spaltennummern 5:131). Das erste (nur aus diesen fehlenden Werten bestehende) Pattern kommt 3 mal vor. Im zweiten Pattern fehlen zusätzlich noch die Variablen age und gender (Spaltennummern 3 und 4) und es kommt 2 mal vor. Im dritte Pattern fehlt zusätzlich noch die Variable consent (Spaltennummer 2) und es kommt 8 mal vor. Die Fälle, welche ins letzte Pattern fallen, haben demnach auf allen Variablen, außer id, fehlende Werte. Zufälligerweise gibt es auch genau 8 fehlende Fälle auf der Variablen consent. Es ist hier naheliegend zu vermuten, dass diese 8 Personen den Fragebogen gar nicht ausgefüllt haben (d.h. die Erhebung abgebrochen haben). Das sind wahrscheinlich auch dieselben Personen, die wir in 2.3 gefunden haben, als wir die Fälle mit den meisten Missings identifiziert haben. # Überprüfung ob Fälle mit max-Anzahl Missings == Fälle mit Missings auf consent which(rowSums(is.na(data_a)) == max(rowSums(is.na(data_a)))) == which(is.na(data_a$consent)) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE Es handelt sich um dieselben Fälle. Wir entfernen diese 8 Personen nun aus dem Datensatz. data_a &lt;- data_a[which(!is.na(data_a$consent)),] # which(!is.na(data_a$consent)) selektiert Fälle mit vorhandenen Werten in consent Lösung B Wir bekommen von summary(aggr()) verschiedene Informationen über die Missings und Missing-Patterns. Neben der Zusammensetzung der Patterns bekommen wir Auskunft über die absoluten und relativen Häufigkeiten der variablenweisen Missings und der Missing-Patterns. In den Grafiken steht blau für vorhandene Werte; rot für Missings. Wir sehen in der rechten Grafik, dass ein Missing-Pattern keine Missings enthält. #install.packages (&quot;VIM&quot;) library(VIM) summary(aggr(data_b)) ## ## Missings per variable: ## Variable Count ## P_ID 0 ## age 0 ## gender 0 ## race_white_yn 1 ## ethn 0 ## race 0 ## group 0 ## cond 0 ## preselect_curr_e1 19 ## preselect_curr_e2 22 ## preselect_curr_e3 13 ## preselect_curr_e4 31 ## preselect_ave_e1 37 ## preselect_ave_e2 44 ## preselect_ave_e3 59 ## preselect_ave_e4 49 ## preselect_e1 10 ## preselect_e2 14 ## preselect_e3 34 ## preselect_e4 16 ## preselect_neg_e1 13 ## preselect_neg_e2 24 ## preselect_neg_e3 37 ## preselect_neg_e4 25 ## preselect_pos_e1 6 ## preselect_pos_e2 7 ## preselect_pos_e3 34 ## preselect_pos_e4 5 ## preselect_peak_e1 24 ## preselect_peak_e2 72 ## preselect_peak_e3 6 ## preselect_peak_e4 51 ## sias1 0 ## sias2 0 ## sias3 1 ## sias4 0 ## sias5 0 ## sias6 1 ## sias7 0 ## sias8 0 ## sias9 0 ## sias10 1 ## sias11 1 ## sias12 1 ## sias13 0 ## sias14 0 ## sias15 0 ## sias16 0 ## sias17 0 ## sias18 0 ## sias19 0 ## sias20 1 ## ## Missings in combinations of variables: ## Combinations ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:1:1:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:1:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:1:1:0:0:1:1:0:0:1:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:1:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:1:0:0:0:1:1:0:0:1:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:1:0:0:0:1:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:1:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:0:0:1:1:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:1:0:0:0:1:0:0:0:0:1:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:1:0:0:0:0:0:0:0:0:0:0:1:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:1:0:0:1:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:1:1:0:0:0:0:0:1:0:1:0:0:0:0:1:0:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:0:1:1:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:0:0:0:0:0:0:0:1:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:0:0:1:0:0:0:1:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:0:1:1:1:1:1:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:0:1:0:0:1:1:1:0:0:1:0:0:0:1:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:0:0:1:1:1:1:0:0:1:0:0:0:1:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:0:1:0:0:1:0:0:0:1:0:0:0:1:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:0:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:1:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:1:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:0:1:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:1:0:0:0:0:0:1:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:0:0:0:0:0:1:1:0:0:1:0:0:0:1:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:0:0:1:0:0:1:1:0:0:0:0:0:0:0:0:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:0:0:1:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:1:1:0:1:1:1:1:0:1:1:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:0:1:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:0:1:1:1:1:1:0:0:1:1:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:1:0:1:1:1:0:0:0:1:0:0:0:0:1:0:0:1:0:1:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:1:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:1:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:0:0:0:0:0:1:1:1:1:1:1:1:1:0:0:1:0:0:1:1:0:0:0:1:0:0:1:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## 0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0 ## Count Percent ## 51 27.2727273 ## 1 0.5347594 ## 1 0.5347594 ## 6 3.2085561 ## 6 3.2085561 ## 4 2.1390374 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 3 1.6042781 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 2 1.0695187 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 2 1.0695187 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 4 2.1390374 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 2 1.0695187 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 2 1.0695187 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 ## 1 0.5347594 # Anzahl der Patterns (minus eins, weil das nur vorhandene Werte enthält): length(summary(aggr(data_b))[[&quot;combinations&quot;]][[&quot;Combinations&quot;]]) - 1 ## [1] 114 Es gibt 114 Patterns mit Missings. Die Missing-Pattern, welche am häufigsten vorkommen, mit jeweils 6 mal, haben nur einen fehlenden Wert auf der Variablen preselect_peak_e4 (Spaltennummer 32) bzw. preselect_peak_e2 (Spaltennummer 30). 9.9.3 Übung 3: Zufälligkeit Nachdem wir uns im vorhergehenden Abschnitt deskriptiv-statistische Kennwerte der Missings und Missing-Patterns angeschaut haben, wollen wir uns im Folgenden auch inferenz-statistisch absichern. Dazu überprüfen wir die Zufälligkeit der Missing-Patterns. Ob fehlende Werte zufällig sind ist eine essentielle Frage, die wir vor der Nutzung der Daten klären müssen. 1.) Sind die fehlenden Werte MCAR? Was hätte es für Konsequenzen für die weitere Datenverarbeitung, wenn die fehlenden Werte MAR oder MNAR wären? Auffrischung: MCAR, MAR und MNAR MCAR: Missings hängen weder von der (fehlenden) Ausprägung auf der Variablen selbst noch von anderen Variablen und deren fehlenden Werten ab. Sie sind also komplett zufällig. MAR: Missings hängen nicht von der (fehlenden) Ausprägung auf der Variablen selbst, aber von anderen Variablen und deren Missings ab. Die Missings sind also zufällig, sobald für diese andere Variablen kontrolliert wird. MNAR: Missings hängen von der (fehlenden) Ausprägung auf der Variablen selbst ab. Sie sind also selbst dann nicht zufällig, wenn für die anderen Variablen kontrolliert wird. Tipp Zur Überprüfung der Zufälligkeit können wir den Test von Little (1988)1 verwenden. Dieser überprüft, ob es signifikante Unterschiede zwischen den variablenweisen Mittelwerten der Missing Patterns gibt. Die \\(H0\\), dass es keine überzufälligen Unterschiede der Mittelwerte in Abhängigkeit der Missing Patterns gibt (= MCAR), ist unsere Wunschhypothese. Der Test ist in der Funktion mcar_test() aus dem Paket naniar implementiert. Achtung: Die Funktion mcar_test() kann wir nur mit Variablen des Typs numeric (integer und double), logical und factor umgehen. Außerdem können keine qualitativen Daten ausgewertet werden. Es gab eine qualitative Variable in Datensatz A, gender...comment, welche wir aber zu Beginn des letzten Abschnitts bereits aus dem Datensatz entfernt haben. Lösung A Die Funktion mcar_test() kann nicht mit Daten vom Typ character umgehen. Daher müssen wir die nominalskalierten Variablen gender und consent zuerst faktorisieren: data_a$gender &lt;- factor(data_a$gender) data_a$consent &lt;- factor(data_a$consent) Das Signifikanzniveau für den Test legen wir auf \\(\\alpha = 0.05\\) fest. # install.packages(&quot;remotes&quot;) # remotes::install_github(&quot;njtierney/naniar&quot;) library(naniar) mcar_test(data_a) ## Warning in norm::prelim.norm(data): NAs introduced by coercion to integer range ## Error in `dplyr::mutate()`: ## ! Problem while computing ## `d2 = purrr::pmap_dbl(...)`. ## ℹ The error occurred in group 1: ## miss_pattern = 1. ## Caused by error in `solve.default()`: ## ! Lapack routine dgesv: system is exactly singular: U[2,2] = 0 Wir bekommen hier nur eine Fehlermeldung ausgegeben. Das liegt daran, dass die Funktion mit unseren Daten nicht klar kommt. Genauer gesagt, wird nur ein Missing-Pattern mittels mcar_test() erkannt, obwohl es nach aggr() 4 gibt (bzw. nun nur noch 3, da wir alle Fälle mit fehlenden Werten auf consent entfernt haben und damit ein Missing-Pattern weniger haben). Leider gibt es keine alternative Implementierung des Tests von Little (für die aktuelle R-Version), und daher können wir nicht weiter an diesen Daten arbeiten. Lösung B Das Signifikanzniveau legen wir auf \\(\\alpha = 0.05\\) fest. # install.packages(&quot;remotes&quot;) # remotes::install_github(&quot;njtierney/naniar&quot;) library(naniar) mcar_test(data_b) ## Warning in norm::prelim.norm(data): NAs introduced by coercion to integer range ## # A tibble: 1 × 4 ## statistic df p.value missing.patterns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 3572. 5355 1 115 Da der \\(p\\)-Wert über unserem á priori festgelegtem Signifikanzniveau \\(\\alpha = 0.05\\) liegt, haben wir Evidenz dafür, dass die Daten MCAR sind. Achtung: Das hier ein \\(p\\)-Wert von \\(1\\) ausgegeben wird ist formal nicht korrekt. Das liegt wahrscheinlich an der Rundung, denn korrekterweise wäre es wohl \\(p=0.99\\). Konsequenzen der Verarbeitung von Daten, die MAR oder MNAR sind Wenn die Missings MAR oder MNAR sind, kann es zu Parameterverzerrungen kommen, wenn wir Methoden nutzen würden, welche die strengere Annahme MCAR annehmen. Beispiele: Wir können keine Fälle löschen wenn die Daten MAR oder MNAR sind. Wenn die Daten MNAR sind, können wir auch die Maximum Likelihood Schätzung sowie die multiple Imputation nicht nutzen. 2.) Wie gehen wir weiter vor (mit Hinblick auf die Ergebnisse des Tests nach Little)? Lösung Wie bereits oben beschrieben können wir nicht (einfach) weiter an Datensatz A arbeiten, solange wir nicht testen konnten, ob die Daten MCAR sind. Eine Möglichkeit wäre es, die Daten mit einer anderen Statistiksoftware auszuwerten, in der es einen implementierten MCAR-Test gibt. Es ist aber nicht ausgeschlossen, dass dieser auf dem gleichen Algorithmus wie mcar_test() beruht und wir das gleiche Problem haben würden. Bezüglich Datensatz B können wir alle Verfahren anwenden, die MCAR oder MAR fordern, weil wir bei der Durchführung des Test nach Little Evidenz für die \\(H_0\\) (d.h. Daten sind MCAR) gefunden haben. 9.9.4 Übung 4: Umgang Achtung: Die folgenden Aufgaben beziehen sich nur auf den Datensatz B. 1.) Berechne die Mittelwerte aller numerischen, mindestens intervallskalierten Variablen jeweils mit listwise und pairwise deletion. Vergleiche die jeweilige Anzahl der für die Berechnung genutzten Datenpunkte. Der Mittelwert welcher Variablen unterscheidet sich am meisten, welcher am wenigsten? Tipp: listwise und pairwise deletion listwise/casewise deletion: Fälle mit mind. einem Missing in einer Variablen werden aus allen Mittelwertsberechnungen ausgeschlossen. pairwise deletion: Fälle mit Missings werden nur aus den jeweiligen Mittelwertsberechnungen derjenigen Variablen ausgeschlossen, in denen der Wert fehlt. Tipp: numerisch Mit is.numeric() können wir überprüfen, ob ein Vektor numerisch ist. Das müssen wir nur noch auf alle Spalten des Datensatzes anwenden. Tipp: Anzahl Beobachtungen Bei listwise deletion gehen für die Berechnung der Mittelwerte in jede Variable die gleichen Datenpunkte ein. Wir können einfach mit nrow() die Anzahl herausfinden. Bei pairwise deletion gehen für die Berechnung der Mittelwerte unterschiedliche Datenpunkte ein. Mit colSums() zählen wir Spaltensummen; jetzt müssen wir nur noch die nicht fehlenden Werte angeben. Tipp: Unterschiedlichkeit der Mittelwerte Dazu berechnen wir die Differenz der Mittelwerte einer Variablen, die mit listwise und pairwise deletion erstellt wurden. Sinnvoll ist es hier, mit abs() die absolute Differenz zu erhalten. Lösung # herausfinden, welche Variablen numerisch sind: which_num &lt;- c() for (i in 1:ncol(data_b)){ if (is.numeric(data_b[, i]) == TRUE) { which_num &lt;- append(which_num, i) } } # Selektion auf Datensatz anwenden: data_b_num &lt;- data_b[, which_num] # Variablen, die nicht (mind.) intervallskaliert sind, aussortieren; # dazu vergleichen wir die Angaben im Codebuch und den Datentyp der Variablen data_b_num &lt;- data_b_num[, -c(1, # P_ID (nominalskaliert) 3 # race_white_yn (nominalskaliert) )] # Mittelwerte berechnen: listwise_mean &lt;- colMeans(na.omit(data_b_num)) pairwise_mean &lt;- colMeans(data_b_num, na.rm=TRUE) # Anzahl genutzter Datenpunkte herausfinden: listwise_n &lt;- nrow(na.omit(data_b)) # gleich für alle Variablen pairwise_n &lt;- colSums(!is.na(data_b_num)) # verschieden für jede Variable # Vergleich der jeweiligen Mittelwerte vergleich &lt;- data.frame(listwise_mean, pairwise_mean, listwise_n, pairwise_n) for (i in 1:nrow(vergleich)){ vergleich$diff[i] &lt;- abs(vergleich[i, 1] - vergleich[i, 2]) } Die Anzahl der in die Berechnungen eingegangenen Datenpunkte unterscheidet sich stark. Während bei pairwise deletion zwischen 115 und 187 Beobachtungen für die Berechnungen genutzt wurden, wurden bei listwise deletion bei der Berechnung der Mittelwerte aller Variablen (die gleichen) 51 Beobachtungen genutzt. vergleich[which(vergleich$diff == min(vergleich$diff)), ] # ähnlichster Mittelwert ## listwise_mean pairwise_mean listwise_n pairwise_n diff ## age 19.14815 19.24064 51 187 0.09249356 vergleich[which(vergleich$diff == max(vergleich$diff)), ] # unterschiedlichster Mittelwert ## listwise_mean pairwise_mean listwise_n pairwise_n diff ## preselect_peak_e3 -26.31481 -44.12155 51 181 17.80673 Die Variable mit dem geringsten Unterschied in den mit listwise und pairwise deletion berechneten Mittelwerten ist age; die Variable mit dem größten Unterschied ist preselect_peak_e3. 2.) Berechne die Korrelationen aller numerischen, mindestens intervallskalierten Variablen jeweils mit listwise und pairwise deletion. Die Korrelation welcher Variablen unterscheidet sich am meisten zwischen listwise und pairwise deletion, welcher am wenigsten? Tipp: listwise und pairwise deletion listwise/casewise deletion: Fälle mit mind. einem Missing in einer Variablen werden aus allen Mittelwertsberechnungen ausgeschlossen. pairwise deletion: Fälle mit Missings werden nur aus der jeweiligen Korrelationsberechnungen derjenigen Variablen ausgeschlossen, in denen der Wert fehlt. Tipp: numerisch Mit is.numeric() können wir überprüfen, ob ein Vektor numerisch ist. Das müssen wir nur noch auf alle Spalten des Datensatzes anwenden. Tipp: Unterschiedlichkeit der Korrelationen Dazu berechnen wir die Differenz der Korrelationen jeder Variablen, die mit listwise und pairwise deletion erstellt wurden. Die Korrelationen von beiden Methoden sind je in einer Matrix gespeichert, d.h. für den Vergleich lohnt es sich, eine Schleife, die über alle Zeilen und Spalten geht, zu erstellen. Sinnvoll ist außerdem, mit abs() die absolute Differenz zu berechnen. Tipp: Korrelationen der Variablen mit den extremsten Unterschieden finden Mit which(..., arr.ind = TRUE) können wir uns Zeilenname, Zeilen- und Spaltenindizes ausgeben lassen. Lösung # herausfinden, welche Variablen numerisch sind: which_num &lt;- c() for (i in 1:ncol(data_b)){ if (is.numeric(data_b[, i]) == TRUE) { which_num &lt;- append(which_num, i) } } # Selektion auf Datensatz anwenden: data_b_num &lt;- data_b[, which_num] # Variablen, die nicht (mind.) intervallskaliert sind, aussortieren; # dazu vergleichen wir die Angaben im Codebuch und den Datentyp der Variablen data_b_num &lt;- data_b_num[, -c(1, # P_ID (nominalskaliert) 3 # race_white_yn (nominalskaliert) )] # Korrelationen berechnen: listwise_corr &lt;- cor(data_b_num, use=&quot;complete.obs&quot;) pairwise_corr &lt;- cor(data_b_num, use=&quot;pairwise.complete.obs&quot;) # Matrix für die Differenzen der Korrelationen vorbereiten: diff_corr &lt;- matrix(nrow=nrow(listwise_corr), # Matrix erstellen ncol=ncol(listwise_corr)) diff_corr &lt;- data.frame(diff_corr, # zu Dataframe umwandeln row.names = colnames(data_b_num)) # Zeilennamen ergänzen colnames(diff_corr) &lt;- colnames(data_b_num) # Spaltennamen ergänzen # Differenzen der Korrelationen berechnen: for (i in 1:nrow(listwise_corr)){ # über alle Zeilen ... for (j in 1:ncol(listwise_corr)){ # ... und über alle Spalten ... diff_corr[i,j] &lt;- abs(listwise_corr[i,j] - pairwise_corr[i,j]) } # ... absolute Abweichungen berechnen } # (zuerst) doppelte Elemente und Diagonalelemente löschen: diff_corr[upper.tri(diff_corr, diag = TRUE)] &lt;- NA # Korrelation mit größtem Unterschied: max(diff_corr, na.rm = TRUE) ## [1] 0.3696347 which(diff_corr == max(diff_corr, na.rm = TRUE), arr.ind = T) # Zeilnenname, Zeilen- und Spaltennummer ## row col ## preselect_peak_e3 24 4 colnames(diff_corr[4]) # Spaltenname ## [1] &quot;preselect_curr_e3&quot; # Korrelation mit kleinstem Unterschied: min(diff_corr, na.rm = TRUE) ## [1] 0.0001259926 which(diff_corr == min(diff_corr, na.rm = TRUE), arr.ind = T) # Zeilenname, Zeilen- und Spaltennummer ## row col ## preselect_neg_e2 15 3 colnames(diff_corr[3]) # Spaltenname ## [1] &quot;preselect_curr_e2&quot; Die Korrelation mit dem größten Unterschied zwischen der listwise und der pairwise deletion Methode ist jene zwischen preselect_peak_e3 und preselect_curr_e3 (0.37); die Korrelation mit dem kleinsten Unterschied jene zwischen preselect_neg_e2 und preselect_curr_e2 (1.3^{-4}). Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] naniar_0.6.1 VIM_6.2.2 readxl_1.3.1 foreign_0.8-82 ## [5] devtools_2.4.5 usethis_2.1.6 ICC_2.4.0 readr_2.1.3 ## [9] Hmisc_4.7-1 Formula_1.2-4 survival_3.2-13 lattice_0.20-45 ## [13] ggplot2_3.4.0 colorspace_2.0-3 psych_2.2.9 car_3.1-1 ## [17] carData_3.0-5 kableExtra_1.3.4 dplyr_1.0.10 htmltools_0.5.3 ## [21] rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] deldir_1.0-6 ellipsis_0.3.2 class_7.3-20 ## [4] visdat_0.5.3 htmlTable_2.4.1 base64enc_0.1-3 ## [7] fs_1.5.2 rstudioapi_0.13 proxy_0.4-27 ## [10] remotes_2.4.2 fansi_1.0.3 ranger_0.14.1 ## [13] xml2_1.3.3 splines_4.2.0 mnormt_2.1.1 ## [16] robustbase_0.95-0 cachem_1.0.6 pkgload_1.3.0 ## [19] jsonlite_1.8.3 cluster_2.1.2 png_0.1-7 ## [22] shiny_1.7.3 compiler_4.2.0 httr_1.4.2 ## [25] backports_1.4.1 assertthat_0.2.1 Matrix_1.5-1 ## [28] fastmap_1.1.0 cli_3.4.1 later_1.3.0 ## [31] prettyunits_1.1.1 tools_4.2.0 gtable_0.3.0 ## [34] glue_1.6.2 Rcpp_1.0.9 cellranger_1.1.0 ## [37] jquerylib_0.1.4 vctrs_0.5.0 svglite_2.1.0 ## [40] nlme_3.1-155 lmtest_0.9-40 laeken_0.5.2 ## [43] xfun_0.34 stringr_1.4.0 ps_1.7.2 ## [46] rvest_1.0.2 mime_0.12 miniUI_0.1.1.1 ## [49] lifecycle_1.0.3 DEoptimR_1.0-11 zoo_1.8-11 ## [52] MASS_7.3-56 scales_1.2.1 hms_1.1.1 ## [55] promises_1.2.0.1 parallel_4.2.0 RColorBrewer_1.1-2 ## [58] yaml_2.3.5 memoise_2.0.1 gridExtra_2.3 ## [61] sass_0.4.2 rpart_4.1.16 latticeExtra_0.6-30 ## [64] stringi_1.7.8 highr_0.9 e1071_1.7-12 ## [67] checkmate_2.0.0 boot_1.3-28 pkgbuild_1.3.1 ## [70] rlang_1.0.6 pkgconfig_2.0.3 systemfonts_1.0.4 ## [73] evaluate_0.15 purrr_0.3.4 htmlwidgets_1.5.4 ## [76] tidyselect_1.2.0 processx_3.8.0 norm_1.0-10.0 ## [79] magrittr_2.0.2 bookdown_0.29 R6_2.5.1 ## [82] generics_0.1.2 profvis_0.3.7 DBI_1.1.2 ## [85] pillar_1.8.1 withr_2.5.0 sp_1.5-0 ## [88] abind_1.4-5 nnet_7.3-17 tibble_3.1.8 ## [91] crayon_1.5.2 interp_1.0-33 utf8_1.2.2 ## [94] tzdb_0.3.0 urlchecker_1.0.1 jpeg_0.1-9 ## [97] data.table_1.14.4 callr_3.7.2 vcd_1.4-10 ## [100] digest_0.6.30 webshot_0.5.4 xtable_1.8-4 ## [103] tidyr_1.2.1 httpuv_1.6.5 munsell_0.5.0 ## [106] viridisLite_0.4.1 bslib_0.4.0 sessioninfo_1.2.2 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. Little, R. J. A. (1988). A test of missing completely at random for multivariate data with missing values. Journal of the American Statistical Association, 83(404), 1198–1202. (für HU-Studierende über ub.hu-berlin.de zugänglich)↩︎ "],["wide--long-format.html", "Chapter 10 Wide- &amp; Long-Format 10.1 Vom Long- ins Wide-Format 10.2 Vom Wide- ins Long-Format 10.3 Andere Funktionen zum Umformatieren", " Chapter 10 Wide- &amp; Long-Format Einleitung Wenn man wissenschaftliche Fragestellungen untersuchen möchte, benötigt man ein geeignetes Format, in dem die Daten vorliegen. Manchmal stehen die Daten, die man auswerten möchte, aber nicht im “richtigen” Format zur Verfügung. Im Wesentlichen gibt es zwei verschiedene Darstellungsformen von Tabellendaten (Datensätzen): das Wide- und das Long-Format. Im Wide-Format entspricht jede Zeile einer Untersuchungseinheit. So liegen beispielsweise messwiederholte Variablen in unterschiedlichen Spalten vor. Beispiel 1 Wide-Format: Messzeitpunkte Untersuchungseinheit t1 t2 t3 1 4 3 1 2 5 2 3 Untersuchungseinheit 1 Untersuchungseinheit 2 Messwiederholung Rater Beispiel 2 Wide-Format: Rater Untersuchungseinheit self friend parent 1 2 1 3 2 3 4 2 Im Long-Format hingegen liegen Messung von einer Untersuchungseinheit in mehreren Zeilen vor. Dabei sind die Werte einer Untersuchungseinheit, die in unterschiedlichen Modi (z.B. Messzeitpunkte, Rater) erhoben wurden, untereinander gelistet. So werden beispielsweise messwiederholte Variablen in einer Spalte dargestellt. Beispiel 1 Long-Format: Messzeitpunkte Untersuchungseinheit Zeitpunkt Messung 1 1 4 1 2 3 1 3 1 2 1 5 2 2 2 2 3 3 Beispiel 2 Long-Format: Rater Untersuchungseinheit Rater Messung 1 self 2 1 friend 1 1 parent 3 2 self 3 2 friend 4 2 parent 2 Wir schauen uns zwei Möglichkeiten an, mit denen man Datensätze umformatieren kann: reshape() aus dem Standardpaket stats gather() bzw. spread() aus dem Paket tidyr Standard- oder auch Basispakete sind Pakete, die von Anfang an in R enthalten sind, ohne dass man sie installieren muss. Wir werden beide Pakete nutzen, um unsere Daten vom Long- ins Wide-Format und vom Wide- ins Long-Format zu überführen. Wir stellen beide Möglichkeiten zum Umformatieren vor, damit jeder ausprobieren kann, mit welcher Funktion sie bzw. er besser arbeiten kann. Während gather() und spread() manchen Nutzern intuitiver und einfacher erscheinen, werden die Daten mit reshape() etwas übersichtlicher dargestellt (z.B. durch Zeilennummerierungen). Falls ihr das Paket tidyr noch nicht installiert habt, könnt ihr das folgendermaßen tun: install.packages(\"tidyr\", dependencies = TRUE) Beispieldatensatz für dieses Kapitel Im Zuge dieses Kapitels werden wir mit dem Datensatz ChickWeight arbeiten, der standardmäßig in R enthalten ist. Wir laden ihn folgendermaßen herunter: data(ChickWeight) Die Daten stammen aus einem Längsschnitt-Experiment, in dem der Einfluss von verschiedenem Futter auf das Wachstum von Küken untersucht wurde. Der Datensatz liegt im Long-Format vor und enthält vier Variablen. Mehr Informationen zu den Variablen finden wir hier. weight: Körpergewicht eines Kükens in Gramm Time: Tage seit der Geburt des Kükens Chick: Identifikationsnummer des Kükens Diet: Nummer der Futtergruppe (1, 2, 3, 4) Von besonderem Interesse für uns ist die Zeitvariable Time. Diese werden wir im Folgenden nutzen, um den Datensatz vom Long- ins Wide-Format, und dann wieder zurück ins Long-Format, zu bringen. Achtung: Der Einfachheit halber wird im Folgenden der Begriff messwiederholte Variable verwendet, wenn von einer Variablen die Rede ist, die mehrfach erhoben wurde (d.h. im Wide-Format in mehreren Spalten und im Long-Format in einer Spalte vorliegt). Im Beispiel zum Wide-Format oben sind demnach die Variablen t1, t2, t_3 sowie self, friend und parent messwiederholt. In unserem Beispiel-Datensatz (der im Long-Format vorliegt) ist das die Variable weight. Gleiches gilt für den Begriff Zeitvariable, die die verschiedenen Modi der messwiederholten Variablen kodiert. Im Beispiel zum Long-Format oben entspricht diese den Variablen Zeitpunkt und Rater. In unserem Beispiel-Datensatz (der im Long-Format vorliegt) ist das die Variable Time. 10.1 Vom Long- ins Wide-Format Hierbei wird eine messwiederholte Variable (weight) in Abhängigkeit der Ausprägungen der Zeitvariable (Time) in mehrere Spalten aufgeteilt. Im Wide-Format gibt es mehr Spalten und weniger Zeilen (als im Long-Format). Die Anzahl der Zeilen entspricht der Anzahl der Untersuchungseinheiten. In unserem Datensatz sind die Untersuchungseinheiten Küken. Sowohl reshape() als auch spread() benötigen zur Formatierung von Long nach Wide eine ID-Variable im Datensatz (auch wenn man diese bei spread() nicht an ein Argument übergeben muss). Eine ID-Variable bezeichnet die Untersuchungsobjekte. Im Eingangsbeispiel wäre dies die Variable Untersuchungseinheit. 10.1.1 stats: reshape() Mit dem reshape()-Befehl kann man Daten sowohl vom Long- ins Wide-Format als auch vom Wide- ins Long-Format bringen. Bei der Formatierung von Long in Wide sind folgende 5 Argumente wichtig: data: Name des Datensatzes v.names: messwiederholte Variable, die im Long-Format in einer Spalte vorliegt und im Wide-Format auf mehrere Spalten ausgedehnt werden soll timevar: Zeitvariable, die im Long-Format die Modi kodiert idvar: ID-Variable, die unterschiedliche Untersuchungseinheiten kodiert direction: vom Long- ins Wide-Format Zusätzlich kann man mit drop=\"Variable\" eine bzw. mit drop=c(\"Variable_1\", \"Variable_2\", ..., \"Variable_x\") mehrere Variablen aus dem umformatierten Datensatz entfernen. reshape_wide &lt;- reshape(data=ChickWeight, # Name des Datensatzes v.names=&quot;weight&quot;, # messwiederholte Variable (in einer Spalte) timevar=&quot;Time&quot;, # (bestehende) Zeitvariable idvar=&quot;Chick&quot;, # (bestehende) ID-Variable direction=&quot;wide&quot;) # vom Long- ins Wide-Format Achtung: Ganz links sehen wir die “alte” Zeilennummerierung von ChickWeight. Die Zeilennummerierung ändert sich in reshape_wide, weil alle Beobachtungen einer Untersuchungseinheit zu unterschiedlichen Messzeitpunkten in einer Zeile vorliegen. Das erkennt man auch daran, dass die Zeilennummerierung von ChickWeight (meistens) in 12-er Schritten vorliegt und es genau 12 Messzeitpunkte gibt. Da bei einigen Küken weniger als 12 Messzeitpunkte vorhanden sind, gilt das aber nicht für den ganzen Datensatz reshape_wide. 10.1.2 tidyr: spread() Hier sind nur drei Argumente wichtig: data: Name des Datensatzes key: Zeitvariable, die im Long-Format unterschiedliche Modi kodiert value: messwiederholte Variable, die im Long-Format in einer Spalte vorliegt und im Wide-Format auf mehrere Spalten ausgedehnt werden soll library(tidyr) # Laden des Pakets spread_wide &lt;- spread(data=ChickWeight, # Name des Datensatzes key=&quot;Time&quot;, # Zeitvariable value=&quot;weight&quot;) # messwiederholte Variable 10.1.3 Unterschied zwischen reshape() und spread() Bei reshape() werden Zeilennummerierungen auf Basis des übergebenen Datensatzes erzeugt (siehe Hinweis im reshape()-Abschnitt). Wide-Format: Argumente von reshape() und spread() gegenübergestellt reshape spread data data v.names value timevar key idvar kein explizites Argument, aber benötigt Während wir in reshape() mit dem Parameter direction fetslegen, welches Format wir erstellen wollen, nutzen wir in tidyr entweder spread() oder gather(). Die Benennung der Spalten der messwiederholten Variable unterscheidet sich in den beiden Ansätzen. Während bei reshape() eine Kombination aus dem Namen der messwiederholten Variablen und der Ausprägung der Zeitvariablen genutzt wird (z.B. weight.0), wird bei spread() nur die Ausprägung der Zeitvariablen genutzt (z.B. 0). 10.2 Vom Wide- ins Long-Format Hierbei wird eine messwiederholte Variable (weight), die in mehreren Spalten vorliegt, zu einer Spalte zusammengefasst und es wird eine dazugehörige Zeitvariable (Time) erstellt. Im Long-Format gibt es weniger Spalten und mehr Zeilen (als im Wide-Format). Die Anzahl der Zeilen entspricht nicht der Anzahl der Untersuchungseinheiten, sondern der Anzahl der Untersuchungseinheiten x Anzahl der (jeweiligen) Messwiederholungen. Deswegen ist es im Long-Format besonders wichtig, eine ID-Variable zu haben, um einzelne Untersuchungseinheiten differenzieren zu können. Zur Erinnerung: In unserem Beispiel sind die Untersuchungseinheiten Küken. Achtung: Manchmal werden nicht alle Untersuchungseinheiten zu jedem Zeitpunkt untersucht. In unserem Datensatz ChickWeight ist das der Fall. Wir können uns folgendermaßen anschauen, wie häufig jede Untersuchungseinheit beobachtet wurde: table(ChickWeight$Chick) ## ## 18 16 15 13 9 20 10 8 17 19 4 6 11 3 1 12 2 5 14 7 24 30 22 23 27 28 ## 2 7 8 12 12 12 12 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 ## 26 25 29 21 33 37 36 31 39 38 32 40 34 35 44 45 43 41 47 49 46 50 42 48 ## 12 12 12 12 12 12 12 12 12 12 12 12 12 12 10 12 12 12 12 12 12 12 12 12 Die ungeraden Zeilen (1,3, und 5) zeigen die ID-Variable Chick; die geraden (2,4, und 6) die Häufigkeit der Messung. Küken 18, 16, 15 und 44 wurden weniger als 12 mal untersucht. Deswegen gibt es auch einen Unterschied zwischen der Anzahl der Zeilen im ursprünglichen Long-Format von ChickWeight und den nachfolgend erstellten Long-Formaten des Datensatzes. Im ursprünglichen Long-Format ChickWeight werden die Zeilen der Küken, die zu bestimmten Messzeitpunkten nicht beobachtet wurden, weggelassen. Mit reshape() und gather() werden diese Zeilen erstellt und die Beobachtung der messwiederholten Variable mit NA (= fehlender Wert) versehen. 10.2.1 stats: reshape() Bei der Formatierung von Wide in Long sind folgende 6 Argumente wichtig: data: Name des Datensatzes varying: messwiederholte Variable, die im Wide-Format auf mehrere Spalten ausgedehnt ist und im Long-Format in einer Spalte vorliegen soll timevar: Zeitvariable, die unterschiedliche Modi kodiert, und die neu im Long-Format erstellt werden soll times: Ausprägungen der Zeitvariable, d.h. die Kodierung der Modi idvar: ID-Variable, die unterschiedliche Untersuchungseinheiten kodiert direction: vom Wide- ins Long-Format Mit v.names kann man die messwiederholte Variable, welche im Wide-Format in mehreren Spalten vorliegt und nun im Long-Format zu einer Spalte zusammengefasst wird, umbenennen. Wenn alle Spaltennamen den gleichen Stamm haben (bei uns weight.0, weight.2, …), dann wird dieser als Name der zusammengefassten Variable genutzt. Wenn die Spalten unterschiedliche Namen haben, sollte man hier einen gemeinsamen Namen angeben. Auch hier kann man einzelne Variablen mit drop=\"Variable\" bzw. mehrere Variablen mit drop=c(\"Variable_1\", \"Variable_2\", ..., \"Variable_x\") aus dem umformatierten Datensatz entfernen. reshape_long &lt;- reshape(data=reshape_wide, # Name des Datensatzes varying=3:14, # messwiederholte Variable (mehrere Spalten) timevar=&quot;Time&quot;, # (neu erstellte) Zeitvariable times=c(seq(0, 20, 2), 21), # Ausprägungen von timevar (Messzeitpunkte) idvar=&quot;Chick&quot;, # (bestehende) ID-Variable direction=&quot;long&quot;) # vom Wide- ins Long-Format Achtung: Ganz links sehen wir die neue Zeilennummerierung von reshape_long. Die Zeilennummerierung hat hier Nachkommastellen (z.B. 1.0), weil hier ID-Variable und Zeitvariable kombiniert wurden. Die Zahl vor dem Punkt steht für die Untersuchungseinheit (Chick); die Zahl nach dem Komma für den Messzeitpunkt (Time). Was macht varying=3:14? Man kann bestehende Variablen auch mit ihren Indizes (d.h. hier: ihren Spaltennamen) ansprechen. Das ist häufig weniger schreibintensiv, als alle Spaltennamen in einem Vektor zu speichern z.B. varying=c(\"weight.0\", \"weight.2\", ..., \"weight.21\"). So könnte man z.B. auch idvar=1 anstatt idvar=\"Chick\" schreiben. Mit den Variablen, die man im Rahmen der Umformatierung erst neu erstellt (z.B. timevar), geht das aber nicht, da diese noch gar nicht existieren und von daher auch keine Indizes haben. Wie können wir verschiedene messwiederholte Variablen jeweils zusammenfassen? bzw. Wie nutzen wir varying=list() und v.names? Wenn man mehrere Variablen zu den verschiedenen Messzeitpunkten erhoben hat, kann man diese mit varying=list() vom Wide- ins Long-Format bringen. Dabei muss man darauf achten, dass alle messwiederholten Variablen die gleiche Anzahl an Spalten besitzen (d.h. alle zu jedem Messzeitpunkt erhoben wurden). Für unseren Datensatz generieren wir ein fiktives Beispiel, in dem wir die 12 messwiederholten weight-Spalten duplizieren und dann jeweils zu einer Variablen zusammenfügen. Deswegen schreiben wir varying=list(3:14, 15:26) in reshape(). Zusätzlich kann man die messwiederholten Variablen mit v.names=c(\"...\", \"...\") umbenennen. Das funktioniert auch für einzelne messwiederholte Variablen: Dann benötigt man keinen Namensvektor c(\"...\", \"...\") mehr, sondern nur v.names=\"...\". Wir nennen die zwei fiktiven Variablen in unserem Beispiel AV_1 und AV_2. # messwiderholte Variablen duplizieren: reshape_dup &lt;- cbind(reshape_wide, reshape_wide[3:14]) reshape_dup_long &lt;- reshape(data=reshape_dup, varying=list(3:14, 15:26), # zwei messwiederholte Variable ... # ... mit jeweils mehreren Spalten v.names=c(&quot;AV_1&quot;, &quot;AV_2&quot;), # Benennung der zwei messwiederholten Variablen # optional timevar=&quot;Time&quot;, times=c(seq(0, 20, 2), 21), # Ausprägungen von timevar (Messzeitpunkte) idvar=&quot;Chick&quot;, direction=&quot;long&quot;) 10.2.2 tidyr: gather() Hierfür benötigt man 5 Argumente: data: Name des Datensatzes key: Zeitvariable, die unterschiedliche Modi kodiert und die neu im Long-Format erstellt werden soll value: Benennung der neu zu erstellenden messwiederholten Variable ...: messwiederholte Variable, die im Wide-Format auf mehrere Spalten ausgedehnt ist und im Long-Format in einer Spalte vorliegen soll factor_key: ob die neu erstellte Zeitvariable als Faktor (TRUE) oder als Character (FALSE; Default) gehandhabt werden soll library(tidyr) # Laden des Pakets gather_long &lt;- gather(data=spread_wide, # Name des Datensatzes key=&quot;Time&quot;, # (neu erstellte) Zeitvariable value=&quot;weight&quot;, # (neu erstellte) messwiederholte Variable 3:14, # messwiederholte Variable (mehrere Spalten) factor_key=TRUE) # ob die Zeitvariable ein Faktor sein soll 10.2.3 Unterschied zwischen reshape() und gather() Bei reshape() werden Zeilennummerierungen auf Basis des übergebenen Datensatzes erzeugt (siehe Hinweis im reshape()-Abschnitt). Long-Format: Argumente von reshape() und gather() gegenübergestellt reshape gather data data varying … timevar key times idvar v.names value drop factor_key Das Argument direction wurde nicht mit in die Übersicht aufgenommen. Während man bei reshape() angeben muss, welches Format man erstellen will, nutzt man in tidyr entweder gather() oder spread(). Bei gather() werden die Ausprägungen der Zeitvariable (Modi) so benannt, wie vorher die einzelnen Spalten der messwiederholten Variablen hießen. Wenn wir uns das anschauen wollen, könnten wir einfach reshape_wide (anstatt spread_wide) mit gather() bearbeiten. Im Gegensatz dazu kann man bei reshape() mit times eine eigene Benennung der Ausprägungen festlegen. Bei reshape() wird mit idvar eine neue ID-Variable erstellt. Dazu muss ich dem Argument nur einen Namen geben. Wenn es bereits eine ID-Variable gibt, sollte man den Namen dieser angeben, weil ansonsten noch eine neue ID-Variable erstellt wird. Mit drop kann man zusätzlich bestimmte Variablen aus dem neu formatierten Datensatz entfernen. Bei gather() kann man mit mit factor_key entscheiden, ob die neu erstellte Zeitvariable als Faktor oder als Character behandelt werden sollen. Mit reshape() ist der Datentyp immer numerisch. 10.3 Andere Funktionen zum Umformatieren Es gibt noch andere Funktionen, mit denen man Datensätze vom Long- ins Wide-Format oder umgekehrt umformatieren kann. In gewisser Hinsicht sind pivot_longer() und pivot_wider() aus dem Paket tidyr die Nachfolger von gather() und spread(). Neben der intuitiveren Benennung enthalten sie einige Neuerungen wie z.B. die Möglichkeit, mit mehreren messwiederholten Variablen mit verschiedenen Datentypen zu arbeiten (dieses Feature wurde von melt() und dcast() übernommen). Außerdem wurde der Support für gather() und spread() eingestellt, d.h. bestehende Probleme mit diesen beiden Funktionen werden nicht mehr behoben werden. Hier finden wir eine Vignette, in der die Handhabung von pivot_longer() und pivot_wider() erklärt wird. Wenn wir wissen wollen, wie man melt() (Wide zu Long) und dcast() (Long zu Wide) aus dem Paket reshape2 nutzt, können wir dafür hier nachschauen. Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] tidyr_1.2.1 naniar_0.6.1 VIM_6.2.2 readxl_1.3.1 ## [5] foreign_0.8-82 devtools_2.4.5 usethis_2.1.6 ICC_2.4.0 ## [9] readr_2.1.3 Hmisc_4.7-1 Formula_1.2-4 survival_3.2-13 ## [13] lattice_0.20-45 ggplot2_3.4.0 colorspace_2.0-3 psych_2.2.9 ## [17] car_3.1-1 carData_3.0-5 kableExtra_1.3.4 dplyr_1.0.10 ## [21] htmltools_0.5.3 rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] deldir_1.0-6 ellipsis_0.3.2 class_7.3-20 ## [4] visdat_0.5.3 htmlTable_2.4.1 base64enc_0.1-3 ## [7] fs_1.5.2 rstudioapi_0.13 proxy_0.4-27 ## [10] remotes_2.4.2 fansi_1.0.3 ranger_0.14.1 ## [13] xml2_1.3.3 splines_4.2.0 mnormt_2.1.1 ## [16] robustbase_0.95-0 cachem_1.0.6 pkgload_1.3.0 ## [19] jsonlite_1.8.3 cluster_2.1.2 png_0.1-7 ## [22] shiny_1.7.3 compiler_4.2.0 httr_1.4.2 ## [25] backports_1.4.1 assertthat_0.2.1 Matrix_1.5-1 ## [28] fastmap_1.1.0 cli_3.4.1 later_1.3.0 ## [31] prettyunits_1.1.1 tools_4.2.0 gtable_0.3.0 ## [34] glue_1.6.2 Rcpp_1.0.9 cellranger_1.1.0 ## [37] jquerylib_0.1.4 vctrs_0.5.0 svglite_2.1.0 ## [40] nlme_3.1-155 lmtest_0.9-40 laeken_0.5.2 ## [43] xfun_0.34 stringr_1.4.0 ps_1.7.2 ## [46] rvest_1.0.2 mime_0.12 miniUI_0.1.1.1 ## [49] lifecycle_1.0.3 DEoptimR_1.0-11 zoo_1.8-11 ## [52] MASS_7.3-56 scales_1.2.1 hms_1.1.1 ## [55] promises_1.2.0.1 parallel_4.2.0 RColorBrewer_1.1-2 ## [58] yaml_2.3.5 memoise_2.0.1 gridExtra_2.3 ## [61] sass_0.4.2 rpart_4.1.16 latticeExtra_0.6-30 ## [64] stringi_1.7.8 highr_0.9 e1071_1.7-12 ## [67] checkmate_2.0.0 boot_1.3-28 pkgbuild_1.3.1 ## [70] rlang_1.0.6 pkgconfig_2.0.3 systemfonts_1.0.4 ## [73] evaluate_0.15 purrr_0.3.4 htmlwidgets_1.5.4 ## [76] tidyselect_1.2.0 processx_3.8.0 norm_1.0-10.0 ## [79] magrittr_2.0.2 bookdown_0.29 R6_2.5.1 ## [82] generics_0.1.2 profvis_0.3.7 DBI_1.1.2 ## [85] pillar_1.8.1 withr_2.5.0 sp_1.5-0 ## [88] abind_1.4-5 nnet_7.3-17 tibble_3.1.8 ## [91] crayon_1.5.2 interp_1.0-33 utf8_1.2.2 ## [94] tzdb_0.3.0 urlchecker_1.0.1 jpeg_0.1-9 ## [97] data.table_1.14.4 callr_3.7.2 vcd_1.4-10 ## [100] digest_0.6.30 webshot_0.5.4 xtable_1.8-4 ## [103] httpuv_1.6.5 munsell_0.5.0 viridisLite_0.4.1 ## [106] bslib_0.4.0 sessioninfo_1.2.2 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["annahmen-der-multiplen-linearen-regression.html", "Chapter 11 Annahmen der Multiplen Linearen Regression 11.1 Linearität 11.2 Exogenität der Prädiktoren 11.3 Homoskedastizität 11.4 Unabhängigkeit der Residuen 11.5 Weitere wichtige Aspekte 11.6 Literatur &amp; weiterführende Hilfe", " Chapter 11 Annahmen der Multiplen Linearen Regression Einleitung Wenn wir uns den gerichteten Zusammenhang von mehr als zwei Variablen anschauen möchten, können wir dafür die multiple lineare Regression nutzen. Bei dieser können wir eine abhängige Variable (AV, nachfolgend Kriterium genannt) durch mehrere (multiple) unabhängige Variablen (UVs, nachfolgend Prädiktoren genannt) vorhersagen. Grundsätzlich gilt, dass das Kriterium metrisch sein muss (d.h. mindestens intervallskaliert). Die Prädiktoren hingegen können auch kategorial (d.h. dichotom, nominal- oder ordinalskaliert) sein, sofern diese korrekt kodiert werden (z.B. als Dummyvariablen). Wie wir u.a. Dummykodierungen erstellen können erfahren wir im Abschnitt Indikatorvariablen: Kodierung nominaler Merkmale im Kapitel Datenvorbereitung. Wie sieht die Regressionsgleichung aus? Nach dem linearen Modell gilt (für Person \\(i=1, ..., n\\) und Prädiktor \\(k=1, ..., K\\)): \\[y_i = b_0 + b_1x_1 + ... + b_Kx_K + e_i\\] \\(y\\): Kriterium \\(b_0\\): y-Achsenabschnitt (Intercept) \\(b_1, ..., b_K\\): Steigungen (Slopes); hier unstandardisiert \\(e_i\\): Residuum (Vorhersagefehler); gibt den Teil von y an, der nicht durch die Regressionsgleichung vorhergesagt werden kann Bei der Anwendung der multiplen linearen Regression müssen allerdings bestimmte Annahmen erfüllt sein. Wenn diese verletzt sind, besteht die Gefahr, dass die Parameterschätzungen inkorrekt (verzerrt) sind und/oder wir inkorrekte Schlussfolgerungen über das Vorhandensein von Effekten in der Population ziehen (z.B. wir aufgrund von verzerrten Standardfehlern fälschlicherweise ein signifikantes Ergebnis erhalten). Die wichtigsten Annahmen sind: Linearität, Exogenität, Homoskedastizität und die Unabhängigkeit der Residuen. Darüber hinaus sollten wir uns auch immer die Normalverteilung der Residuen, Multikollinearität sowie Ausreißer und einflussreiche Datenpunkte ansehen. Gefahren bei Verletzung der Annahmen und weiterer wichtiger Punkte verzerrte Koeffizienten verzerrte Standardfehler Linearität X X Exogenität X X Homoskedastizität X Unabhängigkeit der Residuen X Normalverteilung der Residuen X Multikollinearität X Einflussreiche Datenpunkte X Bei der Prüfung von Annahmen in der multiplen linearen Regression ist die Residualdiagnostik ein wichtiges Verfahren. Residuen \\(\\hat e_i\\) sind Abweichungen der vorhergesagten Werte des Kriteriums \\(\\hat y_i\\) von den beobachteten Werten des Kriteriums \\(y_i\\) von Person \\(i\\). Man schaut sich anstatt der geplotteten Rohdaten häufig die Residualplots an, weil man Plots mit mehr als zwei Achsen (bei mehr als einem Prädiktor) grafisch nicht gut darstellen kann. Zusätzlich visualisieren Residuen die Abweichungen besser und lassen uns so u.a. nicht-lineare Zusammenhänge besser aufdecken. Die verschiedenen Annahmen werden im Verlauf der folgenden Abschnitte kurz erläutert und Möglichkeiten der Überprüfung (v.a. mit Hilfe von Grafiken), sowie zum Umgang mit Verletzung der Annahmen kurz skizziert. Warum werden bevorzugt Grafiken genutzt, um die Annahmen zu prüfen? In geplotteten Daten können verschiedenste Verletzungen (z.B. Missspezifikationen der Form des Zusammenhangs zwischen den Variablen) entdeckt werden, denn graphische Darstellungen machen nur geringe Annahmen über die Art des Problems. Statistische Tests hingegen haben häufig einen eingeschränkten Fokus und sie vergleichen nur, was wir vorgegeben haben. Zusätzlich funktionieren sie nur unter bestimmten Annahmen, liefern lediglich eine 0/1-Aussage ohne die Schwere des Annahmeverstoßes zu quantifizieren und hängen stark von der Stichprobengröße ab. Was ist die Lowess Fit Line? Wenn wir Ergebnisse einer linearen Regression mit plot() darstellen, wird häufig die sogenannte Lowess Fit Line eingezeichnet. Lowess steht für locally weighted scatterplot smoother. Die Lowess (oder auch Loess) Fit Line ist ein Verfahren, welches den besten nonparametrischen Fit für die gegeben Daten anzeigt. Sie ist eine Auswertungshilfe bei der Beurteilung der Form des Zusammenhangs. Dabei macht sie keine Annahmen über die Form des Zusammenhangs zwischen den Variablen. Der Zusammenhang zwischen zwei Variablen wird im Streudiagramm als “smoothe” Linie, die den generellen Trend der Daten beschreibt, dargestellt. Wenn der Zusammenhang zwischen zwei Variablen in der Population linear ist, so sollte sich auch die Lowess Line einer Gerade annähern. Allerdings ist die Lowess Line häufig an den Enden der Verteilung von X weniger präzise, da hier weniger Daten vorhanden sind. Beispieldatensatz für dieses Kapitel Hier sehen wir, wie wir den Datensatz erstis, an dem wir in diesem Kapitel arbeiten werden, einlesen können. load(url(&quot;http://www.beltz.de/fileadmin/beltz/downloads/ OnlinematerialienPVU/R_fuer_Einsteiger/erstis.rda&quot;)) # Zeilenumbruch zwischen der ersten und zweiten Zeile noch entfernen! Die enthaltenen Daten sind aus einer Erhebung mit Erstsemesterstudierenden der Psychologie. Unter diesem Link finden wir das Codebuch zum Datensatz. Exemplarisch schauen wir uns für dieses Kapitel an, wie gut sich lz.1 (Lebenszufriedenheit T1) durch zuf.inh.1 (Zufriedenheit mit Studieninhalten T1) und zuf.bed.1 (Zufriedenheit mit Studienbedingungen T1) vorhersagen lässt. Dazu erstellen wir erst einen neuen Datensatz mit diesen Variablen und führen dann die Regression durch. # Daten aus erstis in neuem Dataframe speichern ... daten &lt;- data.frame(erstis$lz.1, erstis$zuf.inh.1, erstis$zuf.bed.1) # ... und Spalten umbenennen names(daten) &lt;- c(&quot;leb_zufr&quot;, &quot;zufr_inhalt&quot;, &quot;zufr_beding&quot;) # Regression durchführen lm_lz &lt;- lm(daten$leb_zufr ~ daten$zufr_inhalt + daten$zufr_beding, na.action = &quot;na.exclude&quot;) # mit &quot;na.exclude&quot; schließen wir fehlende Werte aus Für mehr Informationen dazu, wie lm() und andere Funktionen mit Missings umgehen, können wir uns das Kapitel Fehlende Werte anschauen. # Die Residuen brauchen wir später, daher fügen wir sie jetzt ... # ... schon als Variable zum Datensatz hinzu. daten$resid &lt;- residuals(lm_lz) # wichtig: residuals() oder resid() nehmen # lm_lz$residuals funktioniert hier nicht, # ... weil die Zeilenanzahl geringerer ist ... # ... weil Zeilen mit Missings gelöscht werden Im Rahmen dieses Kapitels liegt der Fokus nicht auf der inhaltlichen Interpretation der Ergebnisse der multiplen linearen Regressions, sondern darauf, ob die oben genannten Annahmen erfüllt sind. Nachfolgend sehen wir den Output der lm()-Funktion (lm_lz). summary(lm_lz) ## ## Call: ## lm(formula = daten$leb_zufr ~ daten$zufr_inhalt + daten$zufr_beding, ## na.action = &quot;na.exclude&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2274 -3.7319 0.8666 3.8354 8.5019 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.7910 2.3392 5.468 1.7e-07 *** ## daten$zufr_inhalt 2.4993 0.6870 3.638 0.000369 *** ## daten$zufr_beding 1.3128 0.5743 2.286 0.023572 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.137 on 161 degrees of freedom ## (27 observations deleted due to missingness) ## Multiple R-squared: 0.141, Adjusted R-squared: 0.1303 ## F-statistic: 13.21 on 2 and 161 DF, p-value: 4.87e-06 Wenn die Annahmen für die multiple lineare Regression in unserem Beispiel nicht erfüllt sind, besteht die Gefahr, dass unsere Parameterschätzungen (Estimate-Spalte) und/oder Standardfehler (Std.Error-Spalte) inkorrekt sind. Damit wären unsere Ergebnisse u.U. untauglich. Achtung: Um eine (multiple) lineare Regression durchführen zu können, müssen unsere Daten ggf. in einem für unsere Analyse geeigneten (Tabellen-)Format vorliegen. Es gibt das Long- und das Wide-Format. Wie wir beide ineinander überführen können erfahren wir im gleichnamigen Kapitel. 11.1 Linearität Mit Linearität ist die korrekte Spezifikation der Form des Zusammenhangs zwischen Kriterium und Prädiktoren gemeint. Genauer gesagt, meint die Annahme, dass der Erwartungswert des Kriteriums sich als Linearkombination der Prädiktoren darstellen lässt. Achtung: Dies bedeutet jedoch nicht notwendigerweise, dass der Zusammenhang der Variablen linear sein muss. Es muss sich lediglich um eine linear additive Verknüpfung der Regressionsterme handeln. Beispielsweise spezifiziert die folgende Regressionsgleichung \\(y=b_0 + b_1x^2 + e\\) einen quadratischen Zusammenhang zwischen \\(Y\\) und \\(X\\) mittels einer linear additiven Verknüpfung der Regressionsterme (hier nur ein einziger Prädiktor \\(X\\)). Siehe auch den Abschnitt zum Umgang mit Nicht-Linearität. Wenn die Form des Zusammenhangs zwischen Kriterium und Prädiktoren nicht richtig spezifiziert wurde, können ernsthafte Probleme auftreten. Dies wäre zum Beispiel dann der Fall, wenn es zwischen Prädiktoren und Kriterium in Wirklichkeit einen quadratischen Zusammenhang gibt, wir in unserem Regressionsmodell aber nur einen linearen Zusammenhang spezifiziert haben. Sowohl die Regressionskoeffizienten als auch die Standardfehler könnten in einem solchen Fall verzerrt sein. 11.1.1 Überprüfung 11.1.1.1 Bivariate Streudiagramme In einem ersten Schritt schauen wir uns bivariate Streudiagramme an. Das heißt, wir schauen uns nicht das gesamte Modelle (mit mehreren Prädiktoren) an, sondern nur Zusammenhänge zwischen einzelnen Prädiktoren und dem Kriterium. Achtung: Auch wenn die bivariaten Streudiagramme auf Linearität hinweisen, sollten wir nicht vergessen, dass auch Interkationen zwischen Prädiktoren zu nicht-linearen Zusammenhängen führen können. Die Nutzung von bivariaten Streudiagramen zur Überprüfung der Annahme der Linearität ist weder eine notwendige, noch eine hinreichende Bedingung. Sie sind daher mit Vorsicht zu beurteilen. # Lebenszufriedenheit - Zufriedenheit mit Studieninhalten plot(daten$zufr_inhalt, daten$leb_zufr) lz_inh &lt;- lm(daten$leb_zufr ~ daten$zufr_inhalt, na.action=&#39;na.exclude&#39;) # Einfache Regression abline(lz_inh) # Einzeichnen Regressionsgerade Der Plot spricht für einen linearen Zusammenhang zwischen Lebenszufriedenheit (Kriterium) und Zufriedenheit mit Studieninhalten (Prädiktor). # Lebenszufriedenheit - Zufriedenheit mit Studienbedingungen plot(daten$zufr_beding, daten$leb_zufr) lz_bed &lt;- lm(daten$leb_zufr ~ daten$zufr_beding, na.action=&#39;na.exclude&#39;) # Einfache Regression abline(lz_bed) # Einzeichnen Regressionsgerade Der Plot von Lebenszufriedenheit (Kriterium) und Zufriedenheit mit Studienbedingungen (Prädiktor) weist auf einen linearen Zusammenhang hin. 11.1.1.2 Residualplot Das wichtigste Werkzeug zur Prüfung der Linearitätsannahme ist der Residualplot. In einem Residualplot werden die vorhergesagten Werte \\(\\hat y_i\\) (auf der \\(x\\)-Achse) gegen die Residuen \\(\\hat e_i = y_i - \\hat y_i\\) (auf der \\(y\\)-Achse) abgetragen. plot(lm_lz, which = 1) # erster Plot der plot()-Funktion für ein lm-Objekt ist der Residualplot Die gestrichelte Linie bei \\(y = 0\\) zeigt den Erwartungswert der Residuen. Diese ist immer null und die Residuen sollten sich ohne erkennbares Muster, um diese Linie verteilen. Die rote Linie ist die Lowess Fit Line. Diese sollte sich der gestrichelten Linie annähern, wenn der Zusammenhang zwischen Prädiktoren und Kriterium linear ist. In unserem Beispiel legt der Residualplot nahe, dass der Zusammenhang zwischen Lebenszufriedenheit und Zufriedenheit mit Studieninhalten und -bedingungen weitgehend linear ist. Die Annahme der Linearität wäre z.B. verletzt, wenn die Residuen einen U-förmigen Zusammenhang mit den vorhergesagten Werten aufweisen würden. Das würde nahelegen, dass ein quadratischer Zusammenhang zwischen dem Kriterium und den Prädiktoren besteht, der nicht adäquat modelliert wurde. 11.1.2 Umgang Um einen angemessen Weg zu finden, um mit nicht-linearen Zusammenhängen zwischen den Variablen umzugehen, können wir die folgenden vier Fragen zur Eingrenzung nutzen: Gibt es eine Theorie, die einen spezifischen nicht-linearen Zusammenhang zwischen den integrierten Variablen vorhersagt? Wir sollten eine Regressionsgleichung aufstellen, die dem theoretisch implizierten mathematischen Zusammenhang widerspiegelt. Wie sieht der beobachtete bivariate Zusammenhang zwischen den Variablen aus? Wenn der Zusammenhang zwischen dem Kriterium und einzelnen Prädiktoren oder zwischen einzelnen Prädiktoren untereinander nicht-linear ist, können die Prädiktoren transformiert werden (z.B. \\(log X_1\\)). Die Art der Transformation hängt von der Art des Zusammenhangs ab. Wie sieht die Lowess Line in den originalen Daten aus? und Bleibt die Varianz der Residuen über den Bereich des Kriteriums hinweg konstant? (siehe Homoskedastizität) Homoskedastizität: Hierbei könnten wir einzelne Regressionsterme in Polynomen höherer Ordnung darstellen (z.B. \\(X_1^2\\), \\(X_2^3\\)). In einigen Fällen könnten einfache Potenzfunktionen nicht ausreichend sein. Dann könnten nichtparametrische Funktionen die bessere Lösung sein, um den Zusammenhang zwischen Kriterium und Prädiktoren zu spezifizieren. Heteroskedastizität: Wir könnten das Kriterium durch eine nichtlineare mathematische Funktion von \\(Y\\) ersetzen (z.B. \\(log Y\\)). Mit dieser Transformation entsprechen die Residuen nun den beobachteten \\(log Y\\) minus den vorhergesagten \\(log \\hat Y\\). 11.2 Exogenität der Prädiktoren Die Prädiktoren \\(X\\) sind unabhängig vom Fehlerterm der Regressionsgleichung \\(e\\): \\(E(e|X)=0\\). Das impliziert z.B. perfekte Reliabilität und das alle relevanten Variablen im Modell aufgenommen sind, das heißt, dass es keine konfundierenden Variablen gibt. Das ist ein zentrales Anliegen in der Wissenschaft, jedoch ist Exogenität nicht leicht nachzuweisen. Für unser Beispiel der Regression von ‘Zufriedenheit mit Studieninhalten’ und ‘Zufriedenheit mit Studienbedingungen’ auf ‘Lebenszufriedenheit’ müssten wir überlegen, ob noch andere Variablen einen Einfluss haben könnten. Beispielsweise könnten auch verschiedene Persönlichkeitsfaktoren mit ‘Lebenszufriedenheit’ zusammenhängen. Das würde sich dann darin äußern, dass die Prädiktoren noch systematische Varianz mit dem Fehlerterm teilen. Wenn nicht alle relevanten Prädiktoren im Modell spezifiziert sind oder enthaltene Prädiktoren messfehlerbehaftet sind, können daraus verzerrte Regressionskoeffizienten und Standardfehler resultieren. 11.2.1 Überprüfung Vor der Erhebung müssen wir uns sorgfältig Gedanken darüber machen, welche Prädiktoren relevant sind. Diese müssen vollständig in das Modell integriert werden. Beispielsweise sollten wir stets eine Literaturrecherche durchführen, um uns über den derzeitigen Stand der Forschung in einem Themenbereich zu informieren. Zur Überprüfung der Exogenität könnten wir außerdem eine hierarchische Regression durchführen, in der wir schrittweise weitere Variablen aufnehmen. Wenn sich die Regressionsgewichte bei Aufnahme eines neuen Prädiktors ändern, war die Exogenitätsannahme der ursprünglichen Prädiktoren wahrscheinlich nicht erfüllt. Solche Modellvergleiche helfen bei der Beurteilung der Exogenität. Wir sollten uns zusätzlich theoretisch überlegen, ob die gemessenen Prädiktoren messfehlerbehaftet sein könnten. Direkt beobachtbare Variablen (z.B. Alter, höchster Bildungsabschluss oder Körpergröße) stehen weniger im Verdacht, messfehlerbehaftet zu sein. Nicht direkt beobachtbare (latente) Variablen hingegen (z.B. Berufserfolg, Kreativität oder Wohlbefinden) können mit größerer Wahrscheinlichkeit messfehlerbehaftet sein. Unsere Variablen ‘Lebenszufriedenheit’, ‘Zufriedenheit mit Studieninhalten’ und ‘Zufriedenheit mit Studienbedingungen’ sind alle latent. Von daher sind Messfehler wahrscheinlicher. Wir sollten uns außerdem für die reliabelsten Erhebungsinstrumente für die Messung der Prädiktoren entscheiden. Wenn wir nicht an der Erhebung der Variablen beteiligt waren, sollten wir uns nachträglich über die Reliabilität der Erhebungsinstrumente informieren. In unserem Fall des erstis-Datensatz gibt es leider keine weiteren Informationen zu den Erhebungsinstrumenten. So können wir leider nicht einschätzen, wie reliabel die Erhebungsinstrumente sind. Die Reliabilität erhobener Variablen können wir auf verschiedene Arten schätzen. Diese werden in Abhängigkeit des Forschungsdesign und der Fragestellung ausgewählt. Die Reliabilität der Messungen in unserem Beispiel könnten wir beispielsweise mit McDonald’s Omega bzw. dem gewichteten Omega berechnen. 11.2.2 Umgang Wenn die Prädiktoren stark messfehlerbehaftet sind, sollten wir auf Regressionsmodelle mit latenten Variablen zurückgreifen. Beispielsweise können wir Messfehler mittels Strukturgleichungsmodellierung berücksichtigen. 11.3 Homoskedastizität Die Varianz der Residuen \\(s^2_{e}\\) an einer bestimmten Stelle des Prädiktors ist für alle Prädiktorwerte gleich. Diese Varianz entspricht dem quadrierten Standardschätzfehler \\(\\sigma_e^2\\) in der Population. Homoskedastizität wird auch Varianzhomogenität genannt. Die Annahme wäre beispielsweise verletzt, wenn mit steigenden Prädiktorwerten die Residuen größer, d.h. die Vorhersage mittels der Regressionsgerade ungenauer, werden würde. Es kann vielfältige Gründe für Varianzheterogenität geben. So können z.B. stark abweichende Werte dafür verantwortlich sein (siehe Extreme Werte und einflussreiche Datenpunkte). Nur unter Gültigkeit der Annahme ist die Berechnung der Standardfehler korrekt, aber Heteroskedastizität führt nicht zu verzerrten Regressionkoeffizienten. Andere als die vorgestellten Möglichkeiten zur Überprüfung und zum Umgang mit Heteroskedastizität inklusive der Umsetzung in R finden wir z.B. auf R-bloggers. 11.3.1 Überprüfung 11.3.1.1 Residualplot Zur Überprüfung der Homoskedastizität können wir ebenfalls einen Residualplot, wie schon bei der Überprüfung der Annahme der Linearität, verwenden. plot(lm_lz, which = 1) # erster Plot der plot()-Funktion für ein lm-Objekt ist der Residualplot Die gestrichelte Linie bei \\(y = 0\\) zeigt den Erwartungswert der Residuen. Diese ist immer null und die Residuen sollten sich ohne erkennbares Muster, um diese Linie verteilen. Die Annahme wäre z.B. verletzt, wenn die Residuen einen nach rechts geöffneten Trichter bilden würden. Das würde bedeuten, dass die Varianz mit größer werdenden \\(X\\)-Werten wachsen würde wie in nachfolgendem Beispiel illustriert: 11.3.1.2 Scale Location Plot Mit dem Scale Location Plot schauen wir, ob die Standardabweichung der standardisierten Residuen \\(\\frac{e_i}{s_e}\\) über den Bereich der vorhergesagten Werte hinweg gleich bleibt. plot(lm_lz, which = 3) # dritter Plot der plot()-Funktion für ein lm-Objekt ist der Scale Location Plot Die standardisierten Residuen sollten auch hier gleichmäßig (zufällig) über den gesamten Bereich streuen. Außerdem sollte die Lowess Line möglichst horizontal zur \\(x\\)-Achse sein. Auch der Scale Location Plot weist in unserem Beispiel auf ungefähre Varianzhomogenität hin. Das heißt, dass die standardisierten Residuen des Teils vom Kriterium ‘Lebenszufriedenheit’, der nicht durch die Prädiktoren ‘Zufriedenheit mit Studieninhalten’ und ‘Zufriedenheit mit Studienbedingungen’ vorhergesagt werden kann, gleichmäßig über die vorhergesagten Werte vom Kriterium ‘Lebenszufriedenheit’ streut. Exkurs: Quantifizieren des Ausmaßes Bisher haben wir uns angeschaut, ob Heteroskedastizität vorliegt. Diese hat aber erst einen substanziellen Einfluss auf die Regression, wenn das Ausmaß “groß” ist. Wir können die Varianzheterogenität (wenn die graphische Überprüfung darauf hindeutet) quantifizieren, um zu entscheiden, ob wir korrektive Maßnahmen durchführen sollten. Dazu schauen wir uns eine Möglichkeit aus dem Lehrbuch von Cohen, Cohen, West &amp; Aiken (2003, S.146) an. Dabei betrachten wir die konditionale (Fehler-)Varianz \\(s^2_{(e | slice)}\\) in sog. “Slices” (d.h. Gruppen) für einzelne Prädiktoren. Wir schauen uns das exemplarisch bei Zufriedenheit mit Studieninhalten an. Dazu nutzt man die Ergebnisse der einfachen Regression der Prädiktoren. Diese haben wir im Abschnitt Bivariate Streudiagramme schon einmal berechnet und darauf greifen wir jetzt wieder zurück. Das Vorgehen: Sortieren der Residuen nach aufsteigendem Prädiktor X (Zufriedenheit mit Studieninhalten) in ähnlich große Slices einteilen Die Wahl der Anzahl der Slices ist ein Tradeoff zwischen stabiler Varianzschätzung in jedem Slice (d.h. wenig Gruppen) und der Begutachtung verschiedener Anteile der Daten (d.h. viele Gruppen). konditionale Varianz in den Slices berechnen Dazu quadrieren wir die einzelnen Residuen, teilen diese jeweils durch die Anzahl der Personen in diesem Slice \\(n_{slice}\\) minus 2, und summieren die Quotienten auf. Zur Beurteilung der konditionalen Varianz gibt es zwei Kriterien: der Quotient aus dem Slice mit der größten konditionalen Varianz geteilt durch den Slice mit der kleinsten konditionalen Varianz sollte kleiner als 10 sein mit größer werdendem X sollte die konditionale Varianz in den Slices nicht systematisch variieren (z.B. konstant kleiner oder größer werden) Wenn der Quotient \\(&gt; 10\\) ist oder es systematische Variation gibt, sollten wir korrektive Maßnahmen einleiten Nachfolgend schauen wir uns die Umsetzung dazu in R an: # Objekt erzeugen, ... # ... in dem die Residuen nach aufsteigender Größe in X sortiert sind: quant &lt;- data.frame(erstis$zuf.inh.1, residuals(lz_inh))[order(erstis$zuf.inh.1, residuals(lz_inh)),] # data.frame() erstellt einen Dataframe aus den übergebenen Variablen # order() sortiert das erste Argument X aufsteigend (Default) ... # ... und gleiche Ausprägungen in X werden nach den Ausprägungen .. # ... des zweiten Arguments (hier: Residuen) sortiert # das Komma am Ende sagt, dass wir Zeilen sortieren wollen # mit [ ] bekommen wir die Indizes (und nicht die Werte) ausgegeben, ... # ... welche wiederum auf den Dataframe angewendet werden Standardmäßig wird so die erste Variable X aufsteigend sortiert, weil order(..., decreasing=FALSE) der Default ist. Mit decreasing=TRUE können wir absteigend sortieren. &gt; Achtung: Bei order(X, -resid) ändert sich die Sortierung innerhalb der gleichen Prädiktorwerte X in Abhängigkeit davon, ob wir bei order() vor die zweite Variable (resid) ein - setzen oder nicht: Mit resid (so wie wir es im Beispiel machen) wird innerhalb der gleichen Prädiktorwerte aufsteigend sortiert. Mit -resid wird innerhalb der gleichen Prädiktorwerte absteigend sortiert. Dadurch kommen unterschiedliche Residuen in die Slices und folglich werden auch unterschiedliche konditionale Varianzen berechnet! quant &lt;- na.exclude(quant) # Missings raus # Anzahl der Fälle nrow(quant) # 165 # Einteilung: 3 Gruppen á 41 Personen, 1 Gruppe mit 42 Personen slice_1 &lt;- quant$residuals.lz_inh.[1:41] slice_2 &lt;- quant$residuals.lz_inh.[42:82] slice_3 &lt;- quant$residuals.lz_inh.[83:123] slice_4 &lt;- quant$residuals.lz_inh.[124:165] # konditionale Varianz in Slices berechnen var_slice_1 &lt;- sum((slice_1^2/(length(slice_1)-2))) var_slice_2 &lt;- sum((slice_2^2/(length(slice_2)-2))) var_slice_3 &lt;- sum((slice_3^2/(length(slice_3)-2))) var_slice_4 &lt;- sum((slice_4^2/(length(slice_4)-2))) # konditionale Varianzen in Vektor speichern sort_var &lt;- c(var_slice_1 , var_slice_2, var_slice_3, var_slice_4) sort_var ## [1] 40.79699 25.91895 21.15083 24.16912 In unserem Beispiel scheint es keine systematischen Zu- oder Abhnahme in den konditionalen Varianzen der Slices zu geben … round((var_slice_1 / var_slice_3), 3) ## [1] 1.929 … und auch der Quotient zwischem dem Slice mit der größten und der kleinsten konditionalen Varianz ist relativ klein. 11.3.2 Umgang Wenn wir heteroskedastische Residuen haben, sollten wir gegen die potentielle Verzerrung der Standardfehler vorgehen. Dazu könnten wir beispielsweise Methoden zur Korrektur der Standardfehler oder varianzstabilisierende Verfahren, wie die Box-Cox-Transformation, nutzen. Eine andere Alternative wäre es, auf robustere Verfahren wie die gewichtete Regression mittels WLS zurückzugreifen. Exkurs: WLS-Regression Der weighted least squares Schätzer gewichtet jeden Fall (d.h. jede Person) danach, wie präzise die Beobachtung von \\(Y\\) für diesen Fall war. Das bedeutet, dass Beobachtungen mit geringen (Fehler-)Varianzen höher gewichtet werden als Beobachtungen mit großen (Fehler-)Varianzen. Anders ausgedrückt werden Beobachtungen, die näher an der Regressionsgerade sind, stärker gewichtet. Im Vergleich dazu gewichtet der standardmäßig genutzte OLS-Schätzer (Ordinary Least Squares, Methode der kleinsten Quadrate) jede Beobachtung gleich. Es gibt mehrere Möglichkeiten, die Gewichte \\(w_i\\) zu schätzen. Im Folgenden schauen wir uns exemplarisch eine Methode aus dem Buch von Cohen, Cohen, West &amp; Aiken (2003, S.146-147) an. &gt; Achtung: Wenn wir Ausreißer in unseren Daten haben, finden wir in der Fußnote 19 auf S.147 des Lehrbuchs eine geeignetere Methode zur Schätzung der Gewichte \\(w_i\\). Dazu nehmen wir die Residuen aus der OLS-Regression, quadrieren diese und regredieren sie auf unsere Prädiktoren. Die Inversen dieser vorhergesagten quadrierten Residuen sind das unsere Gewichte für die WLS-Regression. weights_lm &lt;- lm(daten$resid ^ 2 ~ daten$zufr_inhalt + daten$zufr_beding, na.action = &quot;na.exclude&quot;) # wieder na.exclude nutzen # sonst hat der Vektor mit den Gewichten wieder eine geringere Länge daten$gewichte &lt;- fitted.values(weights_lm) Zur Modellschätzung können wir wieder lm() nutzen. Dafür müssen wir nur zusätzlich den Gewichtsvektor im Argument weights spezifizieren. # WLS-Regression lm_wls &lt;- lm(daten$leb_zufr ~ daten$zufr_inhalt + daten$zufr_beding, weights = 1 / daten$gewichte, na.action = &quot;na.exclude&quot;) # Vergleich der OLS- und WLS-Schätzung summary(lm_lz)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.791048 2.3391903 5.468152 1.704375e-07 ## daten$zufr_inhalt 2.499315 0.6869872 3.638081 3.694881e-04 ## daten$zufr_beding 1.312796 0.5743345 2.285769 2.357195e-02 summary(lm_wls)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.889931 2.3951572 5.381664 2.566204e-07 ## daten$zufr_inhalt 2.545507 0.6966131 3.654119 3.488119e-04 ## daten$zufr_beding 1.222937 0.5605375 2.181723 3.057982e-02 Wir sehen, dass die Schätzungen von WLS denen von OLS sehr nah ist. Relevant wären hier prinzipiell mehr die Standardfehler (die bei Varianzheterogenität und OLS verzerrt wären). Da wir aber von Varianzhomogenität in unserem Beispiel ausgehen, sind auch diese sehr ähnlich. Achtung: Es gibt zwei Schwierigkeiten im Umgang mit dem WLS-Schätzer: wir müssen ein angemessenes Gewicht für jede Beobachtung wählen wenn die Gewichte nicht angemessen sind, ist WLS kein guter Schätzer daher ist die WLS-Schätzung am besten, wenn die Stichprobengröße \\(N\\) groß ist oder wenn es mehrere Beobachtungen mit identischen \\(X\\)-Werten gibt (d.h. mehrere Personen mit den gleichen Ausprägungen auf dem jeweiligen Prädiktor) standardisierte Effektgrößen, wie z.B. der Determinationskoeffizient \\(R^2\\), haben keine eindeutige Interpretation (wie bei OLS) Daher sollten wir eher OLS als WLS nutzen, außer wenn die Stichprobengröße sehr groß ist oder es ein großes Problem mit Heteroskedastizität gibt. 11.4 Unabhängigkeit der Residuen Die Höhe des Residuums einer Beobachtung \\(\\hat e_i\\) hängt nicht von der Höhe des Residuums einer anderen Beobachtung \\(\\hat e_x\\) ab. Wenn Residuen abhängig sind, kann sich das auch in Heteroskedastizität äußern. Residuen sind abhängig, wenn wir z.B. wiederholte Messungen oder Gruppenstrukturen untersuchen. Ersteres meint, dass mehrere Messungen von einer Person, d.h. zu mehreren Zeitpunkten, vorliegen. In diesem Fall sprechen wir auch von seriellen Abhängigkeiten. Zweiteres meint, dass die Daten “geclustert” sind und es somit systematische Zusammenhänge zwischen Personen gibt z.B. Schüler in einer Schulklasse. Beide Fälle müssen bei der Modellspezifikation berücksichtigt werden. In unserem Beispiel könnte beispielsweise das Kriterium ‘Lebenszufriedenheit’ in Abhängigkeit vom Wohnort variieren. Damit wären die Daten geclustert. Abhängigkeit der Residuen führt zwar nicht zu verzerrten Regressionskoeffizienten, aber es kann zu verzerrten Standardfehlern führen. 11.4.1 Überprüfung Ganz grundsätzlich können wir diese Annahme zuerst einmal theoretisch überprüfen, indem wir uns das Studiendesign der erhobenen Daten anschauen. Wenn nur zu einem Messzeitpunkt Daten erhoben wurden und jede Person nur einmal zu diesem befragt wurde, können wir serielle Abhängigkeiten ausschließen. Beim Clustering ist es etwas schwieriger, dieses nur durch Überlegung auszuschließen. Auch wenn im Studiendesign nicht vorgesehen war, dass Daten von unterschiedlichen Gruppen erhoben wurden, könnte es dennoch Ähnlichkeiten zwischen Personen hinsichtlich bestimmter Variablen geben. Um mögliche Verletzungen der Unabhängigkeitsannahme prüfen zu können, benötigen wir eine Vermutung darüber, welche Gruppierungsvariablen relevant sein könnten. 11.4.1.1 Ausmaß serieller Abhängigkeit Es gibt mehrere Möglichkeiten, das Ausmaß serieller Abhängigkeit zu beurteilen. Im Folgenden werden zwei grafische Visualisierungen vorgestellt. 11.4.1.1.1 Index Plot Einen Eindruck darüber, ob es systematische Abhängigkeiten zwischen den Residuen gibt, können wir mithilfe des Index Plot bekommen. Dieser ist ein Scatterplot, der die Residuen \\(\\hat e_i = y_i - \\hat y_i\\) (\\(y\\)-Achse) gegen den Index \\(i\\) (\\(x\\)-Achse), der zumeist durch die (aufsteigende) Zeilennummerierung repräsentiert wird, plottet. Hierbei können wir u.a. visualisieren, ob es zeitliche Abhängigkeiten gibt, z.B. ob Personen, die später an der Befragung teilgenommen haben, systematische Unterschiede in ihrer Lebenszufriedenheit (Kriterium) zeigen. Um das besser beurteilen zu können, können wir auch hier wieder die Lowess Line einzeichnen. Diese sollte wieder möglichst horizontal zur \\(x\\)-Achse bei \\(y = 0\\) liegen. Beide Linien fügen wir zur Vereinfachung der Beurteilung des Plots hinzu. plot(lm_lz$residuals) lines(lowess(lm_lz$residuals), col = &quot;red&quot;) # Lowess Line einfügen (in rot) abline( # Linie einzeichnen h=0, # h - horizontal, 0 - Schnittpunkt mit y-Achse col=&quot;grey&quot;, # Farbe lty=3) # Linien-Art: gestrichelt Die Abbildung impliziert, dass die Residuen unabhängig sind. Das entspricht unseren Vorüberlegungen, das wir nur Variablen haben, die zum gleichen Messzeitpunkt erhoben wurden. Exkurs: Grafische Interpretationshilfe für den Index Plot Falls wir ein Messwiederholungsdesign haben und die Daten im Long-Format (d.h. Messungen einer Person in mehreren Zeilen) vorliegen, können wir die Messungen verschiedener Person farblich unterschiedlich darstellen. In unserem Fall haben wir zwar keine Messwiederholung, aber wir illustrieren die farbliche Darstellung einmal am gleichen Beispiel. Die Farbe der geplotteten Elemente können wir mit dem Argument col verändern. Um die Farbe ab einer bestimmten Anzahl an Messungen (d.h. Messwiederholungen für eine Person) jeweils zu verändern, können wir rep() benutzen. Diese Funktion wiederholt das erste Argument x (bzw. die einzelnen Elemente des ersten Arguments) jeweils so oft wie das zweite Argument each. plot(lm_lz$residuals, col = rep(1:17, each = 10)) lines(lowess(lm_lz$residuals), col = &quot;red&quot;) abline(h=0, col=&quot;darkblue&quot;, type=&quot;l&quot;, lty=3) ## Warning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): graphical ## parameter &quot;type&quot; is obsolete Die 10 steht dafür, dass wir 10 Messwiederholungen haben. 1 bis 17 kodiert unterschiedliche Farbe. Alternativ können wir auch die Farbnamen in einem Vektor spezifizieren - dann würden wir anstatt 1:17 z.B. c(‘red’, ‘blue’, ‘green’, …) einfügen. Mit dem Argument pch können wir analog die Form der Punkte ändern. plot(lm_lz$residuals, pch = rep(1:17, each = 10)) lines(lowess(lm_lz$residuals), col = &quot;red&quot;) abline(h=0, col=&quot;darkblue&quot;, type=&quot;l&quot;, lty=3) ## Warning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): graphical ## parameter &quot;type&quot; is obsolete Wenn es starke serielle Abhängigkeiten geben würde, würden sich die Fälle sichtlich voneinander unterscheiden bzw. Cluster bilden. Illustriert ist das im folgenden Plot: ## Warning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): graphical ## parameter &quot;type&quot; is obsolete 11.4.1.2 Ausmaß des Clusterings Zu allererst können wir uns den Intraklassenkoeffizienten (IKK) anschauen. Dieser schätzt den Anteil der Varianz einer Variable, die durch Gruppenzugehörigkeit (Clustering) erklärt wird. Achtung: Der IKK sollte allerdings nicht als einzige Methode zur Überprüfung des Clusterings herangezogen werden, weil wir uns hier nur Mittelwertsunterschiede anschauen. Daher wird noch eine weitere Methode, die Betrachtung von Boxplots der geclusterten Gruppen, illustriert. 11.4.1.2.1 Intraklassenkorrelation Beispielsweise können wir mit Hilfe des IKK beurteilen, wie stark sich Personen innerhalb einer Gruppe ähneln. Je näher der Wert an \\(0\\) ist, desto geringer ist der relative Anteil an Unterschieden zwischen den Gruppen (d.h. desto weniger Clustering liegt vor). Je näher der Wert an \\(1\\) ist, desto geringer ist der relative Anteil an Unterschieden innerhalb einer Gruppe (d.h. desto mehr Clustering liegt vor). Zur Überprüfung können wir z.B. die Funktion ICCest aus dem Paket ICC nutzen. Für unser Beispiel schauen wir uns exemplarisch an, inwieweit Unterschiede in ‘Lebenszufriedenheit’ (Kriterium) durch ‘Gruppenzugehörigkeit im Wintersemester’ (erstis$gruppe) erklärt werden können. Dazu fügen wir diese Variable zu unserem Datensatz hinzu. daten$gruppe &lt;- erstis$gruppe library(ICC) ICCest(daten$gruppe, daten$leb_zufr) ## NAs removed from rows: ## 27 140 ## Warning in ICCest(daten$gruppe, daten$leb_zufr): ## $ICC ## [1] 0.005398465 ## ## $LowerCI ## [1] -0.01305084 ## ## $UpperCI ## [1] 0.2595086 ## ## $N ## [1] 4 ## ## $k ## [1] 47.05115 ## ## $varw ## [1] 31.43056 ## ## $vara ## [1] 0.1705978 Die Funktion gibt uns folgende Outputs: welche Missings (d.h. Zeilen) entfernt wurden $ICC: die IKK (Punktschätzung) $LowerCI und $UpperCI: die Grenzen des Konfidenzintervalls für den IKK (Intervallschätzung) $N: die Anzahl an Gruppen $k: die Anzahl der Personen in jeder Gruppe $varw: die Varianz innerhalb einer Gruppe $vara: die Varianz zwischen den Gruppen Bei unbalancierter Gruppengröße ist k, die Anzahl der Personen in jeder Gruppe, kleiner als die mittlere Gruppengröße. Für mehr Informationen dazu schau dir die R-Dokumentation für ICCest() an. Die IKK weist darauf hin, dass wenig Varianz durch die Kurszugehörigkeit erklärt wird. Es ist also nicht notwendig, dass wir die Gruppenzugehörigkeit in unserem Modell berücksichtigen. 11.4.1.2.2 Boxplots der geclusterten Gruppen Zuletzt schauen wir uns die Boxplots der Verteilungen der Residuen in den Gruppen an. Zur Erleichterung der Interpretation zeichnen wir wieder mit Hilfe von abline(h=0, col=\"darkblue\", type=\"l\", lty=3) eine Senkrechte bei \\(y=0\\) (weil der erwartete Mittelwert der Residuen \\(0\\) ist). plot(daten$resid ~ daten$gruppe) abline(h=0, col=&quot;darkblue&quot;, type=&quot;l&quot;, lty=3) ## Warning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): graphical ## parameter &quot;type&quot; is obsolete Die Interpretation hier ist ähnlich zu den Resiualplots: Der Median der einzelnen Gruppen sollte jeweils nah an der gestrichelten Linie bei \\(y=0\\) sein. Zusätzlich sollten die Verteilungen sich stark überlappen. Die Verteilungen der Residuen in den Gruppen sieht sehr ähnlich aus. Die Verteilungen überlappen sich deutlich. Es scheint keine systematischen Streuungsunterschiede in den Residuen in Abhängigkeit der Gruppenzugehörigkeit zu geben. 11.4.2 Umgang 11.4.2.1 Serielle Abhängigkeit Liegen serielle Abhängigkeiten vor, müssen wir auf geeignete Verfahren zur Analyse von Längsschnittdaten zurückgreifen (z.B. gemischte lineare Modelle). 11.4.2.2 Clustering Liegen Abhängigkeiten aufgrund von Gruppenunterschieden vor, können wir versuchen, die Gruppenunterschiede, in Form von weiteren Variablen, ins Modell aufzunehmen. Gelingt dies nicht, müssen wir auf geeignete Verfahren zur Analyse von „geclusterten“ Daten zurückgreifen (z.B. gemischte lineare Modelle). 11.5 Weitere wichtige Aspekte 11.5.1 Normalverteilung der Residuen Die Annahme, dass die Residuen in der Population normalverteilt sind, ist eigentlich gar keine Annahme im engeren Sinne. Aufgrund des zentralen Grenzwertsatzes sind die Regressionskoeffizienten in großen Stichproben selbst dann asymptotisch normalverteilt, wenn die Annahme nicht erfüllt ist. Daher ist eine Verletzung der Annahme eher in kleineren Stichproben problematisch. Da in der Praxis aber unklar ist, wann eine Stichprobe als groß genug anzusehen ist, ist es ratsam, die Verteilung der Residuen immer auf Abweichung von einer Normalverteilung hin zu prüfen. Wenn die Normalverteilungsannahme der Residuen verletzt ist, ist der Standardfehler womöglich verzerrt, was zu falschen inferenzstatistischen Schlüssen führen kann. Nicht normalverteilte Residuen können auch auf andere Probleme wie Modellmissspezifikation hinweisen. 11.5.1.1 Überprüfung 11.5.1.1.1 Histogramm Mit Hilfe eines Histogramms können wir uns die Häufigkeitsverteilung einer metrischen Variable anzeigen lassen. Dafür wird diese in verschiedene Klassen (‘bins’) eingeteilt. Das schauen wir uns für die Residuen an. hist(lm_lz$residuals) Optisch sollte ungefähr eine Normalverteilung zu erkennen sein. In unserem Beispiel sieht es jedoch nach einer linksschiefen (d.h. rechtssteilen) Verteilung der Residuen aus. Exkurs: Graphische Interpretationshilfe für das Histogramm Um die Beurteilung des Histogramms zu erleichtern, können wir zwei Linien einzeichnen - die reale Dichtefunktion der Variablen und die erwartete Dichtefunktion, wenn die Variable normalverteilt wäre. Diese können wir vergleichen, um die Annahme der Normalverteilung zu überprüfen. Eine wichtige Voraussetzung dafür, dass wir über das Histogramm die Dichtefunktionen legen können, ist dass wir das Argument prob=TRUE setzen. Dann zeichnen wir die reale Dichtefunktion der Residuen ein. Dafür kombinieren wir line() und density(). Erstere zeichnet eine Linie in eine Grafik ein und mit zweiterer spezifizieren wir, dass die Linie die geschätzte Dichte (der Residuen) visualisieren soll. Für die normalverteilte Dichtefunktion der Residuen nutzen wir curve() und dnorm(). Mit ersterer wird eine Kurve auf Basis einer mathematischen Funktion eingezeichnet. Mit zweiterer geben wir an, dass es eine Dichtefunktion einer Normalverteilung sein soll, für die wir Mittelwert und Standardabweichung (der Residuen) angeben. Zusätzlich müssen wir hier mit add=TRUE festlegen, dass die Kurve über das Histogramm gelegt werden soll. Um die Normalverteilung besser anschauen zu können, erweitern wir die \\(x\\)-Achse. Das machen wir mit xlim=c(-15, 15). Die Wahl der Grenzen von \\(-15\\) bis \\(+ 15\\) - ist eine theoretische Überlegung. Bei einer Normalverteilung liegen ~ \\(99\\)% der Werte im Bereich \\(+/- 3\\) Standardabweichungen vom Mittelwert entfernt. Die Standardabweichung der Residuen beträgt ~ 5.105. Für eine bessere Übersichtlichkeit stellen wir die reale Dichtefunktion und die erwartete Dichtefunktion farblich dar. hist(lm_lz$residuals, # Histogramm der Residuen ... prob = TRUE, # ... als Dichte ... xlim = c(-20, 20)) # ... mit veränderter X-Achse lines( # erstellt eine Linie ... density( # ... auf Basis der geschätzten Dichtefunktion ... lm_lz$residuals), # ... der Residuen ... col = &quot;orange&quot;) # ... in orange curve( # erstellt eine Kurve ... dnorm( # ... einer normalverteilten Dichtefunktion ... x, mean = mean(lm_lz$residuals), # ... auf Basis des Mittelwerts ... sd = sd(lm_lz$residuals)), # ... und der Standardabweichung der Residuen ... col = &quot;green&quot;, #... in grün ... add = TRUE) # ... die zu bestehendem Plot hinzugefügt wird Anhand dieser beiden Linien wird noch deutlicher, dass die Verteilung der Residuen in unserem Beispiel von einer Normalverteilung abweicht. 11.5.1.1.2 QQ-Plot Der QQ-Plot (Quantil-Quantil-Plot) plottet die aufsteigend geordneten standardisierten Residuen gegen die korrespondierenden Quantile der Normalverteilung. Dafür werden die Residuen durch den Standardschätzfehler geteilt \\(\\frac {e_i} {\\hat {\\sigma}}\\). plot(lm_lz, which = 2) # zweiter Plot der plot()-Funktion für ein lm-Objekt ist der QQ-Plot abline(v = 1, col = &quot;blue&quot;) # zur Illustration für unser Beispiel In der obigen Abbildung werden die theoretischen Quantile der Normalverteilung (\\(x\\)-Achse) gegen die standardisierten Residuen (\\(y\\)-Achse) abgetragen. Wenn die Residuen normalverteilt sind, sollten die Punkte ungefähr auf der winkelhalbierenden Geraden (gestrichelte Linie) liegen. Kleinere Abweichungen, vor allem an den Enden der Geraden (unten links und oben rechts) sind in der Praxis aber nicht ungewöhnlich und oftmals nicht weiter problematisch. In unserem Beispiel weichen viele Punkte mit höheren Ausprägungen (rechts von der blauen Linie) von der winkelhalbierenden Geraden ab, was für eine gewisse Verletzung der Normalverteilungsannahme spricht. Wenn wir sehen wollen, wie der QQ-Plot bei verschiedenen Verteilungsformen aussehen kann, können wir uns diesen Forumseintrag zu QQ-Plots anschauen. Exkurs: Shapiro-Wilk-Test Wir haben eingangs schon angerissen, warum graphische Verfahren besser geeignet sind, um Annahmen zu überprüfen. Für die Überprüfung der Normalverteilung der Residuen mittels statistischer Tests gelten die gleichen Vorbehalte: Wenn die Stichprobe groß ist, haben wir viel Power auch kleinste Abweichungen von der Normalverteilung zu finden. Gerade in dieser Situation (d.h. bei grossem \\(N\\)) sind Abweichungen von der Normalverteilung aber gar nicht so problematisch. Bei kleinem \\(N\\) hingegen (wenn Abweichungen potentiell gefährlich sind), fehlt dem Test dann aber oft die Teststärke Abweichungen korrekt zu erkennen. Mit dem Shapiro-Wilk-Test kann getestet werden, ob Daten normaltverteilt sind (\\(H_0\\)) oder nicht (\\(H_1\\)). Der Test wird mit der Funktion shapiro.test() durchgeführt. Um die Residuen der Regression zu testen, extrahieren wir diese wieder aus dem Ergebnisobjekt mit lm_lz$residuals. shapiro.test(lm_lz$residuals) ## ## Shapiro-Wilk normality test ## ## data: lm_lz$residuals ## W = 0.96198, p-value = 0.0001845 In unserem Beispiel kann die Nullhypothese abgelehnt werden, d.h. dass die Residuen nicht normalverteilt sind. 11.5.1.2 Umgang Zuerst sollten wir überprüfen, ob die Abweichung von der Normalverteilung auf eine Missspezifikation des Modells zurückzuführen ist. Wenn das nicht zutrifft und die Stichprobe klein ist, sollten wir korrektive Maßnahmen einleiten. Wir könnten die Daten transformieren, wie bei Umgang mit Nicht-Linearität, allerdings könnte das auch die getesteten Hypothesen ändern, z.B. vergleichen wir nach der log-Transformation keine arithmetischen Mittel mehr sondern geometrische Mittel. Alternativ könnten wir auch robuste Testverfahren anwenden. 11.5.2 Multikollinearität Wenn Prädiktoren sehr hoch miteinander korrelieren, spricht man von Multikollinearität. Dabei können wir außerdem zwischen non-essentieller und essentieller Mutlikollinearität unterscheiden. Non-essentielle Multikollinearität entsteht dadurch, dass Prädiktoren nicht zentriert sind. Essentielle Multikollinearität entsteht durch tatsächliche Zusammenhänge zwischen Variablen in der Population. Diese Abhängigkeiten zwischen Prädiktoren äußern sich dadurch, dass die Varianz eines Prädiktors großteilig durch die anderen Prädiktoren erklärt werden kann, er sich also aus einer Linearkombination der anderen Prädiktoren ergibt z.B. \\(X_1=1,2,3\\) und \\(X_2=2,4,6\\). Beispielsweise würde zwischen ‘Größe in cm’ und ‘Größe in m’ perfekte Multikollinearität bestehen, weil beide Vielfache voneinander sind. Multikollinearität kann aber beispielsweise auch vorliegen, wenn die Anzahl an Prädiktoren \\(k\\) größer ist als die Größe der Stichprobe \\(N\\) oder wenn wir im Modell versehentlich zweimal den gleichen Prädiktor spezifiziert haben. Multikollinearität tritt außerdem häufig bei Interaktionen zwischen Prädiktoren auf. Das kommt daher, dass eine Interaktion ein Produkt aus zwei Prädiktoren ist und folglich viel Gemeinsamkeit mit beiden Prädiktoren hat. Bei sehr hohen Korrelationen zwischen den Prädiktoren, ergeben sich folgende Probleme: Erschwerte Interpretation der partiellen Korrelationen. Weil sich die Prädiktoren einen großen Anteil an der erklärten Varianz des Kriteriums teilen ist unklar, welchem Prädiktor welcher Anteil zugeschrieben werden soll. Um die Anteile der Varianzaufklärung besser den Prädiktoren zuordnen zu können, könnten wir die Reihenfolge der Aufnahme der Prädiktoren im Modell ändern (hierarchisches Vorgehen). Dabei schauen wir uns die Differenzen der Determinationskoeffizienten \\(R^2_{k+m} - R^2_k\\) der Modelle (die sich durch einen weiteren Prädiktor \\(m\\) unterscheiden) an. Erhöhte Standardfehler. Die Standardfehler der von Multikollinearität betroffenen Regressionkoeffizienten vergrößern sich (siehe Toleranz und Formel des Standardfehlers der Regressionsgewichte), was wiederum zu breiteren Konfidenzintervallen und einer geringeren Wahrscheinlichkeit führt, die Nullhypothese abzulehnen, wenn tatsächlich ein Effekt vorliegt (d.h. geringere Power). Formel des Standardfehlers der Regressionsgewichte: \\(SE_{b_k} = \\frac{sd_y}{sd_x} \\cdot \\sqrt{\\frac{1}{1-R^2_k}} \\cdot \\sqrt{\\frac{1-R^2_y}{N - K - 1}}\\) Toleranz: \\(1-R^2_k\\) Indeterminationskoeffizient: \\(1-R^2_y\\) 11.5.2.1 Überprüfung Um festzustellen, ob Prädiktoren hoch miteinander korreliert sind, können wir uns die Toleranz und den Variance Inflation Factor anschauen. Beide hängen direkt miteinander zusammen, aber ihre Interpretationen sind unterschiedlich. Falls wir ein ernsthaftes Problem mit Multikollinearität haben, wird uns das in R teilweise mit z.B. aliased coefficients oder 1 coefficient not defined because of singularities angezeigt. Im Paket mctest gibt es zwei Funktionen, die helfen, Multikollinearität zu entdecken - omctest() - und zu lokalisieren - imctest(). Hierfür werden jeweils verschiedene Maße zu Rate gezogen u.a. auch die Toleranz und der VIF. Da wir die anderen Maße aber nicht vertiefend behandeln, sei an dieser Stelle nur auf das Paket hingewiesen. Bei Interesse empfielt es sich, den Artikel der Entwickler mctest: An R Package for Detection of Multicollinearity among Regressors zu lesen, in dem die Maße kurz erklärt und weiterführende Quellen angegeben werden. 11.5.2.1.1 Toleranz Die Toleranz sagt uns, wie viel Varianz in \\(X_j\\) unabhängig von den anderen Prädiktoren ist. Dafür wird eine Regression von Prädiktor \\(X_j\\) auf alle anderen Prädiktoren im Modell gerechnet (d.h. das Kriterium bleibt außen vor) und davon der Kehrwert gebildet. Sie ist somit ein Maß für die Uniqueness von \\(X_j\\). Die Tolreanz wird wie folgt berechnet: \\(1 - R^2_j\\) In R können wir die Toleranz z.B. über den Kehrwert des Variance Inflation Factors, mit der Funktion vif() aus dem Paket car, berechnen. Der Funktion übergeben wir unser lm-Objekt. library(car) 1 / vif(lm_lz) ## daten$zufr_inhalt daten$zufr_beding ## 0.8988664 0.8988664 Je kleiner die Toleranz ist, desto größer das Problem mit Multikollinearität. \\(Tol_j = 0\\) impliziert perfekte Multikollinearität. \\(Tol_j~ &lt; 0.10\\) deutet auf ein ernsthaftes Problem mit Multikollinearität hin. Ungefähr 89.9% der Varianz in Zufriedenheit mit Studienbedingungen ist unabhängig von Zufriedenheit mit Studieninhalten (für den Fall von genau zwei Prädiktoren, wie in unserem Beispiel, gilt auch die umgekehrte Interpretation). 11.5.2.1.2 Variance Inflation Factor \\(VIF_j\\) gibt an, um wie viel die Varianz des Regressionskoeffizients \\(X_j\\) (durch die Korrelation mit den anderen Prädiktoren) erhöht wird, verglichen mit dem Fall, dass alle Prädiktoren unkorreliert sind. \\(\\sqrt {VIF_j}\\) gibt an, um welchen Faktor sich der Standardfehler \\(SE_{b_j}\\) durch Einschluss weiterer korrelierter Prädiktoren erhöht (verglichen mit dem Fall, dass alle Prädiktoren unkorreliert sind). Der VIF wird für jeden Prädiktor im Modell berechnet. Er wird folgendermaßen bestimmt: \\(\\frac{1}{1 - R^2_j}\\), wobei \\(R^2_j\\) der Determinationskoeffizient der Regression des \\(j\\)-ten Prädiktors auf alle anderen \\((k - 1)\\) Prädiktoren ist. Das entspricht der quadrierten multiplen Korrelation zwischen \\(X_j\\) und allen anderen Prädiktoren. Achtung: Im Fall von zwei Prädiktoren bleibt die geteilte Varianz zwischen beiden Variablen gleich; egal welche Variable Kriterium und welche Prädiktor ist. Daher sind beide VIF gleich. Dazu nutzen wir aus dem Paket car die Funktion vif(), welcher wir unser lm-Objekt übergeben. library(car) vif(lm_lz) ## daten$zufr_inhalt daten$zufr_beding ## 1.112512 1.112512 Als Daumenregel gilt, dass ein VIF größer als 10 auf ein Problem mit Multikollinearit hindeutet. Der VIF in unserem Beispiel ist sehr klein, d.h. dass unsere beiden Prädiktoren Zufriedenheit mit Studieninhalten und Zufriedenheit mit Studienbedingungen nicht viel gemeinsame Varianz teilen. 11.5.2.2 Umgang In einer Regression, die auch Interaktionen zwischen den Prädiktoren annimmt, können wir die Prädiktoren zentrieren, um non-essentieller Multikollinearität beizukommen. Hierbei sollten wir aber bei Vorhandensein von Dummyvariablen vorsichtig sein. Diese müssen nicht zentriert werden und eine Zentrierung erschwert zusätzlich ihre Interpretation. Bei hoher essentieller Multikollinearität ist es oftmals ratsam, Prädiktoren auszuschließen, wenn diese weitgehend redundante Informationen liefern. 11.5.3 Extreme Werte und einflussreiche Datenpunkte Extreme Werte, auch Ausreißer (Outlier) genannt, meinen untypische Datenpunkte, die nicht zum Rest der Daten passen. Diese können sowohl auf dem Kriterium als auch auf den Prädiktoren vorkommen. Wenn ein extremer Wert auf dem Kriterium vorkommt, heißt das, dass die Prädiktorwerte einer Person \\(i\\) zwar im Wertebereich der anderen Personen liegen, aber der beobachtete Wert \\(y_i\\) stark vom vorhergesagten \\(\\hat y_i\\) abweicht (Abweichung in \\(Y\\)). Wenn extreme Werte auf den Prädiktoren vorkommen, heißt das, dass die Prädiktorwerte einer Person \\(i\\) nicht im Wertebereich der anderen Personen liegen (Abweichung in \\(X\\)), aber der beobachtete Wert \\(y_i\\) dennoch nah am vorhergesagten \\(\\hat y_i\\) ist. Extreme Werte auf den Prädiktoren haben zusätzlich eine hohe Leverage/Hebelwirkung und damit potenziell einen starken Einfluss auf die Regressionsgerade. Die Leverage ist die absolute Abweichung eines beobachteten Wertes einer Person vom Mittel aller Prädiktoren \\(X\\). In den folgenden drei Abbildungen sehen wir verdeutlicht, was mit extremen Abweichungen auf dem Kriterium bzw. den Prädiktoren gemeint ist. Dazu wurden exemplarisch Lebenszufriedenheit (\\(Y\\)) und Zufriedenheit mit Studieninhalten (\\(X_1\\)) gegeneinander geplottet. Hierbei wurde jeweils der \\(Y\\)- bzw. \\(X_1\\)-Wert von Person 50 geändert. Die gestrichelte Linie stellt dabei jeweils die obere Grenze des Wertebereichs des Kriteriums bzw. des Prädiktors dar. Später greifen wir nochmal auf das gleiche Beispiel zurück, um den Unterschied und die Konsequenzen von extremen Abweichungen auf dem Kriterium bzw. den Prädiktoren noch mehr zu verdeutlichen. Wenn die Regressionsgerade stark durch einzelne Beobachtungen beeinflusst wird, bezeichnet man diese als einflussreiche Datenpunkte (influentials). Der Ausschluss dieser Beobachtungen würde stark abweichende Parameterschätzungen hervorbringen. Das gleichzeitige Vorhandensein von Ausreißern und Beobachtungen mit hoher Leverage könnten die Ursache dafür sein. 11.5.3.1 Überprüfung Stoßen wir über auffällige Werte in unserem Datensatz, sollte wir zunächst kontrollieren, ob diese durch Eingabefehler oder fehlerhafte Kodierungen fehlender Werte (Missings) zustande gekommen sind. Beispielsweise werden in manchen Anwendungen Missings nicht mit NA, sondern z.B. wie bei Unipark mit 99 oder -99 kodiert. Wir müssen diese vor der Auswertung auf NA umkodieren, da R diese sonst nicht als Missings erkennt. Für mehr Informationen dazu können wir im Kapitel zu Fehlenden Werten nachschauen. Stark abweichende Werte lassen sich oft einfach mit Hilfe von Plots oder Deskriptivstatistiken identifizieren. 11.5.3.1.1 Extreme Werte auf dem Kriterium Hierbei schaut man sich vor allem die Residuen an. 11.5.3.1.1.1 Plot der studentisierten gelöschten Residuen Zur Exploration von Ausreißern auf dem Kriterium können wir uns die studentisierten gelöschten Residuen, geplottet gegen den Index \\(i\\), anschauen. Studentisierte gelöschte Residuen werden auch als externally studentized residuals bezeichnet. Studentisierte gelöschte Residuen werden wie folgt berechnet: \\(\\frac {e_i} {\\hat {\\sigma} \\cdot \\sqrt{1 - h_m}}\\)   Sie sind theoretisch \\(t\\)-verteilt mit \\(df = N − k − 1\\) (N = Stichprobengröße, k = Anzahl Prädiktoren). Wir erhalten die Anzahl der Freiheitsgrade aus dem lm-Objekt mittels summary(lm_lz)$fstat[“dendf”]. Studentisiert heißt, dass die Residuen durch die geschätzte Populationsstandardabweichung der Residuen an der Stelle \\(x_m\\) (das meint den Hebelwert der Person \\(h_m\\)) geteilt werden. Gelöscht bedeutet geschätzte Abweichung des vorhergesagten Wertes vom beobachteten Wert (das sind die normalen Residuen) in einem Modell ohne die entsprechende Person. Wir sagen also \\(\\hat y_i\\) für Person \\(i\\) mit Hilfe eines Modells hervor, für welches Person \\(i\\) nicht in die Parameterschätzung mit eingegangen ist. Für mehr Informationen zu studentisierten gelöschten Residuen (externally studentized residuals) siehe S.399ff im Lehrbuch von Cohen, Cohen, West &amp; Aiken (2003). Wir extrahieren diese mit rstudent() aus dem lm-Objekt. plot(rstudent(lm_lz)) Es gibt keine einheitlichen Richtlinien darüber, ab wann ein studentisiertes gelöschtes Residuum als extrem groß anzusehen ist. Wir schauen nur, ob einzelne Werte stark vom Cluster der anderen Werte abweichen. In unserem Beispiel gibt es keine Werte, die extrem von der Verteilung der anderen abweichen. Wir schauen uns dennoch exemplarisch einmal die zwei größten Werte an (d.h. die kleiner gleich - 3 sind). In folgender Abbildung schauen wir uns an, wie eine extreme Abweichung eines studentisierten gelöschten Residuums aussehen könnte. In folgendem Plot wurde nur ein studentisiertes gelöschtes Residuum eingefügt; die anderen Werte sind gleich geblieben. which(unname(rstudent(lm_lz) &lt;= -3)) # Indizes aller Werte, die kleiner gleich -3 sind ## [1] 37 154 Mit which() lassen wir uns die Indizes derjenigen studentisierten gelöschten Residuen ausgeben, die \\(\\leq-3\\) sind. Wir nutzen außerdem unname(), weil rstudent() einen benannten numerischen Vektor (named num) erstellt und bei Nutzung von which() die Indizes sonst doppelt ausgegeben werden würden. Bei named nums werden die Indizes nämlich schon im Vektor mitgespeichert (ohne dass dieser dadurch seine Dimensionalität ändert). 11.5.3.1.1.2 Outlier Test Alternativ könnten wir auch einen Signifikanztest nur für das größte studentisierte gelöschte Residuum durchführen. Dabei wird dessen p-Wert mit einer Bonferroni-Korrektur angepasst. Dazu wird der p-Wert mit der Stichprobengröße (hier: \\(N=164\\)) multipliziert. Das an der Stichprobengröße relativierte (d.h. durch dieses geteilt) Signifikanzlevel \\(\\alpha\\) markiert den kritischen Wert (d.h. das korrigierte Signifkanzlevel). Dazu nutzen wir outlierTest() aus dem Paket car. Wir nutzen konventionell \\(\\alpha=0.05\\). library(car) outlierTest(lm_lz) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 37 -3.053226 0.0026521 0.43494 Der Test liefert Evidenz dafür, dass das größte studentisierte gelöschte Residuum nicht extrem von den anderen abweicht. 11.5.3.1.2 Extreme Werte auf den Prädiktoren Hierbei schauen wir uns vor allem die Hebelwerte \\(h_{ii}\\) einzelner Beobachtungen (engl. leverage oder hatvalues) an. Hebelwerte geben die absolute Abweichung eines beobachteten Wertes einer Person vom Mittel aller Prädiktoren \\(X\\) (dem sog. Schwerpunkt) an. Fälle mit großer Hebelwirkung haben potenziell einen größeren Einfluss auf die Parameterschätzungen (\\(0 &lt; h_{ii} &lt; 1\\)). 11.5.3.1.2.1 Plot und Histogramm der Hebelwerte In kleinen bis mittelgroßen Datensätzen ist ein Indexplot der Hebelwerte eine ausreichende Methode, um Ausreißer auf den Prädiktoren zu erkennen. Dazu extrahieren wir die Hebelwerte mit Hilfe der Funktion hatvalues() aus dem lm-Objekt. Damit fehlende Werte (NA) ausgeschlossen werden (ihre Hebelwirkung würde \\(0\\) betragen), wenden wir außerdem [!is.na()] an. is.na() evaluiert, ob die Elemente vorhanden sind (TRUE oder FALSE), ! sorgt dafür, dass keine Missings angewählt werden, und [ ] gibt die Indizes aus. plot(hatvalues(lm_lz)[!is.na(daten$resid)]) In größeren Datensätzen sollten wir die Hebelwerte der Übersichtlichkeit halber in einem Histogramm visualisieren. hist(hatvalues(lm_lz)[!is.na(daten$resid)]) Wir sollten uns hierbei auf die Inspektion weniger, extremer Werte beschränken und vor allem nur auf solche, die weit entfernt von den restlichen (nah beieinander liegenden Werten) sind. Den größten Hebelwert und seinen Index können wir uns folgendermaßen ausgeben lassen: sort((hatvalues(lm_lz)), decreasing=TRUE)[1] ## 151 ## 0.06632099 Wenn wir z.B. die größten drei Werte betrachten möchte, schreiben wir [1:3] anstatt [1]. Wenn wir alle Werte absteigend (weil decreasing=TRUE) betrachten möchten, lassen wir die eckigen Klammern mit den Indizes komplett weg. Der Hebelwert von Person 151 liegt etwas abseits von der Verteilung der anderen. Noch extremere Werte sollten wir uns immer genauer anschauen. In unserem Beispiel liegen aber keine offensichtlich extremen Hebelwerte vor. 11.5.3.1.3 Einflussreiche Datenpunkte 11.5.3.1.3.1 Residuen-Leverage Plot Wenn es große Residuen und Hebelwerte gibt, können wir uns weiterführend den Residuen-Leverage Plot anschauen. Dieser plottet die Hebelwerte (\\(x\\)-Achse) gegen die standardisierten Residuen (\\(y\\)-Achse). Zur Standardisierung werden die Residuen durch den Standardschätzfehler geteilt \\(\\frac {e_i} {\\hat {\\sigma}}\\) plot(lm_lz, which=5) # fünfter Plot der plot()-Funktion für ein lm-Objekt ist der Residuen-Leverage Plot Die Lowess Line sollte auch hier wieder flach und nah an der gestrichelten Linie bei \\(y=0\\) sein. Die Werte der Person mit dem Index 50 weisen eine leicht erhöhte hohe Cooks Distance auf. Allerdings sind die Grenzen bei 0.5 und 1 nicht einmal sichtbar, weil alle Werte der Cooks Distance relativ klein sind. Auch die Lowess Line ist flach an nah bei y=0. Beides spricht dafür, dass in unserem Datensatz keine einflussreichen Datenpunkte, die die Parameter verzerren könnten, enthalten sind. In diesem Forumseintrag wird der Residuen-Leverage-Plot noch ausführlicher erklärt. Was ist die Cooks Distance? Und wie hängt diese mit den standardisierten Residuen, der Leverage und der Lowess Fit Line zusammen? Die Cooks Distance ist ein Maß dafür, wie sich die Regressionsgerade und damit die vorhergesagten Werte ändern würden, wenn wir die Daten einer betrachteten Person \\(i\\) (\\(Cooks D_i\\)) ausschliessen würden, wobei \\(Cooks D_i \\geq 0\\) ist. Je größer die \\(Cooks D_i\\), desto größer der Einfluss der Beobachtungen von Person \\(i\\). Im Residuen-Leverage-Plot werden auch Linien der Cooks Distance mit den Werten \\(0.5\\) und \\(1.0\\) abgetragen, wenn Beobachtungen in die Nähe dieser Grenzen kommen (was im oberen Beispiel nicht der Fall ist). Wir können uns folgendermaßen auch nur die Werte der Cooks Distance gegen den Index geplottet anschauen: plot(lm_lz, which=4) Schauen wir uns den Residuen-Leverage-Plot noch einmal an. In den folgenden Abbildungen wurden die Werte von Person 50 auf dem Kriterium und einem Prädiktor (Zufriedenheit mit Studieninhalten) jeweils einzeln und anschließend gemeinsam manipuliert, um den jeweiligen Einfluss im Vergleich zum bestehenden Datensatz (der schon weiter oben dargestellt war) zu veranschaulichen. In der Einführung zu Extremen Werte und einflussreichen Datenpunkte haben wir den Unterschied zwischen Ausreißern auf dem Kriterium und den Prädiktoren schon angesprochen. Die Abbildungen sollen deren Implikationen noch verdeutlichen. Bei der Interpretation helfen die durchgezogene Lowess Fit Line und die rot gestrichelten Grenzwerte der Cooks Distance. Das standardisierte Residuum ist größer; die Leverage ist gleich. Die Lowess Line ist immer noch flach und nah an der gestrichelten Linie bei \\(y=0\\). Die Cooks Distance ist wesentlich größer, sie beträgt fast \\(1\\). Das standardisierte Residuum ist vom Betrag her ähnlich; die Leverage ist größer. Die Lowess Line weicht wesentlich stärker von der gestrichelten Linie bei \\(y=0\\) ab. Die Cooks Distance ist etwas größer, aber noch \\(&lt; 0.5\\). Das standardisierte Residuum ist größer; die Leverage ist größer. Die Lowess Line weicht extrem von der gestrichelten Linie bei \\(y=0\\) ab. Die Cooks Distance ist \\(&gt; 1\\). 11.5.3.2 Umgang Achtung: Wir sollten abweichende Beobachtungen nicht unbedacht aus der Analyse entfernen. Sie können nicht nur durch fehlerhafte Messung, sondern auch korrekte, aber seltene Messungen zutande gekommen sein (z.B. weil wenig Leute eine gewisse Ausprägung auf einer bestimmten Variable besitzen). Außerdem gibt es keine einheitlichen Richtlinien darüber, ab wann wir Ausreißer und einflussreiche Datenpunkte entfernen sollten. Wir sollte diese immer mit Hinblick auf die Gesamtverteilung bewerten. Jeder Ausschluss sollte plausibel begründet werden können. Am besten ist es immer, zu überprüfen, ob wir robuste Ergebnisse vorliegen haben. Wir führen die vorgestellten Analysen mit und ohne kritische Werte durch und schauen uns an, was passiert. Ändern sich die Befunde nicht, sind wir wahrscheinlich auf der sicheren Seite. 11.6 Literatur &amp; weiterführende Hilfe Dieses Kapitel basiert größtenteils auf dem in der Vorlesung Multivariate Verfahren des Masterstudiengangs Psychologie genutzen Lehrbuch: Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences Hillsdale, NJ: Erlbaum. Abschnitt 4.3 Assumptions and Ordinary Least Squares Regresion (S.117-125) Abschnitt 4.4 Detecting Violations of Assumptions (S.125-141) Abschnitt 4.5 Remedies: Alternative Approaches When Problems Are Detected (S.141-149) Abschnitt 10.3 Detecting Outliers: Regression Diagnostics” (S.394-411) Abschnitt 10.5 Multicollinearity” (S.419-425) (für HU-Studierende über ub.hu-berlin.de zugänglich) Für ein deutschsprachiges Buch:   Gollwitzer, M., Eid, M., &amp; Schmitt, M. (2017). Statistik und Forschungsmethoden. Weinheim: Beltz Verlagsgruppe. (für HU-Studierende online zugänglich) Kapitel “Regressionsdiagnostik” (S. 704-725) (für HU-Studierende über ub.hu-berlin.de zugänglich) Zur weiteren Hilfe bei der Interpretation von Plots, können wir diesen Forumseintrag sowie diese Seite nutzen. Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] tidyr_1.2.1 naniar_0.6.1 VIM_6.2.2 readxl_1.3.1 ## [5] foreign_0.8-82 devtools_2.4.5 usethis_2.1.6 ICC_2.4.0 ## [9] readr_2.1.3 Hmisc_4.7-1 Formula_1.2-4 survival_3.2-13 ## [13] lattice_0.20-45 ggplot2_3.4.0 colorspace_2.0-3 psych_2.2.9 ## [17] car_3.1-1 carData_3.0-5 kableExtra_1.3.4 dplyr_1.0.10 ## [21] htmltools_0.5.3 rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] deldir_1.0-6 ellipsis_0.3.2 class_7.3-20 ## [4] visdat_0.5.3 htmlTable_2.4.1 base64enc_0.1-3 ## [7] fs_1.5.2 rstudioapi_0.13 proxy_0.4-27 ## [10] remotes_2.4.2 fansi_1.0.3 ranger_0.14.1 ## [13] xml2_1.3.3 splines_4.2.0 mnormt_2.1.1 ## [16] robustbase_0.95-0 cachem_1.0.6 pkgload_1.3.0 ## [19] jsonlite_1.8.3 cluster_2.1.2 png_0.1-7 ## [22] shiny_1.7.3 compiler_4.2.0 httr_1.4.2 ## [25] backports_1.4.1 assertthat_0.2.1 Matrix_1.5-1 ## [28] fastmap_1.1.0 cli_3.4.1 later_1.3.0 ## [31] prettyunits_1.1.1 tools_4.2.0 gtable_0.3.0 ## [34] glue_1.6.2 Rcpp_1.0.9 cellranger_1.1.0 ## [37] jquerylib_0.1.4 vctrs_0.5.0 svglite_2.1.0 ## [40] nlme_3.1-155 lmtest_0.9-40 laeken_0.5.2 ## [43] xfun_0.34 stringr_1.4.0 ps_1.7.2 ## [46] rvest_1.0.2 mime_0.12 miniUI_0.1.1.1 ## [49] lifecycle_1.0.3 DEoptimR_1.0-11 zoo_1.8-11 ## [52] MASS_7.3-56 scales_1.2.1 hms_1.1.1 ## [55] promises_1.2.0.1 parallel_4.2.0 RColorBrewer_1.1-2 ## [58] yaml_2.3.5 memoise_2.0.1 gridExtra_2.3 ## [61] sass_0.4.2 rpart_4.1.16 latticeExtra_0.6-30 ## [64] stringi_1.7.8 highr_0.9 e1071_1.7-12 ## [67] checkmate_2.0.0 boot_1.3-28 pkgbuild_1.3.1 ## [70] rlang_1.0.6 pkgconfig_2.0.3 systemfonts_1.0.4 ## [73] evaluate_0.15 purrr_0.3.4 htmlwidgets_1.5.4 ## [76] tidyselect_1.2.0 processx_3.8.0 norm_1.0-10.0 ## [79] magrittr_2.0.2 bookdown_0.29 R6_2.5.1 ## [82] generics_0.1.2 profvis_0.3.7 DBI_1.1.2 ## [85] pillar_1.8.1 withr_2.5.0 sp_1.5-0 ## [88] abind_1.4-5 nnet_7.3-17 tibble_3.1.8 ## [91] crayon_1.5.2 interp_1.0-33 utf8_1.2.2 ## [94] tzdb_0.3.0 urlchecker_1.0.1 jpeg_0.1-9 ## [97] data.table_1.14.4 callr_3.7.2 vcd_1.4-10 ## [100] digest_0.6.30 webshot_0.5.4 xtable_1.8-4 ## [103] httpuv_1.6.5 munsell_0.5.0 viridisLite_0.4.1 ## [106] bslib_0.4.0 sessioninfo_1.2.2 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["grafiken.html", "Chapter 12 Grafiken 12.1 Bevor es losgeht 12.2 Grundlegender Aufbau von ggplot() 12.3 Eine Variable 12.4 Zwei Variablen 12.5 Mehr als zwei Variablen 12.6 Modifikationen 12.7 Kategoriale Variablen: Benennung und Reihenfolge der Ausprägungen 12.8 Grafiken einzelner Gruppen und die Anpassung von Achsengrenzen 12.9 Geraden einzeichnen 12.10 Weitere statistische Kennwerte ergänzen 12.11 Motive (Themes) 12.12 Grafiken exportieren 12.13 Weiterführende Hilfen", " Chapter 12 Grafiken Einleitung Zur Verbreitung von wissenschaftlichen Untersuchungen ist es sinnvoll, Rohdaten oder Ergebnisse von statistischen Analysen übersichtlich darzustellen, um diese besser kommunizieren zu können. Dabei sind Grafiken zumeist die Methode der Wahl. Gute Grafiken sind anschaulich, wirkungsvoll und lassen einen wichtige Informationen auf einen Blick erfassen. Es gibt drei führende Pakete zum Erstellen von Grafiken in R: Basispaket graphics (Funktionen wie z.B. plot() oder hist()) Paket lattice Paket ggplot2 Basispakete sind von Anfang an in R enthalten, ohne dass wir sie herunterladen müssen. Im Rahmen dieses Kapitels werden wir uns auf ggplot2 konzentrieren. Mit diesem Paket haben wir eine große Bandbreite an Möglichkeiten, das Aussehen der Grafiken zu verändern. Außerdem können wir auf eine große Anzahl von verschiedenen Grafiken zurückgreifen wie z.B. den Violinenplot (siehe Abb. unten). Ein Violinenplot zeigt die Dichteverteilung eines metrischen Merkmals in Abhängigkeit eines kategorialen Merkmals. Den Violinenplot schauen wir uns später noch genauer an. Zuerst schauen wir uns die grundlegende Funktionsweise von ggplot an. Dann lernen wir, wie wir (bekannte) Grafiken erstellen und modifizieren können. Abschließend erfahren wir, wie wir Grafiken exportieren können. Am Ende befindet sich außerdem eine Auswahl weiterführender Hilfen, die für die Visualisierung der eigenen Daten nützlich sein können. # Paket installieren ... install.packages(&quot;ggplot2&quot;, dependencies=TRUE) # ... und laden library(ggplot2) Bei Problemen beim Installieren bzw. Laden von Paketen schaut in unserem FAQ-Eintrag dazu vorbei. Cheatsheet zu ggplot2 Um uns die Anwendung von ggplot2 zu erleichtern, können wir ein Cheatsheet in deutsch oder englisch herunterladen. Auf der R-Studio Website gibt es noch mehr Cheat Sheets, v.a. zu tidyverse Paketen. Alternativ finden wir die englischen Cheat Sheets auch direkt in R-Studio. Dazu gehen wir in der Menüleiste am oberen Bildschirmrand auf Help –&gt; Cheatsheets. Beispieldatensatzfür dieses Kapitel Im Rahmen des Kapitels werden wir hauptsächlich mit dem Datensatz ChickWeight arbeiten, der standardmäßig im Basispaket datasets enthalten ist. Wir laden den Dataframe folgendermaßen in unseren Workspace: data(ChickWeight) Die Daten stammen aus einem Experiment, in dem der Einfluss des Futters auf das Wachstum von Küken untersucht wurde. ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 Der Datensatz enthält vier Variablen. Mehr Informationen zu den Variablen finden wir hier. weight: Körpergewicht eines Kükens in Gramm Time: Tage seit der Geburt des Kükens Chick: Identifikationsnummer des Kükens Diet: Nummer der Futtergruppe (1, 2, 3, 4) Für den Abschnitt zu Visualisierungsmöglichkeiten bei mehr als drei Variablen nutzen wir zusätzlich noch den Datensatz mpg, welcher im Paket ggplot2 enthalten ist. Wir laden diesen folgendermaßen in unser Environment: mpg &lt;- mpg ## # A tibble: 6 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compa… Der Datensatz enthält 11 Variablen zum Benzinverbrauch von Automodellen in den Jahren 1999 bis 2008. Mehr Informationen zu den Variablen finden wir mit ?mpg. 12.1 Bevor es losgeht Bevor wir uns in ggplot2 vertiefen, wollen wir überprüfen, ob unsere Daten in der benötigten Datenstruktur vorliegen. Außerdem lernen wir einige Webseiten kennen, die uns dabei helfen, uns für eine Art der Visualisierung unserer Daten zu entscheiden. 12.1.1 Richtiges Datenformat Bevor es losgehen kann, müssen wir erst einmal sicherstellen, dass unsere Daten auch in adäquater Form vorliegen. Um Grafiken mit ggplot() erzeugen zu können, müssen alle genutzen Variablen in einem gemeinsamen Dataframe vorliegen. Das können wir folgendermaßen überprüfen: is.data.frame(ChickWeight) ## [1] TRUE Falls unser Datensatz nicht als Dataframe vorliegt, könnten wir ihn so umwandeln: ChickWeight &lt;- as.data.frame(ChickWeight) In Abhängigkeit der eigenen Fragestellung müssen die Daten im Long- bzw. Wide-Format vorliegen. Schauen wir uns den Unterschied einmal an der Fragestellung, wie sich das Gewicht der Küken (weight) zu unterschiedlichen Zeitpunkten (Time) verändert, an. Wenn wir Time als Prädiktor für weight aufnehmen möchten, müssen die Daten im Long-Format vorliegen, damit Time auch als eigene Variable kodiert ist. Wenn wir uns hingegen für eine Veränderung des Gewichts von Tag 0 (Time 0) zu Tag 2 (Time 2) interessieren, müssen die Daten im Wide-Format vorliegen, damit die Gewichtsdaten zu den Messzeitpunkten in einzelnen Spalten (Variablen) vorliegen. Wenn wir wissen wollen, wie wir Daten vom Wide- ins Long-Format (oder vice versa) bekommen, dann können wir unser Kapitel dazu anschauen. Außerdem kann es Probleme geben, wenn nominal- oder ordinalskalierte Variablen (z.B. Diet) im Datensatz nicht als Faktor vorliegen. Mit str() können wir uns anschauen, in welchem Daten- bzw. Objekttyp die Variablen eines Dataframes vorliegen. str(ChickWeight) # überprüfen ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 578 obs. of 4 variables: ## $ weight: num 42 51 59 64 76 93 106 125 149 171 ... ## $ Time : num 0 2 4 6 8 10 12 14 16 18 ... ## $ Chick : Ord.factor w/ 50 levels &quot;18&quot;&lt;&quot;16&quot;&lt;&quot;15&quot;&lt;..: 15 15 15 15 15 15 15 15 15 15 ... ## $ Diet : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language weight ~ Time | Chick ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Diet ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time&quot; ## ..$ y: chr &quot;Body weight&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(gm)&quot; Mit factor() können wir einzelne Spalten faktorisieren. ChickWeight$Diet &lt;- factor(ChickWeight$Diet) # umwandeln # Diet lag bereits vorher als ungeordneter Faktor vor 12.1.2 Welche Grafik sollte ich für meine Daten nehmen? Falls wir noch auf der Suche nach einer informativen und ansprechenden Grafik für unsere Daten seid, können wir uns einen Überblick über geeignete Grafiken auf from Data to Viz verschaffen. Für mehr Inspiration (sowie teilweise auch den R-Code) können wir auf den folgenden zwei Seiten nachschauen: Auf R Graph Gallery finden wir verschiedene Grafiken nach Oberbegriffen (z.B. Zusammenhänge, Korrelation) sortiert und unterteilt in bestimmte Arten von Grafiken (z.B. Histogramm, Korrelogramm). Wenn wir auf die jeweilige Grafik klicken, kommen wir auf eine Seite, auf der es eine Definition und mehrere Beispielgrafiken, teils mit R-Code, gibt. Wir finden hier auch nicht nur Grafiken, die mit ggplot erstellt wurden. Auf Top 50 ggplot2 Visualizations - The Master List finden wir auch eine gute Übersicht möglicher ggplot-Grafiken für verschiedene Anliegen (z.B. Korrelation, Variation, Veränderung) mit den dazugehörigen R-Codes. Wir finden hier auch sehr einzigartige Visualisierungen wie z.B. Dendrogramme. 12.2 Grundlegender Aufbau von ggplot() Das gg in ggplot() steht für grammar of graphics. ggplot strebt einen intuitiven Ansatz zur Erstellung von Grafiken an. Die essenziellen drei Komponenten, die wir zur Erstellung jeder Grafik benötigen, sind: Datensatz Diesen übergeben wir an das Argument data. ästhetische Mappings (aesthetics) Diese werden in aes() festgelegt. Dazu gehören u.a. die x- und y-Dimensionen, Farben und Größe. geometrische Objekte (geoms) Diese werden in geom() festgelegt. Die Formen, mit denen die Daten dargestellt werden, sind z.B. Punkte, Linien, Balken. Die Besonderheit an ggplot ist, dass wir verschiedene Ebenen übereinander legen. Diese Ebenen verbinden wir syntaktisch jeweils mit einem +. Das Grundgerüst ist dabei das Koordinatensystem (1. Ebene) und die Art der Grafik, die wir mittels geometrischer Objekt festlegen (2. Ebene). Zusätzlich können wir diese beiden Ebenen modifizieren (z.B. Farben der geometrischen Objekte ändern) und ebenso neue Ebenen ergänzen (z.B. Beschriftung). Die Ebenen können wir auch als Teilfunktionen begreifen, die eine Grafik konstituieren. Was wir z.B. bei plot(..., main=\"...\") in einer Funktion realisieren können (das Koordinatensystem, die Art der Grafik - ein Streudiagramm - und die Beschriftung) machen wir in ggplot mit mehreren Teilfunktionen (ggplot() + geom_point() + ggtitle()). 12.2.1 1. Ebene: Grundlegendes Koordinatensystem Das Koordinatensystem spezifizieren wir mit ggplot(). Dem Parameter data übergeben wir den Datensatz (1). Das ästhetische Mapping (2) legen wir mit aes() fest. Diesem geben wir die Namen der Variablen, die auf der x- und y-Achse dargestellt werden sollen. ggplot(data=ChickWeight, aes(x=Time, y=weight)) Wir können aes() aber nicht nur in ggplot(), d.h. dem grundlegenden Koordinatensystem, sondern auch in anderen Ebenen (z.B. geom_point()) nutzen. Wie bei anderen Funktionen können wir die Parameterbezeichnung data, x und y weglassen und nur die Argumente (d.h. die Namen des Datensatzes bzw. der Variablen) angeben. Bei x und y müssen wir dann aber unbedingt die Reihenfolge einhalten: ggplot(ChickWeight, aes(Time, weight)). 12.2.2 Ebene: Art der Grafik Bisher wurde nur das Koordinatensystem mit den jeweiligen Achsengrenzen der Variablen erstellt. Wenn wir die Daten nun plotten möchten, müssen wir noch festlegen, wie die Daten dargestellt werden sollen (z.B. die Häufigkeit als Balken, die bivariate Verteilung als Punkte). Dazu nutzten wir geometrische Objekte(3), die wir mit den geom-Funktionen festlegen (z.B. geom_line() für Linien oder geom_bar() für Balken). Damit ergänzen wir den Plot um eine neue Ebene. ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() Hier sehen wir, wie die 1. Ebene, das grundlegende Koordinatensystem ggplot(...), mit der 2. Ebene, den geplotteten Daten geom_point(...) durch das + verknüpft wurden. Nachdem wir uns nun mit dem grundlegenden Aufbau von ggplot() vertraut gemacht haben, schauen wir uns nun an, wie wir spezifische Arten von Grafiken erstellen. 12.3 Eine Variable Zuerst schauen wir uns an, wie wir die Häufigkeitsverteilung einer Variablen visualisieren können. 12.3.1 Kategorial Hierfür nehmen wir die Variable Diet. Diese kodiert die Gruppen, in denen das Futter der Küken variiert wurde. Später erfahren wir noch, wie wir die Reihenfolge und Benennung von kategorialen Variablen ändern können. 12.3.1.1 Balkendiagramm Um ein Balkendiagramm zu erzeugen, nutzen wir geom_bar(). Standardmäßig werden mit geom_bar() absolute Häufigkeiten (count) geplottet. ggplot(data=ChickWeight, aes(x=Diet)) + geom_bar() Wir können folgendermaßen auch die relativen Häufigkeiten (prob) plotten: ggplot(data=ChickWeight, aes(x=Diet)) + geom_bar(aes(y = ..prop.., group=1)) ## Warning: The dot-dot notation ## (`..prop..`) was deprecated in ## ggplot2 3.4.0. ## ℹ Please use `after_stat(prop)` ## instead. Mit + coord_flip() können wir das Diagramm um 90° nach rechts kippen. Jetzt ist auf der x-Achse die Häufigkeit und auf der y-Achse die Gruppierung abgebildet. ggplot(data=ChickWeight, aes(x=Diet)) + geom_bar() + coord_flip() Wir können auch sogenannte gestapelte Balkendiagramme (stacked bar plots) anfertigen. Dazu müssen erst einen neuen Dataframe erzeugen, in welchem die Gruppennamen sowie die (absoluten) Häufigkeiten der Variablen je als neue Variable gespeichert sind. # Häufigkeitstabelle von Diet erstellen und in Dataframe konvertieren: freq_Diet &lt;-as.data.frame(table(ChickWeight$Diet)) # (optional) Spalten umbenennen: colnames(freq_Diet) &lt;- c(&quot;Futtergruppe&quot;, &quot;Häufigkeit&quot;) Mit diesem Dataframe erstellen wir nun ein gestapeltes Balkendiagramm: ggplot(data=freq_Diet, aes(x=&quot;&quot;, y=Häufigkeit, fill=Futtergruppe)) + geom_bar(stat = &quot;identity&quot;) Auf die Integration von Farben, hier durch den Parameter fill, gehen wir später im Abschnitt zu Farbe noch ausführlicher ein. Mehr Informationen zur Erstellung von gestapelten sowie auch gruppierten Balkendiagrammen finden wir hier. Beide Grafiken eignen sich auch gut, um Gruppierungskombinationen mit noch einer weitere kategorialen Variablen darzustellen. 12.3.1.2 Kreisdiagramm Um ein Kreisdiagramm zu erstellen, müssen wir, analog zur Erstellung von gestapelten Balkendiagrammen, zuerst einen neuen Dataframe erzeugen, in welchem die Namen und (absoluten) Häufigkeiten der Gruppen der Variablen als neue Variablen vorhanden sind. # Häufigkeitstabelle von Diet erstellen und in Dataframe konvertieren: freq_Diet &lt;-as.data.frame(table(ChickWeight$Diet)) # (optional) Spalten umbenennen: colnames(freq_Diet) &lt;- c(&quot;Futtergruppe&quot;, &quot;Häufigkeit&quot;) Auch das weitere Vorgehen überschneidet sich weitest mit dem der Erstellung eines gestapelten Balkendiagrammes. Wir müssen lediglich noch + coord_polar(\"y\") ergänzen, damit die Anteile der Häufigkeiten in Polarkoordinaten überführt werden. ggplot(data=freq_Diet, aes(x=&quot;&quot;, y=Häufigkeit, fill=Futtergruppe)) + geom_bar(stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;) Auf Farben gehen wir im gleichnamigen Abschnitt später noch ausführlicher ein. Auf dieser Seite finden wir weitere Hilfe zur Erstellung und Modifikation von Kreisdiagrammen. 12.3.2 Metrisch Beispielhaft schauen wir uns hierfür das Gewicht der Küken (weight) an. 12.3.2.1 Histogramm Mit Hilfe von Histogrammen können wir uns die Verteilung einer metrischen Variablen anschauen. Um ein Histogramm zu erstellen, nutzen wir geom_histogram(). Richtlinien zur Kategorisierung einer metrischen Variablen Bevor wir eine metrische Variable zum Zwecke der grafischen Darstellung kategorisieren, sollten wir uns Gedanken über die Anzahl und Breite der Kategorien machen, in die wir die Daten einteilen möchten. Es ist meist nicht sinnvoll, die Häufigkeiten der Rohwerte von metrischen Variablen zu plotten, weil identische Messwerte selten vorliegen. Daher ist es sinnvoll, die Rohdaten zu gruppieren. Dazu müssen wir uns Gedanken über die Intervallgröße dieser Kategorien machen (“Problem der Kategorisierung”). Durch Vergrößerung gehen (relevante) Informationen verloren; durch Verkleinerung bleiben (zu viele) idiosynkratische Merkmale erhalten. Prinzipiell gilt: Je größer eine Stichprobe ist, desto schmaler können die einzelnen Kategorien sein (d.h. mehr Kategorien). Als Orientierung können wir hierfür die Sturges Regel nutzen: \\(m ≈ 1 + 3.32 \\cdot lg(N)\\) (\\(m\\) = Anzahl der Kategorien, \\(N\\) = Stichprobengröße, \\(lg\\) = dekadischer Logarithmus) Schauen wir uns die ungefähre Anzahl der Kategorien nach der Sturges Regel an. Dazu nutzen wir die Funktion nclass.Sturges(1:Stichprobengröße). # Gibt es fehlende Werte? anyNA(ChickWeight$weight) ## [1] FALSE ## wenn nein: nclass.Sturges(1:nrow(ChickWeight)) ## [1] 11 ## wenn ja: # nclass.Sturges(1:table(is.na(ChickWeight$weight))[[2]]) # table(is.na(ChickWeight$weight))[[2]] gibt uns die Anzahl der vorhanden werte Nach Sturges Regel sollten wir für weight in etwa 11 Kategorien bilden. Je größer die Streuung der Variablen, desto breiter können die einzelnen Kategorien sein (d.h. weniger Kategorien). min(ChickWeight$weight) # kleinster Wert ## [1] 35 max(ChickWeight$weight) # größter Wert ## [1] 373 # Variationsbreite (Streuung) = größter minus kleinster Wert: max(ChickWeight$weight) - min(ChickWeight$weight) ## [1] 338 # bei Vorhandensein von fehlenden Werten: # min(..., na.rm=TRUE) bzw. max(..., na.rm=TRUE) Es ist sinnvoll, zuerst nach Sturges Regel eine Anzahl an Bins zu errechnen, und diese dann ggf. in Abhängigkeit der Variationsbreite des Merkmals zu verändern. Generell sollten wir nicht mehr als 20 Kategorien bilden. Diese sollten im Regelfall auch ungefähr die gleiche Breite aufweisen. Wir erstellen ein Histogramm von weight mit den empfohlenen 11 Kategorien, welche wir mit dem Argument bins erstellen. ggplot(data=ChickWeight, aes(x=weight)) + geom_histogram(bins=11) Problem der korrekten Darstellung der Variationsbreite Grundsätzlich ist es, sowohl in base graphics als auch in ggplot, problematisch, den wahren Range (d.h. die Variationsbreite) einer Variablen darzustellen. Ohne explizite Eingabe der Grenzen werden diese leider nicht berücksichtigt. Das kann zu einem falschen Eindruck der Variationsbreite der Variablen führen. Dieses Problem demonstrieren wir einmal an obigem Beispiel. Mit geom_vline(xintercept) erstellen wir zwei vertikale Linien, die den kleinsten bzw. größten beobachteten Wert von weight markieren. Diese nutzen wir zur Veranschaulichung der Problematik. ggplot(data=ChickWeight, aes(x=weight)) + geom_histogram(bins=11) + geom_vline(xintercept=min(ChickWeight$weight)) + # kleinster Wert geom_vline(xintercept=max(ChickWeight$weight)) # größter Wert Wie wir sehen haben wir zwar die gewünschten 11 Kategorien, aber diese sind leider so eingeteilt, dass die äußeren Kategorien über den kleinsten und größten beobachteten Wert von weight hinausgehen (d.h. sie sind zu groß). Um das Problem in den Griff zu bekommen, nutzen wir das Argument breaks, mit dem wir die Kategoriengrenzen manuell festlegen. Diesem übergeben wir die Funktion seq(from, to, by), die uns eine reguläre Sequenz erstellt (damit wir nicht alle Grenzen einzeln eingeben müssen). Auch hier wollen wir wieder die empfohlenen 11 Kategorien haben, daher teilen wir den Range (373-35) von weight durch 11. ggplot(data=ChickWeight, aes(x=weight)) + geom_histogram(breaks=seq(35, 373, (373-35)/11)) + geom_vline(xintercept=min(ChickWeight$weight)) + # kleinster Wert geom_vline(xintercept=max(ChickWeight$weight)) # größter Wert Wie wir an den vertikalen Linien sehen, hält die Breite der Kategorien insgesamt jetzt den Range von weight ein. Weiterführend finden wir zusätzliche Modifikationen für Histogramme auf dieser Seite. 12.3.2.2 Dichtefunktion Die Dichteverteilung einer kontinuierlichen Variablen können wir mit geom_density() einzeichnen. Per Default wird dafür die Gaußsche Dichtefunktion (kernel = \"gaussian\") genutzt. ggplot(data=ChickWeight, aes(x=weight)) + geom_density() 12.4 Zwei Variablen Nun schauen wir uns die grafische Darstellung von Zusammenhängen zwischen zwei Variablen an. 12.4.1 X kategorial, Y metrisch Später erfahren wir noch, wie wir die Reihenfolge und Benennung von kategorialen Variablen ändern können. 12.4.1.1 Boxplot Wenn wir uns die Merkmalsverteilung einer metrischen Variable in Abhängigkeit einer kategorialen Variablen anschauen möchten, können wir Boxplots nutzen. Welche Kennwerte werden in Boxplots dargestellt? In Boxplots werden mehrere deskriptiv-statistische Kennwerte dargestellt: zentrale Tendenz: Median: dargestellt durch die dicken, horizontalen Linien in den Boxen Variabilität: Interquartilsrange (IQR): mittlere 50% der Verteilung, dargestellt durch die Boxen Whisker: größter bzw. kleinster beobachteter Wert innerhalb der oberen bzw. unteren Ausreißergrenzen, dargestellt durch die vertikale Linien an den Boxen Ausreißer: einzelne Beobachtungen außerhalb der Whisker, dargestellt durch Punkte Einen Boxplot erhalten wir mit geom_boxplot(). Wenn wir fehlende Werte in unseren Daten haben, müssen wir noch das Argument na.rm = TRUE ergänzen, welches diese (aus der Grafik) entfernt. Wir schauen uns das Gewicht der Küken (weight) zum zweiten Messzeitpunkt (Time 2) in Abhängigkeit der Fütterung (Diet) an. ggplot(data=ChickWeight[ChickWeight$Time == 2,], aes(x=Diet, y=weight)) + geom_boxplot() Erweiterte Indexierung: Wie können wir nur bestimmte Daten (z.B. nur Daten zum zweiten Messzeitpunkt) für ggplot() auswählen? Wenn wir nicht alle Daten, sondern nur solche, auf die bestimmte Bedingungen zutreffen, auswählen wollen, können wir dazu die Indexierung mittels der eckigen Klammern [] nutzen. Für unser Beispiel wollen wir uns nur die Daten des zweiten Messzeitpunkts (Time 2) anschauen. Formal würde (nur) die Bedingung so aussehen: ChickWeight$Time == 2. Diese wollen wir noch auf unseren Dataframe anwenden; dazu nutzen wir die eckigen Klammern: ChickWeight[ChickWeight$Time == 2,]. Mit dem Komma legen wir fest, welche der beiden Dimensionen unseres Dataframes wir meinen (Zeile, Spalte). Weil wir Fälle mit bestimmten Ausprägungen (Bedingungen) auswählen wollen, setzen wir das Komma am Ende. Zu beachten ist außerdem, dass wir die Indexierung bereits in der grundlegenden Ebene, d.h. in ggplot(data, ...), festlegen müssen. Zu Beginn haben wir gelernt, dass wir durch die Übergabe des Dataframes an ggplot(data, ...) das grundlegende Koordinatensystem erstellen und damit die Anzahl der Datenpunkte (Beobachtungen) festlegen. Folglich würde es nicht funktionieren, die Selektion erst in einer späteren Ebene vorzunehmen, weil die an Anzahl der Datenpunkte sich unterscheiden würde. 12.4.1.2 Violinenplot Der Violinenplot ist eine Variante von Boxplots, in dem die Dichtefunktion eines metrischen Merkmals grafisch dargestellt wird. Diese wird an der Senkrechten zur x-Achse gespiegelt. Um die Grafiken mit dem Boxplot aus dem letzten Abschnitt vergleichen zu können, schauen wir uns hier ebenfalls das Gewicht der Küken (weight) zum zweiten Messzeitpunkt (Time 2) in Abhängigkeit der Diät (Diet) an. Hier wird das Indexieren der Daten (zum zweiten Messzeitpunkt) erklärt. Generell können wir den Violinenplot mit geom_violin() erstellen. Zusätzlich gibt es hier einen sehr wichtigen Parameter: scale. Mit diesem legen wir fest, ob die einzelnen “Violinen” in der gleichen Größe (area; voreingestellt) oder in Größen proportional zur Anzahl der Beobachtungen in jeder Gruppe (count) dargestellt werden sollen. Wir schauen uns unsere Daten einmal in beiden Modi an. ggplot(data= ChickWeight[ChickWeight$Time == 2,], aes(x=Diet, y=weight)) + geom_violin(scale = &quot;area&quot;) ggplot(data=ChickWeight[ChickWeight$Time == 2,], aes(x=Diet, y=weight)) + geom_violin(scale = &quot;count&quot;) Wir sehen, dass die Größen derselben “Violinen” sich in den beiden Grafiken unterscheiden. Das liegt daran, dass bei ersterer Grafik (scale=area) die Anzahl der Beobachtungen pro Gruppe unbeachtet bleibt, während sie in zweiter Grafik (scale=count) über die relative Größe der “Violinen” visualisiert wird. Wenn wir gleich große Gruppen hätten, würden wir keinen Unterschied zwischen den beiden Grafiken erkennen. 12.4.1.3 Gruppenmittelwerte visualisieren Im Folgenden schauen wir uns die finalen (Time 21) Mittelwerte des Gewichts der Küken (weight) in den einzelnen Futtergruppen (Diet) an. Dabei wollen wir außerdem die 95%-Konfidenzintervalle für die einzelnen Mittelwerte einzeichnen. Hier wird das Indexieren der Daten (zum finalen Messzeitpunkt) erklärt. Wenn wir aggregierte Daten einer statistischen Analyse (z.B. Gruppenmittelwerte bei einem \\(t\\)-Test) darstellen möchten, ist ein weit verbeiteter Ansatz, Balkendiagramme mit Fehlerbalken zu nutzen. Negativbeispiel Achtung: Das Problem bei diesem Ansatz ist, dass Personen dazu neigen, die Werte (innerhalb des 95% Konfidenzintervalls), die innerhalb der Balken liegen, als statistisch wahrscheinlicher wahrzunehmen als jene, die außerhalb der Balken liegen. Dieses Phänomen ist als Within-the-bar Bias bekannt. Es ist daher sinnvoll, für die Darstellung von Mittelwerten keine Balkendiagramme zu nutzen, sondern auf geeignetere Visualisierungen zurückzugreifen. Beispielsweise vermitteln einzelne Punkte (für die Mittelwerte) mit Fehlerbalken die relevanten Informationen besser. Generell um statistische Kennwerte zu ergänzen, nutzen wir die Funktion stat_summary() (mehr dazu hier). Mit fun.data=\"mean_cl_normal\" bekommen wir Mittelwerte mit 95% Konfindenzintervallen. ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + stat_summary(fun.data=&quot;mean_cl_normal&quot;) Positivbeispiel Wenn wir zusätzlich noch unsere Informationsdichte erhöhen wollen, d.h. nicht nur den Mittelwert, sondern die Verteilung in den einzelnen Gruppen visualisieren wollen, können wir Violinenplots integrieren. ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + geom_violin(scale=&quot;count&quot;) + stat_summary(fun.data=&quot;mean_cl_normal&quot;) # Reihenfolge beachten! erst Violinenplot, dann Mittelwerte darüber Positivbeispiel 12.4.2 X und Y metrisch 12.4.2.1 Streudiagramm Mit Streudiagrammen (Scatterplots) können wir Wertepaare zweier kontinuierlicher Variablen grafisch darstellen. Das machen wir mit geom_point(). Wir schauen uns im Folgenden den Zusammenhang von Zeit seit dem Schlüpfen (Time) und dem Gewicht in Gramm (weight) der Küken an. ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() Wie wir die Datenpunkte in Abhängigkeit einer weiteren kategorialen Variablen einfärben können, erfahren wir im Abschnitt Farben. Overplotting vermeiden Wenn wir sehr große Datensätze haben, könnten wir das Problem haben, dass wir individuelle Daten optisch nicht mehr gut unterscheiden können, weil viele Datenpunkte übereinander liegen. Dieses Problem nennt man auch Overplotting. Im Folgenden schauen wir uns einige Möglichkeiten an, wie wir das Problem beheben können. Hierfür schauen wir uns wieder den Zusammenhang von Zeit seit dem Schlüpfen (Time) und dem Gewicht in Gramm (weight) der Küken an. Punkte modifizieren Wir können die Formen verkleinern mit dem Parameter size (Breite der Linie in mm), … ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point(size = 0.1) … die Form ändern mit dem Parameter shape, … ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point(shape = 1) … oder die Form transparent machen mit dem Parameter alpha (0 &lt; alpha &lt; 1). ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point(alpha = 0.1) Jittern “Jittern” heißt, dass wir etwas zufälliges Rauschen einfügen, damit die Datenpunkte etwas voneinander abweichen. Achtung: Weil wir hiermit aber einen falschen Eindruck von den Daten vermitteln könnten, sollten wir die Verwendung von gejitterten Daten immer kennzeichnen. Per default werden die Punkte in 80% der Breite der implizierten Bins (z.B. der Bin von Time 0) geplottet, sodass die Bins optisch noch gut voneinander zu trennen sind. Mit dem Parameter width können wir die Breite anpassen. ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_jitter() # mehr (aber zu viel) Variation: ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_jitter(width=1) Es gibt noch weitere Möglichkeiten Overplotting zu vermeiden. Auf dieser Seite finden wir noch andere Beispiele sowie ihre Umsetzung in R. Regressionsgerade einzeichnen Wir können unsere bivariaten Daten zusätzlich noch durch eine Funktion beschreiben lassen. Häufig nutzen wir dafür Regressionsmodelle. Die Regressionsgerade eines einfachen linearen Regressionsmodells können wir mit geom_smooth(method=\"lm\") über unser Streudiagramm legen. Per default wird nicht nur eine Regressionsgerade eingezeichnet, sondern auch das 95%-Konfidenzintervall um die Gerade gelegt. ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + geom_smooth(method=&quot;lm&quot;) Das Konfidenzintervall um die Regressionsgerade könnten wir mit se=FALSE weglassen. Generell ist es aber sinnvoll, die Genauigkeit unseres Regressionsmodells in die Grafik miteinzubeziehen. Wenn wir die Datenpunkte in einem Streudiagramm mit Regressionsgerade einfärben wollen ohne gruppenspezifische Regressionsgeraden zu erhalten, müssen wir aes(col) in geom_point(), und nicht in ggplot(), spezifizieren. Paket ggiraphextra: Interaktive Plots und weitere Darstellungen von Regressionsmodellen Mit der Funktion ggPredict() aus dem Paket ggiraphExtra können wir interaktive Streudiagramme mit Regressionsgeraden (von einfachen und multiplen linearen Regressionsmodellen) erstellen. Hierfür benötigen wir die Basisfunktion ggplot() gar nicht (aber ggiraphExtra baut auf ggplot2 auf). Allerdings müssen wir zuerst manuell mit lm() die Regression rechnen und das Ergebnisobjekt an ggPredict() übergeben. Im Folgenden schauen wir uns an, wie wir interaktive Streudiagramme mit einer Regressionsgeraden (eines einfachen linearen Regressionsmodells) erstellen können. Weitere Hilfe zu ggPredict(), z.B. wie wir gruppenspezifische Ergebnisse einer multiplen linearen Regression mit Interaktion oder einer logistischen Regression visualisieren können, findet wir hier. Interaktiv bedeutet, dass wir in den Plot reinzoomen, uns die Funktion der Regressionsgerade sowie die ID und die Prädiktor- und Kriteriumswerte der Personen anzeigen lassen können. Um den Plot interaktiv zu machen, müssen wir interactive=TRUE festlegen. Auch hier können wir ein 95%-iges Konfidenzintervall um die Regressionsgerade legen, indem wir se=TRUE spezifizieren. Per Default ist hier (im Gegensatz zu geom_smooth) kein Konfidenzintervall eingezeichnet. Die Visualisierung der (Un)Genauigkeit unseres Regressionsmodells ist allerdings zu empfehlen. # install.packages(&quot;ggiraphExtra&quot;, dependencies=TRUE) # Regression rechnen: reg_ggP &lt;- lm(formula = weight ~ Time, data = ChickWeight) # Ergebnisobjekt plotten library(ggiraphExtra) ggPredict(reg_ggP, se = TRUE, interactive = TRUE) Achtung: Leider ist die Nutzung von ggPredict() teilweise noch fehleranfällig, weil von ggiraphExtra erst eine Beta-Version vorliegt (Stand: Version 0.2.9). Es kann u.a. zu Problemen kommen, wenn man ordinalskalierte (ordered factors) oder nominalskalierte Variablen (factors) ins Modell aufnimmt. Trotzdem ist das interaktive Streudiagramm ein sinnvolles Feature, z.B. für Ergebnispräsentation in html-Dokumenten. Man sollte die Weiterentwicklung des Pakets ggiraphExtra also verfolgen. 12.5 Mehr als zwei Variablen Wie wir bis hier gesehen haben, können wir bis zu zwei Variablen sehr gut in einer Grafik visualisieren. Wenn wir aber mehr (kategoriale) Variablen aufnehmen wollen, müssen wir darauf achten, dass die zusätzlichen Informationen nicht zu Lasten der Verständlichkeit der Grafik sind. Mit Facetten erstellen wir eine Matrix aus (Unter)Grafiken einer Art für verschiedene Gruppen. Mit einem Alluvial Plot können wir die Häufigkeiten der Zugehörigkeit zu verschiedenen Gruppen visualisieren. Beide Grafiken können genutzt werden, wenn wir Variablen visualisieren wollen, von denen mindestens eine kategorial ist. Wir können mit ihnen mehr als drei Variablen visualisieren, wenn alle (Alluvial Plot) bzw. alle bis auf maximal zwei (Facetten Plots) Variablen kategorial sind. Werte kontinuierliche Variablen können wir auch in Kategorien einteilen um sie hier zu nutzen. Im Folgenden nutzen wir auch den Datensatz mpg, weil dieser mehr kategoriale Varialen enhält als ChickWeight. 12.5.1 Mindestens eine kategoriale Variable 12.5.1.1 Facetten (Facet Grids) Wenn wir uns eine Art von Grafik (z.B. Streudiagramm) in Abhängigkeit einer kategorialen Variablen (d.h. für verschiedene Gruppen) separat anschauen wollen, können wir + facet_grid() nutzen. Mit dieser Funktion bekommen wir “Facetten”, d.h. (Unter)Grafiken für jede Ausprägung der kategorialen Variablen. Wir können außerdem entscheiden, wie die Facetten angeordnet sein sollen: facet_grid(Variable ~ .) Die Variable wird zeilenweise angeordnet, indem wir sie vor die Tilde schreiben und einen Punkt dahinter setzen. facet_grid( ~ Variable) Die Variable wird spaltennweise angeordnet, indem wir sie nach der Tilde schreiben. Die Anordnung von Zeile und Spalte in facet_grid() ist analog zum Indexieren von zweidimensionalen Datenstrukturen (z.B. Matrizen, Dataframes): Zuerst die Zeile, dann die Spalte. Wir schauen uns die Gewichtszunahme (weight) über die Zeit (Time) getrennt für die einzelnen Experimentalgruppen (Diet) an. ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + facet_grid(Diet ~ .) # Zeilen ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + facet_grid( ~ Diet) # Spalten Wir können auch Facetten für mehrere kategoriale Variablen erstellen. Das Produkt der Anzahl der Ausprägungen der Variablen gibt die Menge der (Unter)Grafiken an (z.B. drei Variablen mit jeweils zwei Ausprägungen: 2 x 2 x 2 = 8 Grafiken). Wenn wir mehr als zwei Variablen haben, werden Variablen, die zusammen in einer Zeile bzw. einer Spalte gestaffelt werden, mit einem + verbunden. Damit die einzelnen Grafiken aber noch übersichtlich bleiben, sollten wir ihre Anzahl tendenziell gering halten. Wir schauen uns aus dem Datensatz mpg Histogramme des Hubraums (displ) der Autos an. Die Histogramme sind nach Jahr (year), Antrieb (drv) und Brennstofftrieb (fl) gestaffelt. ggplot(data=mpg, aes(x=displ)) + geom_histogram() + facet_grid(fl ~ year + drv) # Zeilen ~ Spalten Idealerweise würde man vor der Verarbeitung nominal- und ordinalskalierte Variablen, die als numerische Vektoren vorliegen (wie drv und fl), noch faktorisieren. Mehr dazu auch im Kapitel zu Datenvorbereitung Weitere Informationen zu facet_grid(), z.B. wie man hier die Achsengrenzen und -beschriftung anpassen kann, befinden sich auf dem Cheat Sheet. Mehr Informationen dazu, wie wir nur Plots für einige Gruppen erstellen, erhalten wir später. 12.5.2 Nur kategoriale Variablen 12.5.2.1 Alluvial Plot Alluvial Plots sind eine Art von Flussdiagrammen, die die Zugehörigkeit zu mehreren Gruppierungen visualisieren. Sie stellen somit auch eine grafische Alternative zu Kontingenztabellen dar. Wir wollen das Gewicht der Küken zum ersten und letzten Messzeitpunkt (Time) in Abhängigkeit der Fütterung (Diet) visualisieren. Dazu werden wir die Werte der metrischen Variablen weight in zwei Gruppen aufteilen: größer bzw. kleiner als der Mittelwert. Für die Visualisierung längsschittlicher Daten müssen wir die folgenden Schritte durchlaufen: Zuerst wählen wir die relevanten Fälle, d.h. jene zum ersten und letzten Messzeitpunkt, aus und speichern sie in einem neuen Dataframe (optional; nur wenn wir nicht alle Zeitpunkte visualisieren wollen). Diesen wandeln wir dann ins Wide-Format um. Danach erstellen wir eine neue Variable, die kodiert, ob das Gewicht eines Kükens zu einem gewissen Zeitpunkt kleiner oder größer als der Mittelwert ist. Zuletzt entfernen die nicht länger benötigten Variablen aus dem Dataframe. Diese Schritte werden im folgenden nur knapp erklärt. Mehr Hilfe finden wir im Kapitel zu Datenvorbereitung in den Abschnitten Daten extrahieren und Kodierung ändern. library(dplyr) # neuer Datensatz nur mit Werten von erster und letzter Messung: copy &lt;- filter(ChickWeight, Time == 0 | Time == 21) # ins Wide-Format bringen: library(tidyr) cont_Chick &lt;- spread(copy, key=&quot;Time&quot;, value=&quot;weight&quot;) # Die Variablen 0 und 21 wurden erzeugt. Diese müssen wir bei der Indexierung # immer mit &quot;..&quot; umschließen, da sie sonst nicht als solche erkannt werden. # für beide Messzeitpunkte eine neue Variable erstellen: cont_Chick$t0 &lt;- case_when(cont_Chick$&quot;0&quot; &lt; mean(cont_Chick$&quot;0&quot;, na.rm=TRUE) ~ &quot;kleiner&quot;, cont_Chick$&quot;0&quot; &gt; mean(cont_Chick$&quot;0&quot;, na.rm=TRUE) ~ &quot;größer&quot;) cont_Chick$t21 &lt;- case_when(cont_Chick$&quot;21&quot; &lt; mean(cont_Chick$&quot;21&quot;, na.rm=TRUE) ~ &quot;kleiner&quot;, cont_Chick$&quot;21&quot; &gt; mean(cont_Chick$&quot;21&quot;, na.rm=TRUE) ~ &quot;größer&quot;) # nicht mehr benötigte Variablen raus: cont_Chick &lt;- select(cont_Chick, -&quot;0&quot;, -&quot;21&quot;, -Chick) # Die ID-Variable &quot;Chick&quot; benötigen wir nicht mehr und sie würde uns die # im Anschluss erstellte Kontingenztabelle unnötig erweitern. Für jeden Alluvial Plot (egal ob längsschnittliche oder querschnittliche Daten genutzt wurden) müssen wir die Daten in eine Kontingenztabelle überführen (damit die Häufigkeiten der Gruppierungskombinationen explizit in einer neuen Spalte gespeichert werden) und anschließend wieder in einen Dataframe umwandeln. # Kontingenztabelle erstellen cont_Chick &lt;- table(cont_Chick) # in Dataframe umwandeln cont_Chick &lt;- as.data.frame(cont_Chick) Um besser zu verstehen, warum das sein muss, können wir uns den Datensatz vor und nach der Umwandlung anschauen. Vergleich unseres Dataframes vor und nach der Umwandlung Vor der Umwandlung steht jede Zeile für eine Beobachtung (d.h. ein Küken). Diet t0 t21 1 größer kleiner 1 kleiner kleiner 1 größer kleiner 1 größer kleiner 1 kleiner größer 1 kleiner kleiner 1 kleiner größer 1 größer 1 größer kleiner 1 kleiner kleiner 1 größer kleiner 1 kleiner kleiner 1 kleiner kleiner 1 kleiner größer 1 kleiner 1 kleiner 1 größer kleiner 1 kleiner 1 größer kleiner 1 kleiner kleiner 2 kleiner größer 2 kleiner kleiner 2 größer kleiner 2 größer kleiner 2 kleiner größer 2 größer größer 2 kleiner kleiner 2 kleiner größer 2 kleiner größer 2 größer kleiner 3 größer größer 3 kleiner größer 3 kleiner kleiner 3 kleiner größer 3 kleiner größer 3 kleiner größer 3 kleiner kleiner 3 kleiner größer 3 größer größer 3 kleiner größer 4 größer kleiner 4 größer größer 4 größer kleiner 4 größer 4 kleiner kleiner 4 kleiner größer 4 kleiner kleiner 4 kleiner größer 4 kleiner größer 4 kleiner größer Nach der Umwandlung (in eine Kontingenztabelle und wieder zurück in einen Dataframe) steht in jeder Zeile eine Kombination (welche Futtergruppe Diet und jeweils kleiner oder größer zum ersten und zum zweiten Messzeitpunkt) und die Anzahl der Beobachtungen. Diet t0 t21 Freq 1 größer größer 0 2 größer größer 1 3 größer größer 2 4 größer größer 1 1 kleiner größer 3 2 kleiner größer 4 3 kleiner größer 6 4 kleiner größer 4 1 größer kleiner 7 2 größer kleiner 3 3 größer kleiner 0 4 größer kleiner 2 1 kleiner kleiner 6 2 kleiner kleiner 2 3 kleiner kleiner 2 4 kleiner kleiner 2 Bevor wir den Alluvial Plot erstellen, laden wir das benötigte Paket ggalluvial und überprüfen, ob unser neu erstellter Dataframe auch wirklich im korrekten Alluvial Format vorliegt. library(ggalluvial) is_alluvia_form(cont_Chick) ## [1] TRUE Es ist zu empfehlen, den nachfolgenden Code für die eigenen Daten größtenteils zu kopieren. Lediglich in Zeilen (a) und (d) müssen die Argumente von fill bzw. limit angepasst werden. ggplot(data=cont_Chick, aes(y=Freq, axis1=t0, axis2=t21)) + # (a) grundlegender Alluvial Plot: geom_alluvium(aes(fill=Diet)) + # (b) Balken zur Visualisierung der Häufigkeiten in t0 und t21: geom_stratum(fill=&quot;white&quot;, width=1/12) + # (c) Einfügen der Gruppennamen von t0 und t21: geom_label(stat = &quot;stratum&quot;, aes(label=after_stat(stratum))) + # (d) Einfügen bzw. Ändern der Benennung der Variablen t0 und 21: scale_x_discrete(limits = c(&quot;erste Messung&quot;, &quot;letzte Messung&quot;), # (e) Verringern der Fläche außerhalb der (äußersten) Strata: expand = c(.05, .05)) ## Warning in to_lodes_form(data = data, axes = axis_ind, discern = ## params$discern): Some strata appear at multiple axes. ## Warning in to_lodes_form(data = data, axes = axis_ind, discern = ## params$discern): Some strata appear at multiple axes. ## Warning in to_lodes_form(data = data, axes = axis_ind, discern = ## params$discern): Some strata appear at multiple axes. ## Warning: Using the `size` aesthetic in ## this geom was deprecated in ## ggplot2 3.4.0. ## ℹ Please use `linewidth` in the ## `default_aes` field and ## elsewhere instead. Um den Plot (und den Code) besser zu verstehen, werden wir noch einige wichtige Begriffe erläutern: axis: vertikale Dimension einer Variablen auf der x-Achse (weiße Balken von t0 und t21) stratum: Gruppierung von axis (für t0 und t21 jeweils größer und kleiner) alluvium: horizontale “Strömungen” im Hintergrund, die die kombinierten Gruppenzugehörigkeiten beschrieben (z.B. größer zur ersten Messung und kleiner zur letzten Messung) und nach einer weiteren Gruppe farblich kodiert sind (Diet) Die verschiedenen Strata zeigen uns, wie groß die Anteile an Küken sind, die ihr Gewicht gehalten oder verändert haben (jeweils im Vergleich zum Mittelwert zum jeweiligen Zeitpunkt). Wir können auch vergleichen, ob sich die Trends in den Gruppen von Diet unterscheiden z.B. hat Gruppe 3 den größten Zuwachs bekommen (erste Messung: kleiner; letzte Messung: größer). Mehr Informationen und weitere Beispiele für Alluvial Plots finden wir auch in der Dokumentation des Pakets. Im folgenden Abschnitt werden noch weitere Erweiterungen vorgestellt z.B. wie wir einen Titel und andee Achsenbeschriftungen einfügen können. 12.6 Modifikationen Nachfolgend schauen wir uns einige Möglichkeiten der Modifikation von Grafiken an. Als Beispiele nutzen wir dafür die bisher erstellten Grafiken. Achtung: Wenn wir in der R-Dokumentation nach ggplot2-specs suchen, finden wir eine Übersicht der ästhetischen Spezifikationen wie z.B. Farben, Linientypen, Punktformen, Schriftarten, Textausrichtung etc. 12.6.1 Farbe Nachfolgend schauen wir an, wie wir bestimmte Elemente bzw. Teile von Elementen einer Grafik farblich hervorheben können und welche Möglichkeiten es gibt (einzelne) Farben und Farbpaletten zu nutzen. Um unsere Farbwahl auch für Menschen mit verschiedenen Sehschwächen geeignet zu gestalten, können wir verschiedene Farbwahrnehmungen mit Coblis simulieren. Grundlegendes Es gibt zwei Parameter, mit denen wir jeweils festlegen können, ob Elemente farblich umrandet und/oder komplett ausgefüllt werden sollen. col, color oder colour: farbliche Umrandung eines Elements In verschiedenen Funktionen ist das Argument zur farblichen Umrandung unterschiedlich benannt (d.h. col, color oder colour). Es gibt aber auch viele Funktionen, in denen alle Parameter funktionieren (und dasselbe bewirken). Genauere Informationen erhalten wir in der Dokumentation der jeweiligen Funktion. fill: farbliche Füllung eines Elements Weil col und fill separate Parameter sind, können wir auch beide gleichzeitig nutzen, um sowohl die Umrandung als auch die Ausfüllung eines Elements (geom) verschieden einzufärben. Achtung: Allerdings gibt es Elemente (geoms), die davon ausgenommen sind. Bei Linien (z.B. geom_line()) und Text (z.B. geom_text()) können wir nur col nutzen. Bei Punkten (z.B. geom_point()) gibt es einige Formen (21-24, siehe unten), die col und fill nutzen können. Alle anderen Formen können auch nur col nutzen. Die Punktform legen wir mit dem Argument shape fest. ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point(shape=23, fill=&quot;red&quot;, col=&quot;blue&quot;) Abbildung aus der Dokumentation von “ggplot2-specs” Zusäzlich können wir zwei Farbmodi unterscheiden: statische Farben gelten für alle (spezifizierten) Elemente außerhalb von aes() variable Farben unterschiedliche Farben für verschiedene Ausprägungen einer Variablen es wird automatisch eine Legende erstellt innerhalb von aes() Nachfolgend finden wir eine Übersicht, in der die jeweiligen Unterschiede zwischen col und fill sowie statischen und variablen Farben verdeutlicht werden sollen. Der Code zur Erstellung der jeweiligen Grafik befindet sich über den Abbildungen. statisch Variablenausprägung fill col (Einzelne) Farben Wir können in R standardmäßig implementierte (“built-in”) Farben sowie Hex-Farben nutzen. Auf dieser Seite finden wir eine Übersicht der built-in Farben. Diese können wir einfach mit ihrem Namen auswählen z.B. \"red\" oder \"seagreen1\". Achtung: Diese built-in Farben sind nicht mit allen Anwendungen in R kompatibel. Beispielsweise können wir \"seagreen\", aber nicht \"seagreen1\" in der Funktion kable() aus dem Paket knitr, welche eine Tabelle erstellt, nutzen. Darüber hinaus können wir auch Hex-Farben (mit dem Hexadezimal-Zahlensystem kodierte Faben) nutzen, die aus einem # und einer 6-stelligen Zeichenfolge bestehen, z.B. \"#53FD9F\". Dabei werden jeweils zwei Zahlen genutzt um rot, blau und grün zu kodieren (RGB). Beispielsweise können wir hier eigene Farben erstellen und uns den Hex-Code kopieren. Grafische Elemente transparent machen Wir können einzelne Elemente einer Grafik transparent machen. Wenn wir die Grafik exportieren wollen, müssen wir nicht nur in der Erstellung der Grafik sondern auch beim Export angeben, dass ein bestimmtes Element transparent sein soll. Nur Grafiken im PNG-Format können transparent sein. Folgendermaßen können wir beispielsweise den Hintergrund der Grafik (plot.background) transparent machen: ggplot(ChickWeight[ChickWeight$Time == 8,], aes(Diet, weight)) + geom_violin(scale = &quot;count&quot;) + theme(plot.background = element_rect(fill = &quot;transparent&quot;)) # Erstellung ggsave(&quot;Grafik_transp.png&quot;, bg = &quot;transparent&quot;) # Export Um zu sehen, dass die sonst weiße Hintergrundfläche der Grafik nun transparent ist, müssten wir die Grafik runterladen, weil der Hintegrund hier auch weiß ist. Farbpaletten Es ist zu empfehlen bei der farblichen Kodierung von Variablenausprägungen auf Farbpaletten zurückgreifen, da diese bereits gut durchdachte Farbkombinationen enthalten. Die Idee dahinter ist bei kategorialen Variablen verschiedene Farben und bei metrischen Variablen ähnliche Farben für ähnliche Ausprägungen zu nutzen. Neben den voreingestellten Farbpaletten, die wir automatisch nutzen, wenn wir col oder fill innerhalb von aes() spezifizieren, können wir auch weitere Farbpaletten nutzen. Auf colorbrewer2.org finden wir jeweils angemessene Farbkombinationen zur Kodierung von kategorialen Daten. Teilweise sind diese Farbpaletten bereits in ggplot2 implementiert z.B. in der Funktion scale_colour_brewer() bzw. scale_fill_brewer(), mit der wir Farbanpassungen vornehmen können. Für kategoriale Daten ist type=\"qual\" (qualitative) am besten geeignet, da es Paletten mit sehr unterschiedliche Farben nutzt. Es gibt noch \"seq\" (sequential) und \"div\" (diverging), mit denen wir eine Reihenfolge bzw. eine Grenze farblich darstellen können. Für mehr Informationen lohnt es sich auf der Webseite nachzuschauen. Dort sehen wir z.B. auch die verschiedenen Paletten, die wir mit dem Parameter palette ändern können. ggplot(ChickWeight[ChickWeight$Time == 8,], aes(Diet, weight)) + geom_violin(scale = &quot;count&quot;, aes(fill=Diet)) + scale_fill_brewer(type=&quot;qual&quot;) 12.6.2 Legenden modifizieren Wenn wir Farben, Punktformen o.ä. auf Variablen(ausprägungen) anwenden (d.h. diese in aes() spezifizieren), wird automatisch eine Legende erstellt. Es gibt verschiedene Aspekte von Legenden, die wir ändern können. Im Folgenden schauen wir uns Text und Positionierung an. Weitere Modifikationen finden wir hier. Text Wir können sowohl den Titel als auch die einzelnen Elemente einer Legende anders benennen. Wenn es sich um eine kategoriale Variable handelt, die wir mit fill farblich kodiert haben, können wir die Änderungen mit scale_fill_discrete(name, labels) vornehmen. Dem Parameter name übergeben wir den Titel der Legende, labels einen Vektor mit den Namen der Bezeichnung der Ausprägungen. ggplot(data=ChickWeight, aes(x=Diet)) + geom_bar(aes(fill = Diet)) + # fill ... scale_fill_discrete(name = &quot;Fütterung&quot;, # ... deswegen _fill_ labels = c(&quot;Diät 1&quot;, &quot;Diät 2&quot;, &quot;Diät 3&quot;, &quot;Diät 4&quot;)) Analog dazu nutzen wir scale_color_discrete(), wenn wir das Argument color (zur farblichen Umrandung) genutzt haben. Wenn wir eine metrische Variable farblich kodiert haben, nutzen wir scale_fill_continuous bzw. scale_colour_continuous (). Position Die Position der Legende können wir mit theme(legend.position) ändern. Wir können zwischen oben (\"top\"), unten (\"bottom\"), links (\"left\") und rechts (\"right\") wählen. Mit \"none\" können wir die Legende entfernen. ggplot(data=ChickWeight, aes(x=Diet)) + geom_bar() + geom_bar(aes(fill = Diet)) + theme(legend.position = &quot;bottom&quot;) 12.6.3 Beschriftung Im Folgenden schauen wir uns an, wie wir Titel und Achsenbeschriftung, sowie Beschriftungen in der Grafik ändern bzw. hinzufügen können. Überschrift Um dem Plot (nur) eine Überschrift zu verpassen nutzen wir ggtitle(). ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + ggtitle(&quot;Gewichtsveränderung über die Zeit&quot;) Achsenbeschriftung Wenn wir (nur) die Beschriftung der x- oder y-Achse ändern möchten (z.B. weil diese anders benannt werden sollen als die Variablen im Datensatz), können wir xlab() bzw. ylab() nutzen. ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + xlab(&quot;Zeit in Tagen&quot;) + ylab(&quot;Gewicht in Gramm&quot;) Überschrift, Achsenbeschriftung und Bildunterschrift Wir können sowohl eine Überschrift als auch Achsenbeschriftungen ergänzen, indem wir labs(title, x, y) nutzen. Außerdem können wir der Grafik mit caption noch eine Bildunterschrift verpassen. ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + labs(title =&quot;Gewichtsveränderung über die Zeit&quot;, x = &quot;Zeit in Tagen&quot;, y = &quot;Gewicht in Gramm&quot;, caption=&quot;Weitere Notizen&quot;) Beschriftung in der Grafik Ausprägung von Variablen Mit geom_text() (Bsp. 1 und 2) oder geom_label() (Bsp. 3 und 4) können wir Ausprägungen einer Variablen als Text in unsere Grafik einfügen. Achtung: Allerdings können wir die Funktionen nicht bei Facetten anwenden. Beide Funktionen machen fast das Gleiche. Optisch unterscheiden sie sich dadurch, dass geom_label() den Text zusätzlich mit einem weißen Feld hinterlegt. Unseren Text übergeben wir an aes(label), welches sowohl in ggplot() (Bsp. 1 und 3), als auch in geom_text() bzw. geom_label() (Bsp. 2 und 4) spezifiziert werden kann. Zusätzlich können wir die Ausrichtung des Texts anpassen. Dazu nutzen wir hjust (horizontal) und vjust (vertikale) (Bsp. 2 und 4), welchen wir einen Wert zwischen 0 und 1 übergeben (Default: hjust=0.5 und vjust=0.5). Wir können auch Werte außerhalb von 0 bis 1 spezifizieren; allerdings wird in der Dokumentation davon abgeraten (ohne weitere Begründung; mehr Informationen dazu in der Dokumentation von ggplot2-specs). Beispiel 1 und 2 zeigen die Gewichtsveränderung (x-Achse: Time, y-Achse: weight) über die Zeit von Küken 1 (ChickWeight$Chick == 1). Wir beschriften die Datenpunkte mit den jeweiligen Messzeitpunkten (aes(label=Time)). ggplot(data=ChickWeight[ChickWeight$Chick == 1,], aes(x=Time, y=weight, label=Time)) + geom_point() + geom_text() ggplot(data=ChickWeight[ChickWeight$Chick == 1,], aes(x=Time, y=weight)) + geom_point() + geom_text(aes(label=Time), vjust=1) Beispiel 3 und 4 zeigen die Gewichtsverteilung der Küken in den Futtergruppen (x-Achse: Diet, y-Achse: weight) zum letzten Messzeitpunkt (Time 21). Wir beschriften die Datenpunkte mit den jeweiligen Nummern der Küken (aes(label=Chick)). ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight, label=Chick)) + geom_point() + geom_label() ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + geom_point() + geom_label(aes(label=Chick), hjust=1.2) Es gibt noch weitere Parameter, mit denen wir u.a. das Padding des Textes, die Rundung des Textfeldes, die Schriftgröße oder -farbe ändern können. Weitere Hilfe zur Nutzung und Modifikation von geom_text() und geom_label() finden wir hier. Beliebiger Text an beliebiger Stelle Wenn wir einen eigenen Text in eine Grafik schreiben wollen, können wir annotate(geom = \"text\") und annotate(geom = \"label\") nutzen. Analog zu den Funktionen im vorhergehenden Abschnitt sind beide geom-Optionen gleich, außer dass der Text bei \"label\" mit einem weißen Feld hinterlegt wird. Anders als bei den Funktionen im vorhergehenden Abschnitt müssen wir eine Position festlegen (weil unser Text nicht zu bestehenden Datenpunkten gematcht wird). Dafür können wir x und y nutzen. Diese entsprechen den Maßen der Einheiten der jeweiligen Achsen. Beispiel 1 zeigt die Gewichtsveränderung (x-Achse: Time, y-Achse: weight) über die Zeit von Küken 1 (ChickWeight$Chick == 1). Beispiel 2 zeigt die Gewichtsverteilung der Küken in den Futtergruppen (x-Achse: Diet, y-Achse: weight) zum letzten Messzeitpunkt (Time 21). ggplot(data=ChickWeight[ChickWeight$Chick == 1,], aes(x=Time, y=weight)) + geom_point() + annotate(geom = &quot;text&quot;, label=&quot;Küken 1&quot;, x=5, y=200) ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + geom_point() + annotate(geom=&quot;label&quot;, label=&quot;Messzeitpunkt 21&quot;, x=1.7, y=370) Wir können die Darstellung des Textes (wie im vorhergehenden Abschnitt) auch anpassen. Mehr Informationen dazu in der Dokumentation von annotate(). 12.7 Kategoriale Variablen: Benennung und Reihenfolge der Ausprägungen Benennung Der Einfachheit halber nutzen wir zur Kodierung von Gruppen häufig Zahlen, wie z.B. bei Diet: 1, 2, 3, 4. In einer Grafik hingegen ist es aber sinnvoller, Begriffe zu nutzen. Mit levels() können wir uns nicht nur die verschiedenen Stufen eines Faktors ausgeben lassen. Wir können die Funktion außerdem nutzen, um die Benennung der Stufen zu ändern. Im Folgenden ändern wir die Benennungen der Gruppen von Diet. ChickWeight$Diet_name &lt;- ChickWeight$Diet # Kopie von `Diet` als neue Spalte levels(ChickWeight$Diet_name) &lt;- c(&quot;Diät 1&quot;, &quot;Diät 2&quot;, &quot;Diät 3&quot;, &quot;Diät 4&quot;) ggplot(data=ChickWeight, aes(x=Diet_name, y=weight)) + geom_boxplot() Reihenfolge Die Reihenfolge von Faktorstufen ist bei Wortketten standardmäßig alphabetisch und bei Zahlen aufsteigend. Wenn wir das ändern möchten, müssen wir die Sortierung des Faktors ändern (dadurch ändert sich nicht die Sortierung der Variablen im Datensatz). Die Sortierung des Faktors können wir mit der Funktion factor() ändern. Dem Parameter x übergeben wir die Variable; levels einen character-Vektor mit der neuen Sortierung. Wir ändern die Reihenfolge der Gruppen von Diet. # Kopie von `Diet` erstellen (optional): ChickWeight$Diet_sort &lt;- ChickWeight$Diet # Änderung der Reihenfolge: ChickWeight$Diet_sort &lt;- factor(x=ChickWeight$Diet_sort, levels=c(&quot;3&quot;, &quot;4&quot;, &quot;2&quot;, &quot;1&quot;)) ggplot(data=ChickWeight, aes(x=Diet, y=weight)) + geom_boxplot() + ggtitle(&quot;unbearbeitet&quot;) ggplot(data=ChickWeight, aes(x=Diet_sort, y=weight)) + geom_boxplot() + ggtitle(&quot;bearbeitet&quot;) 12.8 Grafiken einzelner Gruppen und die Anpassung von Achsengrenzen Manchmal ist es sinnvoll, die Achsengrenzen von x- und y-manuell zu ändern, beispielsweise wenn wir Grafiken einzelner Gruppen vergleichen möchten. Im Abschnitt zu Facetten haben wir gelernt, wie wir einzelne Grafiken für jede Ausprägung einer kategorialen Variablen (bzw. für jede Kombination von Ausprägungen mehrerer kategorialer Veriablen) erstellen können. Diese werden jedoch kleiner dargestellt, je mehr Gruppierungen es gibt. Wenn wir ohnehin nur spezifische Gruppen vergleich wollen, können wir für diese manuell Grafiken erstellen. Wenn wir uns für die Gewichtszunahme (weight) in den Gruppen (Diet) zu Beginn (Time 0) und zum Ende (Time 21) des Experiments interessieren, dann können manuell beide Grafiken erstellen. Hier wird das Indexieren der Daten erklärt. ggplot(data=ChickWeight[ChickWeight$Time == 0,], aes(x=Diet, y=weight)) + geom_point() ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + geom_point() Das Problem hierbei ist, dass sich die Streubreiten von weight, und damit die Achsengrenzen von y, zu beiden Zeitpunkten stark unterscheiden. Dadurch könnten wir einen falschen Eindruck von den Daten bekommen. Um das zu verhindern, können wir die Achsengrenzen anpassen. Das machen wir mit xlim() und ylim(). Diesen übergeben wir jeweils einen numerischen Vektor, der den unteren und den oberen Grenzwert der Achse enthält. ggplot(data=ChickWeight[ChickWeight$Time == 0,], aes(x=Diet, y=weight)) + geom_point() + ylim(c(0,375)) ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + geom_point() + ylim(c(0,375)) Bei genauer Betrachtung fällt weiterhin auf, dass unsere beiden y-Achsen nicht bei 0 beginnen, obwohl wir das mit ylim(c(0,...)) scheinbar so festgelegt haben. Standardmäßig werden Achsen (kontinuierlicher Variablen) etwas erweitert. Dieses Verhalten können wir z.B. mit scale_y_continuous(expand = c(0, 0)) auf der y-Achse (und scale_x_coninuous() auf der x-Achse) ändern. Achtung: Etwaige Änderungen der Achsengrenzen müssen wir dann aber auch in scale_y_continuous(limits) machen. ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + geom_point() + scale_y_continuous(limits=c(0,375), expand = c(0, 0)) 12.9 Geraden einzeichnen Wir können auch Geraden einzeichnen (z.B. alternativ zu Achsengrenzen oder um manuell Regressionsgeraden einzuzeichnen). Vertikale Gerade können wir mit geom_vline(xintercept) einfügen; horizontale mit geom_hline(yintercept). Dabei können wir den Parametern xintercept und yintercept auch mehrere Werte übergeben. Dazu packen wir diese in einen Vektor mit c(). Wir schauen uns dazu eines der Beispiele aus dem Abschnitt vorher an: die Gewichtszunahme (weight) in den Gruppen (Diet) zum Ende (Time 21) des Experiments. Wir zeichnen jeweils den kleinsten und größten Wert, sowie den Mittelwert (über alle Diet-Gruppen) von weight als (horizontale) Gerade ein. Hier wird das Indexieren der Daten erklärt. # Maxima von weight (zu Time 21) herausfinden: min &lt;- min(ChickWeight$weight, na.rm=TRUE) max &lt;- min(ChickWeight$weight, na.rm=TRUE) ggplot(data=ChickWeight[ChickWeight$Time == 21,], aes(x=Diet, y=weight)) + geom_point() + # Min &amp; Max: geom_hline(yintercept=c(min, max)) + # Mittelwert: geom_hline(yintercept=mean(ChickWeight$weight[ChickWeight$Time == 21]), col=&quot;red&quot;) So können wir die Verteilungen von weight in den Diet-Gruppen zu den zwei Zeitpunkten besser vergleichen. Wir können nicht nur sehen, dass die Verteilung an Tag 21 wesentlich breiter gestreut ist als an Tag 0 (weil dadurch die Achsengrenzen angepasst werden), sondern auch … 12.10 Weitere statistische Kennwerte ergänzen Manchmal möchten wir in einer bestehende Grafik noch zusätzliche statistische Kennwerte einfügen. Dazu können wir stat_summary() nutzen. Damit können wir verschiedenen Kennwerte von y (z.B. den Mittelwert, den Median, oder den minimalen und maximalen Wert) in eine Grafik einzeichnen. Als Beispiel schauen wir uns ein Streudiagramm des Gewichts der Küken (weight) in Abhängigkeit ihrer Diät (Diet) am 10. Tag (Time 10) an. Per Default wird mit stat_summary() (ohne Spezifikation von Argumenten) der Mittelwert (als Punkt), umschlossen vom Standardfehler des Mittelwerts (als Linie), eingezeichnet. Wir können dem Parameter fun aber auch die Argumente \"median\" oder \"mean\" übergeben. Letzteres zeichnet ebenfalls den Mittelwert ein (aber ohne Standardfehler). Wenn wir die Extrema einfügen wollen, nutzen wir fun.min = \"min\" und fun.max = \"max\". ggplot(data=ChickWeight[ChickWeight$Time == 10,], aes(x=Diet, y=weight)) + geom_point() + stat_summary(col=&quot;blue&quot;) + # Mittelwert + SE: blau stat_summary(fun = &quot;median&quot;, col=&quot;red&quot;, pch=3) # Median: rotes Kreuz ## No summary function supplied, ## defaulting to `mean_se()` ## Warning: Removed 4 rows containing ## missing values ## (`geom_segment()`). ggplot(data=ChickWeight[ChickWeight$Time == 10,], aes(x=Diet, y=weight)) + stat_summary(fun = &quot;mean&quot;, fun.min = &quot;min&quot;, fun.max = &quot;max&quot;) Mehr Informationen zur Funktion finden wir in der R-Dokumentation von stat_summary() bzw. eine ausführlichere Version davon mit Beispielabbildungen hier. 12.11 Motive (Themes) Es gibt einige Optionen, den Hintergrund des Koordinatensystems zu verändern.Standarmäßig haben wir einen grauen Hintergrund mit weißen Rasterlinien; das entspricht der Einstellung theme_grey(). Wenn wir beispielsweise einen weißen Hintergrund mit grauen Rasterlinien haben möchten, nutzen wir dafür theme_minimal(). ggplot(data=ChickWeight, aes(x=Time, y=weight)) + geom_point() + theme_minimal() Wir können unsere Palette möglicher Motiven mit dem Paket ggthemes erweitern. In diesem finden wir u.a. die Funktion theme_tufte(), mit der Grafiken nach dem Vorbild von Edward Tufte auf das Notwendige reduziert werden (z.B. weißer Hintergrund, keine unnötigen Rasterlinien eingezeichnet etc.). 12.12 Grafiken exportieren Wenn wir unsere Grafik erstellt haben, müssen wir diese noch exportieren, um sie außerhalb von R nutzen zu können. Grundsätzlich gibt es zwei Möglichkeiten dafür: über das untere rechte Panel bei Plots &gt; Export (nur in RStudio möglich) mit Hilfe von Funktionen Im Folgenden schauen wir uns an, wie wir den Export-Button und die Funktion ggsave(), mit der wir mit ggplot2 erstellte Grafiken exportieren können, nutzen. Eine Übersicht über das Exportieren von Grafiken mit Funktionen aus dem Basispaket grDevices, finden wir hier. Mit diesen Funktionen können wir Grafiken, die mit einem beliebigen R-Paket erstellt wurden, exportieren. Achtung: Wenn wir mehrere Garfiken exportieren wollen, müssen wir darauf achten, dass wir diesen unterschiedliche Namen zuweisen. Sowohl innerhalb von R als auch beim Export werden bestehende Objekte mit dem gleichen Namen überschrieben. Achtung: Hinweis für Windows-NutzerInnen: Manchmal kann es zu Probleme mit Aliasing kommen. Das ist, wenn Grafikelemente und/oder Text nach dem Export “krisselig” dargestellt werden. Beispiel für Aliasing Bildquelle: https://www.r-bloggers.com/exporting-nice-plots-in-r/ Um das zu verhindern, können wir das Paket Cairo, mit dessen gleichnamiger Funktion, nutzen. Mehr Informationen zur Anwendung können wir auf R-bloggers finden. 12.12.1 Export-Button Wenn wir die Grafik mittels der grafischen Benutzeroberfläche exportieren möchten, klicken wir auf Export im unteren rechten Panel. Wenn wir diese als Bild exportieren wollen, wählen wir Save as Image… aus. Wenn wir mehrere Grafiken geplottet haben können wir mit den Pfeiltasten (ganz links unter dem Reiter Files) auswählen, welche Grafik wir exportieren wollen. Daraufhin öffnet sich ein neues Fenster, in dem wir einige Einstellungen tätigen können. Bildformat Hier können wir wählen, in welchem Bildformat wir unsere Grafik abspeichern wollen. Standardmäßig wird sie als PNG gespeichert. Weitere Optionen sind JPEG, TIFF, BMP, SVG und EPS. Verzeichnis Wenn wir auf den Button Directory… klicken, können wir festlegen, wo die Grafik gespeichert werden soll. Standardmäßig wird sie in der obersten Ordnerstruktur, bzw. wenn wir ein Workind Directory gesetzt haben in diesem, gespeichert. Name der Datei Hier können wir unserer Datei einen Namen geben. Standardmäßig wird als Name Rplot vorgeschlagen, wenn wir in dem aktuellen Verzeichnis keine andere Datei (im gleichen Format) besitzen, die den gleichen Namen trägt. Wenn es bereits eine solche Datei gibt, wird als Name für die neue Datei automatisch Rplot01 (bzw. Rplot02 usw.) vorgeschlagen. Größe bearbeiten Für die Bearbeitung der Größe der Grafik haben wir zwei Möglichkeiten: Angeben der Maße für Breite und Höhe Großziehen der Grafik Bei beiden Möglichkeiten können wir entweder willkürlich die Maße anpassen oder das Verhältnis zwischen Breite und Höhe beibehalten indem wir ein Häkchen bei Maintain aspect ratio setzen. Hier geben wir jeweils die Maße für Breite und Höhe in px (Pixel) an. Wenn wir das Häkchen bei Maintain aspect ratio gesetzt haben, müssen wir nur ein Maß angeben; das andere wird automatisch berechnet. Danach können wir auf Update Preview klicken, um uns eine Vorschau der Grafik anzuschauen (aber das geht nur, solange die Maße nicht zu groß für die Vorschau sind). Die Standardmaße 398 x 375 px entsprechen 14,04 x 13,23 cm (B x H). Analog können wir die Größe der Grafik auch mit der Maus verändern. Dazu ziehen wir einfach an der unteren rechten Ecke der Grafikvorschau. Wenn wir eine gewisse Größe in cm haben wollen, können wir die px-Angaben z.B. auf dieser Seite umrechnen lassen. Achtung: Die Auflösung der Grafik können wir in dem Export-Fenster nicht ändern. Standardmäßig beträgt diese 72 dpi. 12.12.2 ggsave() Mit ggplot2 erstellte Grafiken können wir mit ggsave() exportieren. Schauen wir uns einmal an, was wir in der Funktion spezifizieren können. ggsave( filename = &quot;Grafik.png&quot;, # Name und Bildformat der zu exportierenden Grafik path = &quot;/Users/...&quot;, plot = last_plot(), # letzter Plot als Default oder alternativ Name der Grafik in R width = 12, # Breite height = 9, # Höhe units = &quot;cm&quot;, # in welcher Einheit angegeben dpi = 72 # Default; äquivalent zu: dpi = &quot;screen&quot; ) Das gewünschte Bildformat können wir einfach als Endung an den Namen der Grafik anhängen. Beides übergeben wir in \" \" an das Argument filename. Wir haben hier mehr Auswahl an möglichen Outputformaten. Es gibt z.B. PNG, PDF, JPEG, TIFF, EPS, PS, TEX und SVG. Wir legen mit dem Argument plot fest, welche Grafik wird exportieren wollen. Das funktioniert nur wenn wir diese vorher in einer Variablen gespeichert haben (z.B. name &lt;- ggplot(...)). Dann erscheint der Name auch im Global Environment (oberer rechtes Panel in R). Standarmäßig wird der zuletzt erstellte Plot exportiert, wenn wir das Argument plot weglassen (oder plot=last_plot() eingeben). Das Verzeichnis, in dem die Grafik abgelegt werden soll, können wir mit dem Argument path festlegen. Diesem übergeben wir den Ordnerpfad, welchen wir auch in \" \" setzen müssen. Standardmäßig wird die Grafik in der obersten Ordnersturktur bzw. im derzeitigen Working Directory (wenn eins gesetzt wurde) abgelegt. Die Größe der zu exportierenden Grafik können wir mit width und height ändern. Mit units legen wir dabei fest, in welcher Einheit die Maße sein sollen. Wenn wir width, height und units weglassen, wird die Größe des current graphics device genutzt. Diese ist von unseren jeweiligen globalen Computer-Einstellungen abhängig. Manchmal müssen wir mit den Maßen ein bisschen herumspielen, bis wir die perfekte Größe gefunden haben. Im Gegensatz zur Möglichkeiten des Exportierens via Export-Button können wir in ggsave() mit dem Argument dpi die Bildauflösung verändern. Zusäzlich zu der Standardeinstellung von 72 dpi (screen), können wir 300 dpi (print) und 320 dpi (retina) nutzen. Wir können sowohl die Zahl, als auch die Bezeichnung in \" \" angeben. 12.13 Weiterführende Hilfen 12.13.1 Eine einfache ggplot-Funktion: qplot() Es gibt im Paket ggplot2 auch eine einfacher handzuhabende Variante zu ggplot() - das ist qplot() (“quick plot”). Diese ist an den Aufbau der base graphics Funktion plot() angelehnt. Mit ihr können wir verschiedenen Grafiken erstellen, aber sie ist weniger flexibel und modifizierbar als ggplot(). Wir erstellen exemplarisch ein Streudiagramm vom Gewicht der Küken (weight) in Abhängigkeit der Zeit (Time) und ergänzen eine Überschrift sowie Achsenbeschriftungen. qplot(x=Time, y=weight, data=ChickWeight, geom=&quot;point&quot;, main=&quot;Gewichtsveränderung über die Zeit&quot;, xlab=&quot;Zeit in Tagen&quot;, ylab=&quot;Gewicht in Gramm&quot;) ## Warning: `qplot()` was deprecated in ## ggplot2 3.4.0. Das gleiche Beispiel haben wir auch im Abschnitt Beschriftung. Für mehr Informationen zu qplot() (vor allem zu den möglichen Argumenten) können wir hier nachschauen. 12.13.2 Mehrere Plots zusammenführen Wenn wir mehrere Grafiken haben, die wir in einer gemeinsamen Datei speichern wollen, dann müssen wir dafür auf zusätzliche Pakete zurückgreifen. Diese Seite gibt einen guten Überblick darüber, welche Funktionen wir dafür nutzen können. Am Ende der Seite unter Alternative options befindet sich eine kleine Tabelle, in der die Pakete mit ihren Funktionen hinsichtlich ihrer ggsave()-Kompatibilität und Möglichkeit zur Anordnung der Plots verglichen werden. Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ggalluvial_0.12.3 ggiraphExtra_0.3.0 lsr_0.5.2 tidyr_1.2.1 ## [5] naniar_0.6.1 VIM_6.2.2 readxl_1.3.1 foreign_0.8-82 ## [9] devtools_2.4.5 usethis_2.1.6 ICC_2.4.0 readr_2.1.3 ## [13] Hmisc_4.7-1 Formula_1.2-4 survival_3.2-13 lattice_0.20-45 ## [17] ggplot2_3.4.0 colorspace_2.0-3 psych_2.2.9 car_3.1-1 ## [21] carData_3.0-5 kableExtra_1.3.4 dplyr_1.0.10 htmltools_0.5.3 ## [25] rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] uuid_1.0-3 backports_1.4.1 systemfonts_1.0.4 ## [4] plyr_1.8.7 sp_1.5-0 splines_4.2.0 ## [7] mycor_0.1.1 digest_0.6.30 fansi_1.0.3 ## [10] magrittr_2.0.2 checkmate_2.0.0 memoise_2.0.1 ## [13] cluster_2.1.2 tzdb_0.3.0 remotes_2.4.2 ## [16] svglite_2.1.0 prettyunits_1.1.1 jpeg_0.1-9 ## [19] rvest_1.0.2 xfun_0.34 callr_3.7.2 ## [22] crayon_1.5.2 jsonlite_1.8.3 zoo_1.8-11 ## [25] glue_1.6.2 gtable_0.3.0 ppcor_1.1 ## [28] webshot_0.5.4 sjmisc_2.8.9 pkgbuild_1.3.1 ## [31] DEoptimR_1.0-11 abind_1.4-5 scales_1.2.1 ## [34] DBI_1.1.2 miniUI_0.1.1.1 Rcpp_1.0.9 ## [37] viridisLite_0.4.1 xtable_1.8-4 laeken_0.5.2 ## [40] htmlTable_2.4.1 proxy_0.4-27 vcd_1.4-10 ## [43] profvis_0.3.7 htmlwidgets_1.5.4 httr_1.4.2 ## [46] RColorBrewer_1.1-2 ellipsis_0.3.2 urlchecker_1.0.1 ## [49] pkgconfig_2.0.3 farver_2.1.1 nnet_7.3-17 ## [52] sass_0.4.2 deldir_1.0-6 utf8_1.2.2 ## [55] reshape2_1.4.4 tidyselect_1.2.0 labeling_0.4.2 ## [58] rlang_1.0.6 later_1.3.0 munsell_0.5.0 ## [61] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [64] cli_3.4.1 generics_0.1.2 sjlabelled_1.2.0 ## [67] ranger_0.14.1 evaluate_0.15 stringr_1.4.0 ## [70] fastmap_1.1.0 yaml_2.3.5 processx_3.8.0 ## [73] fs_1.5.2 robustbase_0.95-0 purrr_0.3.4 ## [76] visdat_0.5.3 nlme_3.1-155 mime_0.12 ## [79] ggiraph_0.8.4 xml2_1.3.3 compiler_4.2.0 ## [82] rstudioapi_0.13 png_0.1-7 e1071_1.7-12 ## [85] tibble_3.1.8 bslib_0.4.0 stringi_1.7.8 ## [88] highr_0.9 ps_1.7.2 Matrix_1.5-1 ## [91] vctrs_0.5.0 pillar_1.8.1 norm_1.0-10.0 ## [94] lifecycle_1.0.3 lmtest_0.9-40 jquerylib_0.1.4 ## [97] insight_0.18.6 data.table_1.14.4 httpuv_1.6.5 ## [100] R6_2.5.1 latticeExtra_0.6-30 bookdown_0.29 ## [103] promises_1.2.0.1 gridExtra_2.3 sessioninfo_1.2.2 ## [106] boot_1.3-28 MASS_7.3-56 assertthat_0.2.1 ## [109] pkgload_1.3.0 withr_2.5.0 mnormt_2.1.1 ## [112] mgcv_1.8-39 parallel_4.2.0 hms_1.1.1 ## [115] rpart_4.1.16 class_7.3-20 shiny_1.7.3 ## [118] base64enc_0.1-3 interp_1.0-33 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["r-markdown.html", "Chapter 13 R Markdown 13.1 Neues R Markdown Dokument anlegen 13.2 Text 13.3 R-Code 13.4 Weiterführend", " Chapter 13 R Markdown Mit R Markdown (Endung .Rmd) können wir R Code (z.B. für Datenanalyse und -visualisierung) und Text in einem Dokument verbinden. Dabei dient das R Markdown als “Bauplan” für das finale Output-Dokument (z.B. html, pdf oder Word). Beispielsweise sind alle Inhalte der R Lernplattform mit R Markdown erstellt worden. Was ist Markdown? Markdown ist eine Auszeichnungsprache (markup language). Diese werden für Formatierung und Gliederung von Text genutzt. Weitere Auszeichnungsprachen wie YAML und LaTex, die wir uns später noch genauer anschauen werden, können auch in R Markdown integriert werden. Warum R Markdown? Wir können R Markdown nicht nur nutzen, um schöne Zusammenfassungen für R Übungen zu schreiben. Wir können ganz einfach Tabellen (z.B. Korrelationstabellen oder Ergebnistabellen für eine ANOVA) oder Grafiken in R erstellen und diese dann in unsere Abschlussarbeit übernehmen2. So entfällt das lästige händische Erstellen der Tabellen, bei dem sich häufig Flüchtigkeitsfehler einschleichen. Im Rahmen dieses Kapitels lernen wir, wie wir R Markdown Dokumente erstellen und verändern können, welche Möglichkeiten es gibt, R Code zu integrieren und wie wir Text gestalten können. Achtung: Wenn nicht anders vermerkt, gelten alle vorgestellten Inhalte in diesem Kapitel zu Text- und Codebarbeitung für die Outputformate pdf, Word und html. Bevor es los gehen kann müssen wir zuerst das Paket rmarkdown installieren, um R Markdown Dokumente erstellen zu können install.packages(&quot;rmarkdown&quot;) 13.1 Neues R Markdown Dokument anlegen Nachfolgend sehen wir, wie wir ein neues R Markdown Dokument (Rmd) anlegen können. Wir können zwischen verschiedenen Outputformaten, u.a. html, pdf und Word, wählen. html (hypertext markup language) ist auch eine Auszeichnungsprache und die Basis von Inhalten im Internet. Unser Webbrowser (z.B. Firefox, Chrome, Safari) stellt die Inhalte des html dar. Die Inhalte der Lernplattform sind ebenfalls html Dokumente. So sieht das neu geöffnete Rmd aus. Hier wurde als Outputformat html festgelegt. Das sehen wir ganz oben in output: html_document. Mit dem Knit-Button können wir unser Output-Dokument erstellen. Alternativ können wir zum Knitten einen Kurzbefehl nutzen: Windows: shift + ctrl + K Mac: shift + cmd + K Für die Outputformate html und pdf bekommen wir in einem neuen Fenster in R eine Vorschau davon. Für Word Dokumente öffnet sich (ggf.) Microsoft Word. html pdf Word Dies ist nur die erste Seite des Word Dokuments. Die anderen Inhalte sind auf die zweite Seite gerutscht. 13.1.1 YAML Gehen wir nochmal einen Schritt zurück und schauen uns das Rmd genauer an. Der Code ganz oben im Dokument ist der YAML Header. In diesem legen wir globale Dokument-Optionen fest. Nachfolgend sehen wir einige. Achtung: Standardmäßig steht hier output: html-document (bzw. output: pdf_document oder output: word_document) hintereinander in einer Zeile. Um weitere YAML Optionen ergänzen zu können, müssen wir html_document (bzw. pdf_document oder word_document) in eine neue Zeile schreiben und einen Doppelpunkt dahinter ergänzen. Die YAML Optionen kommen jeweils auch in eine eigene Zeile. Mit toc: true können wir eine Gliederung (table of contents) einfügen. Diese können wir permanent anzeigen lassen, sodass sie trotz Scrollen immer an der gleichen Stelle (links) bleibt. Dazu ergänzen wir toc_float: true. Achtung: Diese Option funktioniert nur in html. Wir können unsere Überschriften auch automatisch nummerieren lassen indem wir number_sections: true nutzen. Die Gestaltung können wir mit der Option theme anpassen. Hier wurde das Theme journal genutzt. Hier finden wir eine Übersicht über alle bestehenden Möglichkeiten. Achtung: Diese Option funktioniert nur in html. Einen Überblick über bestehende YAML Optionen finden wir hier. 13.2 Text Normaler Text wird in R Markdown einfach geschrieben. Im Gegensatz dazu konnten wir im R Skript nur Anmerkungen machen indem wir ein # vor unseren Text gesetzt haben. Nachfolgend schauen wir uns Möglichkeiten der Gestaltung von Text (d.h. die nutzbaren Auszeichnungssprachen) an. Das sind Markdown, html und LaTex. Eine kurze Übersicht über Textgestaltung mit Markdown können wir auch in RStudio angezeigt bekommen, wenn wir in der oberen Taskleiste in RStudio auf Help und dann auf Markdown Quick Reference klicken. 13.2.1 Überschriften Um Überschriften zu setzen, schreiben wir ein bzw. mehrere # an den Anfang einer Zeile. Die Überschrift wird dann im Rmd farblich hervorgehoben. Je mehr # wir setzen, desto kleiner wird die Überschrift. Wie die einzelnen Überschriften aussehen, hängt von unserem Design (theme im YAML Header oder Template) ab. Markdown Die Überschriften erscheinen auch in der Gliederung, wenn wir im YAML Header eine festgelegt haben (toc: true). Mit der Option toc_depth können wir festlegen, bis zu welcher Stufe die Überschriften inkludiert werden sollen. Achtung: Im R-Skript konnten wir # zum Kommentieren nutzen. Das funktioniert hier auch, aber nur in den Code Chunks. Um Textpassagen auszukommentieren, können wir den html-Kommentar-Tag &lt;!-- --&gt; nutzen. Mit folgendem Kurzbefehl können wir sowohl Text als auch Code auskommentieren (funktioniert in html und pdf): Windows/Linux: ctrl + shift + C Mac: cmd + shift + C 13.2.2 Hervorhebung Wir können unseren Text auch fett und kursiv schreiben. Markdown Es gibt zwei verschiedenen Optionen (* oder _) damit wir beide kombinieren können, d.h. fett und kursiv schreiben können. Dabei ist Reihenfolge (ob zuerst fett oder kursiv) egal. Wir müssen nur darauf achten, dass das Zeichen, welches zuletzt eingefügt wurde, auch zuerst beendet wird (z.B. **_eine Möglichkeit_**). 13.2.3 Zeilenumbruch Es gibt mehrere Möglichkeiten, einen Zeilenumbruch herbeizuführen. Entweder wir machen mindestens zwei Leerzeichen ans Ende einer Zeile oder wir nutzen einen Backslash (\\). Markdown 13.2.4 Verlinkung Wir können in unserem Output-Dokument auch auf externe (Web-)Quellen verlinken. Dazu nutzen wir [verlinkter Text](http://example.com). Wenn unser Output-Dokument ein html ist, ist es benutzerfreundlicher, wenn sie die Webseite in einem neuen Fenster öffnet. Dazu nutzen wir den html Tag &lt;a href=\"http://example.com\" target=\"_blank\"&gt;verlinkter Text&lt;/a&gt;. In html haben wir zusätzlich die Möglichkeit, innerhalb eines Dokuments zu verlinken. Am einfachsten ist es, auf eine Überschrift zu verlinken. Dazu nutzen wir einfach eckige Klammern, in die wir den Namen einer Überschrift schreiben: [Überschrift]. 13.2.5 Formeln Wir können Formeln und mathematische Symbole mittels LaTex integrieren. Bei der Installation von dem Paket rmarkdown wird (standarmäßig) automatisch das Paket tinytex heruntergeladen. Das ermöglicht uns die Nutzung von LaTex. Das machen wir mit $ und speziellem LaTex-Code. Einen guten Überblick darüber, wie wir wichtige mathematische Symbole in LaTex schreiben, gibt uns diese Seite. Um Formeln im Fließtext zu schreiben (sog. Inline Formeln), nutzen wir einfache Dollarzeichen z.B. $\\alpha=0.05$ wird zu \\(\\alpha=0.05\\). Um Formeln als (zentrierten) Blocksatz zu schreiben nutzen wir doppelte Dollarzeichen z.B. $$ \\sigma^2= \\sum_{i =1}^{n} \\frac{(x_i - \\bar{x}) ^2}{n-1} $$ wird zu \\[ \\sigma^2= \\sum _{i =1} ^{n} \\frac{(x_i - \\bar{x}) ^2} {n-1} \\] &gt; Achtung: Leerzeichen haben im LaTex Code keinen Einfluss. Sie sind hier nur der Übersichtlichkeit halber eingefügt. Wenn wir in den Formeln Leerzeichen ahben wollen, müssen wir dafür speziellen LaTex Code nutzen. 13.3 R-Code Nun schauen wir uns an, welche Möglichkeiten es gibt, R-Code und/oder deren Outputs in unser Dokument zu integrieren. 13.3.1 Code Chunk Allen Code, denn wir sonst in ein R-Skript geschrieben haben, und der dann in der Konsole ausgeführt wurde (und ggf. im Viewer angezeigt wurde, d.h. Plots), schreiben wir in sogenannte Code Chunks. Wir können Code Chunks einfügen indem wir auf Insert &gt; R klicken … … oder einen Kurzbefehl nutzen: Windows: ctrl + alt + I Mac: alt + cmd + I Im Rmd sind die Code Chunks (in der Standardeinstellung) grau hinterlegt. Wenn wir auf den Pfeil ganz rechts klicken, können wir den Code in dem Chunk ausführen. Darunter erscheint ggf. der Output des Codes. Zum Ausführen des Code Chunks gibt es können wir auch einen Kurzbefehl nutzen: Windows: shift + ctrl + Enter Mac: shift + cmd + Enter Achtung: Wenn wir beim Knitten Fehlermeldungen angezeigt bekommen, die auf den Code zurückzuführen sind, sollten wir diesen erstmal in einem “normalen” R-Skript ausführen. So können wir schauen, ob das Problem direkt im Code oder in der Einbettung in R Markdown liegt. Im Output-Dokument werden standarmäßig auch der Code und ggf. der Output angezeigt. 1+2+3 ## [1] 6 vektor &lt;- c(1, 2, 3) vektor ## [1] 1 2 3 mean(vektor) ## [1] 2 Es gibt Optionen zur Einstellung des Verhaltens der Code Chunks, die wir oben im Code Chunk innerhalb der geschweiften Klammern {r, ...} festlegen. Zwei davon werden wir häufiger brauchen: echo und eval. Mit echo legen wir fest, ob wir den Code im Output-Dokument an´zeigen wollen ({r, echo=TRUE}) oder nicht ({r, echo=FALSE}). Mit eval legen wir fest, ob wir den Code ausführen wollen ({r, eval=TRUE}) oder nicht ({r, eval=FALSE}). Es gibt auch die Möglichkeit, globale Optionen für das Verhalten der Chunks festzulegen. Diese werden auf jeden Code Chunk angewendet (außer wenn wir in einem Code Chunk eine andere Option spezifizieren) und verhindern dadurch überflüssige Wiederholungen. Globale Optionen legen wir mit knitr::opts_chunk$set(...) fest. Die Funktion kommt direkt in den Code Chunk (und nicht wie bei den Chunk-spezifischen Optionen in {r}). include=FALSE führt den Code aus, zeigt den Output des Codes aber nicht im Output-Dokument an. Das ist eine kürzere Alternative zu {r, echo=FALSE, eval=TRUE}, wenn der Output aus Zeichen besteht (z.B. das Ergebnis einer Rechnung; der Inhalt eines Objektes) bzw. {r, echo=FALSE, fig.show='hide'} wenn der Output eine Grafik ist. Code Chunks können auch benannt werden, indem in den geschweiften Klammern nach einem Leerzeichen hinter dem r der gewünschte Name eingetragen wird, z.b. {r, ...} für den Namen setup. Das hilft uns bei der Orientierung, weil wir die Namen in der Dokumentübersicht in RStudio angezeigt bekommen. Wir müssen allerdings darauf achten, denselben Namen nicht mehrmals zuzuweisen. Sonst bekommen wir eine Fehlermeldung. Eigentlich benennen wir einen Chunk mit einem Tag: {r label=setup, …}. Diesen können wir aber weglassen, solange wir den Namen an die erste Stelle schreiben. 13.3.2 Inline Code Wenn es sich bei dem Output (von Code) um Zeichen handelt (und nicht um z.B. Tabellen und Grafiken), können wir diesen auch direkt in den Fließtext integrieren. Dazu nutzen wir . Wir können damit z.B. Inhalte eines Objektes oder das Ergebnis einer (mathematischen) Funktion (ohne den Code) ausgeben lassen. Der Output wird außerdem so formatiert wie der Fließtext. Im vorhergehenden Abschnitt haben wir beispielsweise das Objekt vektor in einem Code Chunk definiert. Dessen Inhalte können wir mit ausgeben lassen: 1, 2, 3 Wir können auch den Mittelwert von vektor berechnen mit : 2 Nur mit den (ohne r) können wir außerdem Text hervorheben. 13.3.3 Grafiken, Bilder und Tabellen Wir können mit R-Code in den Code Chunks außerdem auch Grafiken und Tabellen erstellen und ausgeben, sowie auch Bilder, die nicht im Rmd erstellt wurden, anzeigen. Bei Grafiken müssen wir nichts weiter beachten. Wir können denselben R-Code nutzen wie auch in einem R-Skript (.R). Wenn wir lokal gespeicherte Bilder integrieren wollen, können wir die Funktion include_graphics(“dateipfad”) aus dem Paket knitr nutzen. Dieses wurde (standardmäßig) automatisch heruntergeladen, als wir rmarkdown installiert haben. knitr ist das Paket, welches unseren R-Code im Rmd ausführt. library(knitr) include_graphics(&quot;dateipfad&quot;) Schauen wir uns noch Code Chunk Optionen an, die wir für Grafiken und Bilder nutzen können. Diese schreiben wir wieder oben in den Code Chunk in {r, ...}. Mit out.width (Breite) und out.height (Höhe) können wir die Größe der Abbildungen im Outputdokument kontrollieren. Wir nehmen dafür am besten ‘…px’ (pixel). Wenn wir wollen, dass das Seitenverhältnis beibehalten wird, sollten wir immer nur die Breite oder die Höhe ändern. Beispiel: {r, out.width='300px'} Mit fig.align können wir die Ausrichtung beeinflussen. Gültige Optionen sind: 'left', 'right' and 'center'. Beispiel: {r, fig.align='center'} Um Tabellen zu erstellen, gibt es verschiedene Funktionen (die nur in Rmd funktionieren). Wir schauen uns im Folgenden eine an (und zwar dieselbe, die auch für die Inhalte der R-Lernplattform genutzt wird). Zuerst benötigen wir einen Beispieldatensatz. Wir nutzen den Datensatz women, der im Basispaket datasets enthalten ist und den wir mit der Funktion data() in unser Environment bekommen. data(&quot;women&quot;) women ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 ## 7 64 132 ## 8 65 135 ## 9 66 139 ## 10 67 142 ## 11 68 146 ## 12 69 150 ## 13 70 154 ## 14 71 159 ## 15 72 164 So sieht es aus, wenn wir den Datensatz einfach ausgeben. Schönere Tabellen erhalten wir mit der Funktion kable() aus dem Paket kableExtra. install.packages(&quot;kableExtra&quot;) library(kableExtra) kable(women) height weight 58 115 59 117 60 120 61 123 62 126 63 129 64 132 65 135 66 139 67 142 68 146 69 150 70 154 71 159 72 164 Die gelbe Hintergrundfarbe sowie die grüne Schriftfarbe, die erscheinen, wenn man mit der Maus über die Zeilen geht, sind zusätzlich eingestellt für diese Seite. Wenn ihr kable() bei euch ausführt, ist das nicht enthalten. Es gibt noch diverse weitere Optionen, Tabellen zu verschönern. Eine ausführliche Dokumentation dazu gibt es auf dieser Seite. 13.4 Weiterführend 13.4.1 Templates Templates (Vorlagen) besitzen ein vorgefertigtes Design und ggf. sogar weitere Funktionalitäten für unser Output-Dokument. Wenn wir Templates nutzen, sind in diesen ggf. nicht alle bzw. noch zusätzliche YAML Optionen nutzbar. Wir wollen nun noch zwei Templates vorstellen, die für Projektdokumentation bzw. Veröffentlichungen hilfreich sein können. Für beide müssen wir jeweils ein zusätzliches Paket herunterladen. Nach der Installation der Pakete finden wir die Templates hier: 13.4.1.1 distill Mit dem Template Distill Article aus dem Paket distill können wir html Dokumente erstellen. Die Inhalte der R Lernplattform basieren auch auf diesem Template. Allerdings wurden einige Designänderungen vorgenommen und zusätzliche Funktionen eingefügt. install.packages(&quot;distill&quot;) In diesem haben wir u.a. folgende zusätzliche (bzw. geänderte) Funktionalitäten: Seitennotizen (am rechten Rand) Bilder und Grafiken können über die Seitenränder hinaus gehen (siehe Beispiel unten) Fußnoten werden angezeigt, wenn man mit der Maus über sie fährt3 Gliederung (nur) ganz oben im Dokument Die YAML Option toc_float bewirkt in distill etwas anderes. Mit true wird die Gliederung am linken Rand angezeigt, wenn das Fenster, in dem das Outputdokument geöffnet wird (zumeist der Brwoser), größer gleich 1000px ist. Mit false wird die Gliederung immer mittig angezeigt. Appendix (wenn z.B Fußnoten oder Referenzen eingefügt) Seitennotiz Eine ausführliche Dokumentation zu den Funktionalitäten des Templates und ihrer Nutzung finden wir hier. Figure 13.1: Bildquelle: https://psyteachr.github.io/msc-data-skills/images/memes/better_graphs.png 13.4.1.2 papaja Mit dem Paket papaja (Preparing APA Journal Articles) bekommen wir ein Template, um Artikel ganz einfach im APA-Style zu schreiben. Das Outputformat ist pdf. Achtung: Bevor wir das Paket herunterladen, müssen wir schauen, ob die folgenden zwei Voraussetzungen erfüllt sind: RStudio Version größer gleich 0.98.932 TeX Distribution (neuer als 2013; Links zu den Tex Distributions sind z.B. auf der Entwicklerseite, welche unten verlinkt ist, zu finden) Unsere R Studio Version finden wir z.B. in R Studio unter Help &gt; About RStudio heraus. Weil das Paket noch nicht über den CRAN Mirror verfügbar ist, müssen wir es über die Github-Seite des Entwicklers herunterladen. Dazu benötigen wir wiederum das Paket devtools. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;crsh/papaja&quot;) In diesem Video wird die Nutzung des Templates erklärt. Wenn wir das kostenlose Literaturverwaltungsprogramm Zotero nutzen, können wir zusätzlich direkt daraus unsere Zitationen importieren. Damit wird auch automatisch eine Bibliographie erstellt. Um Zotero und R Makrdown gemeinsam nutzen zu können, müssen wir das Paket citr (in R) sowie die Software Better BibTex (auf unserem Rechner) installieren. # Installation von citr: install.packages(&quot;citr&quot;) # Installation von Better BibTex siehe Link Weitere Informationen finden wir auf der Githubseite des Entwicklers. Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. Achtung: Es ist sinnvoll, die nachfolgende Funktion in einen Code Chunk im eigenen Rmd einzubetten. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ggalluvial_0.12.3 ggiraphExtra_0.3.0 lsr_0.5.2 tidyr_1.2.1 ## [5] naniar_0.6.1 VIM_6.2.2 readxl_1.3.1 foreign_0.8-82 ## [9] devtools_2.4.5 usethis_2.1.6 ICC_2.4.0 readr_2.1.3 ## [13] Hmisc_4.7-1 Formula_1.2-4 survival_3.2-13 lattice_0.20-45 ## [17] ggplot2_3.4.0 colorspace_2.0-3 psych_2.2.9 car_3.1-1 ## [21] carData_3.0-5 kableExtra_1.3.4 dplyr_1.0.10 htmltools_0.5.3 ## [25] rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] uuid_1.0-3 backports_1.4.1 systemfonts_1.0.4 ## [4] plyr_1.8.7 sp_1.5-0 splines_4.2.0 ## [7] mycor_0.1.1 digest_0.6.30 fansi_1.0.3 ## [10] magrittr_2.0.2 checkmate_2.0.0 memoise_2.0.1 ## [13] cluster_2.1.2 tzdb_0.3.0 remotes_2.4.2 ## [16] svglite_2.1.0 prettyunits_1.1.1 jpeg_0.1-9 ## [19] rvest_1.0.2 xfun_0.34 callr_3.7.2 ## [22] crayon_1.5.2 jsonlite_1.8.3 zoo_1.8-11 ## [25] glue_1.6.2 gtable_0.3.0 ppcor_1.1 ## [28] webshot_0.5.4 sjmisc_2.8.9 pkgbuild_1.3.1 ## [31] DEoptimR_1.0-11 abind_1.4-5 scales_1.2.1 ## [34] DBI_1.1.2 miniUI_0.1.1.1 Rcpp_1.0.9 ## [37] viridisLite_0.4.1 xtable_1.8-4 laeken_0.5.2 ## [40] htmlTable_2.4.1 proxy_0.4-27 vcd_1.4-10 ## [43] profvis_0.3.7 htmlwidgets_1.5.4 httr_1.4.2 ## [46] RColorBrewer_1.1-2 ellipsis_0.3.2 urlchecker_1.0.1 ## [49] pkgconfig_2.0.3 farver_2.1.1 nnet_7.3-17 ## [52] sass_0.4.2 deldir_1.0-6 utf8_1.2.2 ## [55] reshape2_1.4.4 tidyselect_1.2.0 labeling_0.4.2 ## [58] rlang_1.0.6 later_1.3.0 munsell_0.5.0 ## [61] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [64] cli_3.4.1 generics_0.1.2 sjlabelled_1.2.0 ## [67] ranger_0.14.1 evaluate_0.15 stringr_1.4.0 ## [70] fastmap_1.1.0 yaml_2.3.5 processx_3.8.0 ## [73] fs_1.5.2 robustbase_0.95-0 purrr_0.3.4 ## [76] visdat_0.5.3 nlme_3.1-155 mime_0.12 ## [79] ggiraph_0.8.4 xml2_1.3.3 compiler_4.2.0 ## [82] rstudioapi_0.13 png_0.1-7 e1071_1.7-12 ## [85] tibble_3.1.8 bslib_0.4.0 stringi_1.7.8 ## [88] highr_0.9 ps_1.7.2 Matrix_1.5-1 ## [91] vctrs_0.5.0 pillar_1.8.1 norm_1.0-10.0 ## [94] lifecycle_1.0.3 lmtest_0.9-40 jquerylib_0.1.4 ## [97] insight_0.18.6 data.table_1.14.4 httpuv_1.6.5 ## [100] R6_2.5.1 latticeExtra_0.6-30 bookdown_0.29 ## [103] promises_1.2.0.1 gridExtra_2.3 sessioninfo_1.2.2 ## [106] boot_1.3-28 MASS_7.3-56 assertthat_0.2.1 ## [109] pkgload_1.3.0 withr_2.5.0 mnormt_2.1.1 ## [112] mgcv_1.8-39 parallel_4.2.0 hms_1.1.1 ## [115] rpart_4.1.16 class_7.3-20 shiny_1.7.3 ## [118] base64enc_0.1-3 interp_1.0-33 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. Wir können die Tabellen und Grafiken z.B. aus einem Word Dokument, welches wir mit R Markdown erstellt haben, rauskopieren und in das Word Dokument einfügen, in welchem wir unsere Arbeit schreiben. Wir können aber auch direkt unsere Arbeit in einem R Markdown Dokument schreiben (und daraus ein Word Dokument o.ö. erstellen).↩︎ wie hier↩︎ "],["outputs---alm---faq.html", "Chapter 14 Outputs - ALM - FAQ 14.1 \\(t\\)-Test 14.2 Lineare Regression 14.3 ANOVA 14.4 ALM: Zusammenhänge der drei Verfahren", " Chapter 14 Outputs - ALM - FAQ Das Allgemeine Lineare Modell (ALM) umfasst verschiedene inferenzstatistische und varianzanalytische Verfahren, darunter \\(t\\)-Test (t.test()), ANOVA (aov()) und lineare Regression (lm()). Im Folgenden schauen wir uns an, wie die Outputs der jeweiligen Funktionen t.test(), aov() und lm() aufgebaut sind und wie diese interpretiert werden. Dafür nutzen wir jeweils dieselben zwei Variablen. Wir werden zudem vor der Durchführung der Methoden deren Annahmen prüfen. Abschließend beschäftigen wir uns mit den Gemeinsamkeiten der Verfahren (für unseren speziellen Fall von einer metrischen abhängigen Variablen und einer kategorialen unabhängigen Variablen). Wir gehen nur knapp auf die Prüfung der Annahmen ein. Eine ausführlichere Einleitung befindet sich hier. Beispieldatensatz für dieses Kapitel Wir schauen uns im folgenden den Datensatz erstis an. Dieser enthalt Daten aus einer Erhebung mit Erstsemesterstudierenden der Psychologie. Das zugehörige Codebook finden wir hier. Den Datensatz laden wir mit der load()-Funktion in unsere Environment: load(url(&quot;http://www.beltz.de/fileadmin/beltz/downloads/ OnlinematerialienPVU/R_fuer_Einsteiger/erstis.rda&quot;)) # Zeilenumbruch zwischen der ersten und zweiten Zeile noch entfernen! Wir wollen untersuchen, ob sich das Vorhandenseins eines Nebenjobs (job) auf die Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung, erster Messzeitpunkt) auswirkt. Dazu speichern wir beide Variablen in einem neuen Datensatz. # Daten aus erstis in neuem Dataframe speichern und umbennenen daten &lt;- data.frame(job = erstis$job, zf_belastung = erstis$zuf.bel.1) Wir müssen außerdem noch überprüfen, ob die Variablen entsprechend ihres Messniveaus kodiert sind: str(daten) ## &#39;data.frame&#39;: 191 obs. of 2 variables: ## $ job : Factor w/ 2 levels &quot;ja&quot;,&quot;nein&quot;: 2 1 1 2 2 1 NA NA NA 2 ... ## $ zf_belastung: num 1.33 2.67 3.67 2.33 NA ... Wie erhofft liegt die nominalskalierte Variable job als ungeordneter Faktor und die intervallskalierte Variable zf_belastung als numeric vor. Mehr Informationen zu Datentypen und angemessener Kodierung finden wir im Kapitel Einführung in R. 14.1 \\(t\\)-Test Der \\(t\\)-Test untersucht, ob es signifikante Mittelwertsunterschiede in der Population hinsichtlich einer metrischen Variablen gibt. Der Begriff umfasst eine Gruppe von Hypothesentests, darunter den \\(t\\)-Test für unabhängige Stichproben und den \\(t\\)-Test für abhängige Stichproben (Beobachtungspaare). In unserem Beispiel schauen wir, ob sich Personen mit und ohne Nebenjob (job) hinsichtlich der Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) unterscheiden. Weil es sich bei Studierenden mit und ohne Nebenjob um zwei voneinander unabhängige Stichproben (d.h. unterschiedliche Personen) handelt, verwenden wir den \\(t\\)-Test für unabhängige Stichproben. Überprüfung der Annahmen des \\(t\\)-Tests Achtung: An dieser Stelle sei wieder auf das ausführlichere Skript zur Annahmenprüfung verwiesen. Im Folgenden schauen wir uns jeweils nur ein mögliches Verfahren zur Überprüfung der spezifischen Annahmen an. Zur Durchführung eines \\(t\\)-Tests für unabhängige Zufallsstichproben müssen folgende Annahmen erfüllt sein: es muss sich um einfache, voneinander unabhängige Stichproben handeln Normalverteilung des untersuchten Merkmals in beiden Populationen Varianzhomogenität des untersuchten Merkmals in beiden Populationen Normalverteilung Schauen wir uns dazu jeweils für beide Gruppen von job einen QQ-Plot an. library(ggplot2) ggplot(data = daten, aes(sample = zf_belastung)) + # Koordinatensystem stat_qq() + stat_qq_line() + # QQ-Plot + Hilfslinie facet_grid(~ job) # einzelne Plots nach den Ausprägungen von job # die Kategorie NA fasst die fehlenden Werte auf der Variablen job zusammen # diese können wir hier unbeachtet lassen Mehr Informationen zum Erstellen von Grafiken mit dem Paket ggplot2 finden wir im Kapitel zu Grafiken. Achtung: Hilfe zur Interpretation von QQ-Plots finden wir hier. Hinweis: Die Residuen in der linearen Regression sind beim \\(t\\)-Test dasselbe wie in den Gruppen zentrierte Werte der \\(AV\\) (d.h. Abweichungen vom gruppenspezifischen Mittelwert) in unserem Beispiel. In beiden Diagrammen weichen die Punkte leicht von der Linie ab, d.h. dass die Daten nicht perfekt normalverteilt sind. Wir können uns aber auf den zentralen Grenzwertsatz stützen, welcher besagt, dass der Mittelwert eines Merkmals bei wachsender Stichprobengröße approximativ normalverteilt ist. Generell gibt es keinen Richtwert, der besagt, wann Stichproben hinreichend groß sind. Mit unseren Stichprobengrößen (\\(n_{ja} = 87\\) und \\(n_{nein} = 86\\)) und den geringfügigen Abweichungen von einer Normalverteilung in den beiden Gruppen können wir erwarten, dass die Annahme hinreichend erfüllt ist. Varianzhomogenität Varianzhomogenität bedeutet, dass die Varianzen in den untersuchten Populationen gleich sind. Wenn dies der Fall ist, sollten sich auch die Stichprobenvarianzen ähneln. Ist diese Annahme nicht erfüllt, müssen wir auf robustere Methoden, z.B. den Welch’s \\(t\\)-Test für unabhängige Stichproben, zurückgreifen. Zur Überprüfung der Varianzhomogenität können wir beispielsweise den Levene Test nutzen. In diesem wird die Nullhypothese überprüft, dass die Populationsvarianzen homogen sind (\\(\\sigma^2_{ja} = \\sigma^2_{nein}\\)). Weil die \\(H_0\\) die Wunschhypothese ist, ist es von größerer Relevanz, diese nicht fälschlicherweise abzulehnen, d.h. einen \\(\\beta\\)-Fehler zu machen (relativ gesehen zum \\(\\alpha\\)-Fehler). Wir können das Risiko für einen \\(\\beta\\)-Fehler nur indirekt kontrollieren, indem wir das \\(\\alpha\\)-Level erhöhen. Daher legen wir unser Signifikanzniveau auf \\(\\alpha=.20\\) (zweiseitig) fest. Wir führen den Test mit der Funktion leveneTest() aus dem Paket car durch. library(car) leveneTest(zf_belastung ~ job, daten) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.807 0.3703 ## 167 Da der \\(p\\)-Wert (Pr(&gt;F)) größer als unser \\(\\alpha\\)-Level ist, können wir die \\(H_0\\) beibehalten. Nachdem die Annahmen weitgehend erfüllt erscheinen, können wir den \\(t\\)-Test für unabhängige Stichproben anwenden. Mit der Funktion t.test() aus dem Basispaket stats können unterschiedliche Arten von \\(t\\)-Tests durchgeführt werden. Welches Verfahren verwendet werden soll, legen wir mit den Parametern paired und var.equal fest. Mit paired spezifizieren wir, ob eine Abhängigkeit der Stichproben besteht. TRUE zu einem \\(t\\)-Test für Beobachtungspaare, FALSE führt zur Durchführung eines \\(t\\)-Tests für unabhängige Stichproben. Mit var.equal spezifizieren wir, ob Varianzhomogenität vorliegt. Wenn die Annahme nicht erfüllt ist, legen wir mit FALSE fest, dass der Welch’s \\(t\\)-Test durchgeführt werden soll. Wir wollen einen ungerichteten \\(t\\)-Test für unabhängige Stichproben durchführen und legen unser Signifikanzlevel \\(\\alpha=.05\\) fest. t.test(formula = zf_belastung ~ job, data = daten, paired = FALSE, # unabhängige SP var.equal = TRUE, # Annahme Varianzhomogenität erfüllt alternative =&quot;two.sided&quot;, # zweiseitige (ungerichtete) Testung na.action = &quot;na.exclude&quot;) # Ausschluss fehlender Werte Mehr Informationen zum Umgang mit fehlenden Werten finden wir im Kapitel Fehlende Werte. Wir erhalten u.a. folgende Informationen: t: empirische Prüfgröße des \\(t\\)-Tests df: Anzahl der Freiheitsgerade der \\(t\\)-Verteilung (entspricht \\(N-2\\)) Wir kommen wir auf \\(N=169\\)? Wir haben alle Fälle mit fehlenden Werten auf einer der beiden Variablen ausgeschlossen (casewise deletion durch na.action = \"na.exclude\"). Wir können diese Bedingung folgendermaßen auf unseren Datensatz anwenden, um \\(N\\) für unsere spezifische Analyse zu berechnen: nrow(daten[!is.na(daten$job) &amp; !is.na(daten$zf_belastung),]) ## [1] 169 Für mehr Informationen siehe das Kapitel zu Fehlenden Werten. p-value: in \\(p\\)-Wert umgerechneter empirischer \\(t\\)-Wert Durch den Vergleich mit unserem \\(\\alpha\\)-Niveau können wir anhand dieser Information zu einer Testentscheidung gelangen. Ist der \\(p\\)-Wert kleiner als \\(\\alpha\\), verwerfen wir die \\(H_0\\). 95 percent confidence interval: obere und untere Grenze des 95% Konfidenzintervalls der Mittelwertsdifferenz sample estimates: Stichprobenmittelwerte der Gruppen Durch Betrachtung des \\(p\\)-Wertes stellen wir fest, dass die Wahrscheinlichkeit unter Gültigkeit der \\(H_O\\), einen empirischen \\(t\\)-Wert von \\(\\mid -1.278 \\mid\\) oder extremer zu erhalten, bei 20.3% liegt. Da dieser \\(p\\)-Wert deutlich über \\(\\alpha=.05\\) liegt, behalten wir die Nullhypothese bei. Wir gehen davon aus, dass Studierende mit und ohne Nebenjob sich nicht statistisch signifikant in ihrer Zufriedenheit mit der Bewältigung von Studienbelastungen unterscheiden. 14.2 Lineare Regression Die lineare Regression ermöglicht es, eine abhängige Variable (AV, Kriterium) durch eine oder mehrere unabhängige Variablen (UVs, Prädiktoren) vorherzusagen. Das Kriterium muss metrisch sein, wohingegen die Prädiktoren auch kategorial sein können, wenn sie adäquat kodiert sind (siehe z.B. Indikatorvariablen: Kodierung nominaler Merkmale im Kapitel Datenvorbereitung). ein Prädiktor: einfache lineare Regression; mehrere Prädiktoren: multiple lineare Regression Wir führen eine einfache lineare Regression durch, in der wir die Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) auf das Vorhandensein eines Nebenjobs (job) zurückführen. Wir legen unser Signifikanzlevel auf \\(\\alpha=.05\\) fest. Bei der linearen Regression ist die sogenannte Residualdiagnostik ein essentieller Teil der Annahmenprüfung. Allerdings müssen wir dafür die Regression bereits durchgeführt haben. Wir können die Annahmenprüfung also erst nach der Regression machen. Mit der Funktion lm() aus dem Basispaket stats können wir eine lineare Regression durchführen. lm_belastung &lt;- lm(formula = zf_belastung ~ job, data = daten, na.action = &quot;na.exclude&quot;) # Ausschluss fehlender Werte Mehr Informationen zum Umgang mit fehlenden Werten finden wir im Kapitel Fehlende Werte. Das Ergebnisobjekt lm_belastung schauen wir uns erst nach der Annahmenprüfung an. Überprüfung der Annahmen der (einfachen) linearen Regression Achtung: Im Rahmen dieses Kapitels besprechen wir die Annahmen und deren Überprüfung nicht im Detail. Mehr Informationen finden wir im Kapitel zur Prüfung der Annahmen der multiplen linearen Regression, die denen der einfachen linearen Regression sehr ähnlich ist. Folgende vier Annahmen sind bei der einfachen linearen Regression mit der inferenzstatistischen Absicherung verbunden: Linearität Homoskedastizität Normalverteilung der Residuen Unabhängigkeit der Residuen Linearität Die Abhängigkeit zwischen Erwartungswert des Kriteriums und Prädiktor ist linear. Diese Annahme lässt sich mittels eines Residualplots untersuchen. Dieser plottet die vorhergesagten Kriteriumswerte \\(\\hat y_i\\) gegen die Residuen \\(\\hat e_i\\). plot(lm_belastung, which=1) Weil wir nur eine kategoriale \\(UV\\) mit zwei Ausprägungen haben, ordnen sich die Punkte in zwei vertikalen Linien an. Da sich die Lowess Fit Line (rote gestrichelte Linie), welche den generellen (nonparametrischen) Trend der Daten beschreibt, dem Erwartungswert der Residuen bei \\(y=0\\) (schwarze gestrichelten Linie) annähert, können wir annehmen, dass Linearität vorliegt. Mehr Informationen zur Lowess Fit Line und zum Residuenplot finden wir im Kapitel zur Prüfung der Annahmen der multiplen linearen Regression. Homoskedastizität Die Varianz der \\(y\\)-Werte, die an einer bestimmten Stelle des Prädiktors vorliegt, ist für alle Prädiktorwerte gleich (Varianzhomogenität). Diese haben wir bereits im Abschnitt zum \\(t\\)-Test mittels des Levene-Tests überprüft. Den Residualplot, den wir gerade zur Überprüfung der Annahme der Linearität genutzt haben, können wir auch zur Überprüfung der Annahme der Homoskedatizität nutzen. Weil die Residuen sich ohne erkennbares Muster um den Erwartungswert der Residuen bei \\(y=0\\) (schwarze gestrichelten Linie) verteilen, nehmen wir Homoskedastizität an. Normalverteilung der Residuen Die Verteilung der \\(y\\)-Werte an einer bestimmten Stelle der \\(UV\\) ist eine Normalverteilung. Eine Verletzung dieser Annahme ist eher in kleineren Stichproben problematisch. In großen Stichproben sind die Regressionskoeffizienten aufgrund des zentralen Grenzwertsatzes selbst dann asymptotisch normalverteilt, wenn die Annahme nicht erfüllt ist. Es gibt jedoch keinen Richtwert, ab wann eine Stichprobe als hinreichend groß gilt. Wir sollten die Annahme immer überprüfen. Dazu schauen wir uns einen QQ-Plot der Residuen an. plot(lm_belastung, which=2) Unser Kriterium Zufriedenheit mit der Bewältigung von Studienbelastungen scheint in Abhängigkeit der Gruppenzugehörigkeit leicht von einer Normalverteilung abzuweichen. Die Größe unserer Stichprobe, \\(N = 169\\), legt nahe, dass (nach dem zentralen Grenzwertsatz) die Regressionskoeffizienten approximativ normalverteilt sind und der Standardfehler der Steigung nicht verzerrt ist. Unabhängigkeit der Residuen Die Höhe des Residuums einer Beobachtung ist unabhängig von der Höhe des Residuums einer anderen Beobachtung. Serielle Abhängigkeit, d.h. mehrere Messungen von einer Person, können wir ausschließen, da es sich nicht um ein Messwiederholungs-Design handelt. Zur Überprüfung auf Clustering, d.h. systematische Zusammenhänge zwischen Personen einer Gruppe, müssten wir im Verdacht stehende (erhobene) Gruppenvariablen begutachten. Da dies aber den Rahmen dieses Kapitels sprengen würde, lassen wir das außen vor. Da wir keine Hinweise auf Verletzung der Annahmen gefunden haben, schauen wir uns nun die Ergebnisse der einfachen linearen Regression an. Wir schauen uns die Ergebnis unseres Regressionsmodells mittels summary() an: summary(lm_belastung) Der Output sagt uns, inwiefern wir Zufriedenheit mit der Bewältigung von Studienbelastungen mit dem Vorliegen eines Nebenjobs vorhersagen können. Wir bekommen Auskunft über: die Signifikanztests der geschätzten Populationskoeffizienten \\(\\hat b_0\\) und \\(\\hat b_1 ... \\hat b_k\\) (unserer \\(k\\) Prädiktoren) den Signifikanztest der insgesamt aufgeklärten Varianz des gesamten Regressionsmodells \\(R^2\\) In unserem Fall einer einfachen linearen Regression sind die Ergebnisse der Signifikanztestung von Schätzung der Steigung und Schätzung der Varianzaufklärung des Gesamtmodells identisch. Unter Coefficients finden wir Informationen zu den geschätzten Regressionskoeffizienten - Intercept und Steigungskoeffizient(en): Estimate: Schätzung der (Populations-)Regressionskoeffizienten (Intercept): vorhergesagter Wert des Kriteriums wenn alle Prädiktoren 0 sind (bei metrischen Prädiktoren) bzw. vorhergesagter Wert des Kriteriums in den jeweiligen Referenzgruppen (bei kategorialen Prädiktoren); Wert von zf_belastung in der Referenzgruppe job == ja jobnein: Steigungskoeffizient, der die erwartete Veränderung im Kriterium angibt, wenn der Prädiktor um eine Einheit erhöht wird (bei metrischen Prädiktoren) bzw. in (einer) der Vergleichsgruppen (bei kategorialen Prädiktoren); Unterschied von job == nein im Vergleich zur Referenzgruppe job == ja im Hinblick auf zf_belastung Std.Error: Standardfehler des Regressionskoeffizienten t.value: empirischer \\(t\\)-Wert für die Signifikanztestung des (partiellen) Regressionskoeffizienten Pr(&gt;|t|): in \\(p\\)-Wert umgerechneter empirischer \\(t\\)-Wert Sign.codes: durch Sternchen gekennzeichnete Signifikanzniveaus der geschätzten Regressionskoeffizienten Der Intercept \\(b_0 = 2.679\\) gibt uns den Mittelwert der Personen mit Nebenjob (job == \"ja\", Referenzgruppe) auf der Skala von Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) an. Wir gelangen durch Betrachtung von \\(p &lt; 2^{-16}\\) zu der Testentscheidung, die \\(H_0\\), dass der Intercept in der Population 0 ist, abzulehnen. \\(t\\)-Test des Intercepts \\(b_0\\): \\(H_0\\): \\(\\beta_0 = 0\\) \\(H_1\\): \\(\\beta_0 &gt; 0\\) Der Steigungskoeffizient \\(b_1 = 0.131\\) sagt uns, dass Personen ohne Nebenjob (job == \"nein\") sich im Mittel 0.131 Punkte höher auf der Skala von Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) beschreiben. Zudem gelangen wir durch Betrachtung von \\(p = 0.203\\) zu der Testentscheidung, die \\(H_0\\), dass der Populationskoeffizient \\(\\beta_1 = 0\\) ist, beizubehalten. \\(t\\)-Test des Steigungs-koeffizienten \\(b_1\\): \\(H_0\\): \\(\\beta_1 = 0\\) \\(H_1\\): \\(\\beta_1 \\neq 0\\) Im unteren Abschnitt finden wir Informationen bezüglich des Gesamtmodells: Residual Standard Error: Standardschätzfehler \\(s_e\\) Gütemaß für die Genauigkeit der Regressionsvorhersage Multiple R-squared: Determinationskoeffizient \\(R^2\\) Gütemaß für die Schätzgenauigkeit Adjusted R squared: korrigierter Determinationskoeffizient \\(R_{korr}^2\\); gilt durch Korrektur der Freiheitsgrade als erwartungstreuer Schätzer für die Population F-statistic: empirischer Wert des \\(F\\)-Tests des Determinationskoeffizienten \\(R^2\\) (d.h. des Gesamtmodells) mit Anzahl der Zähler- (\\(k\\)) und Nenner-Freiheitsgrade (\\(N-k-1\\)) der \\(F\\)-Verteilung mit zugehörigem \\(p\\)-Wert. Handelt es sich um eine einfache lineare Regression mit nur einem Prädiktor (wie in unserem Fall), führt der \\(F\\)-Test des Determinations-koeffizienten zum selben Ergebnis wie der \\(t\\)-Test des Steigungs-koeffizienten. Der Determinationskoeffizienten \\(R^2 = 0.009685\\) sagt aus, dass in unserer Stichprobe knapp 1% der Variation in der Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) durch das Vorhandensein eines Nebenjobs (job) erklärt werden kann. Wir können die \\(H_0\\), dass der Determinationskoeffizient in der Population \\(0\\) ist (\\(\\rho^2 = 0\\)), durch Betrachtung von \\(p = 0.203\\) beibehalten. \\(F\\)-Test des Determinations-koeffizienten \\(R^2\\): \\(H_0: \\rho^2 = 0\\) \\(H_1: \\rho^2 &gt; 0\\) 14.3 ANOVA Eine ANOVA (Analysis of Variance) überprüft den Einfluss von einer bzw. mehreren kategorialen unabhängigen Variablen (UVs, Faktoren) mit \\(p\\) Faktorstufen auf eine metrische abhängige Variable (AV). Dazu wird eine Varianzdekomposition durchgeführt, welche die Gesamtvarianz in systematische und Fehlervarianz zerlegt. Wir wollen in unserem Beispiel überprüfen inwiefern Unterschiede in der Zufriedenheit mit der Bewältigung von Studienbelastungen (zf_belastung) auf das Vorhandensein eines Nebenjobs (job) zurückzuführen ist. Wir haben ein minimal unbalanciertes Design, weil die Anzahl der Beobachtungen in den Faktorstufen von job sich geringfügig unterscheiden mit \\(n_{ja} = 87\\) und \\(n_{nein} = 86\\). Unser Signifikanzlevel legen wir auf \\(\\alpha=.05\\) fest. Überprüfung der Annahmen der ANOVA Vor der Durchführung der ANOVA mit unseren Beispieldaten müssen wir zwei Annahmen prüfen: Normalverteilung des untersuchten Merkmals in beiden Populationen Homoskedastizität des untersuchten Merkmals in beiden Populationen Beides haben wir im Rahmen der Durchführung des \\(t\\)-Test bereits untersucht und Evidenz dafür gefunden. Zur Durchführung der ANOVA verwenden wir die aov()Funktion. anova &lt;- aov(zf_belastung ~ job, data = daten, na.action = &quot;na.exclude&quot;) # Ausschluss fehlender Werte summary(anova) Mehr Informationen zum Umgang mit fehlenden Werten im gleichnamigen Kapitel. Der Output liefert die folgenden Informationen für den Gruppierungsfaktor (job) und für die Residuen (Residuals): Df: Anzahl der Freiheitsgrade der \\(F\\)-Verteilung … des Gruppierungsfaktor: \\(df_{job} = p - 1\\) … der Residuen: \\(df_{e} = N-p\\) Sum Sq: Quadratsumme … des Gruppierungsfaktors (“Treatmentquadratsumme”) … der Residuen (“Fehlerquadratsumme”) Mean Sq: Mittlere Quadratsumme (MQ) relativieren die Quadratsumme eines Effekts an seinen Freiheitsgraden F value: empirische Prüfgröße des \\(F\\)-Tests entspricht dem Quotienten \\(\\frac {MQ_{job}} {MQ_e}\\) Pr(&gt;F): in \\(p\\)-Wert umgerechneter empirischer \\(F\\)-Wert Unser \\(p\\)-Wert deutet darauf hin, dass unter Gültigkeit der \\(H_0\\) die Wahrscheinlichkeit, den vorliegenden \\(F\\)-Wert von 1.633 oder einen größeren zu erhalten, 20,3% beträgt. Wir kommen somit zu der Testentscheidung, die \\(H_0\\) beizubehalten. Das bedeutet, dass wir davon ausgehen, dass zwischen den Populationen von Studierenden mit und ohne Nebenjob keine überzufälligen Mittelwertsunterschiede bezüglich der Zufriedenheit mit der Bewältigung von Studienbelastungen existieren. 14.4 ALM: Zusammenhänge der drei Verfahren Wie bereits eingangs erwähnt, gehören \\(t\\)-Test, lineare Regression und ANOVA alle zum Allgemeinen Linearen Modell (ALM). Für den hier betrachteten Spezialfall von nur einer UV mit nur zwei Stufen (und adäquater Kodierung; siehe z.B. Indikatorvariablen: Kodierung nominaler Merkmale aus dem Kapitel Datenvorbereitung) kommen die drei Verfahren zum selben Ergebnis bei der Signifikanztestung, obwohl scheinbar andere Hypothesen getestet werden. Empirische Prüfgröße Verfahren getestete Hypothesen \\(t\\) \\(F\\) \\(p\\) \\(t\\)-Test für unabhängige Stichproben Mittelwertsunterschied in den Gruppen \\(H_0\\): \\(\\mu_1 = \\mu_2\\) \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\) (ungerichtet) -1.278 0.203 einfache lineare Regression Steigungskoeffizient \\(b_1\\) \\(H_0\\): \\(\\beta_1 = 0\\) \\(H_1\\): \\(\\beta_1 \\neq 0\\) (ungerichtet) 1.278 0.203 einfache lineare Regression Determinationskoeffizient \\(R^2\\) \\(H_0\\): \\(\\rho^2 = 0\\) \\(H_1\\): \\(\\rho^2\\) &gt; \\(0\\) (gerichtet) 1.633 0.203 ANOVA Mittelwertsunterschied in den Gruppen \\(H_0\\): \\(\\mu_1 = \\mu_2\\) \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\) (ungerichtet) 1.633 0.203 Wir können \\(t\\)- und \\(F\\)-Werte ineinander überführen durch folgende Formel: \\(F = t^2 \\longrightarrow t^2=1.278^2=1.633=F\\). Achtung: Dass der \\(t\\)-Wert einmal negativ (\\(t\\)-Test) und einmal positiv (einfache lineare Regression) ist liegt daran, dass die Gruppen jeweils vertauscht wurden. Das Allgemeine Lineare Modell (ALM) umfasst varianzanalytische Verfahren sowie (multiple) Korrelations- und Regressionsrechnung. Dadurch können nicht nur metrische sondern auch kategoriale Merkmale (als \\(UV\\)s) untersucht werden, sofern die kategorialen Merkmale in geeigneter Form kodiert sind. Mehr Informationen zu Kodierung gibt es im Kapitel Einführung in R. In unserem Fall von einer intervallskalierten \\(AV\\) und einer nominalskalierten \\(UV\\) werden in den drei Verfahren jeweils die Mittelwerte der \\(AV\\) für die jeweiligen \\(UV\\) gebildet. Beim \\(t\\)-Test und der einfachen linearen Regression haben wir diese auch ausgegeben bekommen: \\(t\\)-Test: mean in group ja \\(= 2.67\\) mean in group nein \\(= 2.80\\) Einfache lineare Regression: \\(Intercept = 2.67\\) \\(b_1 = 0.131\\) \\(Intercept + b_1 = 2.81\\) Dadurch sind die getesteten Hypothesen der drei Verfahren in diesem speziellen Fall (\\(AV\\): intervallskaliert, \\(UV\\): nominalskaliert) äquivalent. Um eine möglichst exakte Replikation der Funktionen zu gewährleisten gibt es im folgenden relevante Angaben zum System (R-Version, Betriebssystem, geladene Pakete mit Angaben zur Version), mit welchem diese Seite erstellt wurde. sessionInfo() ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.1 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ggalluvial_0.12.3 ggiraphExtra_0.3.0 lsr_0.5.2 tidyr_1.2.1 ## [5] naniar_0.6.1 VIM_6.2.2 readxl_1.3.1 foreign_0.8-82 ## [9] devtools_2.4.5 usethis_2.1.6 ICC_2.4.0 readr_2.1.3 ## [13] Hmisc_4.7-1 Formula_1.2-4 survival_3.2-13 lattice_0.20-45 ## [17] ggplot2_3.4.0 colorspace_2.0-3 psych_2.2.9 car_3.1-1 ## [21] carData_3.0-5 kableExtra_1.3.4 dplyr_1.0.10 htmltools_0.5.3 ## [25] rmarkdown_2.17 knitr_1.40 ## ## loaded via a namespace (and not attached): ## [1] uuid_1.0-3 backports_1.4.1 systemfonts_1.0.4 ## [4] plyr_1.8.7 sp_1.5-0 splines_4.2.0 ## [7] mycor_0.1.1 digest_0.6.30 fansi_1.0.3 ## [10] magrittr_2.0.2 checkmate_2.0.0 memoise_2.0.1 ## [13] cluster_2.1.2 tzdb_0.3.0 remotes_2.4.2 ## [16] svglite_2.1.0 prettyunits_1.1.1 jpeg_0.1-9 ## [19] rvest_1.0.2 xfun_0.34 callr_3.7.2 ## [22] crayon_1.5.2 jsonlite_1.8.3 zoo_1.8-11 ## [25] glue_1.6.2 gtable_0.3.0 ppcor_1.1 ## [28] webshot_0.5.4 sjmisc_2.8.9 pkgbuild_1.3.1 ## [31] DEoptimR_1.0-11 abind_1.4-5 scales_1.2.1 ## [34] DBI_1.1.2 miniUI_0.1.1.1 Rcpp_1.0.9 ## [37] viridisLite_0.4.1 xtable_1.8-4 laeken_0.5.2 ## [40] htmlTable_2.4.1 proxy_0.4-27 vcd_1.4-10 ## [43] profvis_0.3.7 htmlwidgets_1.5.4 httr_1.4.2 ## [46] RColorBrewer_1.1-2 ellipsis_0.3.2 urlchecker_1.0.1 ## [49] pkgconfig_2.0.3 farver_2.1.1 nnet_7.3-17 ## [52] sass_0.4.2 deldir_1.0-6 utf8_1.2.2 ## [55] reshape2_1.4.4 tidyselect_1.2.0 labeling_0.4.2 ## [58] rlang_1.0.6 later_1.3.0 munsell_0.5.0 ## [61] cellranger_1.1.0 tools_4.2.0 cachem_1.0.6 ## [64] cli_3.4.1 generics_0.1.2 sjlabelled_1.2.0 ## [67] ranger_0.14.1 evaluate_0.15 stringr_1.4.0 ## [70] fastmap_1.1.0 yaml_2.3.5 processx_3.8.0 ## [73] fs_1.5.2 robustbase_0.95-0 purrr_0.3.4 ## [76] visdat_0.5.3 nlme_3.1-155 mime_0.12 ## [79] ggiraph_0.8.4 xml2_1.3.3 compiler_4.2.0 ## [82] rstudioapi_0.13 png_0.1-7 e1071_1.7-12 ## [85] tibble_3.1.8 bslib_0.4.0 stringi_1.7.8 ## [88] highr_0.9 ps_1.7.2 Matrix_1.5-1 ## [91] vctrs_0.5.0 pillar_1.8.1 norm_1.0-10.0 ## [94] lifecycle_1.0.3 lmtest_0.9-40 jquerylib_0.1.4 ## [97] insight_0.18.6 data.table_1.14.4 httpuv_1.6.5 ## [100] R6_2.5.1 latticeExtra_0.6-30 bookdown_0.29 ## [103] promises_1.2.0.1 gridExtra_2.3 sessioninfo_1.2.2 ## [106] boot_1.3-28 MASS_7.3-56 assertthat_0.2.1 ## [109] pkgload_1.3.0 withr_2.5.0 mnormt_2.1.1 ## [112] mgcv_1.8-39 parallel_4.2.0 hms_1.1.1 ## [115] rpart_4.1.16 class_7.3-20 shiny_1.7.3 ## [118] base64enc_0.1-3 interp_1.0-33 Für Informationen zur Interpretation dieses Outputs schaut auch den Abschnitt Replizierbarkeit von Analysen des Kapitels zu Paketen an. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
